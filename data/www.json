[{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e282"
  },
  "title": "Start free, scale effortlessly.",
  "description": "For production applications at any scale.",
  "text": "Available on the AWS Marketplace",
  "url": "https://aws.amazon.com/marketplace/pp/prodview-xhgyscinlz4jk",
  "- tier": "Standard",
  "price": "From <span>$0.096/hour</span>",
  "supportTitle": "Standard Support",
  "ctaText": "Get Started",
  "ctaLink": "https://app.pinecone.io/",
  "ctaInfo": "No credit card required",
  "content": "    ctaLink: https://app.pinecone.io/     ctaInfo: Upgrade after creating account    - tier: Enterprise     description: For mission-critical production applications.     price: From <span>$0.144/hour</span>     hardware:       - Any number of pods and replicas       - Zero-downtime scaling     features:       - Save vector data in Collections ($0.025/GB/month)       - Multiple projects and users       - Choose your environment (GCP US-West, GCP EU-West, AWS US-East)       - Multi-AZ       - Prometheus Metrics       - Multiple payment options     supportTitle: Premium Support     support:       - Slack and Email       - 24/7/365 for <span class='tooltip-item'>Sev-1<span class='tooltip'>Production system is down or is severely impacted such that routine operation is impossible.</span></span> and <span class='tooltip-item'>Sev-2<span class='tooltip'>Production issue where system is functional but offers service in degraded or restricted capacity.</span></span>, <span class=\"tooltip-item\">Business hours<span class=\"tooltip\">Monday–Friday 9am–6pm EST</span></span> for <span class='tooltip-item'>Sev-3<span class='tooltip'>Production system has minor impact or has issue in development.</span></span> and <span class='tooltip-item'>Sev-4<span class='tooltip'>No production impact, questions or request for feature.</span></span>       - Up to 4 technical contacts       - \"Response-time SLA (<span class='tooltip-item'>Sev-1<span class='tooltip'>Production system is down or is severely impacted such that routine operation is impossible.</span></span>: 1 hour; <span class='tooltip-item'>Sev-2<span class='tooltip'>Production issue where system is functional but offers service in degraded or restricted capacity.</span></span>: 4 hours; <span class='tooltip-item'>Sev-3<span class='tooltip'>Production system has minor impact or has issue in development.</span></span>: 1 business day; <span class='tooltip-item'>Sev-4<span class='tooltip'>No production impact, questions or request for feature.</span></span>: 2 business days)\"       - <span class='tooltip-item'>Uptime<span class='tooltip'>Defined as total minutes in that month minus downtime, divided by total minutes in that month.</span></span> SLA (99.9%)     ctaText: Get Started     ctaLink: https://app.pinecone.io/     ctaInfo: Upgrade after creating account ---  <a name=\"cost\"><h3>Usage Pricing</h3></a>  When you [create a Pinecone Index](/docs/manage-indexes/), you choose the pod type and number of pods to use for the index. Each pod comes with a fixed amount of vCPU, RAM, and SSD coupled with the software. You will be billed at the end of the month for total pod-hours used. Pod usage is rounded up to 15-minute increments.  Overview of pod types (see [documentation](/docs/indexes/) for more details):  - s1: Optimized for storage and cost, with low overall cost and low throughput. - p1: Optimized for balanced performance, with moderate search latency and throughput. - p2: Optimized for high performance, with very low search latency and high throughput.  {{< tabs totalTabs=\"2\">}} {{< tab tabName=\"Google Cloud Platform\" tabId=\"gcp_tab\" tabIcon=\"/images/gcp_logo.svg\" >}}  <div class=\"responsive-table pricing-table\">  | Pod Type |       Starter       |       Standard        |      Enterprise       | | :------: | :-----------------: | :-------------------: | :-------------------: | |    s1    | Free <br>(Limit: 1) | $0.0960 <br>/pod-hour | $0.1440 <br>/pod-hour | |    p1    | Free <br>(Limit: 1) | $0.0960 <br>/pod-hour | $0.1440 <br>/pod-hour | |    p2    | Free <br>(Limit: 1) | $0.1440 <br>/pod-hour | $0.2160 <br>/pod-hour |  </div>  {{< /tab >}}  {{< tab tabName=\"Amazon Web Services\" tabId=\"aws_tab\" tabIcon=\"/images/aws_logo.svg\" >}}  <div class=\"responsive-table pricing-table\">  | Pod Type |    Starter    |       Standard        |       Enterprise       | | :------: | :-----------: | :-------------------: | :--------------------: | |    s1    | Not available | $0.1110 <br>/pod-hour | $0.1665 <br>/pod-hour  | |    p1    | Not available | $0.1110 <br>/pod-hour | $0.1665 <br>/pod-hour  | |    p2    | Not available | $0.1665 <br>/pod-hour | $0.24975 <br>/pod-hour |  </div>  {{< /tab >}} {{< /tabs >}}  [Scaling pod sizes](/docs/manage-indexes/#changing-pod-sizes) to x2, x4, and x8 will change the pricing proportionally. For instance, a p1.x2 pod is twice the price of a p1 pod.  [Contact us](/contact/) for pre-commitment discounts and Enterprise Dedicated pricing.  ### Common Questions  **What will I be charged for?**  You will be charged for \"pod hours\" in 15-minute increments, which is the time an index is running. Pod-hours are counted even when you're not sending queries to the index.  **How many pods do I need?**  Since each pod is a bundle of hardware resources (vCPU, RAM, and disk), the required number of pods is affected by multiple factors such as data size (volume, dimensionality, and metadata), metadata cardinality, and desired latency or throughput. We strongly recommend testing with _your own data_ on different pod types and number of pods, and following our [tips for performance tuning](https://www.pinecone.io/docs/performance-tuning/).  As a _very rough_ starting guideline: each s1 pod fits 5M 768-dim vectors, each p1 pod fits 1M 768-dim vectors, and each p2 pod fits 1M 768-dim vectors.  You can also [contact us](/contact/) for help with sizing and testing.  **How secure is Pinecone?**  Pinecone is SOC2 Type II compliant, and we take security seriously for all users on all plans. Read about [our security practices](/security/).  **Where is the user license or the terms of service?**  Users accept the [End-User License Agreement](/user-agreement/) when creating an account.  **Do you offer pre-commitment discounts?**  Yes we do. [Contact us](/contact/) for more details.  **How do I cancel or downgrade?**  You can manage your subscription directly in the console. If you attempt to downgrade to Starter (Free) while exceeding the limits of the Starter plan, you will first need to delete projects or indexes until you are within the limits.  ---  For other questions, [contact us](/contact/) or [ask the community](https://community.pinecone.io). ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e284"
  },
  "title": "\"Customer Data Protection Addendum\"",
  "headline": "\"Customer Data Protection Addendum\"",
  "cta": "false",
  "intro": "\"\"",
  "content": "This Customer Data Protection Addendum (“**DPA**”) is incorporated into and forms part of the Pinecone Services Agreement (the “**Agreement**”) by and between you (“**Customer**”) and Pinecone Systems, Inc. (“**Pinecone**”), under which Pinecone has agreed to provide the certain Hosted Services described therein (“**Hosted Services**”) to Customer.  This DPA forms part of the Agreement and specifies the data protection obligations of the parties that may arise from the Processing of Personal Data by Pinecone on behalf of Customer in the course of providing Hosted Services to Customer under the Agreement.  1. **Definitions**      For purposes of this DPA, the terms below have the meanings set forth below. Capitalized terms that are used but not defined in this DPA have the meanings given in the Agreement.      1. **Affiliate** means any entity that directly or indirectly controls, is controlled by, or is under common control with the subject entity, where “control” refers to the power to direct or cause the direction of the subject entity, whether through ownership of voting securities, by contract or otherwise.      2. **Applicable Data Protection Laws** means European Data Protection Laws and the CCPA, in each case, to the extent applicable to the relevant Personal Data or Processing thereof under the Agreement.      3. **CCPA** means the California Consumer Privacy Act of 2018 and any regulations promulgated thereunder, in each case, as amended from time to time.      4. **Data Subject** means an identified or identifiable natural person.      5. **EEA** means the European Economic Area.      6. **EU** means the European Union.      7. **European Data Protection Laws** means the GDPR and other data protection laws of the EU, its Member States, Switzerland, Iceland, Liechtenstein, Norway and the United Kingdom, in each case, to the extent it applies to the relevant Personal Data or Processing thereof under the Agreement.      8. **GDPR** means Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016, as amended from time to time.      9. **Information Security Incident** means a breach of Pinecone’s security leading to the accidental or unlawful destruction, loss, alteration, unauthorized disclosure of, or access to, Personal Data in Pinecone’s possession, custody or control. Information Security Incidents do not include unsuccessful attempts or activities that do not compromise the security of Personal Data, including unsuccessful log-in attempts, pings, port scans, denial of service attacks, or other network attacks on firewalls or networked systems.      10. **Personal Data** means the Customer Data that constitutes “personal data”, “personal information” or similar information governed by Applicable Data Protection Laws. For purposes of this DPA, Personal Data does not include Personal Data of representatives of Customer with whom Pinecone has business relationships independent of the Hosted Services where Pinecone acts as a controller of such Personal Data.      11. **Process** or **Processing** means any operation or set of operations that is performed on Personal Data or on sets of Personal Data, whether or not by automated means, such as collection, recording, organization, structuring, storage, adaptation or alteration, retrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, restriction, erasure or destruction.      12. **Security Measures** has the meaning given in Section 4.1 (Pinecone’s Security Measures).      13. **Standard Contractual Clauses** means the mandatory provisions of the standard contractual clauses for the transfer of personal data to processors established in third countries in the form set out by European Commission Decision 2010/87/EU.      14. **Subprocessors** means third parties authorized under this DPA to Process Personal Data in relation to the Hosted Services.      15. **Third Party Subprocessors** has the meaning given in Section 5 (Subprocessors) of Annex 1.      16. The terms **controller**, **processor** and **supervisory authority** as used in this DPA have the meanings given in the GDPR.  2. **Duration and Scope of DPA**      1. This DPA will, notwithstanding the expiration of the Agreement, remain in effect until, and automatically expire upon, Pinecone’s deletion or return of all Personal Data.      2. <u>Annex 1</u> (EU Annex) to this DPA applies to Personal Data or the Processing thereof subject to European Data Protection Laws. Annex 2 (California Annex) to this DPA, applies to Personal Data or the Processing thereof subject to the CCPA with respect to which Customer is a Business (as defined in the CCPA).  3. **Customer Instructions**      Pinecone will Process Personal Data only in accordance with Customer’s instructions. By entering into this DPA, Customer instructs Pinecone to Process Personal Data to provide the Hosted Services. Customer acknowledges and agrees that such instruction authorizes Pinecone to Process Personal Data (a) to perform its obligations and exercise its rights under the Agreement; (b) perform its legal obligations and to establish, exercise or defend legal claims in respect of the Agreement; (c) pursuant to any other written instructions given by Customer and acknowledged in writing by Pinecone as constituting instructions for purposes of this DPA; and (d) as reasonably necessary for the proper management and administration of Pinecone’s business.  4. **Security**      1. <u>Pinecone Security Measures</u>. Pinecone will implement and maintain technical and organizational measures designed to protect Personal Data against accidental or unlawful destruction, loss, alteration, unauthorized disclosure of or access to Personal Data as described in Annex 3 (the “Security Measures”). Pinecone may update the Security Measures from time to time, provided the updated measures do not decrease the overall protection of Personal Data.      2. <u>Information Security Incidents</u>. If Pinecone becomes aware of an Information Security Incident, Pinecone will (a) notify Customer of the Information Security Incident without undue delay after becoming aware of the Information Security Incident and (b) take reasonable steps to identify the cause of such Information Security Incident and prevent a recurrence. Notifications made pursuant to this Section 4.2 will describe, to the extent possible, details of the Information Security Incident, including steps taken to mitigate the potential risks and steps Pinecone recommends Customer take to address the Information Security Incident. Pinecone’s notification of or response to an Information Security Incident under this Section 4.2 will not be construed as an acknowledgement by Pinecone of any fault or liability with respect to the Information Security Incident.      3. <u>Customer’s Security Responsibilities and Assessment</u>          1. <u>Customer’s Security Responsibilities</u>. Customer agrees that, without limitation of Pinecone’s obligations under Section 4.1 (Pinecone Security Measures) and Section 4.2 (Information Security Incidents), Customer is solely responsible for its use of the Hosted Services, including (a) making appropriate use of the Hosted Services to ensure a level of security appropriate to the risk in respect of the Personal Data; (b) securing the account authentication credentials, systems and devices Customer uses to access the Hosted Services; (c) securing Customer’s systems and devices that Pinecone uses to provide the Hosted Services; and (d) backing up Personal Data.          2. <u>Customer’s Security Assessment</u>. Customer is solely responsible for evaluating for itself whether the Hosted Services, the Security Measures and Pinecone’s commitments under this DPA will meet Customer’s needs, including with respect to any security obligations of Customer under Applicable Data Protection Laws or other laws. Customer acknowledges and agrees that (taking into account the state of the art, the costs of implementation and the nature, scope, context and purposes of the Processing of Personal Data as well as the risks to individuals) the Security Measures implemented and maintained by Pinecone provide a level of security appropriate to the risk in respect of the Personal Data.  5. **Data Subject Rights**      1. <u>Customer’s Responsibility for Requests</u>. If Pinecone receives any request from a Data Subject in relation to the Data Subject’s Personal Data, Pinecone will advise the Data Subject to submit the request to Customer and Customer will be responsible for responding to any such request.      2. <u>Pinecone’s Data Subject Request Assistance</u>. Pinecone will (taking into account the nature of the Processing of Personal Data) provide reasonable assistance as necessary for Customer to perform its obligation under Applicable Data Protection Laws to fulfill requests by Data Subjects to exercise their rights under Applicable Data Protection Laws, including if applicable, Customer’s obligation to respond to requests for exercising the Data Subject’s rights set out in Chapter III of the GDPR. Customer shall reimburse Pinecone for any such assistance, at Pinecone’s then-current professional Hosted Services rates, which shall be made available to Customer upon request.  6. **Customer Responsibilities**      Customer represents and warrants to Pinecone that (a) all notices have been given to, and consents and rights have been obtained from, the relevant Data Subjects and any other party as may be required by Applicable Data Protection Laws and any other laws for Processing under the Agreement; and (b) Personal Data does not and will not contain any (i) Social Security numbers or other government-issued identification numbers, (ii) protected health information subject to the Health Insurance Portability and Accountability Act (HIPAA) or other information regarding a Data Subject’s medical history, mental or physical condition or medical treatment or diagnosis by a health care professional, (iii) health insurance information, (iv) biometric information, (v) passwords to any online accounts, (vi) credentials to any financial accounts, (vii) tax return data, (viii) payment card information subject to the Payment Card Industry Data Security Standard, (ix) personal data of children under 16 years of age, or (x) other information that falls within any special categories of data (as defined in the GDPR).  7. **Notices**      Notwithstanding anything to the contrary in the Agreement, any notices required or permitted to be given by Pinecone to Customer may be given (a) in accordance with any notice clause of the Agreement; (b) to Pinecone’s primary points of contact with Customer; or (c) to any email provided by Customer for the purpose of providing it with Hosted Services-related communications or alerts. Customer is solely responsible for ensuring that such email addresses are valid.  8. **Miscellaneous**      1. <u>Liability Cap</u>.      The total combined liability of either party and its Affiliates towards the other party and its Affiliates, whether in contract, tort or any other theory of liability, under or in connection with this DPA and the Standard Contractual Clauses if entered into as described in Annex 1, Section 4 (Transfers out of the EEA) combined will be limited to limitations on liability or other liability caps agreed to by the parties in the Agreement, subject to Section 8.2 of this DPA (Liability Cap Exclusions)       2. <u>Liability Cap Exclusions</u>.      Nothing in Section 8.1 of this DPA (Liability Cap) will affect any party’s liability to Data Subjects under the third-party beneficiary provisions of the Standard Contractual Clauses to the extent limitation of such rights is prohibited by European Data Protection Laws, where applicable.      3. <u>Conflict</u>.      Except as expressly modified by the DPA, the terms of the Agreement remain in full force and effect. To the extent of any conflict or inconsistency between this DPA and the other terms of the Agreement, this DPA will govern.  ## Annex 1  ### EU Annex  1. **Processing of Data**      1. <u>Subject Matter and Details of Processing</u>. The parties acknowledge and agree that (a) the subject matter of the Processing under the Agreement is Pinecone’s provision of the Hosted Services; (b) the duration of the Processing is from Pinecone’s receipt of Personal Data until deletion of all Personal Data by Pinecone in accordance with the Agreement; (c) the nature and purpose of the Processing is to provide the Hosted Services; (d) the data subjects to whom the Processing pertains are Customer’s employees, personnel and other individuals to whom Customer has a relationship; and (e) the categories of Personal Data are provided by Customer or its Authorized Users in connection with the Hosted Services.      2. <u>Roles and Regulatory Compliance; Authorization</u>. The parties acknowledge and agree that (a) Pinecone is a processor of that Personal Data under European Data Protection Laws; (b) Customer is a controller (or a processor acting on the instructions of a controller) of that Personal Data under European Data Protection Laws; and (c) each party will comply with the obligations applicable to it in such role under the European Data Protection Laws with respect to the Processing of that Personal Data. If Customer is a processor, Customer represents and warrants to Pinecone that Customer’s instructions and actions with respect to Personal Data, including appointment of Pinecone as another processor, have been authorized by the relevant controller.      3. <u>Pinecone’s Compliance with Instructions</u>. Pinecone will only Process Personal Data in accordance with Customer’s instructions described in this DPA unless European Data Protection Laws requires otherwise, in which case Pinecone will notify Customer (unless that law prohibits Pinecone from doing so on important grounds of public interest).      4. <u>Data Deletion</u>. Upon termination of Customer’s access to the Hosted Services, Customer instructs Pinecone to delete all Personal Data from Pinecone’s systems as soon as reasonably practicable, unless European Data Protection Laws requires otherwise.  2. **Data Security**      1. <u>Pinecone Security Measures, Controls and Assistance</u>          1. <u>Pinecone Security Assistance</u>. Pinecone will (taking into account the nature of the Processing of Personal Data and the information available to Pinecone) provide Customer with reasonable assistance necessary for Customer to comply with its obligations in respect of Personal Data under European Data Protection Laws, including Articles 32 to 34 (inclusive) of the GDPR, by (a) implementing and maintaining the Security Measures; (b) complying with the terms of Section 4.2 (Information Security Incidents) of the DPA; and (c) complying with this Annex 1.          2. <u>Security Compliance by Pinecone Staff</u>. Pinecone will grant access to Personal Data only to personnel who need such access for the scope of their job duties, and are subject to appropriate confidentiality arrangements.      2. <u>Reviews and Audits of Compliance</u>          1. Customer may audit Pinecone’s compliance with its obligations under this DPA up to once per year and on such other occasions as may be required by European Data Protection Laws, including where mandated by Customer’s supervisory authority. Pinecone will contribute to such audits by providing Customer or Customer’s supervisory authority with the information and assistance reasonably necessary to conduct the audit.          2. If a third party is to conduct the audit, Pinecone may object to the auditor if the auditor is, in Pinecone’s reasonable opinion, not independent, a competitor of Pinecone, or otherwise manifestly unsuitable. Such objection by Pinecone will require Customer to appoint another auditor or conduct the audit itself.          3. To request an audit, Customer must submit a detailed proposed audit plan to Pinecone at least two weeks in advance of the proposed audit date and any third party auditor must sign a customary non-disclosure agreement mutually acceptable to the parties (such acceptance not to be unreasonably withheld) providing for the confidential treatment of all information exchanged in connection with the audit and any reports regarding the results or findings thereof. The proposed audit plan must describe the proposed scope, duration, and start date of the audit. Pinecone will review the proposed audit plan and provide Customer with any concerns or questions (for example, any request for information that could compromise Pinecone security, privacy, employment or other relevant policies). Pinecone will work cooperatively with Customer to agree on a final audit plan. Nothing in this Section 2.2 shall require Pinecone to breach any duties of confidentiality.          4. If the controls or measures to be assessed in the requested audit are addressed in an SOC 2 Type 2, ISO, NIST or similar audit report performed by a qualified third party auditor within twelve (12) months of Customer’s audit request and Pinecone has confirmed there are no known material changes in the controls audited, Customer agrees to accept such report lieu of requesting an audit of such controls or measures.          5. The audit must be conducted during regular business hours, subject to the agreed final audit plan and Pinecone’s safety, security or other relevant policies, and may not unreasonably interfere with Pinecone business activities.          6. Customer will promptly notify Pinecone of any non-compliance discovered during the course of an audit and provide Pinecone any audit reports generated in connection with any audit under this Section 2.2, unless prohibited by European Data Protection Laws or otherwise instructed by a supervisory authority. Customer may use the audit reports only for the purposes of meeting Customer’s regulatory audit requirements and/or confirming compliance with the requirements of this DPA.          7. Any audits are at Customer’s expense. Customer shall reimburse Pinecone for any time expended by Pinecone or its Third Party Subprocessors in connection with any audits or inspections under this Section 2.2 at Pinecone’s then-current professional Hosted Services rates, which shall be made available to Customer upon request. Customer will be responsible for any fees charged by any auditor appointed by Customer to execute any such audit. Nothing in this DPA shall be construed to require Pinecone to furnish more information about its Third Party Subprocessors in a connection with such audits than such Third Party Subprocessors make generally available to their customers.  3. **Impact Assessments and Consultations**      Pinecone will (taking into account the nature of the Processing and the information available to Pinecone) reasonably assist Customer in complying with its obligations under Articles 35 and 36 of the GDPR, by (a) making available documentation describing relevant aspects of Pinecone’s information security program and the security measures applied in connection therewith; and (b) providing the other information contained in the Agreement including this DPA.  4. **Data Transfers**      1. <u>Data Processing Facilities</u>. Pinecone may, subject to Section 4.2 to this Annex 1 (Transfers out of the EEA, Switzerland or United Kingdom), store and process Personal Data in the United States or anywhere Pinecone or its Subprocessors maintains facilities.      2. <u>Transfers out of the EEA, Switzerland or United Kingdom</u>. If Customer transfers Personal Data out of the EEA, Switzerland or United Kingdom to Pinecone in a country not deemed by the European Commission to have adequate data protection, such transfer will be governed by the Standard Contractual Clauses, the terms of which are hereby incorporated into this DPA. In furtherance of the foregoing, the parties agree that:          1. for purposes of the Standard Contractual Clauses, (a) Customer will act as the data exporter and (b) Pinecone will act as the data importer;          2. for purposes of Appendix 1 to the Standard Contractual Clauses, the categories of data subjects, data, special categories of data (if appropriate), and the Processing operations shall be as set out in Section 1.1 to this Annex 1 (Subject Matter and Details of Processing);          3. for purposes of Appendix 2 to the Standard Contractual Clauses, the technical and organizational measures shall be the Security Measures;          4. upon data exporter’s request under the Standard Contractual Clauses, data importer will provide the copies of the subprocessor agreements that must be sent by the data importer to the data exporter pursuant to Clause 5(j) of the Standard Contractual Clauses, and that data importer may remove or redact all commercial information or clauses unrelated the Standard Contractual Clauses or their equivalent beforehand;          5. Customer agrees that the provisions of Section 4.2 of the DPA (Information Security Incidents) satisfy the requirements of the Standard Contractual Clauses between Customer and Pinecone under Clause 5(d)(ii);          6. the audits described in Clause 5(f) and Clause 12(2) of the Standard Contractual Clauses shall be performed in accordance with Section 2.2 of this Annex 1 (Reviews and Audits of Compliance);          7. Customer’s authorizations in Section 5 of this Annex 1 (Subprocessors) will constitute Customer’s prior written consent to the subcontracting by Pinecone of the Processing of Personal Data if such consent is required under Clause 5(h) and 11(1) of the Standard Contractual Clauses; and          8. certification of deletion of Personal Data as described in Clause 12(1) of the Standard Contractual Clauses shall be provided only upon Customer’s request.         Notwithstanding the foregoing, the Standard Contractual Clauses (or obligations the same as those under the Standard Contractual Clauses) will not apply to the extent an alternative recognized compliance standard for the lawful transfer of Personal Data outside the EEA, Switzerland or United Kingdom (e.g., binding corporate rules) applies to the transfer.  5. **Subprocessors**      1. <u>Consent to Subprocessor Engagement</u>. Customer specifically authorizes the engagement of Pinecone’s Affiliates as Subprocessors. In addition, Customer generally authorizes the engagement of any other third parties as Subprocessors (“**Third Party Subprocessors**”).      2. <u>Information about Subprocessors</u>. Information about Subprocessors, including their functions and locations, is available at: https://www.pinecone.io/subprocessors/ (as may be updated by Pinecone from time to time) or such other website address as Pinecone may provide to customer from time to time (the “Subprocessor Site”).      3. <u>Requirements for Subprocessor Engagement</u>. When engaging any Subprocessor, Pinecone will enter into a written contract with such Subprocessor containing data protection obligations not less protective than those in this DPA with respect to Personal Data to the extent applicable to the nature of the Hosted Services provided by such Subprocessor. Pinecone shall be liable for all obligations subcontracted to, and all acts and omissions of, the Subprocessor.      4. <u>Opportunity to Object to Subprocessor Changes</u>. When any new Third Party Subprocessor is engaged during the term of the Agreement, Pinecone will notify Customer of the engagement (including the name and location of the relevant Subprocessor and the activities it will perform) by updating the website listed in Section 5.2 of this Annex 1 (Information about Subprocessors). If Customer objects to such engagement in a written notice to Pinecone within 15 days of being informed thereof on reasonable grounds relating to the protection of Personal Data, Customer and Pinecone will work together in good faith to find a mutually acceptable resolution to address such objection. If the parties are unable to reach a mutually acceptable resolution within a reasonable timeframe, Customer may, as its sole and exclusive remedy, terminate the Agreement and cancel the Hosted Services by providing written notice to Pinecone.  ## Annex 2  ### California Annex  1. For purposes of this Annex 2, the terms “business,” “commercial purpose,” “sell” and “service provider” shall have the respective meanings given thereto in the CCPA, and “personal information” shall mean Personal Data that constitutes personal information governed by the CCPA.  2. Pinecone shall not retain, use, or disclose any Personal Data that constitutes “personal information” under the CCPA (“**CA Personal Information**”) for any purpose other than for the specific purpose of providing the Hosted Services, or as otherwise permitted by CCPA, including retaining, using, or disclosing the CA Personal Information for a commercial purpose other than providing the Hosted Services.  3. Pinecone shall not (a) sell any CA Personal Information; (b) retain, use or disclose any CA Personal Information for any purpose other than for the specific purpose of providing the Service, including retaining, using, or disclosing the CA Personal Information for a commercial purpose other than provision of the Hosted Services; or (c) retain, use or disclose the CA Personal Information outside of the direct business relationship between Pinecone and Customer. Pinecone hereby certifies that it understands its obligations under this Section 3 and will comply with them.  4. It is the parties’ intent that with respect to any CA Personal Information, Pinecone is a service provider.  5. Provision of the Hosted Services encompasses the Processing authorized by Customer’s instructions described in Section 3 of the DPA (Customer Instructions). The parties acknowledge that Pinecone’s retention, use and disclosure of CA Personal Information authorized by Customer’s instructions are integral to Pinecone’s provision of the Hosted Services and the business relationship between the parties.  6. Notwithstanding anything in the Agreement or any order form entered in connection therewith, the parties acknowledge and agree that Pinecone’s access to CA Personal Information or any other Personal Data does not constitute part of the consideration exchanged by the parties in respect of the Agreement.  ## Annex 3  ### Security Measures  As from the DPA Effective Date, Pinecone will implement and maintain the Security Measures set out in this <u>Annex 3</u>. Pinecone may update or modify such Security Measures from time to time provided that such updates and modifications do not materially decrease the overall security of the Hosted Services.  1. Organizational management and dedicated staff responsible for the development, implementation and maintenance of Pinecone’s information security program.  2. Audit and risk assessment procedures for the purposes of periodic review and assessment of risks to Pinecone’s organization, monitoring and maintaining compliance with Pinecone’s policies and procedures, and reporting the condition of its information security and compliance to internal senior management.  3. Data security controls which include at a minimum, but may not be limited to, logical segregation of data, restricted (e.g. role-based) access and monitoring, and utilization of commercially available and industry standard encryption technologies for Personal Data that is transmitted over public networks (i.e. the Internet) or when transmitted wirelessly.  4. Logical access controls designed to manage electronic access to data and system functionality based on authority levels and job functions, (e.g. granting access on a need-to-know basis, use of unique IDs and passwords for all users, periodic review and revoking/changing access when employment terminates or changes in job functions occur).  5. Password controls designed to manage and control password strength, expiration and usage including prohibiting users from sharing passwords and requiring that Pinecone passwords that are assigned to its employees: (i) be at least eight (8) characters in length, (ii) not be stored in readable format on Pinecone’s computer systems; (iii) must be changed every ninety (90) days; must have defined complexity; (iv) must have a history threshold to prevent reuse of recent passwords; and (v) newly issued passwords must be changed after first use.  6. Physical and environmental security of data center, server room facilities and other areas containing Personal Data designed to:  (i) protect information assets from unauthorized physical access, (ii) manage, monitor and log movement of persons into and out of Pinecone facilities, and (iii) guard against environmental hazards such as heat, fire and water damage.  7. Change management procedures and tracking mechanisms designed to test, approve and monitor all changes to Pinecone’s technology and information assets.  8. Incident / problem management procedures design to allow Pinecone to investigate, respond to, mitigate and notify of events related to Pinecone’s technology and information assets.  9. Business resiliency/continuity and disaster recovery procedures designed to maintain service and/or recovery from foreseeable emergency situations or disasters.  ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e286"
  },
  "title": "Open Roles",
  "headline": "\"We're hiring!\"",
  "intro": "\"Join our growing team and help shape the future of our industry.\"",
  "description": "|",
  "src": "/images/pinecone-careers-hero.png",
  "alt": "Pinecone careers",
  "anchorID": "open-roles",
  "text": "At Pinecone, we’re proud to be an equal opportunity employer. We realize the key to creating a company with a world-class culture and employee experience comes from whom we hire and from creating a workplace that celebrates everyone. We proudly consider qualified applicants without regard to race, religion, gender, sexual identity, national origin, age, disability, or any other basis.",
  "- image": "/images/pinecone-team-collage-3.png",
  "# - image": "/images/working-at-pinecone.jpg",
  "#   alt": "Pinecone team member",
  "- title": "Be a friend",
  "- text": "|",
  "name": "Daniel Margulis",
  "image": "\"/images/daniel-margulis.jpg\"",
  "content": "",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e288"
  },
  "title": "\"Community\"",
  "headline": "\"Pinecone Community\"",
  "content": "  title: Hi! We’re happy <span>you’re here</span>.   description: |     The Pinecone Community is for engineers, data scientists, and anyone else involved in the new frontier of (vector) search.   emailSubmit: true info:   title: Ways to get and <span>stay involved</span>   forum:     description: |       [Join the forum](https://community.pinecone.io/) for:     list:       - title: Announcements         text: \"- News and updates from the Pinecone team.\"       - title: Support         text: \"- Ask questions about and get help using Pinecone.\"       - title: Search, No Filter         text: \"- General discussions about search, models, data, use cases, and anything else. Share your projects, tutorials, and ideas with the community.\"     ctaText: Join the forum     ctaLink: https://community.pinecone.io/   events:     description: Live presentations and workshops featuring experts from Pinecone and other organizations.     list:       - date: \"2022-02-16\"         event: Pinecone Office Hours         url: https://pinecone-io.zoom.us/webinar/register/WN_kd6WSbQ5TSONOSqMF0vRdg       - date: \"2022-02-24\"         event: \"Ask Like You Mean It: Build a custom ML-powered Q&A app in a day\"         url: https://pinecone-io.zoom.us/webinar/register/WN_31NCjwJjRBOxZy1AnKTArw       - date: \"2022-03-09\"         event: \"Question, Answered: How to build AI-powered Q&A applications with Haystack and Pinecone\"         url: https://pinecone-io.zoom.us/webinar/register/WN_EOJJDPrbTc2yyH9KpbTcIQ   showcase:     description: |       Want to see your Pinecone project here? Tell us: info@pinecone.io     list:       - title: All-In On AI         description: A semantic search index built with Pinecone and OpenAI that lets you search across every episode of the All-In podcast.         url: https://all-in-on-ai.vercel.app/       - title: NFT Semantic Search         description: Visual semantic search for a million NFTs with Alchemy, OpenAI's CLIP & Pinecone.         url: https://abhaykashyap.com/blog/visual-semantic-search-nfts-alchemy-openai-clip-pinecone/       - title: Parent Resemblence Detection         description: Use Pinecone to detect which parent looks like their children the most.         url: https://medium.com/@timtullydevnull/use-pinecone-to-see-which-parent-most-resembles-your-kids-862588d1c1a       - title: Bible Semantic Search         description: A streamlit app for performing semantic search on the King James Bible.         url: https://share.streamlit.io/chrislee973/bible-semantic-search/main/app.py       - title: Mood Surf         description: A discovery engine to explore topics that intrigue you. Made by [Re:Search](https://re-search.xyz/).         url: https://mood.surf/       - title: FlixRec         description: A similar movies recommendation system built using Pinecone. Test it out at the link above and [read the article here](https://web.navan.dev/posts/2022-05-21-Similar-Movies-Recommender.html).         url: https://flixrec.navan.dev/       - title: Not Slack         description: Not Slack chatbot.         url: https://share.streamlit.io/pinecone-io/playground/not_slack_chatbot/src/server.py       - title: Pinecone.jl         description: Pinecone.jl is a Julia API for the Pinecone vector database.         url: https://github.com/tullytim/Pinecone.jl       - title: ML Q&A         description: The Q&A tool takes discussions and docs from some of the best Python ML libraries and collates their content into a natural language search and Q&A tool.         url: https://share.streamlit.io/pinecone-io/playground/beyond_search_openai/src/server.py       - title: Semantic Search with Sentence Transformers         description: A demo on semantic search with sentence transformers and Pinecone.         url: https://colab.research.google.com/github/pinecone-io/examples/blob/master/semantic_search/light_demo.ipynb       - title: Semantic Search with Pinecone and OpenAI         description: A demo example of building a SOTA semantic search tool with OpenAI vector embeddings and the Pinecone vector DB.         url: https://colab.research.google.com/drive/1t15pC65wFcynCD-VcuRjutysItZM2iII?usp=sharing       - title: ML Doc Q&A         description: The ML Doc Q&A tool takes discussions and docs from some of the best Python ML libraries and collates their content into a natural language search tool.         url: https://share.streamlit.io/pinecone-io/playground/doc_search/src/server.py       - title: Face Recognition         description: Facial recognition using Pinecone.         url: https://github.com/serengil/tensorflow-101/blob/master/python/Pinecone-Face-Recognition.ipynb       - title: GPL for Semantic Search by James Briggs         description: Domain Adaptation for Dense Retrieval.         url: https://gist.github.com/jamescalam/d2c888775c87f9882bb7c379a96adbc8#file-gpl-domain-adaptation-ipynb       - title: Hacker News Doppelgänger         description: This app compares the semantic meaning of your comment history with those of all other users, and finds the top ten users whose comment histories are most similar to yours.         url: https://share.streamlit.io/pinecone-io/playground/hacker_news/src/server.py       - title: Haystack-Pinecone Integration Live App by James Briggs and Brandon Chan         description: Explore the world - Ask any question on this topic and see if Haystack can find the correct answer to your query!         url: https://haystack-demo.deepset.ai/       - title: ODQA Pipeline         description: A question answering demonstration by James Briggs to show how useful Pinecone can be in developing ML-powered applications. Take a look at his work at the link above, and [try out the demo](https://share.streamlit.io/pinecone-io/playground/doc_search/src/server.py).         url: https://colab.research.google.com/drive/1sZmvJ7qpuX7uzD3f8gb7PhxCA9VPkqSF#scrollTo=WRkkhlQOz5yL   newsletter:     description: Don't miss community news and events. pioneers:   title: Pinecone <span>Pioneers</span>   intro: We want to celebrate those who are revolutionizing search technology. The people below are leading the way on adopting, advancing, and advocating for vector search technology.   list:     - name: Romana Dorfer       position: Co-Founder, Factinsect       image: /images/romana-dorfer.jpeg       bio: Romana is an experienced software developer and AI expert, whose company makes an AI-based tool for fact-checking online content.     - name: Dan Whalen       position: Manager R&D, Expel.io       image: /images/dan-whalen.png       bio: Dan is a Principal Researcher at Expel, a transparent Managed Detection and Response vendor. Being in the information security field for 8+ years, he’s passionate about building tech solutions that help protect people from security breaches. Lately, he's focused on applying data science solutions to unique security problems.     - name: André Mourão       position: Senior Software Engineer, Mem Labs       image: /images/andre-mourao.jpeg       bio: \"André is at the forefront of Information Retrieval and Machine Learning. The Principal Senior Engineer at Searchable.ai, making it easy for you to find your files that are spread across multiple cloud services, and founder of Revionista.PT, uncovering post-publication changes in Portuguese news, he’s spent his career ensuring technology makes life easier and more accessible for others. He holds a Computer Science PhD in multimodal search systems. \"     - name: Isabella Fulford       position: Solutions Architect, OpenAI       image: /images/isabella-fulford.png       bio: \"Isabella is a software engineer at Mem Labs, the world's first self-organizing workspace to help you find information and store it better, faster. While pursuing her Bachelor's and completing her Master's in Computer Science at Stanford as a Mayfield Fellow, she also worked at Amazon Web Services and Inspire AI.\"     - name: Craig Schmidt       position: ML Engineer, Delomore       image: /images/craig-schmidt.png       bio: Craig is a former Principal Machine Learning Engineer at TripAdvisor. In his seven years there, he worked extensively with the search group. Before that, he worked at several startups in greater Boston in various Machine Learning (ML) roles. His current startup, Delomore, is a search engine for Shopify shops. It is a perfect application for an embedding-based vector search.     - name: Diego Lopez Yse       position: ML Engineer, Moody's       image: /images/diego-lopez-yse.png       bio: \"Diego is a data scientist working on applied Machine Learning solutions in the Life Sciences. He’s also a writer and content creator, helping others expand their digital skills. He enjoys hackathons and every opportunity to tackle challenges differently: If we've always done it this way, let's change it.\"     - name: George Mathew       position: Founder, Nyckel       image: /images/george-mathew.png       bio: \"George is a computer engineering veteran. After working at companies like Microsoft and Oracle, he founded Nyckel, which makes machine learning accessible to software developers.\"     - name: Aiden Lee       position: Co-Founder & CTO, Twelve Labs       image: /images/aiden-lee.png       bio: \"Aiden has been in the AI/Deep Learning space for years, including work as an AI Researcher, a Deep Learning Research Scientist, and now as the Founder and CTO of Twelve Labs. Twelve Labs is building a video understanding AI infrastructure where users can easily train and integrate SOTA video understanding models into their system. Their AI team won 1st place in the video retrieval track of IEEE ICCV VALUE Challenge 2021 hosted by Microsoft and UNC.\"     - name: George Williams       position: Head of AI, Smile Identity       image: /images/george-williams.png       bio: \"George is the Head of AI at Smile Identity, an identity management and biometrics provider. He has held senior leadership roles at Apple's New Product Architecture group and NYU's Courant Institute.  He is the author of several research papers in computer vision and deep learning and regularly presents at meetups and tech conferences. He has also held the position of Embedded AI Chair for the Valleyml.ai conference, serves on the content committee for the Open Data Science Conference, and is a chair at the Neural Information Processing Conference.\"     - name: Pratik Bhavsar       position: Senior NLP Scientist, Enterpret       image: /images/pratik-bhavsar.png       bio: \"Pratik has worked as a data scientist and NLP engineer throughout his entire career. He’s the founding engineer and senior NLP scientist at Enterpret and is the founder of Maxpool, a community for data scientists to discuss practical ML problems. Enterpret provides automated customer feedback to help bridge the gap between product builders and consumers.\"     - name: Alex Lee       position: Engineering Manager, Goodnotes       image: /images/alex-lee.jpg       bio: \"Alex is currently an Engineering Manager at GoodNotes, where he works on GoodNotes Community, a platform that enables students to explore and exchange lecture notes and other study materials.\"     - name: Oded Kalev       position: Detection Group Leader, Perception Point       image: /images/oded-kalev.jpeg       bio: \"Oded is the ML and Data team lead at Perception Point cyber security where he is leveraging data to improve detection in Files, Emails, URLs and more. He's been interested in science and coding since he was eight years old.\"     - name: Chaymae Chaali       position: Data Scientist, Sealk       image: /images/chaymae-chaali.jpg       bio: \"Chaymae is a Data Scientist at Sealk, a french Fintech offering an artificial intelligence solution to facilitate deal sourcing for M&A and Private Equity.\"     - name: Alvise Sembenico       position: ML Engineer, Klue       image: /images/alvise-sembenico.png       bio: \"Alvise is a machine learning engineer at Klue, an AI-powered Competitive Enablement platform. He’s also the founder of Intrical AI, which ensures due diligence in the AI age. \"     - name: Özge Karakaya       position: ML Engineer, Klue       image: /images/ozge-karakaya.png       bio: \"Ozge is a machine learning engineer at Klue. He’s worked as an engineer in many industries at major companies including Sony, GE Aviation, ING Nederland, and more before joining Klue in July 2021. \"     - name: Nichita Diaconu       position: ML Engineer, Klue       image: /images/nichita-diaconu.png       bio: \"Nichita is a machine learning engineer at Klue. Prior to joining, he was a research engineer at Philips where he wrote his MSc thesis on self-attention in vision models, using data dependent filters that also take advantage of the rotational and translational symmetries of images.\"     - name: Björn Burscher       position: ML Engineering Manager, Klue       image: /images/bjorn-burscher.png       bio: \"Björn is an engineering manager in machine learning at Klue. Before that, he was a machine learning engineer at Klue and received a PhD in Information Science from the University of Amsterdam.\"     - name: Morgan Gallant       position: Co-Founder, Operand       image: /images/morgan-gallant.png       bio: Morgan is the co-founder of Operand, a startup working to \"make knowledge come alive\" by leveraging the latest and greatest in ML-powered search technologies. Specifically, he is interested in the idea of \"[human]-computer symbiosis\", where machines are constantly working on users' behalf to surface information proactively when it's needed.     - name: Bhairav Mehta       position: CEO & Co-Founder, Buzzle.ai       image: /images/bhairav-mehta.png       bio: Bhairav, CEO/Co-founder at Buzzle.ai, uses his background in deep learning research to build innovative NLP solutions that extract strategic insight for product and marketing teams, directly from the sales conversations they are already recording. Before starting Buzzle, Bhairav was a PhD student at MIT studying the theoretical foundations of deep and multi-task learning. His work as a Masters student at Mila (fka the Montreal Institute for Learning Algorithms) is heavily-cited in the fields of learning-based robotics and empirical applications of Stein’s Method.     - name: Adithya Ramanathan       position: Co-Founder, Buzzle.ai       image: /images/adithya-ramanathan.png       bio: Adithya is a co-founder at Buzzle.ai where they leverage cutting edge NLP techniques to turn recorded sales and customer success conversations into strategic insight for product and marketing teams. Prior to building Buzzle, Adithya was a Principal Machine Learning Engineer at Capital One’s Center for Machine Learning where he built and deployed several state of the art NLP solutions.     - name: Michael Staunton       position: Co-Founder, Buzzle.ai       image: /images/michael-staunton.png       bio: Michael, a co-founder at Buzzle.ai, leads development of the core Buzzle services where they apply NLP modeling to take a company's customer conversations and transform them into actionable insight for product and marketing teams. Previously, Michael worked at Capital One where he helped build their CCPA portal.     - name: Val Jones       position: CTO, StoryFile       image: /images/val-jones.png       bio: Val is the CTO of StoryFile, a cloud-based, no-code, automatic platform that would bring the power of conversational video into everyone’s hands. They were one of the first employees Raxium, is the author of over 20 academic publications and five patents, and comes to StoryFile with more than 20 years of experience helping develop cutting edge tech.     - name: Rafal Cycon       position: Chief Data Scientist, Form.com       image: /images/rafal-cycon.png       bio: Rafal is the Chief Data Scientist at Form.com, the digital assistant for the frontline. According to Kaggle, he is the highest rated data science competitor in Poland and one of the Top 30 in the world. Previously, he led ShelfWise and FORNAX and was a computer vision and ML engineer at a variety of companies, as well.     - name: Matt Sonnati       position: CEO & Co-Founder, Inokufu       image: /images/matt-sonnati.png       bio: Matt is the co-founder, CEO and CTO of Inokufu. He has a very successful educational background, holding a PhD in chemistry, a MBA, and awarded Innovator Under 35 France by the MIT Technology Review in 2012. Having two dyslexic brothers, he quickly realized that this educational and career path was not equally accessible to everyone. He then co-founded Inokufu, which offers a semantic search engine and a multi-view recommendation engine, with the aim of offering personalized learning on a large scale.     - name: Guillaume Lefebvre       position: Head of R&D, Inokufu       image: /images/guillaume-lefebvre.jpg       bio: Guillaume has always been passionate about artificial intelligence and more particularly NLP, which he discovered in computer science engineering school and deepened in his master's degree in data science. Today, he continues to work in the field of NLP as a PhD student and head of R&D at Inokufu. At Inokufu, Guillaume aims to revolutionize the field of education and professional training with a disruptive technology of semantic search and multi-view recommendation.     - name: Gabin Desserprit       position: Founder, OriginMatter.com       image: /images/gabin-desserprit.jpg       bio: As a lead product engineer at Origin Matter, Gabin is developing their reverse content search engine that takes images, videos, texts, and audio and reverses them into searchable content like movies, music, NFTs, products, stores and more.     - name: Gabriel Jimenez       position: Software Engineer & Architect       image: /images/gabriel-jimenez.jpg       bio: Gabe is an experienced Software Engineer and Architect, with a passion for learning and software development. With over 15 years of experience in large and small projects, in all kinds of organizations from large multi-nationals to startups, he has developed software in every paradigm from embedded to systems to web development.     - name: Hannah Kolbeck       position: Software Developer       image: /images/hannah-kolbeck.jpg       bio: Hannah is a software developer in Portland, Oregon who meddles in microcontroller hardware and laser cutting. She maintains several Twitter bots centered around alt text and the alt-text.org database. In her spare time she plays soccer, volunteers doing mutual aid, and hangs out with her cat and her wife. She is deeply committed to improving the lives of the people around her, and believes that technology can be a force for immense good in the world is freed from the requirement that it be profitable.     - name: Henry Mao       position: Entrepreneur & Machine Learning Researcher       image: /images/henry-mao.jpg       bio: Henry is a technology entrepreneur with expertise in deep learning, music generation and natural language processing. He co-founded a startup in 2016 called Altum with the mission to usher in a new era of human creativity through artificial intelligence. They are currently developing [Jenni](https://jenni.ai/), an AI content writer.   signUp:     text: |       Want to be recognized or nominate someone as a pioneer in vector search? Email [info@pinecone.io](mailto:info@pinecone.io) with your or their name, title, and a short bio!   perks:     title: Perks of <span>Pioneering</span>     text: \"As part of this diverse and invite-only group you’ll get:\"     list:       - Special recognition throughout the Pinecone community.       - Early access to new features, with opportunities to provide feedback and help shape the product roadmap.       - Shirts and other goodies from Pinecone.       - Opportunities to publish content in the <a href=\"https://www.pinecone.io/learn/\">learning center</a> and present at our events. # announcements: #   text: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nunc vulputate libero et velit interdum, ac aliquet odio mattis. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Curabitur tempus urna at turpis condimentum lobortis. #   ctaText: Learn More #   ctaLink: / --- ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e28a"
  },
  "title": "\"Pinecone Software End User License Agreement\"",
  "headline": "\"Pinecone Software End User License Agreement\"",
  "cta": "false",
  "intro": "\"\"",
  "content": "*Last Revised: May 26, 2022*  IMPORTANT – THIS PINECONE SOFTWARE END USER LICENSE AGREEMENT (EULA) IS A LEGALLY BINDING AGREEMENT BETWEEN YOU (“**YOU**”) AND PINECONE SYSTEMS, INC. (“**PINECONE**” OR “**WE**”). THIS EULA GOVERNS YOUR DOWNLOADING, INSTALLATION AND USE OF SOFTWARE THAT PINECONE MAKES AVAILABLE TO YOU FOR INSTALLATION AS A THIN-CLIENT TO ENABLE YOU TO ACCESS AND USE PINECONE’S PROPRIETARY PLATFORM THAT IT OFFERS AS A HOSTED SOLUTION (SUCH SOFTWARE, THE “**LICENSED SOFTWARE**”, AND THE PINECONE PLATFORM, THE “**PLATFORM**”).  BY DOWNLOADING, INSTALLING OR USING THE LICENSED SOFTWARE,  YOU AGREE TO BE BOUND BY THE TERMS OF THIS EULA AND THE [PINECONE PRIVACY POLICY](/product-privacy/). YOU (A) ACKNOWLEDGE THAT YOU HAVE READ, UNDERSTAND, AND AGREE TO BE BOUND BY THIS EULA; AND (B) REPRESENT THAT YOU HAVE THE AUTHORITY TO ENTER INTO THIS EULA AS AN INDIVIDUAL, OR ON BEHALF OF THE ENTITY LICENSING THE LICENSED SOFTWARE, AND TO BIND SUCH ENTITY TO THE TERMS HEREIN. IF YOU DO NOT AGREE TO ALL TERMS AND CONDITIONS IN THIS EULA, OR IF YOU DO NOT HAVE SUCH AUTHORITY, DISCONTINUE THE DOWNLOAD OF THE LICENSED SOFTWARE.  1. **Scope**. This EULA governs your use of the Licensed Software. If the parties have entered or subsequently enter into a written agreement that purports to govern or that includes provisions governing use of the Licensed Software (“**Other Agreement**”), and the Other Agreement contains any provision that conflicts with any term of this EULA, the conflicting provision in the Other Agreement will govern, but only to extent expressly specified in the Other Agreement. 2. **License**. Subject to the terms of this EULA, Pinecone grants to you a nontransferable, nonexclusive, royalty-free, fully paid, worldwide license (without the right to sublicense) to install and execute the Licensed Software, in executable object code format only, solely on computers that you own or control and for the sole purpose of obtaining access and use of the Platform as permitted under a separate agreement with Pinecone that permits your access and use of the Platform. 3. **Restrictions**. The rights granted hereunder are subject to the following restrictions: (a) you shall not license, sell, rent, lease, transfer, assign, distribute, host, outsource, disclose or otherwise commercially exploit the Licensed Software or make the Licensed Software available to any third party (other than the entity on whose behalf you enter into this EULA); (b) you shall not modify, make derivative works of, disassemble, reverse compile or reverse engineer any part of the Licensed Software; (c) you shall not access the Licensed Software in order to build a similar or competitive product or service; (d) except as expressly stated herein, no part of the Licensed Software may be copied, reproduced, distributed, republished, downloaded, displayed, posted or transmitted in any form or by any means, including but not limited to electronic, mechanical, photocopying, recording or other means; and (e) any future release, update, or other addition to the functionality of the Licensed Software provided by Pinecone (if any) shall be subject to the terms of this EULA unless Pinecone expressly states otherwise. You shall preserve all copyright and other proprietary rights notices on the Licensed Software and all copies thereof. 4. **Responsibility**. You are responsible and liable for all actions and failures to take required actions with respect to the Licensed Software by any party to whom you may provide access to or use of the Licensed Software, whether such access or use is permitted by or in violation of this EULA. 5. **Ownership**. The Licensed Software and all worldwide copyrights, trade secrets, and other intellectual property rights therein, are the exclusive property of Pinecone and its suppliers. All rights in and to the Licensed Software not expressly granted to you in this EULA are reserved by Pinecone and its suppliers. 6. **Third Party Software**. Certain items of software included as part of the Licensed Software are licensed from third parties and are subject to terms and conditions provided by such third parties (“**Third Party Software**”). The Third Party Software is not subject to the terms and conditions of Sections 1 and 2 of this EULA. Instead, each item of Third Party Software is licensed under the terms of the license that accompanies such Third Party Software. Nothing in this EULA limits your rights under, or grants you rights that supersede rights available in, the terms and conditions of any applicable license for the Third Party Software. 7. **Disclaimer of Warranties**. THE LICENSED SOFTWARE AND ANY THIRD PARTY SOFTWARE IS PROVIDED TO YOU ON AN “AS-IS” BASIS. EXCEPT AS EXPRESSLY STATED HEREIN, PINECONE PROVIDES NO TECHNICAL SUPPORT, WARRANTIES OR REMEDIES FOR THE LICENSED SOFTWARE UNDER THIS EULA. PINECONE AND ITS SUPPLIERS, EMPLOYEES, AGENTS, OFFICERS AND PARTNERS (THE “**PINECONE PARTIES**”) DISCLAIM ALL EXPRESS, IMPLIED OR STATUTORY WARRANTIES RELATING TO THE LICENSED SOFTWARE, INCLUDING BUT NOT LIMITED TO, MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, AND NON-INFRINGEMENT. PINECONE DOES NOT WARRANT THAT USE OF THE LICENSED SOFTWARE WILL BE UNINTERRUPTED, OR ERROR-FREE, THAT DEFECTS WILL BE CORRECTED, OR THAT THE LICENSED SOFTWARE IS FREE OF VIRUSES OR OTHER HARMFUL COMPONENTS. IF APPLICABLE LAW REQUIRES ANY WARRANTIES WITH RESPECT TO THE LICENSED SOFTWARE, ALL SUCH WARRANTIES ARE LIMITED IN DURATION TO NINETY (90) DAYS FROM THE DATE OF DOWNLOAD. THE WARRANTY DISCLAIMER SET FORTH ABOVE IS A FUNDAMENTAL ELEMENT OF THE BASIS OF THE AGREEMENT BETWEEN PINECONE AND YOU. PINECONE WOULD NOT BE ABLE TO PROVIDE THE LICENSED SOFTWARE ON AN ECONOMIC BASIS WITHOUT SUCH LIMITATIONS. THE WARRANTY DISCLAIMER INURES TO THE BENEFIT OF THE PINECONE PARTIES. 8. **Registration Information**. YOU ACKNOWLEDGE AND AGREE THAT WHEN YOU INSTALL AND REGISTER THE LICENSED SOFTWARE, THE LICENSED SOFTWARE TRANSMITS TO PINECONE CERTAIN INFORMATION THAT YOU PROVIDE DURING THE INSTALLATION OR REGISTRATION PROCESS, AS WELL AS COMPUTER OR DEVICE CONFIGURATION INFORMATION. YOU AGREE THAT PINECONE MAY COLLECT AND USE THIS DATA TO FACILITATE THE PROVISION OF SOFTWARE UPDATES, PRODUCT SUPPORT OR OTHER SERVICES TO YOU (IF ANY) RELATED TO THE LICENSED SOFTWARE. PINECONE MAY USE THIS INFORMATION PROVIDED SUCH USE IS IN ACCORDANCE WITH ITS PRIVACY POLICY. 9. **Limitation on Liability**. TO THE MAXIMUM EXTENT PERMITTED UNDER APPLICABLE LAW, IN NO EVENT SHALL ANY PINECONE PARTY BE LIABLE FOR ANY INDIRECT, EXEMPLARY, SPECIAL, CONSEQUENTIAL OR INCIDENTAL DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, LOST PROFITS, REVENUES OR DATA, OR COSTS OF REPLACEMENT GOODS OR SERVICES, ARISING IN ANY WAY OUT OF THIS EULA OR YOUR USE OF OR INABILITY TO USE THE LICENSED SOFTWARE, HOWEVER CAUSED, REGARDLESS OF THE THEORY OF LIABILITY (CONTRACT, TORT, OR OTHERWISE) AND EVEN IF IT HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES AND NOTWITHSTANDING THE FAILURE OF ANY LIMITED REMEDY OF ITS ESSENTIAL PURPOSE. SOME JURISDICTIONS DO NOT ALLOW THE LIMITATION OF LIABILITY FOR PERSONAL INJURY, OR OF INCIDENTAL OR CONSEQUENTIAL DAMAGES, SO THIS LIMITATION MAY NOT APPLY TO YOU. In no event shall Pinecone’s total liability to you for all damages (other than as may be required by applicable law in cases involving personal injury) exceed the amount of fifty dollars ($50.00). 10. **Term and Termination**. This EULA and the licenses granted hereunder are effective on the date you download the Licensed Software and shall continue unless this EULA is terminated by either party pursuant to this section. Pinecone may terminate this EULA immediately upon notice to you in the event that you materially breach any of the terms hereof. You may terminate this EULA at any time, with or without cause, by sending to Pinecone written notice indicating your intent to terminate your license (such notice to include your name and the subject “REMOVE”), either via email to info@pinecone.io or via mail or courier service to Pinecone Systems Inc, 548 Market St, PMB 19327, San Francisco, CA 94104-5401. Upon termination, the license granted hereunder shall terminate and you shall immediately destroy any copies of the Licensed Software in your possession, but the terms of Sections 2-15 will remain in effect. 11. **For U.S. Government End Users**. The Licensed Software is a “commercial item” as that term is defined at 48 C.F.R. 2.101 (OCT 1995), and more specifically is “commercial computer software” and “commercial computer software documentation,” as such terms are used in 48 C.F.R. 12.212 (SEPT 1995). Consistent with 48 C.F.R. 12.212 and 48 C.F.R. 227.7202-1 through 227.7202-4 (JUNE 1995), the Licensed Software is provided to U.S. Government End Users (a) only as a commercial end item and (b) with only those rights as are granted to all other customers pursuant to the terms and conditions herein. 12. **Export**. The Licensed Software and related technology are subject to U.S. export control laws and may be subject to export or import regulations in other countries. You agree to strictly comply with all such laws and regulations and acknowledge that you have the responsibility to obtain authorization to export, re-export, or import the Licensed Software and related technology, as may be required. You will indemnify and hold the Pinecone Parties harmless from any and all claims, losses, liabilities, damages, fines, penalties, costs and expenses (including attorney’s fees) arising from or relating to any breach by you of your obligations under this section. 13. **Governing Law and Venue**. This EULA will be governed by the laws of the California without regard to its principles of conflicts of law. Any action or proceeding arising from or relating to this EULA must be brought in a federal or state court located in San Francisco, California, and each party irrevocably submits to the jurisdiction and venue of any such court in any such action or proceeding. 14. **Miscellaneous**. Neither the rights nor the obligations arising under this EULA are assignable by you, and any such attempted assignment or transfer shall be void and without effect. The United Nations Convention on Contracts for the International Sale of Goods shall not apply to this EULA. Any notice to you may be provided by email. Any modifications of this Agreement must be in writing and agreed to by both parties. 15. **Questions or Additional information**. If you have questions regarding this EULA, or wish to obtain additional information about the Licensed Software license, please send an e-mail to info@pinecone.io.  ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e28c"
  },
  "title": "Talk to the <br><span>Vector Search</span> Experts",
  "headline": "\"Talk to an ML Infrastructure Expert\"",
  "cta": "false",
  "intro": "\"Contact us with questions or to see a personalized demo of Pinecone.\"",
  "description": "Send us your questions about Pinecone or details about your vector search needs. We’ll schedule a time to learn and share more with you.",
  "info": "|",
  "content": "",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e28e"
  },
  "title": "\"Privacy Policy\"",
  "headline": "\"Privacy Policy\"",
  "cta": "false",
  "intro": "\"\"",
  "content": "*Updated: December 3, 2020*  This Website Privacy Policy (“Privacy Policy”) describes the privacy practices of Pinecone Systems, Inc. and that of its subsidiaries and affiliates (collectively, “Pinecone,” “we,” “us,” or “our”). This Privacy Policy explains how we collect, use, disclose, secure and otherwise process personal information from individuals in connection with our website and any other website that we own or control and which posts or links to this Privacy Policy (collectively, the “Websites”), and the rights and choices available to individuals with respect to their information. Pinecone may provide additional or supplemental privacy policies to individuals for specific products or services that we offer at the time we collect personal information. These supplemental privacy policies will govern how we may process the personal information in the context of the specific product or service.  We have designed the Websites for businesses and they are not intended for personal or household use. Accordingly, we treat all personal information covered by this Privacy Policy, including information about any visitors to our Websites, as pertaining to individuals acting in their capacity as business representatives (including as representatives our enterprise customers), rather than in their personal capacity.  This Privacy Policy does not govern how we may process personal information on behalf of our enterprise customers as part of the Pinecone Services. Please review the [Product Privacy Statement](/product-privacy/) to understand how we collect, use and otherwise process customer personal information in connection with our products and services (collectively, the “Hosted Services”).  We provide important information for individuals located in the European Union, European Economic Area, Switzerland and United Kingdom (collectively, “Europe” or “European”) [below](#notice-to-european-users).  ## Information Collected  ### _Information Collected Directly from You._  We collect personal information directly from you through your use of our Websites. This information may include:  - **Business contact data**, such as your name, email address, and phone number. - **Employment information**, such as your employer’s name and job title. - **Location information**, such as when you authorize our Websites to access your general location information such as city, state or geographic area. - **Feedback or correspondence**, such as information you provide when you contact us with questions, feedback, or otherwise correspond with us online. - **Usage information**, such as information about how you use the Websites and interact with us, including information associated with any information you provide when you use any features of the Websites. - **Marketing information**, such as your preferences for receiving communications from us and details about how you engage with our communications. - **Other information** that we may collect that is not specifically listed here but that we will use in accordance with this Privacy Policy or as otherwise disclosed at the time of collection.  ### _Information we obtain from social media platforms._  We may maintain pages about us on social media platforms such as Facebook, LinkedIn, Twitter and other third-party platforms. When you visit or interact with our pages on those platforms, the platform provider’s privacy policy will apply to your interactions and their collection, use and processing of your personal information. You or the platforms may provide us with information through the platforms, and we will treat such information in accordance with this Privacy Policy.  ### _Information that we obtain from other third parties._  We may receive personal information about you from third-party sources. For example, a business partner may share your contact information with us if you have expressed interest in learning specially about our services. We may obtain your personal information from other third parties such as marketing and advertising partners, publicly-available sources and data providers.  ### _Referrals._  Website visitors may have the opportunity to refer contacts to us. You may only submit a referral if you have permission to provide the referral’s contact information to us so that we may contact them.  ### _Automatically Collected Data._  We and our service providers as well as our advertising and business partners may use cookies, beacons, pixel tags and other tracking technologies to automatically log information about you, your computer or mobile device and your activity over time on the Websites, including:  - **Device Data**, such as your computer or mobile device operating system type and version number, manufacturer and model, browser type, screen resolution, IP address, device identifier (such as the Google Advertising ID or Apple ID for Advertising), the website you visited before browsing our Websites and general location information such as city, state or geographic area. - **Online Activity Data**, such as browsing history, search history, whether you clicked on or opened one of our emails, which of our pages or screens you viewed, how long you spent on a page or screen, navigation paths between pages or screens, information about your activity on a page or screen, access times and duration of access.  Please visit the “[Options](#options)” section of this Privacy Policy for information on how to disable or opt-out of certain cookies and similar technologies.  ## Use of Information  We may use your personal information for the following purposes or as otherwise described in this Privacy Policy or at the time of collection:  **<span id=\"to-operate-the-websites\">To operate the Websites</span>**. We may use personal information to operate the Websites and to provide related services, including to:  - provide, operate and improve the Websites; - provide information about our services and product offerings; - enable security features of the Websites; - understand your needs and interests, and personalize your experience with the Websites and our communications; - respond to your requests, questions and feedback; - protect the security of the Websites and our systems; and - provide you with customer support when needed.  **<span id=\"for-research-and-development\">For research and development</span>**. We may analyze use of the Websites to evaluate and improve the Websites, including by studying user demographics and use of the Websites. We may use Google Analytics for this purpose.  **Advertising**. We may also work with third-party advertising partners who use cookies (such as the DoubleClick cookie) and similar technologies to deliver targeted advertising that is displayed on unaffiliated websites, to measure the effectiveness of advertising on behalf of our advertising partners, and to identify the audience most likely to respond to an advertisement. These advertisements are delivered by our advertising partners and may be targeted based on your use of the Websites or your activity elsewhere online.  **<span id=\"to-comply-with-law\">To comply with law</span>**. We may use your personal information as we believe necessary or appropriate to comply with applicable laws, lawful requests, and legal process, such as to respond to subpoenas or requests from government authorities.  **<span id=\"for-compliance-fraud-prevention-and-safety\">For compliance, fraud prevention and safety</span>**. We may use your personal information and disclose it to law enforcement, government authorities, and private parties as we believe necessary or appropriate to: (a) protect our, your or others’ rights, privacy, safety or property (including by making and defending legal claims); (b) enforce the terms and conditions that govern the Websites; and (c) protect, investigate and deter against fraudulent, harmful, unauthorized, unethical or illegal activity.  **<span id=\"with-your-consent\">With your consent</span>**. In some cases, we may specifically ask for your consent to collect, use or share your personal information, such as when required by law.  **To create anonymous, aggregated or de-identified data**. We may create anonymous, aggregated or de-identified data from your personal information and other individuals whose personal information we collect, by removing the information that makes the data personally identifiable to you. We may use and share such anonymous, aggregated or de-identified data for any purpose we deem appropriate, such as to maintain and improve the Website.  **<span id=\"to-send-you-marketing-and-promotional-communications\">To send you marketing and promotional communications</span>**. We may send you Pinecone-related marketing and promotional communications about the products or services that we offer. You will have the ability to opt-out of our marketing and promotional communications as described in the “Options” section below.  ## Sharing of Information   We may share personal information with the following categories of recipients:  **Service providers**. We may share your personal information with third-party companies and individuals that provide services on our behalf or help us operate the Websites (such as customer support, hosting, analytics, email delivery, marketing and database management services).  **Advertising partners**. We may enable advertising partners to automatically collect information directly from our Websites for targeted marketing purposes.  **Professional advisors**. We may share your personal information with professional advisors, such as lawyers, auditors, bankers and insurers, where necessary in the course of the professional services that they render to us.  **Authorities and others**. We may share your personal information with law enforcement, government authorities, and private parties, as we believe in good faith to be necessary or appropriate for the compliance, fraud prevention and safety purposes described above.  **Business transferees**. We may share your personal information with relevant participants in business transactions (or potential transactions) involving a corporate divestiture, merger, consolidation, acquisition, reorganization, sale or other disposition of all or any portion of the business or assets of, or equity interests in, (including, in connection with a bankruptcy or similar proceedings).  ## Options  **Access or update information**. If your personal information changes, or if you no longer desire to have a relationship with us, you may correct or update by emailing privacy@pinecone.io. We will respond to your request within a reasonable timeframe.  **Cookies & browser web storage**. Most browsers let you remove or reject cookies. To do this, follow the instructions in your browser settings. To prevent the use of Google Analytics relating to your use of our Websites, you can download and install the browser plug-in available here. Many browsers accept cookies by default until you change your settings. Please note that if you set your browser to disable cookies, the Websites may not work properly. Similarly, your browser settings may allow you to clear your browser web storage.  **Targeted online advertising**. Some of the business partners that collect information about users’ activities on or through the Websites may be members of organizations or programs that provide choices to individuals regarding the use of their browsing behavior for purposes of targeted advertising. Users may opt out of receiving targeted advertising by:  - **Blocking cookies in your browser**. Most browsers let you remove or reject cookies, including cookies used for interest-based advertising. To do this, follow the instructions in your browser settings. Many browsers accept cookies by default until you change your settings. For more information about cookies, including how to see what cookies have been set on your device and how to manage and delete them, visit www.allaboutcookies.org. - **Blocking advertising ID use in your mobile settings**. Your mobile device settings may provide functionality to limit use of the advertising ID associated with your mobile device for interest-based advertising purposes. - **Using privacy plug-ins or browsers**. You can block our websites from setting cookies used for interest-based ads by using a browser with privacy features, like [Brave](https://brave.com/), or installing browser plugins like [Privacy Badger](https://privacybadger.org/), [Ghostery](https://www.ghostery.com/) or [uBlock Origin](https://ublock.org/), and configuring them to block third party cookies/trackers. - **Platform opt-outs**. The following advertising partners offer opt-out features that let you opt-out of use of your information for interest-based advertising:      - Google: https://adssettings.google.com  **Do Not Track**. Some Internet browsers may be configured to send “Do Not Track” signals to the online services that you visit. We currently do not respond to “Do Not Track” or similar signals. To find out more about “Do Not Track,” please visit http://www.allaboutdnt.com.  **Opt out of marketing communications**. If at any time you wish to opt-out of our marketing and promotional emails, you may click the “unsubscribe” link in the email or otherwise contact us at privacy@pinecone.io. It may take up to 10 business days before you stop receiving promotional emails. This opt-out does not apply to operational communications, for example, confirmation emails and you may continue to receive service-related and other non-marketing emails.  ## Information about Children  Our Websites are not directed to children under 16. If a parent or guardian becomes aware that his or her child has provided us with information without their consent, he or she should contact us at privacy@pinecone.io. We will delete such information from our files as soon as reasonably practicable.  ## Security  We maintain various physical, electronic and procedural safeguards designed to protect the personal information we collect. However, security risk is inherent in all internet and information technologies and we cannot guarantee the security of personal information.  ## Third-Party Websites  The Websites may contain links to websites and other online services operated by third parties. These links are not an endorsement of, or representation that we are affiliated with, any third-party and we cannot control, and take no responsibility for, the content or privacy practices of such third-party websites or online services operated by third parties. We encourage you to review the privacy policy and any other applicable terms for such third-party websites or online services operated by third parties.  ## International Data Transfers  We are headquartered in the United States and may have service providers in other countries, and your personal information may be transferred to the United States or other locations outside of your state, province, or country where privacy laws may not be as protective as those in your state, province, or country.  European users should read the important information provided in the “[Notice to European Users](#notice-to-european-users)” section to learn more about transfer of personal information outside of Europe.  ## Privacy Policy Updates  We reserve the right to modify this Privacy Policy at any time. If we make material changes to this Privacy Policy, we will notify you by updating the date of this Privacy Policy and posting it on the Websites. If required by law, we will also provide notification of changes in another way that we believe is reasonably likely to reach you, such as via email or another manner through the Websites.  Any modifications to this Privacy Policy will be effective upon our posting the modified version (or as otherwise indicated at the time of posting). In all cases, your continued use of the Websites after the effective date of any modified Privacy Policy indicates your acceptance of the modified Privacy Policy.  ## Contact  If you have any further questions concerning this Privacy Policy, or would like to request to correct, update, or delete such information, please contact privacy@pinecone.io or mail us at:  Pinecone Systems, Inc<br> 548 Market St<br> PMB 19327<br> San Francisco, CA 94104-5401  ## Notice to European Users  The information provided in this “Notice to European Users” section applies only to individuals in Europe.  **Personal information**. References to “personal information” in this Privacy Policy are equivalent to “personal data” governed by European data protection laws.  **Controller and EU Representative**. Pinecone Systems, Inc. is the controller of your personal information covered by this Privacy Policy for purposes of European data protection laws.  **Legal bases for processing**. We use your personal information only as permitted by law. Our legal bases for processing the personal information described in this Privacy Policy are described in the table below.  <table class=\"table table-bordered\">   <tbody>     <tr>       <td style=\"width:50%\"><b>Processing purpose</b> (click link for details)<br>Details regarding each processing purpose listed below are provided in the section above titled “Use of Personal Information”.</td>       <td style=\"width:50%\"><b>Legal basis</b></td>     </tr>     <tr>       <td>         <ul class=\"my-0\">           <li><a href=\"#to-operate-the-websites\">To operate the Websites</a></li>         </ul>       </td>       <td>Processing is necessary to perform the contract governing our provision of services to you or to take steps that you request prior to entering an agreement for our services. If we have not entered into a contract with you, we process your personal information based on our legitimate interest in providing our services (including our Website) to you.</td>     </tr>     <tr>       <td>         <ul class=\"my-0\">           <li><a href=\"#for-research-and-development\">For research and development</a></li>           <li><a href=\"#to-send-you-marketing-and-promotional-communications\">To send you marketing and promotional communications</a></li>           <li><a href=\"#for-compliance-fraud-prevention-and-safety\">For compliance, fraud prevention and safety</a></li>         </ul>       </td>       <td>These activities constitute our legitimate interests. We do not use your personal information for these activities where our interests are overridden by the impact on you (unless we have your consent or are otherwise required or permitted to by law).</td>     </tr>     <tr>       <td>         <ul class=\"my-0\">           <li><a href=\"#to-comply-with-law\">To comply with law</a></li>         </ul>       </td>       <td>Processing is necessary to comply with our legal obligations.</td>     </tr>     <tr>       <td>         <ul class=\"my-0\">           <li><a href=\"#with-your-consent\">With your consent</a></li>         </ul>       </td>       <td>Processing is based on your consent. Where we rely on your consent you have the right to withdraw it any time in the manner indicated when you consented.</td>     </tr>   </tbody> </table>  **Use for new purposes**. We may use your personal information for reasons not described in this Privacy Policy where permitted by law and the reason is compatible with the purpose for which we collected it. If we need to use your personal information for an unrelated purpose, we will notify you and explain the applicable legal basis.  ## Sensitive personal information  We ask that you not provide us with any sensitive personal information (e.g., information related to racial or ethnic origin, political opinions, religion or other beliefs, health, biometrics or genetic characteristics, criminal background or trade union membership) on or through the Websites, or otherwise to us. If you provide us with any sensitive personal information when you use the Websites, you must consent to our processing and use of such sensitive personal information in accordance with this Privacy Policy. If you do not consent to our processing and use of such sensitive personal information, you must not submit such sensitive personal information through our Websites.  ## Retention  We retain personal information for as long as necessary to fulfill the purposes for which we collected it, including for the purposes of satisfying any legal, accounting, or reporting requirements, to establish or defend legal claims, or for fraud prevention purposes. To determine the appropriate retention period for personal information, we consider the amount, nature, and sensitivity of the personal information, the potential risk of harm from unauthorized use or disclosure of your personal information, the purposes for which we process your personal information and whether we can achieve those purposes through other means, and the applicable legal requirements. When we no longer require the personal information we have collected about you, we will either delete or anonymize it or, if this is not possible (for example, because your personal information has been stored in backup archives), then we will securely store your personal information and isolate it from any further processing until deletion is possible. If we anonymize your personal information (so that it can no longer be associated with you), we may use this information indefinitely without further notice to you.  ## Your rights  European data protection laws give you certain rights regarding your personal information. If you are located within Europe, you may ask us to take the following actions in relation to your personal information that we hold:  - **Access**. Provide you with information about our processing of your personal information and give you access to your personal information. - **Correct**. Update or correct inaccuracies in your personal information. - **Delete**. Delete your personal information. - **Transfer**. Transfer a machine-readable copy of your personal information to you or a third party of your choice. - **Restrict**. Restrict the processing of your personal information. - **Object**. Object to our reliance on our legitimate interests as the basis of our processing of your personal information that impacts your rights.  You may submit these requests by email to privacy@pinecone.io or our postal address provided above. We may request specific information from you to help us confirm your identity and process your request. Applicable law may require or permit us to decline your request. If we decline your request, we will tell you why, subject to legal restrictions. If you would like to submit a complaint about our use of your personal information or our response to your requests regarding your personal information, you may contact us or submit a complaint to the data protection regulator in your jurisdiction. You can find your data protection regulator here.  ## Cross-Border Data Transfer  If we transfer your personal information out of Europe to a country not deemed by the European Commission to provide an adequate level of personal information protection, the transfer will be performed:  - Pursuant to the recipient’s compliance with standard contractual clauses, or Binding Corporate Rules; - Pursuant to the consent of the individual to whom the personal information pertains; or - As otherwise permitted by applicable European requirements.  You may contact us if you want further information on the specific mechanism used by us when transferring your personal information out of Europe. ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e290"
  },
  "title": "\"Pinecone Services Agreement\"",
  "headline": "\"Pinecone Services Agreement\"",
  "cta": "false",
  "intro": "\"\"",
  "content": "*Last updated: May 26, 2022*  Please read this Pinecone Services Agreement (**“Agreement”**) before clicking the “I accept” button, and/or using the Pinecone Systems, Inc. (“**Pinecone**”) Hosted Services or the underlying Platform (as defined below).  By clicking the “I Accept” button, and by using the Hosted Services in any way, you and the entity that you represent (**“Customer”** or **“you”**) are unconditionally consenting to be bound by and becoming a party to this Agreement with Pinecone and you represent and warrant that you have the authority to bind such entity to this Agreement. If you do not unconditionally agree to all of the terms of this Agreement, use of the Hosted Services is strictly prohibited. If Customer has executed, or subsequently executes, a separate agreement with Pinecone with respect to use of the Hosted Services (“**Other Agreement**”), then the terms and conditions of such Other agreement shall govern and control your use of the Hosted Services.  PLEASE NOTE THAT THIS AGREEMENT IS SUBJECT TO CHANGE BY PINECONE IN ITS SOLE DISCRETION AT ANY TIME.  When changes are made, Pinecone will make a new copy of this Agreement available on the Platform.  We will also update the “Last Updated” date at the top of the Agreement.  If we make material changes to this Agreement, we may (and, where required by law, will) also provide notification of changes in another way that we believe is reasonably likely to reach you, such as via e-mail or another manner through the Platform (which may include posting an announcement on our Platform). Pinecone may require you to provide consent to the updated Agreement in a specified manner before further use of the Hosted Services is permitted.  If you do not agree to any of the changes after receiving a notice of such changes, you shall stop accessing the Platform and using Hosted Services. Otherwise, your continued use of the Hosted Services (or access to the Platform) constitutes your acceptance of such changes.    1. **PLATFORM AND HOSTED SERVICES**.        1. <span style=\"text-decoration:underline;\">Access</span>.  Pinecone’s proprietary solution is a platform that leverages machine learning to improve serving capacity, including hosting, real time scoring, operations, scaling, reporting, monitoring, and testing to solve problems related to “many-to-many relationships,” such as search, retrieval, ranking, matching, and deduplication (the “**Platform**”). Customer wishes to access and utilize the Platform, and Pinecone desires to make the Platform available to Customer, subject to the following terms and conditions (the provisioning of the Platform, the “**Hosted Services**”). Pinecone hereby grants Customer, during the term of this Agreement, a non-exclusive, non-transferable, non-sublicensable right and license to access and use the Hosted Services, including by installing any downloadable components of the Platform made available by Pinecone and using and accessing any other materials provided to Customer by Pinecone in connection with the Hosted Services (“**Materials**”), solely for Customer’s internal business purposes. Customer agrees to provide only truthful and accurate information in connection with obtaining usernames and passwords (including API access keys) (collectively, “**User IDs**”) to access the Hosted Services. Customer is responsible for all acts and omissions of any users of Customer’s account or User ID and will undertake reasonable efforts to make all such users aware of the provisions of this Agreement as applicable to such users’ use of the Hosted Services, and will cause such users to comply with such provisions. Any act or omission by any such third party which, if undertaken by Customer, would constitute a breach of this Agreement, will be deemed a breach of this Agreement by Customer.  Customer is responsible for maintaining the confidentiality of the User IDs, and is solely responsible for all activities that occur thereunder.  Customer agrees to notify Pinecone promptly of any actual or suspected unauthorized use of its account or User ID, or any other breach or suspected breach of this Agreement. Pinecone reserves the right to terminate any User ID that Pinecone reasonably determines may have been used by an unauthorized third party.       2. <span style=\"text-decoration:underline;\">Free or Trial Subscriptions</span>. From time to time and in its sole discretion, Pinecone may offer limited free access to the Hosted Services (for purposes of this Section, “**Trial Services**”) so that you can test whether the Hosted Services meet your needs.  Because Trial Services are limited and are provided free of charge, to the extent permitted by law, (i) we make them available to you solely “AS IS” without any warranties of any kind (and we are under no obligation to provide you with support for Trial Services); (ii) we may discontinue the Trial Services or your ability to use them at any time, with or without notice and without any further obligations to you; (iii) you agree that our indemnification obligations under Section 12 do not extend to any claims related to your use or inability to use the Trial Services; and (iv) you agree that our support obligations under Section 5 do not apply to your use of the Trial Services. Except as expressly stated in this Section 1.2, the Trial Services shall be subject in all respects to the terms and conditions of this Agreement. Pinecone may allow you to continue using Hosted Services provided as Trial Services on a paid basis, but your continued use is subject to the payment of the applicable Fees in accordance with Section 6.       3. <span style=\"text-decoration:underline;\">Open Source</span>. Certain items of software that may be provided to Customer as part of the Platform  are subject to “open source” or “free software” licenses (**“Open Source Software”**).  Some of the Open Source Software is owned by third parties.  The Open Source Software is not subject to the terms and conditions of Sections 1.1 or 12.  Instead, each item of Open Source Software is licensed under the terms of the end-user license that accompanies such Open Source Software. Nothing in this Agreement limits Customer’s rights under, or grants Customer rights that supersede, the terms and conditions of any applicable end user license for the Open Source Software. If required by any license for particular Open Source Software, Pinecone makes such Open Source Software, and Pinecone’s modifications to that Open Source Software, available by written request at the notice address specified herein.       4. <span style=\"text-decoration:underline;\">Changes</span>.  From time to time, Pinecone reserves the right to release updates to or upgrades of the Hosted Services, including new versions of the Hosted Services, and to otherwise change or discontinue any aspect or feature of the Hosted Services. Changes may not be consistent across all platforms and devices. Pinecone will use commercially reasonable efforts to notify Customer (including posting through the Hosted Services) of changes to the Hosted Services that Pinecone believes will materially affect use of the Hosted Services.   2. **RESTRICTIONS ON USE.** Customer shall not itself, or through any parent, subsidiary, affiliate, agent or other third party: (i) license, sublicense, sell, resell, transfer, assign, distribute or make available, in whole or in part, the Hosted Services or the Materials to any third party; (ii) copy, translate, decompile, disassemble, reverse-engineer or otherwise modify or make derivative works based upon the Hosted Services or the Materials; (iii) build a product using similar ideas, features, functions or graphics of the Hosted Services or Materials or otherwise engage in competitive benchmarking; (iv) disclose the results of any benchmark test of the Hosted Services or the Materials to any third party without Pinecone’s prior written approval; (v) use the Hosted Services to (A) send or store infringing, threatening, harassing, defamatory, libelous, obscene, pornographic, indecent or otherwise unlawful or tortious materials, including materials harmful to children or violating third-party privacy rights, (B) send or store materials containing software viruses, worms, Trojan horses or other harmful computer code, files, scripts, agents or programs, or (C) engage in any of Customer’s time-critical, or mission-critical functions; (vi) interfere with or disrupt the integrity or performance of the Hosted Services or the data contained therein, or attempt to probe, scan or test vulnerability of the Hosted Services without prior authorization of Pinecone; or (vii) attempt to gain unauthorized access to the Hosted Services or its related systems or networks, including the Platform itself. <br> <br> 3. **DATA**.       1. <span style=\"text-decoration:underline;\">License to Customer Data</span>. “**Customer Data**” means any data, information, or materials that Customer discloses or submits to Pinecone in the course of using the Hosted Services.  Customer hereby grants Pinecone and its suppliers a non-exclusive, royalty-free license to access, use, reproduce, modify and display the Customer Data for the purposes of (i) providing the Hosted Services, (ii) exercising its rights and obligations under this Agreement; (iii) generating Aggregated and Anonymous Data (as defined below); and (iv) complying with its obligations under law.  All rights in and to the Customer Data not expressly granted herein are retained by Customer. Without limiting the foregoing, Customer will be solely responsible for providing all appropriate notices to third parties (including all employees, agents, and independent contractors (collectively, “**Personnel**”)) and obtaining from third parties (including Personnel) all necessary consents and rights for Pinecone to use the Customer Data submitted by or on behalf of Customer for the purposes set forth in this Agreement, including all consents required in accordance with all applicable privacy laws. Customer shall immediately notify, and address with, Pinecone any complaints or claims by Personnel with respect to the sharing of the Customer Data involving such Personnel.       2. <span style=\"text-decoration:underline;\">Performance Data</span>. “**Performance Data**” means any analytics or similar data collected, generated or processed by Pinecone based on Customer’s access to and use of the Platform or Hosted Services (“**Performance Data**”).  Performance Data will be owned by Pinecone, and Pinecone may collect and use such Performance Data for any lawful purpose, _provided_ Pinecone will only disclose Performance Data to third parties, including its subcontractors, for the purposes of facilitating the Hosted Services, for internal purposes, including to improve its products and services, to perform its other obligations and exercise its rights under this Agreement, or as otherwise required by law.         7. <span style=\"text-decoration:underline;\">Aggregated and Anonymous Data</span>. Notwithstanding anything to the contrary herein, Customer agrees that Pinecone may obtain and use Customer Data and Performance Data to create aggregated, anonymized or deidentified data or information of similar form that does not permit the identification of Customer or any individual or entity (the “**Aggregated and Anonymous Data**”).  Customer further agrees that Pinecone shall own such Aggregated and Anonymous Data and may retain, use and disclose such data for any lawful business purpose, including to improve its products and services.       8. <span style=\"text-decoration:underline;\">Customer Responsibility</span>. Customer will be responsible for providing all Customer Data to Pinecone and will provide such Customer Data in a format consistent with the requirements set forth in the documentation (or as otherwise specified by Pinecone). Errors in loading Customer Data into the Platform may cause Customer Data to be rejected by the Platform and Pinecone will have no responsibility for any related impact on Customer’s ability to access or use the Platform.    4. **PROPRIETARY RIGHTS.**       1. <span style=\"text-decoration:underline;\">Ownership</span>. Pinecone and its suppliers own all right, title and interest in and to the Hosted Services (including the Platform, but excluding any Customer Data hosted therein), Performance Data, Aggregated and Anonymous Data, and any Materials, including but not limited to concepts, specifications, integration scenarios and examples of code, and all intellectual property rights in each of the foregoing.  All rights in and to the Hosted Services, including the Platform, and Materials not expressly granted herein are retained by Pinecone.       10. <span style=\"text-decoration:underline;\">Feedback</span>. Notwithstanding anything to the contrary herein, Pinecone may freely use and incorporate into Pinecone’s products and services any suggestions, enhancement requests, recommendations, corrections, or other feedback provided by Customer or by any users of the Hosted Services, the Platform, and the Materials (“**Feedback**”). Customer acknowledges and agrees that all Feedback and all intellectual property rights therein are the exclusive property of Pinecone, and hereby assigns to Pinecone, all right, title and interest to any and all Feedback.       11. <span style=\"text-decoration:underline;\">Publicity</span>. Pinecone may use Customer’s name and logo (“**Customer** **Marks**”) in its Customer list (including on Pinecone’s website, social media and in sales and marketing materials) in the same manner in which it uses the names of its other customers. Pinecone shall use Customer Marks in accordance with Customer’s applicable branding guidelines and Pinecone may not use Customer’s name in any other way without Customer’s prior written consent (with email consent deemed sufficient).   5. **SUPPORT SERVICES.** Subject to the terms and conditions of this Agreement, Pinecone will exercise commercially reasonable efforts to provide support for the use of the Platform and Hosted Services to Customer.<br><br>  6. **Fees, Payment, and Taxes.**      1. <span style=\"text-decoration:underline;\">Fees</span>. The fees for access to and use of the Hosted Services (“**Fees**”) are based on Platform usage and, unless otherwise specified herein, are charged at the rates set forth on [the Pinecone Pricing webpage](https://www.pinecone.io/pricing/). Pinecone reserves the right to change the Fees or its pricing model at any time during the term.  Any such change to Fees, rates or pricing shall go into effect no earlier than thirty (30) days after the change is posted to the Pinecone Pricing webpage. Unless otherwise expressly specified, the Fees are calculated at the end of each month based on Customer’s usage of the Hosted Services.        13. <span style=\"text-decoration:underline;\">Invoicing and Payment</span>. All Fees are quoted in United States Dollars and, except as set forth otherwise in this Agreement, are non-refundable. Pinecone will invoice Customer monthly for the Fees.  Fees are payable thirty (30) days from the date of invoice and will be deemed overdue if they remain unpaid thereafter.         14. <span style=\"text-decoration:underline;\">Late Payments</span>. Payments by Customer that are past due will be subject to interest at the rate of one and one-half percent (1½%) per month (or, if less, the maximum allowed by applicable law) on that overdue balance. Customer will be responsible for any costs resulting from collection by Pinecone of any such overdue balance, including, without limitation, reasonable attorneys’ fees and court costs.  Pinecone reserves the right (in addition to any other rights or remedies Pinecone may have) to suspend Customer’s access to the Platform and the Hosted Services if any Fees are more than fifteen (15) days overdue until such amounts are paid in full.       15. <span style=\"text-decoration:underline;\">Taxes</span>. The Fees do not include taxes, duties or charges of any kind.  If Pinecone is required to pay or collect any local, value added, goods and services taxes or any other similar taxes or duties arising out of or related to this Agreement (not including taxes based on Pinecone’s income), then such taxes and/or duties shall be billed to and paid by Customer.        16. <span style=\"text-decoration:underline;\">Withholding Payments</span>.  If any applicable law requires Customer to withhold amounts from any payments to Pinecone hereunder, then Customer will perform such obligations consistent with the provisions of this section.  Customer will effect such withholding, remit such amounts to the appropriate taxing authorities and promptly furnish Pinecone with tax receipts evidencing the payments of such amounts. The sum payable by Customer upon which the deduction or withholding is based will be increased to the extent necessary to ensure that, after such deduction or withholding, Pinecone receives and retains, free from liability for such deduction or withholding, a net amount equal to the amount Pinecone would have received and retained in the absence of such required deduction or withholding.    7. **CONFIDENTIALITY.**        1. “**Confidential Information**” means any proprietary, confidential and/or trade secret information concerning or relating to the property, business and affairs of the party disclosing such information (the “**Disclosing Party**”) to the other party (the “**Receiving Party**”) under this Agreement, or any other information that the Receiving Party would reasonably understand to be confidential given the nature of the information or the circumstances surrounding disclosure. For the avoidance of doubt, Customer’s Confidential Information includes Customer Data, other than **Personal Data** (as defined in the Customer Data Protection Addendum). Such Personal Data is governed by Section 8 of this Agreement, and the Customer Data Protection Addendum incorporated herein by reference. Pinecone’s Confidential Information includes Performance Data, the proprietary and non-public portions of the Hosted Services, and the Materials provided in connection with this Agreement. Confidential Information shall not include information that a Receiving Party can demonstrate by reasonably sufficient evidence (i) was known to the Receiving Party before receipt thereof under this Agreement, (ii) is disclosed to the Receiving Party by a third party who has a right to make such disclosure without any obligation of confidentiality to the Disclosing Party, (iii) is or becomes generally known to the public or in the trade without violation of either this Agreement by the Receiving Party or any confidentiality obligation owed to the Disclosing Party by any third party, (iv) is furnished by the Disclosing Party to a third party without restriction on subsequent disclosure, or (v) is independently developed by the Receiving Party or its employees or subcontractors without reliance on such Confidential Information.      18. The Receiving Party shall (i) not disclose Confidential Information to third parties (except to its directors, employees, agents or subcontractors to the extent such disclosure is necessary for the performance of this Agreement and who have agreed to restrictions similar to those set forth in this Section or except as may be required by law), (ii) not use Confidential Information except for the purposes contemplated by this Agreement and (iii) use at least the same degree of care to safeguard Confidential Information that it uses to protect its own confidential and proprietary information, but in no event less than a reasonable degree of care under the circumstances.       19. Upon expiration or termination of this Agreement, or upon request of the Disclosing Party, the Receiving Party shall return to the Disclosing Party or destroy all Confidential Information in the possession of the Receiving Party. Each party acknowledges that it will not obtain any right, title or interest in or to the Confidential Information of the other Party as a result of disclosure under this Agreement.       20. The parties acknowledge that the Confidential Information is unique and valuable, and that the Disclosing Party will have no adequate remedy at law if the Receiving Party does not comply with its obligations under this Agreement.  Therefore, the Disclosing Party shall have the right, in addition to any other rights it may have, to seek in any court of competent jurisdiction temporary, preliminary and permanent injunctive relief to restrain any breach, threatened breach, or otherwise to specifically enforce any obligations of the Receiving Party if the Receiving Party fails to perform any of its obligations under this Agreement.    8. **DATA PRIVACY; SECURITY**.       1. <span style=\"text-decoration:underline;\">Data Privacy</span>. Each party shall comply with their respective obligations under the Customer Data Processing Addendum located at https://www.pinecone.io/dpa/ (or such successor URL as may be designated by Pinecone) (“**DPA**”), which is incorporated herein by this reference. By each party’s acceptance and agreement to the terms and conditions of this Agreement, each party agrees to the terms of the DPA, including the Standard Contractual Clauses as “Data exporter” in the case of Customer, and as “Data importer” in the case of Pinecone.      22. <span style=\"text-decoration:underline;\">Security</span>. Pinecone will use reasonable technical and organizational measures designed to prevent unauthorized access, use, alteration, or disclosure of, Customer Data. However, Pinecone shall have no responsibility for errors in transmissions or any other causes beyond Pinecone’s reasonable control.      23. <span style=\"text-decoration:underline;\">Customer Responsibility for Data and Security</span>. Customer will have access to the Customer Data and will be responsible for all changes to and/or deletions of Customer Data and the security of all passwords and other usernames and passwords required in order the access the Platform and the Services. Upon request to Customer’s account manager, Pinecone may facilitate for Customer the ability to export Customer Data from the Platform. Customer will have the sole responsibility for the accuracy, quality, integrity, legality, reliability, and appropriateness of all Customer Data.  Pinecone is not obligated to back up any Customer Data; the Customer is solely responsible for creating backup copies of any Customer Data at Customer’s sole cost and expense.   9. **WARRANTIES.**      1. <span style=\"text-decoration:underline;\">Pinecone Limited Warranty</span>.  Pinecone warrants to you that during the term of this Agreement the Platform will perform materially in accordance with the functionality described in the documentation that Pinecone makes available for the Platform. Your sole and exclusive remedy for a breach of this warranty will be that Pinecone will use commercially reasonable efforts to modify the applicable Platform to achieve the functionality described above. This warranty is void in the event you are in breach of this Agreement. For clarity, this warranty will not apply to any trial or beta services.       25. <span style=\"text-decoration:underline;\">Customer Warranty</span>. Customer represents and warrants that:         * it has procured all applicable consents required to provide the Customer Data to Pinecone for the performance of the Hosted Services, including in accordance with Section 3.1, and all applicable privacy laws;         * the Customer Data will not: (a) infringe or misappropriate any third party’s intellectual property rights; (b) be deceptive, defamatory, obscene, pornographic or unlawful; (c) contain any viruses, worms or other malicious computer programming codes intended to damage the Hosted Services, the Platform or Materials; and (d) otherwise violate the rights of a third party (including under all applicable privacy laws);          * it will use the Hosted Services, Platform and Materials in accordance with the terms herein and all applicable laws; and          * Customer shall not upload to the Hosted Services any Customer Data that contains any sensitive personal information (such as financial, medical or other sensitive personal information such as government IDs, passport numbers or social security numbers).          Customer agrees that any use of the Hosted Services, Platform or Materials contrary to or in violation of the representations and warranties of Customer in this Section 9.2 constitutes unauthorized and improper use of the Hosted Services, Platform or Materials, as applicable.  10. **DISCLAIMER.**       1. <span style=\"text-decoration:underline;\">Disclaimer</span>. TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE HOSTED SERVICES, THE PLATFORM, AND ALL OTHER MATERIALS ARE PROVIDED “AS IS” AND WITH ALL FAULTS. EXCEPT FOR THE LIMITED WARRANTY PROVIDED IN SECTION 9.1, PINECONE MAKES NO WARRANTIES WITH RESPECT TO THE HOSTED SERVICES, THE PLATFORM OR THE MATERIALS, WHETHER EXPRESS OR IMPLIED, INCLUDING WARRANTIES OF TITLE, ACCURACY, INTERFERENCE WITH CUSTOMER’S QUIET ENJOYMENT, SYSTEM INTEGRATION, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK ARISING OUT OF THE USE OR PERFORMANCE OF THE HOSTED SERVICES, THE PLATFORM, OR THE MATERIALS IS WITH CUSTOMER.  NO ORAL OR WRITTEN INFORMATION OR ADVICE GIVEN BY PINECONE OR ITS AGENTS OR EMPLOYEES SHALL IN ANY WAY INCREASE THE SCOPE OF THIS WARRANTY.      27. CUSTOMER ACKNOWLEDGES AND AGREES THAT PINECONE IS NOT LIABLE, AND CUSTOMER AGREES NOT TO SEEK TO HOLD PINECONE LIABLE, FOR THE CONDUCT OF THIRD PARTIES, INCLUDING PROVIDERS OF THE THIRD-PARTY SERVICES, AND THAT THE RISK OF INJURY  FROM SUCH THIRD-PARTY SERVICES RESTS ENTIRELY WITH CUSTOMER.      28. FROM TIME TO TIME, PINECONE MAY OFFER NEW “BETA” FEATURES OR TOOLS WITH WHICH CUSTOMER MAY EXPERIMENT. SUCH FEATURES OR TOOLS ARE OFFERED SOLELY FOR EXPERIMENTAL PURPOSES AND WITHOUT ANY WARRANTY OF ANY KIND, AND MAY BE MODIFIED OR DISCONTINUED AT PINECONE’S SOLE DISCRETION.  THE PROVISIONS OF THIS SECTION APPLY WITH FULL FORCE TO SUCH FEATURES OR TOOLS.   11. **LIMITATION OF LIABILITY.**      1. <span style=\"text-decoration:underline;\">Generally</span>. NEITHER PARTY SHALL BE LIABLE TO THE OTHER PARTY NOR TO ANY THIRD PARTIES FOR LOST PROFITS OR LOST DATA OR FOR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, RELIANCE OR PUNITIVE LOSSES OR DAMAGES HOWSOEVER ARISING UNDER THIS AGREEMENT OR IN CONNECTION WITH THE PLATFORM, WHETHER UNDER CONTRACT, TORT OR OTHERWISE, WHETHER FORESEEABLE OR NOT, AND REGARDLESS WHETHER SUCH PARTY HAS BEEN ADVISED OF THE POSSIBILITY THAT SUCH DAMAGES MAY ARISE, OCCUR OR RESULT.  IN NO EVENT SHALL PINECONE BE LIABLE FOR PROCUREMENT COSTS OF SUBSTITUTE PRODUCTS OR SERVICES. EACH PARTY’S AGGREGATE CUMULATIVE LIABILITY ARISING OUT OF OR IN ANY WAY CONNECTED TO THIS AGREEMENT WILL IN NO EVENT EXCEED THE GREATER OF (A) THE AMOUNT OF FEES PAID BY CUSTOMER UNDER THIS AGREEMENT IN THE TWELVE (12) MONTHS IMMEDIATELY PRECEDING THE EVENT GIVING RISE TO THE CLAIM OR (B) ONE HUNDRED UNITED STATES DOLLARS ($100.00). THE PARTIES AGREE THAT THE LIMITATIONS OF LIABILITY SET FORTH IN THIS SECTION SHALL SURVIVE AND CONTINUE IN FULL FORCE AND EFFECT DESPITE ANY FAILURE OF CONSIDERATION OR OF AN EXCLUSIVE REMEDY. THE PARTIES ACKNOWLEDGE THAT THIS AGREEMENT HAS BEEN ENTERED INTO IN RELIANCE UPON THESE LIMITATIONS OF LIABILITY AND THAT ALL SUCH LIMITATIONS FORM AN ESSENTIAL BASIS OF THE BARGAIN BETWEEN THE PARTIES.        30. <span style=\"text-decoration:underline;\">Basis of the Bargain</span>. THESE LIMITATIONS OF LIABILITY WILL APPLY NOTWITHSTANDING ANY FAILURE OF ESSENTIAL PURPOSE OF ANY LIMITED REMEDY.  THE PARTIES ACKNOWLEDGE THAT THE PRICES HAVE BEEN SET AND THE AGREEMENT ENTERED INTO IN RELIANCE UPON THESE LIMITATIONS OF LIABILITY AND THAT ALL SUCH LIMITATIONS FORM AN ESSENTIAL BASIS OF THE BARGAIN BETWEEN THE PARTIES.  THE PROVISIONS OF THIS AGREEMENT ALLOCATE THE RISKS UNDER THIS AGREEMENT BETWEEN PINECONE AND CUSTOMER.  PINECONE’S FEES FOR THE HOSTED SERVICES REFLECTS THIS ALLOCATION OF RISK AND THE LIMITATION OF LIABILITY SPECIFIED HEREIN.      31. <span style=\"text-decoration:underline;\">Exclusions</span>. THESE LIMITATIONS OF LIABILITY DO NOT APPLY TO: (A) A BREACH BY A PARTY OF SECTIONS 1.1, 2,  OR 7; (B) CUSTOMER’S OBLIGATIONS UNDER SECTION 9.2; (C) INDEMNIFICATION OBLIGATIONS; OR (D) ANY DEATH OR PERSONAL INJURY CAUSED BY EITHER PARTY’S NEGLIGENCE, GROSS NEGLIGENCE, OR WILLFUL MISCONDUCT.  12. **INDEMNIFICATION**      1. <span style=\"text-decoration:underline;\">By Pinecone</span>. Pinecone will defend at its expense any suit brought against Customer, and will pay any settlement Pinecone makes or approves, or any damages finally awarded in such suit, insofar as such suit is based on a claim by any third party alleging that the Platform or the Hosted Services infringes such third party’s patents, copyrights or trade secret rights under applicable laws of any jurisdiction within the United States of America. If any portion of the Platform or Hosted Services becomes, or in Pinecone’s opinion is likely to become, the subject of a claim of infringement (“**Infringing Technology**”), Pinecone may, at Pinecone’s option: (i) procure for Customer the right to continue using the Infringing Technology; (ii) replace the Infringing Technology with non-infringing software or services which do not materially impair the functionality of the Platform or Hosted Services; (iii) modify the Infringing Technology so that it becomes non-infringing; or (iv) terminate this Agreement and refund any unused prepaid Fees for the remainder of the term then in effect, and upon such termination, Customer will immediately cease all use of the Platform and Hosted Services. Notwithstanding the foregoing, Pinecone will have no obligation under this section or otherwise with respect to any infringement claim based upon: (A) any use of the Platform or Hosted Services not in accordance with this Agreement or as specified in the Documentation; (B) any use of the Platform or Hosted Services in combination with other products, equipment, software or data not supplied by Pinecone; or (C) any modification of the Platform or Hosted Services by any person other than Pinecone or its authorized agents (collectively, the “**Exclusions**” and each, an “**Exclusion**”). This section states the sole and exclusive remedy of Customer and the entire liability of Pinecone, or any of the officers, directors, employees, shareholders, contractors or representatives of the foregoing, for infringement claims and actions.        33. <span style=\"text-decoration:underline;\">By Customer</span>. Customer will defend at its expense any suit brought against Pinecone, and will pay any settlement Customer makes or approves, or any damages finally awarded in such suit, insofar as such suit is based on a claim arising out of or relating to: (a) an Exclusion, or (b) Customer’s breach or alleged breach of Section 9.2. This section states the sole and exclusive remedy of Pinecone and the entire liability of Customer, or any of its officers, directors, employees, shareholders, contractors or representatives, for the claims and actions described herein.       34. <span style=\"text-decoration:underline;\">Procedure</span>. The indemnifying Party’s obligations as set forth above are expressly conditioned upon each of the foregoing: (a) the indemnified Party promptly notifying the indemnifying Party in writing of any threatened or actual claim or suit; (b) the indemnifying Party having sole control of the defense or settlement of any claim or suit; and (c) the indemnified Party cooperating with the indemnifying Party to facilitate the settlement or defense of any claim or suit.   13. **TERM.**      1. <span style=\"text-decoration:underline;\">Term and Termination</span>. The term of this Agreement commences on the earlier of your clicking of the “I ACCEPT” button, and when you first access the Hosted Services and shall continue for so long as you access the Hosted Services. If Pinecone becomes aware of any possible violations by Customer of this Agreement, Pinecone may, in its discretion, immediately terminate or suspend Customer’s access to the Hosted Services (including the Platform). Customer can discontinue using the Hosted Services at any time. Upon termination, Customer shall immediately cease all use of the Hosted Services (including the Platform), and delete or destroy all copies of any other Materials in the possession or control of Customer.        36. <span style=\"text-decoration:underline;\">Survival</span>. Sections 1.1, 1.2, 1.3, 2, 3 - 7, 9 - 12, 13.2, and 14 shall survive termination or expiration of this Agreement.    14. **GENERAL**.        1. <span style=\"text-decoration:underline;\">Trade Control Laws</span>.  Customer shall comply with all export control and economic sanctions laws and regulations (collectively, “**Trade Control Laws**”) applicable to Customer in the performance of this Agreement.  Pinecone shall not be required under this Agreement to be directly or indirectly involved in the provision of goods, software, services and/or technical data that may be prohibited by applicable Trade Control Laws.  Customer represents and covenants that it (a) is not identified on, or owned or controlled by or acting on behalf of any individuals or entities identified on, applicable government restricted party lists (“**Restricted Parties**”); (b) is not located in, organized under the laws of or ordinarily resident in Cuba, Iran, North Korea, Sudan, Syria or Crimea (region of Ukraine/Russia) (“**Restricted Countries**”); and (c) will not directly or indirectly export, re-export or otherwise transfer any goods, technology or services covered by the Agreement to or for use in or from Restricted Countries or Restricted Parties.       38. <span style=\"text-decoration:underline;\">Assignment</span>.  Customer may not assign or otherwise transfer this Agreement or any of its rights or obligations, in whole or in part, without the prior written consent of Pinecone, and any unauthorized assignment or transfer shall be void, provided, however, that either party shall have the right to assign the Agreement, without the prior written consent of the other party, to the successor entity in the event of merger, corporate reorganization or a sale of all or substantially all of such party’s assets. This Agreement shall be binding upon the parties and their respective successors and permitted assigns.       39. <span style=\"text-decoration:underline;\">Notices</span>.  Where Pinecone requires that you provide an e-mail address, you are responsible for providing Pinecone with your most current e-mail address.  In the event that the last e-mail address you provided to Pinecone is not valid, or for any reason is not capable of delivering to you any notices required/ permitted by the Agreement, Pinecone’s dispatch of the e-mail containing such notice will nonetheless constitute effective notice.  You may give notice to Pinecone at the following address: Pinecone Systems Inc, 548 Market St, PMB 19327, San Francisco, CA 94104-5401, Attn: NOTICE, or info@pinecone.io. Such notice shall be deemed given when received by Pinecone by letter delivered by nationally recognized overnight delivery service or first class postage prepaid mail at the above address, or by electronic mail.       40. <span style=\"text-decoration:underline;\">Choice of Law; Venue</span>.  This Agreement shall be governed by laws of the State of California, without regard to the choice of conflicts of law provisions of any jurisdiction.  Customer submits to the exclusive jurisdiction and venue of the federal and state courts located in Santa Mateo County, California for any disputes arising out of or related to this Agreement.         41. <span style=\"text-decoration:underline;\">Severability</span>.  If any term of this Agreement is found by competent judicial authority to be unenforceable in any respect, the validity of the remainder of this Agreement will be unaffected, provided that such unenforceability does not materially affect the parties’ rights under this Agreement.       42. <span style=\"text-decoration:underline;\">Waiver</span>.  An effective waiver under this Agreement must be in writing signed by the party waiving its right.  A waiver by either party of any instance of the other party’s noncompliance with any obligation or responsibility under this Agreement will not be deemed a waiver of subsequent instances.        43. <span style=\"text-decoration:underline;\">Independent Contractor</span>.  Neither this Agreement nor the cooperation of the parties contemplated under this Agreement shall be deemed or construed to create any partnership, joint venture or agency relationship between the parties.  Except as otherwise expressly permitted in this Agreement, neither party is, nor will either party hold itself out to be, vested with any power or right to bind the other party contractually or act on behalf of the other party as a broker, agent or otherwise       44. <span style=\"text-decoration:underline;\">Force Majeure</span>.  Any delay in the performance of any duties or obligations of either party (except for the obligation to pay Fees owed) will not be considered a breach of this Agreement if such delay is caused by a labor dispute, shortage of materials, war, fire, earthquake, typhoon, flood, natural disasters, governmental action, pandemic/epidemic, cloud-service provider outages any other event beyond the control of such party, provided that such party uses reasonable efforts, under the circumstances, to notify the other Party of the circumstances causing the delay and to resume performance as soon as possible.       45. <span style=\"text-decoration:underline;\">U.S. Government Restricted Rights</span>. If Customer is a government end user, then this provision also applies to Customer. The software contained within the Platform and the Services and provided in connection with this Agreement has been developed entirely at private expense, as defined in FAR section 2.101, DFARS section 252.227-7014(a)(1) and DFARS section 252.227- 7015 (or any equivalent or subsequent agency regulation thereof), and is provided as “commercial items,” “commercial computer software” and/or “commercial computer software documentation.” Consistent with DFARS section 227.7202 and FAR section 12.212, and to the extent required under U.S. federal law, the minimum restricted rights as set forth in FAR section 52.227-19 (or any equivalent or subsequent agency regulation thereof), any use, modification, reproduction, release, performance, display, disclosure or distribution thereof by or for the U.S. Government shall be governed solely by this Agreement and shall be prohibited except to the extent expressly permitted by this Agreement.       46. <span style=\"text-decoration:underline;\">Entire Agreement</span>.  This Agreement constitutes the entire understanding of the parties with respect to the transactions and matters contemplated hereby and supersedes all previous communications, representations, agreements and understanding relating to the Hosted Services, the Platform, and the Materials.  No representations, inducements, promises or agreements, whether oral or otherwise, between the parties not contained in this Agreement shall be of any force or effect.  ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e292"
  },
  "title": "Features Highlights",
  "headline": "\"Pinecone for Semantic Search\"",
  "description": "|",
  "src": "/images/illustration-scale-performance.svg",
  "alt": "Pinecone Diagram",
  "url": "/docs/examples/extractive-question-answering/",
  "text": "Initiate the functionality of the database from any environment that can make HTTPS calls. Add semantic search to your application with just a few lines of code.",
  "- eyebrow": "Performance at Scale",
  "maxWidth": "80%",
  "- title": "Question answering example",
  "thumbnail": "\"/images/question-answering-example.png\"",
  "content": "",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e294"
  },
  "title": "\"Subprocessors\"",
  "headline": "\"Subprocessors\"",
  "cta": "false",
  "intro": "\"\"",
  "content": "*Updated: September 7, 2022*  Information about Subprocessors, including their functions and locations.  | Service/Vendor | Function | Address | |--|--|--| | Confluent | Event streaming | California, USA | | Datadog | Cloud Monitoring Service Provider |  New York, USA | | Zendesk | Customer service and support ticketing | California, USA | | AWS | Cloud Infrastructure | Washington, USA | | Google | Internal collaboration and cloud infrastructure | California, USA | | Databricks | Data ingestion | California, USA | ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e296"
  },
  "title": "In the <span>News</span>",
  "headline": "\"We are Engineers, Scientists, and Researchers\"",
  "intro": "\"The vector database for machine learning is brought to you by a dedicated team of engineers, scientists, and researchers with a passion for harnessing AI technology to drive business success.\"",
  "description": "",
  "src": "/images/team-elan-dekel.jpg",
  "alt": "Pinecone Diagram",
  "width": "447",
  "height": "548",
  "- name": "Will Freiberg",
  "position": "CEO Crux Informatics",
  "eyebrow": "Our investors",
  "href": "'#press-kit'",
  "text": "Download Our Press Kit",
  "content": "",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e298"
  },
  "title": "\"Product Privacy Statement\"",
  "headline": "\"Product Privacy Statement\"",
  "cta": "false",
  "intro": "\"\"",
  "content": "*Updated: May 26, 2022*  Pinecone Systems, Inc. (“Pinecone”) is a managed database for working with vectors. It provides the infrastructure for machine-learning applications that need to search and rank results based on similarity, such as recommendations, personalization, image and semantic search, and more.  This Product Privacy Statement explains how we collect, use, disclose, and otherwise process personal information on behalf of our enterprise customers (any, a “Customer”) in connection with our products and Hosted Services (collectively, the “Hosted Services”). As a service provider/data processor, Pinecone provides the Hosted Services to its Customers. Pinecone’s Customers are the data owners/data controllers with respect to such personal information processed through the Hosted Services. This Product Privacy Statement does not apply to any websites or other online Hosted Services that do not link to this Product Privacy Statement. To the extent Pinecone shares information with the Customer as this Product Privacy Statement describes, this Product Privacy Statement does not apply to the Customer’s subsequent use of that information. Pinecone’s processing of personal information in connection with the Hosted Services is governed by this Product Privacy Statement and the relevant Customer agreement. In the event of any conflict between this Product Privacy Statement and the relevant Customer agreement, the customer agreement will control to the extent permitted by applicable law.  This Product Privacy Statement is not a substitute for any privacy notice that Pinecone Customers are required to provide to their employees or other authorized users.  ## Information We Collect  _Information collected via the Hosted Services_. The Customer is responsible for determining the scope of information that Pinecone may collect and process. The Hosted Services process information through copying the data catalogue – upon your request - from your cloud servers into our systems, transform the data into vectors, store those vectors, and allow you to add, edit or delete them by changing your data catalogue. This processing is augmented by Pinecone’s intelligence tools. Such information may include personal information. We ask that our Customers not provide us with information that may be deemed sensitive under applicable laws (including, without limitation, financial information, health information, and information about children).  _Information provided to us by authorized users in connection with their use of the Hosted Services_. We collect information about the Customer’s “authorized users” — individuals (typically personnel) that Customer authorizes to create an account or use an API key to access the Hosted Services. This may include personal information that authorized users provide when they:  - Register for an account or create a user profile (such as first and last name, email address, physical address, telephone number(s), fax number, employer name, department and job title, device ID, and profile picture); - Upload content to the Hosted Services; and - Contact customer support or otherwise correspond with us by phone, email, or other means.  _Information collected about authorized users_. We may collect information about authorized users, such as:  - Information provided to us by the Customer about its authorized users. This may include business contact information, such as name, email address, and phone number. - Information about authorized users’ use of the Hosted Services. This may include information about authorized users’ use of the Hosted Services, including computer or mobile device operating system type and version number, manufacturer and model, device identifier, browser type, screen resolution, IP address, general location information such as city, state or geographic area; and information about authorized users’ use of and actions on the Hosted Services, such as pages you viewed, how much time was spent on a page, navigation paths between pages, information about activity on a page, access times, and length of access. This information is collected using cookies and similar technologies.  ## How We Use Information  We use the information we collect at the instruction of the relevant Customer and in accordance with the relevant Customer agreement, to provide the Hosted Services and for related purposes, including to:  - enable authorized users to access and use the Hosted Services; - provide information about the Hosted Services, such as important updates or changes to the Hosted Services and security alerts; - customize the end user experience, such as personalizing content and features to better match interests and preferences; - derive anonymized, aggregated or de-identified data for our subsequent use; - measure performance of and improve the Hosted Services and develop new products and Hosted Services; and - respond to inquiries, complaints, and requests for customer support.  We may also use personal information as we believe necessary or appropriate to (a) comply with applicable law; (b) enforce the terms and conditions that govern the Service; (c) protect our rights, privacy, safety or property, and/or that of you or others; and (d) protect, investigate and deter against fraudulent, harmful, unauthorized, unethical or illegal activity.  ## How We Share Information  We share the information we collect:  - with the relevant Customer from which we obtained the information; - with such third parties as the relevant Customer may direct; and - with third-party service providers that help us manage and improve the Hosted Services.  We may also share personal information with government, law enforcement officials or private parties as required by law, when we believe such disclosure is necessary or appropriate to (a) comply with applicable law; (b) enforce the terms and conditions that govern the Hosted Services; (c) protect our rights, privacy, safety or property, and/or that of you or others; and (d) protect, investigate and deter against fraudulent, harmful, unauthorized, unethical or illegal activity.  We may sell, transfer or otherwise share some or all of Pinecone’s business or assets, including personal information that we process as part of the Hosted Services, in connection with a business transaction (or potential business transaction) such as a merger, consolidation, acquisition, reorganization or sale of assets or in the event of bankruptcy.  ## Information Security  Pinecone uses physical, electronic, and procedural safeguards designed to protect personal information from loss, theft, misuse, and unauthorized access, disclosure, alteration, and destruction. We cannot, however, guarantee that any safeguards or security measures will be sufficient to prevent a security problem. We recommend that our Customers take steps to protect against unauthorized access to any devices or networks used to access the Hosted Services. See the relevant Customer agreement for additional information regarding Pinecone’s information security practices.  ## Data Subject Rights  The relevant Customer is the data owner/data controller of authorized users’ or others’ personal information processed through the Hosted Services. As the data owner/data controller, the relevant Customer is responsible for receiving and responding to authorized users’ and others’ requests to exercise any rights afforded to them under applicable data protection law. Pinecone will assist Customers in responding to such requests as set forth in the relevant Customer agreement.  ## Cross Border Data Transfer Pinecone may transfer personal information collected and processed through the Hosted Services outside of the country from which it originated, including to the United States. See the relevant Customer agreement for additional information regarding how Pinecone safeguards the personal information it transfers across borders.  ## Data Retention  Pinecone retains personal information for as long as necessary to (a) provide the Hosted Services; (b) comply with legal obligations; (c) resolve disputes; and (d) enforce the terms of the relevant Customer agreement. See the relevant Customer agreement for additional information regarding Pinecone’s data retention practices.  ## Third-Party Products and Hosted Services  The Hosted Services may integrate with or enable access to third-party tools. Third-party tools registered, installed, or accessed by authorized users are governed by those third-party providers’ privacy notices. Please review those notices carefully, as Pinecone does not control and cannot be responsible for these providers’ privacy or information security practices.  ## Changes to this Privacy Statement  Pinecone reserves the right to modify this Product Privacy Statement at any time. Similar to Pinecone’s Hosted Services, laws, regulations and industry standards may evolve which may make changes to this Product Privacy Statement appropriate. Pinecone will post the changes to this page. Pinecone encourages you to review the Product Privacy Statement to stay informed. In accordance with applicable law, Pinecone will notify you of any material changes in its collection, use, or disclose of your information by posting a notice on its website. Any material changes to this Product Privacy Statement will be effective thirty (30) calendar days following notice of the changes on the website. These changes will be effective immediately for new users of the Hosted Services. If you object to any such changes, you must notify Pinecone prior to the effective date of such changes that you wish to deactivate your account. Continued use of the Hosted Services following the effective date of such changes indicates your agreeing to such changes.  ## Contact Us  If you have any question about this Product Privacy Statement, you can contact us at privacy@pinecone.io or mail us at:  Pinecone Systems, Inc<br> 548 Market St<br> PMB 19327<br> San Francisco, CA 94104-5401<br> ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e29a"
  },
  "title": "\"Cookie Policy\"",
  "headline": "\"Cookie Policy\"",
  "cta": "false",
  "intro": "\"\"",
  "content": "*Last Revised: December 3, 2020*  This Cookie Policy explains how Pinecone Systems, Inc. (“Pinecone”, “we”, “us” or “our”) uses cookies and similar technologies in connection with the https://www.pinecone.io/ website and any other website that we own or control and which posts or links to this Privacy Policy (collectively, the “Websites”).  Our [Privacy Policy](/privacy/) explains how we collect and use information from and about you when you use our Websites. This Cookie Policy explains more about how we use cookies and your related choices.  ## What are cookies?  Cookies are small data files that are placed on your computer or mobile device when you visit a website.  Cookies serve different purposes, like helping us understand how a website is being used, letting you navigate between pages efficiently, remembering your preferences and generally improving your browsing experience.  Our Websites may use both session cookies (which expire once you close your web browser) and persistent cookies (which stay on your computer or mobile device until you delete them).  We may use two broad categories of cookies: (1) first party cookies, served directly by us to your computer or mobile device, which we use to recognize your computer or mobile device when it revisits our Websites; and (2) third party cookies, which are served by service providers or business partners on our Websites, and can be used by these parties to recognize your computer or mobile device when it visits other websites. Third party cookies can be used for a variety of purposes, including website analytics and social media features.  ## What types of cookies and similar tracking technologies does Pinecone use on the Websites?  On the Websites, we may use cookies and other tracking technologies in the following categories described in the table below.  <table class=\"table table-bordered\">   <thead>     <tr>       <th scope=\"col\">Type</th>       <th scope=\"col\">Description</th>     </tr>   </thead>   <tbody>     <tr>       <td>Advertising</td>       <td>These cookies are used by advertising companies to collect information about how you use our Websites and other websites over time.  These companies use this information to show you ads they believe will be relevant to you within our Websites and elsewhere, and to measure how the ads perform.</td>     </tr>     <tr>       <td>Analytics</td>       <td>These cookies help us understand how our Websites is performing and being used.  These cookies may work with web beacons included in emails we send to track which emails are opened and which links are clicked by recipients.</td>     </tr>     <tr>       <td>Essential</td>       <td>These cookies are necessary to allow the technical operation of our Websites (e.g., they enable you to move around on a website and to use its features).</td>     </tr>     <tr>       <td>Functionality/performance</td>       <td>These cookies enhance the performance and functionality of our Websites.</td>     </tr>   </tbody> </table>  ## Other technologies  In addition to cookies, our Websites may use other technologies, such as Flash technology and pixel tags to collect information automatically.  ### Browser Web Storage  We may use browser web storage (including via HTML5), also known as locally stored objects (“LSOs”), for similar purposes as cookies. Browser web storage enables the storage of a larger amount of data than cookies. Your web browser may provide functionality to clear your browser web storage.  ### Web Beacons  We may also use web beacons (which are also known as pixel tags and clear GIFs) on our Websites and in our HTML formatted emails to track the actions of users on our Websites and interactions with our emails. Unlike cookies, which are stored on the hard drive of your computer or mobile device by a website, pixel tags are embedded invisibly on webpages or within HTML formatted emails. Pixel tags are used to demonstrate that a webpage was accessed or that certain content was viewed, typically to measure the success of our marketing campaigns or engagement with our emails and to compile statistics about usage of the Website, so that we can manage our content more effectively.  ## Your choices  Most browsers let you remove or reject cookies.  To do this, follow the instructions in your browser settings.  Many browsers accept cookies by default until you change your settings.  Please note that if you set your browser to disable cookies, the Websites may not work properly. For more information about cookies, including how to see what cookies have been set on your computer or mobile device and how to manage and delete them, visit https://www.allaboutcookies.org.  If you do not accept our cookies, you may experience some inconvenience in your use of our Websites. For example, we may not be able to recognize your computer or mobile device and you may need to log in every time you visit our Websites. Users may opt out of receiving targeted advertising on websites through members of the Network Advertising Initiative by [clicking here](http://www.networkadvertising.org/choices) or the Digital Advertising Alliance by [clicking here](http://www.aboutads.info/choices). European users may opt out of receiving targeted advertising on websites through members of the European Interactive Digital Advertising Alliance by [clicking here](https://www.youronlinechoices.eu/), selecting the user’s country, and then clicking “Choices” (or similarly-titled link). Please note that we also may work with companies that offer their own opt-out mechanisms and may not participate in the opt-out mechanisms that we linked above.  If you choose to opt-out of targeted advertisements, you will still see advertisements online but they may not be relevant to you. Even if you do choose to opt out, not all companies that serve online behavioral advertising are included in this list, and so you may still receive some cookies and tailored advertisements from companies that are not listed.  ## Changes  Information about the cookies we use may be updated from time to time, so please check back on a regular basis for any changes.  ## Questions  If you have any questions about this Cookie Policy, please contact us by email at info@pinecone.io or mail us at:  Pinecone Systems, Inc.\\ 548 Market St\\ PMB 19327\\ San Francisco, CA 94104-5401",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e29c"
  },
  "title": "\"Website Terms of Service\"",
  "headline": "\"Website Terms of Service\"",
  "cta": "false",
  "intro": "\"\"",
  "content": "*Last Revised: May 26, 2022*  The Pinecone website located at https://www.pinecone.io (the “**Site**”) is a copyrighted work belonging to Pinecone Systems Inc., (“**Pinecone**”, “**us**”, “**our**”, and “**we**”). The Site is for informational purposes only; it contains general information about Pinecone and is directed at businesses and their representatives seeking information on our products and services.  Certain features of the Site may be subject to additional guidelines, terms, or rules, which will be posted on the Site in connection with such features. All such additional terms, guidelines, and rules are incorporated by reference into these Terms of Use (“**Terms**”). Your access to and use of the Site is also subject to our [Privacy Policy](/privacy/).  Pinecone also provides a proprietary data and analytics platform, (the “**Platform**”, and the provision of which, the “**Hosted Services**”). Access to the Platform and the Hosted Services is subject to the terms and conditions of the Pinecone End User License Agreement, or any other executed agreement between the parties (either of which, a “**Hosted Services Agreement**”).  THESE TERMS SET FORTH THE LEGALLY BINDING TERMS AND CONDITIONS THAT GOVERN YOUR USE OF THE SITE. BY ACCESSING OR USING THE SITE, YOU ARE ACCEPTING THESE TERMS (ON BEHALF OF YOURSELF OR THE ENTITY THAT YOU REPRESENT), AND YOU REPRESENT AND WARRANT THAT YOU HAVE THE RIGHT, AUTHORITY, AND CAPACITY TO ENTER INTO THESE TERMS (ON BEHALF OF YOURSELF OR THE ENTITY THAT YOU REPRESENT). YOU MAY NOT ACCESS OR USE THE SITE OR ACCEPT THESE TERMS IF YOU ARE NOT AT LEAST 18 YEARS OLD. IF YOU DO NOT AGREE WITH ALL OF THE PROVISIONS OF THESE TERMS, DO NOT ACCESS AND/OR USE THE SITE.  THESE TERMS REQUIRE THE USE OF ARBITRATION (SECTION 12.2) ON AN INDIVIDUAL BASIS TO RESOLVE DISPUTES, RATHER THAN JURY TRIALS OR CLASS ACTIONS, AND ALSO LIMIT THE REMEDIES AVAILABLE TO YOU IN THE EVENT OF A DISPUTE.  1. **ACCOUNTS**      1. **Account Creation**. In order to use certain features of the Platform and/or Hosted Services, you may register for an account (“**Hosted Services Account**”) on the Site and provide certain information about yourself as prompted by the account registration form. You represent and warrant that: (a) all required registration information you submit is truthful and accurate; and (b) you will maintain the accuracy of such information. You may delete your Account in accordance with the instructions set forth in the applicable Hosted Services Agreement. Pinecone may suspend or terminate your Hosted Services Account in accordance with the terms of the applicable Hosted Services Agreement.      2. **Account Responsibilities**. You are responsible for maintaining the confidentiality of your Account login information and any API Keys you are provided in connection with registration, in accordance with the terms set forth herein, and the applicable Hosted Services Agreement.  2. **ACCESS TO THE SITE**      1. **License**. Subject to these Terms, Pinecone grants you a non-transferable, non-exclusive, revocable, limited license to use and access the Site solely for your own personal or internal business use.      2. **Certain Restrictions**. The rights granted to you in these Terms are subject to the following restrictions: (a) you shall not license, sell, rent, lease, transfer, assign, distribute, host, or otherwise commercially exploit the Site, whether in whole or in part, or any content displayed on the Site; (b) you shall not modify, make derivative works of, disassemble, reverse compile or reverse engineer any part of the Site; (c) you shall not access the Site in order to build a similar or competitive website, product, or service; and (d) except as expressly stated herein, no part of the Site may be copied, reproduced, distributed, republished, downloaded, displayed, posted or transmitted in any form or by any means. Unless otherwise indicated, any future release, update, or other addition to the functionality of the Site shall be subject to these Terms. All copyright and other proprietary notices on the Site (or on any content displayed on the Site) must be retained on all copies thereof.      3. **Modification**. Pinecone reserves the right, at any time, to modify, suspend, or discontinue the Site (in whole or in part) with or without notice to you. You agree that Pinecone will not be liable to you or to any third party for any modification, suspension, or discontinuation of the Site or any part thereof.      4. **No Support or Maintenance**. You acknowledge and agree that Pinecone will have no obligation to provide you with any support or maintenance in connection with the Site.      5. **Ownership**. You acknowledge that all intellectual property rights, including copyrights, patents, trademarks, and trade secrets, in the Site and its content are owned by Pinecone or Pinecone’s suppliers. Neither these Terms (nor your access to the Site) transfers to you or any third party any rights, title or interest in or to such intellectual property rights, except for the limited access rights expressly set forth in Section 2.1. Pinecone and its suppliers reserve all rights not granted in these Terms. There are no implied licenses granted under these Terms.  3. **ACCEPTABLE USE POLICY**. The following terms constitute our “**Acceptable Use Policy**.” You agree not to: (i) upload, transmit, or distribute to or through the Site any computer viruses, worms, or any software intended to damage or alter a computer system or data; (ii) send through the Site unsolicited or unauthorized advertising, promotional materials, junk mail, spam, chain letters, pyramid schemes, or any other form of duplicative or unsolicited messages, whether commercial or otherwise; (iii) use the Site to harvest, collect, gather or assemble information or data regarding other users, including e-mail addresses, without their consent; (iv) interfere with, disrupt, or create an undue burden on servers or networks connected to the Site, or violate the regulations, policies or procedures of such networks; (v) attempt to gain unauthorized access to the Site (or to other computer systems or networks connected to or used together with the Site); (vi) interfere with any other user’s use and enjoyment of the Site; or (vi) use software or automated agents or scripts to produce multiple accounts on the Site, or to generate automated searches, requests, or queries to (or to strip, scrape, or mine data from) the Site (provided, however, that we conditionally grant to the operators of public search engines revocable permission to use spiders to copy materials from the Site for the sole purpose of and solely to the extent necessary for creating publicly available searchable indices of the materials, but not caches or archives of such materials, subject to the parameters set forth in our robots.txt file).  4. **ENFORCEMENT**. We reserve the right (but have no obligation) to investigate and/or take appropriate action against you in our sole discretion if you violate the Acceptable Use Policy or any other provision of these Terms or otherwise create liability for us or any other person. Such action may include reporting you to law enforcement authorities.  5. **FEEDBACK**. If you provide Pinecone with any feedback or suggestions regarding the Site (“**Feedback**”), you hereby assign to Pinecone all rights in such Feedback and agree that Pinecone shall have the right to use and fully exploit such Feedback and related information in any manner it deems appropriate. Pinecone will treat any Feedback you provide to Pinecone as non-confidential and non-proprietary. You agree that you will not submit to Pinecone any information or ideas that you consider to be confidential or proprietary.  6. **PINECONE COMMUNICATIONS**.      1. **Generally**. You may have the opportunity to provide us with your phone number or e-mail address. By providing your phone number or email address to us, you consent to receive phone calls, SMS/text messages, and email communications from Pinecone. Communications from us and our affiliated companies may include communications about your use of the Site, and communications containing your API keys in connection with registration for the Pinecone Platform.      2. **Promotional Email Communications**. If you opt-in to receive marketing or promotional email communications from us, you will have the ability to opt out of receiving such communications by following the unsubscribe instructions in the communication itself. YOU ACKNOWLEDGE THAT YOU ARE NOT REQUIRED TO CONSENT TO RECEIVE PROMOTIONAL EMAILS AS A CONDITION OF USING THE SITE. CONSENT TO THESE PROMOTIONAL MESSAGES IS NOT REQUIRED TO ACCESS THE SITE.      3. **Electronic Communications**. The communications between you and Pinecone use electronic means, whether you use the Site or send us emails, or whether Pinecone posts notices on the Site or communicates with you via email. For contractual purposes, you (a) consent to receive communications from Pinecone in an electronic form; and (b) agree that all terms and conditions, agreements, notices, disclosures, and other communications that Pinecone provides to you electronically satisfy any legal requirement that such communications would satisfy if they were to be in a hardcopy writing. The foregoing does not affect your non-waivable rights.  7. **INDEMNIFICATION**. You agree to indemnify and hold Pinecone (and its officers, employees, and agents) harmless, including costs and attorneys’ fees, from any claim or demand made by any third party due to or arising out of (a) your use of the Site, (b) your violation of these Terms, or (c) your violation of applicable laws or regulations. Pinecone reserves the right, at your expense, to assume the exclusive defense and control of any matter for which you are required to indemnify us, and you agree to cooperate with our defense of these claims. You agree not to settle any matter without the prior written consent of Pinecone. Pinecone will use reasonable efforts to notify you of any such claim, action or proceeding upon becoming aware of it.  8. **THIRD-PARTY LINKS AND APPLICATIONS; RELEASE**      1. **Third-Party Links and Applications**. The Site may contain links to third-party websites and services and applications for third parties (collectively, “**Third-Party Links and Applications**”). Such Third-Party Links and Applications are not under the control of Pinecone, and Pinecone is not responsible for any Third-Party Links and Applications. Pinecone provides access to these Third-Party Links and Applications only as a convenience to you, and does not review, approve, monitor, endorse, warrant, or make any representations with respect to Third-Party Links and Applications. You use all Third-Party Links and Applications at your own risk, and should apply a suitable level of caution and discretion in doing so. When you click on any of the Third-Party Links and Applications, the applicable third party’s terms and policies apply, including the third party’s privacy and data gathering practices. You should make whatever investigation you feel necessary or appropriate before proceeding with any transaction in connection with such Third-Party Links and Applications.      2. **Release**. You hereby release and forever discharge Pinecone (and our officers, employees, agents, successors, and assigns) from, and hereby waive and relinquish, each and every past, present and future dispute, claim, controversy, demand, right, obligation, liability, action and cause of action of every kind and nature (including personal injuries, death, and property damage), that has arisen or arises directly or indirectly out of, or that relates directly or indirectly to, the Site (including interactions with any other Site users or any Third-Party Links and Applications). If you are a California resident, you hereby waive California Civil Code 1542 in connection with the foregoing, which states, “A general release does not extend to claims that the creditor or releasing party does not know or suspect to exist in his or her favor at the time of executing the release and that, if known by him or her, would have materially affected his or her settlement with the debtor or released party.”  9. **DISCLAIMER**      1. **As Is**. THE SITE IS PROVIDED ON AN “AS-IS” AND “AS AVAILABLE” BASIS, WITH ALL FAULTS AND NO GUARANTEES REGARDING OUTCOMES OR PERFORMANCE. PINECONE (AND OUR SUPPLIERS) EXPRESSLY DISCLAIM ANY AND ALL WARRANTIES AND CONDITIONS OF ANY KIND, WHETHER EXPRESS, IMPLIED, OR STATUTORY, INCLUDING ALL WARRANTIES OR CONDITIONS OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, QUIET ENJOYMENT, ACCURACY, OR NON-INFRINGEMENT. WE (AND OUR SUPPLIERS) MAKE NO WARRANTY THAT THE SITE WILL MEET YOUR REQUIREMENTS, WILL BE AVAILABLE ON AN UNINTERRUPTED, TIMELY, SECURE, OR ERROR-FREE BASIS, OR WILL BE ACCURATE, RELIABLE, FREE OF VIRUSES OR OTHER HARMFUL CODE, COMPLETE, LEGAL, OR SAFE. IF APPLICABLE LAW REQUIRES ANY WARRANTIES WITH RESPECT TO THE SITE, ALL SUCH WARRANTIES ARE LIMITED IN DURATION TO NINETY (90) DAYS FROM THE DATE OF FIRST USE.      SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES, SO THE ABOVE EXCLUSION MAY NOT APPLY TO YOU. SOME JURISDICTIONS DO NOT ALLOW LIMITATIONS ON HOW LONG AN IMPLIED WARRANTY LASTS, SO THE ABOVE LIMITATION MAY NOT APPLY TO YOU.  10. **LIMITATION ON LIABILITY**       TO THE MAXIMUM EXTENT PERMITTED BY LAW, IN NO EVENT SHALL PINECONE (OR OUR SUPPLIERS) BE LIABLE TO YOU OR ANY THIRD PARTY FOR ANY LOST PROFITS, LOST DATA, COSTS OF PROCUREMENT OF SUBSTITUTE PRODUCTS, OR ANY INDIRECT, CONSEQUENTIAL, EXEMPLARY, INCIDENTAL, SPECIAL OR PUNITIVE DAMAGES ARISING FROM OR RELATING TO THESE TERMS OR YOUR USE OF, OR INABILITY TO USE, THE SITE, EVEN IF PINECONE HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. ACCESS TO, AND USE OF, THE SITE IS AT YOUR OWN DISCRETION AND RISK, AND YOU WILL BE SOLELY RESPONSIBLE FOR ANY DAMAGE TO YOUR DEVICE OR COMPUTER SYSTEM, OR LOSS OF DATA RESULTING THEREFROM.      TO THE MAXIMUM EXTENT PERMITTED BY LAW, NOTWITHSTANDING ANYTHING TO THE CONTRARY CONTAINED HEREIN, OUR LIABILITY TO YOU FOR ANY DAMAGES ARISING FROM OR RELATED TO THESE TERMS (FOR ANY CAUSE WHATSOEVER AND REGARDLESS OF THE FORM OF THE ACTION), WILL AT ALL TIMES BE LIMITED TO A MAXIMUM OF FIFTY US DOLLARS (U.S. $50.00). THE EXISTENCE OF MORE THAN ONE CLAIM WILL NOT ENLARGE THIS LIMIT. YOU AGREE THAT OUR SUPPLIERS WILL HAVE NO LIABILITY OF ANY KIND ARISING FROM OR RELATING TO THESE TERMS.      SOME JURISDICTIONS DO NOT ALLOW THE LIMITATION OR EXCLUSION OF LIABILITY FOR INCIDENTAL OR CONSEQUENTIAL DAMAGES, SO THE ABOVE LIMITATION OR EXCLUSION MAY NOT APPLY TO YOU.  11. **TERM AND TERMINATION**.  Subject to this Section, these Terms will remain in full force and effect while you use the Site. We may suspend or terminate your rights to access or use the Site at any time for any reason at our sole discretion, including for any use of the Site in violation of these Terms. Upon termination of your rights under these Terms, your right to access and use the Site will terminate immediately. Pinecone will not have any liability whatsoever to you for any suspension or termination of your rights under these Terms. Even after your rights under these Terms are terminated, the following provisions of these Terms will remain in effect: Sections 1, 2.2-2.5, and 3 through 12.  12. **GENERAL**      1. **Changes**. These Terms are subject to occasional revision, and if we make any substantial changes, we may notify you by sending you an e-mail to the last e-mail address you provided to us (if any), and/or by prominently posting notice of the changes on our Site. You are responsible for providing us with your most current e-mail address. In the event that the last e-mail address that you have provided us is not valid, or for any reason is not capable of delivering to you the notice described above, our dispatch of the e-mail containing such notice will nonetheless constitute effective notice of the changes described in the notice. Any changes to these Terms will be effective upon the earlier of thirty (30) calendar days following our dispatch of an e-mail notice to you (if applicable) or thirty (30) calendar days following our posting of notice of the changes on our Site. These changes will be effective immediately for new users of our Site. Continued use of our Site following notice of such changes shall indicate your acknowledgement of such changes and agreement to be bound by the terms and conditions of such changes.      2. **Dispute Resolution. PLEASE READ THIS CAREFULLY. IT AFFECTS YOUR RIGHTS.**          (a) Any and all controversies, disputes, demands, counts, claims, or causes of action (including the interpretation and scope of this clause, and the arbitrability of the controversy, dispute, demand, count, claim, or cause of action) between you and Pinecone and our employees, agents, successors, or assigns, regarding or relating to the Site or these Terms shall exclusively be settled through binding and confidential arbitration.          (b) Arbitration shall be subject to the Federal Arbitration Act and not any state arbitration law. The arbitration shall be conducted before one commercial arbitrator with substantial experience in resolving commercial contract disputes from the American Arbitration Association (“**AAA**”) or Judicial Arbitration and Mediations Services (“**JAMS**”). As modified by these Terms, and unless otherwise agreed upon by the parties in writing, the arbitration will be governed by the AAA’s or JAMS’s rules for commercial arbitration and, if the arbitrator deems them applicable, the procedures for consumer-related disputes.          (c) You are thus GIVING UP YOUR RIGHT TO GO TO COURT to assert or defend your rights EXCEPT for matters that may be taken to small claims court. Your rights will be determined by a NEUTRAL ARBITRATOR and NOT a judge or jury. You are entitled to a FAIR HEARING, BUT the arbitration procedures are SIMPLER AND MORE LIMITED THAN RULES APPLICABLE IN COURT. Arbitrator decisions are as enforceable as any court order and are subject to VERY LIMITED REVIEW BY A COURT.          (d) You and we must abide by the following rules: (1) ANY CLAIMS BROUGHT BY YOU OR US MUST BE BROUGHT IN THE PARTIES’ INDIVIDUAL CAPACITY, AND NOT AS A PLAINTIFF OR CLASS MEMBER IN ANY PURPORTED CLASS OR REPRESENTATIVE PROCEEDING; (2) THE ARBITRATOR MAY NOT CONSOLIDATE MORE THAN ONE PERSON’S CLAIMS, MAY NOT OTHERWISE PRESIDE OVER ANY FORM OF A REPRESENTATIVE OR CLASS PROCEEDING, AND MAY NOT AWARD CLASS-WIDE RELIEF; (3) in the event that you are able to demonstrate that the costs of arbitration will be prohibitive as compared to costs of litigation, we will pay as much of your filing and hearing fees in connection with the arbitration as the arbitrator deems necessary to prevent the arbitration from being cost-prohibitive as compared to the cost of litigation; (4) we also reserve the right in our sole and exclusive discretion to assume responsibility for all of the costs of the arbitration; (5) the arbitrator shall honor claims of privilege and privacy recognized at law; (6) the arbitrator’s award shall be final and may be enforced in any court of competent jurisdiction; (7) the arbitrator may award any individual relief or individual remedies that are permitted by applicable law; and (8) each side pays its own attorneys’ fees and expenses unless there is a statutory provision that requires the prevailing party to be paid its fees’ and litigation expenses, and then in such instance, the fees and costs awarded shall be determined by the applicable law.          (e) Notwithstanding the foregoing, either you or we may bring an individual action in small claims court. Further, claims of infringement or misappropriation of the other party’s patent, copyright, trademark, or trade secret rights shall not be subject to this arbitration agreement. Such claims shall be exclusively brought in the state or federal courts located in San Francisco County, California. Additionally, notwithstanding this agreement to arbitrate, either party may seek emergency equitable relief before the state or federal courts located in San Francisco, California in order to maintain the status quo pending arbitration, and hereby agree to submit to the exclusive personal jurisdiction of the courts located within San Francisco County, California for such purpose. A request for interim measures shall not be deemed a waiver of the right to arbitrate.          (f) With the exception of subparts (1) and (2) in Section 12.2(d) above (prohibiting arbitration on a class or collective basis), if any part of this arbitration provision is deemed to be invalid, unenforceable or illegal, or otherwise conflicts with these Terms, then the balance of this arbitration provision shall remain in effect and shall be construed in accordance with its terms as if the invalid, unenforceable, illegal or conflicting provision were not contained herein. If, however, either subparts (1) and (2) in Section 12.2(d) (prohibiting arbitration on a class or collective basis) is found to be invalid, unenforceable or illegal, then the entirety of this arbitration provision shall be null and void, and neither you nor we shall be entitled to arbitration. If for any reason a claim proceeds in court rather than in arbitration, the dispute shall be exclusively brought in state or federal court in San Francisco County, California.          (g) Notwithstanding any provision in these Terms to the contrary, if we seek to terminate the Dispute Resolution section as included in these Terms, any such termination shall not be effective until 30 days after the version of these Terms not containing the agreement to arbitrate is posted to the Site, and shall not be effective as to any claim of which you provided Pinecone with written notice prior to the date of termination.          (h) For more information on AAA, its Rules and Procedures, and how to file an arbitration claim, you may call AAA at 800-778-7879 or visit the AAA website at http://www.adr.org. For more information on JAMS, it’s Rules and Procedures, and how to file an arbitration claim, you may call JAMS at 800-352-5267 or visit the JAMS website at http://www.jamsadr.com.          (i) Any and all controversies, disputes, demands, counts, claims, or causes of action between you and Pinecone and our employees, agents, successors, or assigns, regarding or relating to these Terms or the Site shall exclusively be governed by the internal laws of the State of California, without regard to its choice of law rules and without regard to conflicts of laws principles except that the arbitration provision shall be governed by the Federal Arbitration Act. The United Nations Convention on Contracts for the International Sale of Goods shall not apply to these Terms.      3. **Export**. The Site may be subject to U.S. export control laws and may be subject to export or import regulations in other countries. You agree not to export, reexport, or transfer, directly or indirectly, any U.S. technical data acquired from Pinecone, or any products utilizing such data, in violation of the United States export laws or regulations.      4. **Disclosures**. Pinecone is located at the address in Section 12.7. If you are a California resident, you may report complaints to the Complaint Assistance Unit of the Division of Consumer Product of the California Department of Consumer Affairs by contacting them in writing at 400 R Street, Sacramento, CA 95814, or by telephone at (800) 952-5210.      5. **Entire Terms**. These Terms constitute the entire agreement between you and us regarding use of the Site. Our failure to exercise or enforce any right or provision of these Terms shall not operate as a waiver of such right or provision. The section titles in these Terms are for convenience only and have no legal or contractual effect. The word “including” means “including without limitation.” If any provision of these Terms is, for any reason, held to be invalid or unenforceable, the other provisions of these Terms will be unimpaired and the invalid or unenforceable provision will be deemed modified so that it is valid and enforceable to the maximum extent permitted by law. These Terms, and your rights and obligations herein, may not be assigned, subcontracted, delegated, or otherwise transferred by you without Pinecone’s prior written consent, and any attempted assignment, subcontract, delegation, or transfer in violation of the foregoing will be null and void. Pinecone may freely assign these Terms. The terms and conditions set forth in these Terms shall be binding upon assignees.      6. **Copyright/Trademark Information**. Copyright © 2020, Pinecone Systems Corporation. All rights reserved. All trademarks, logos and service marks (“**Marks**”) displayed on the Site are our property or the property of other third parties. You are not permitted to use these Marks without our prior written consent or the consent of such third party which may own the Marks. All goodwill generated from the use of any Pinecone Marks will inure to Pinecone’s benefit.      7. **Contact Information:**        Pinecone Systems, Inc<br>       548 Market St<br>       PMB 19327<br>       San Francisco, CA 94104-5401<br>       info@pinecone.io ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e29e"
  },
  "title": "Frequently Asked <span>Questions</span>",
  "class": "early-access",
  "description": "We'll reach out to you directly within a week of submitting the form.",
  "- question": "How do I know if I’ve been granted early access?",
  "answer": "If accepted, we will email you to set up a short onboarding call.",
  "content": "",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2a0"
  },
  "title": "Technology Partners",
  "headline": "\"Pinecone Partners\"",
  "description": "Cohere makes it possible for every developer to harness the power of NLP by allowing them to easily generate, categorize, and organize text at a scale that was previously unimaginable.",
  "emailSubmit": "false",
  "- title": "Cohere",
  "url": "https://www.cohere.ai/",
  "image": "\"/images/cohere.png\"",
  "content": "        - title: \"Try: Semantic-search example notebook with Cohere and Pinecone\"           url: https://github.com/pinecone-io/examples/blob/master/integrations/cohere/semantic_search_trec.ipynb         - title: \"Read: Cohere integration docs\"           url: https://www.pinecone.io/docs/integrations/cohere/     - title: Deepset       url: https://www.deepset.ai/       image: \"/images/deepset.png\"       description: Deepset provides developers with the right tools to build production-ready NLP systems quickly and efficiently.       links:         - title: \"Watch: How to Build AI-powered Q&A Applications with Haystack and Pinecone\"           url: https://www.youtube.com/watch?v=ZdS_V1A5r44         - title: \"Try: Q&A example notebook with Haystack and Pinecone\"           url: https://colab.research.google.com/drive/18YAEm1jHr73S5Lo799GdyXR_EayYDWiY?usp=sharing         - title: \"Read: Haystack integration docs\"           url: https://www.pinecone.io/docs/integrations/haystack/     - title: OpenAI       url: https://www.openai.com/       image: \"/images/openai-logo.png\"       description: OpenAI is an AI research and deployment company. Their mission is to ensure that artificial general intelligence benefits all of humanity by building safe and beneficial AGI, as well as helping others achieve this outcome with their work.       links:         - title: \"Watch: Beyond Semantic Search with OpenAI and Pinecone\"           url: https://www.youtube.com/watch?v=HtI9easWtAA         - title: \"Demo: Q&A demo app made with OpenAI and Pinecone\"           url: https://share.streamlit.io/pinecone-io/playground/beyond_search_openai/src/server.py         - title: \"Read: OpenAI integration docs\"           url: https://www.pinecone.io/docs/integrations/openai/  serviceProviderPartners:   title: Service Provider Partners   description: We partner with service providers to achieve success for their customers by leveraging Pinecone solutions and expertise.    partners:     - title: Strong       url: https://www.strong.io       image: /images/strong.png       description: Strong helps top companies of all sizes in all industries design, engineer, and deploy custom end-to-end machine learning and AI products and solutions with a team that brings diverse, deep knowledge of ML/AI and state-of-the-art techniques to bring these solutions to fruition.       # links:       #   - title: Youtube Links       #     url: /       #   - title: Demo apps       #     url: /       #   - title: Documentation       #     url: /     - title: SmartCat       image: /images/smartcat.png       url: https://www.smartcat.io       description: SmartCat helps partners develop meaningful data solutions and functional data strategies for challenging business problems. We provide an end-to-end solution by having various expertise under the same roof.       # links:       #   - title: Youtube Links       #     url: /       #   - title: Demo apps       #     url: /       #   - title: Documentation       #     url: /  becomeAPartner:   title: Become A Partner   list:     - title: \"Why partner with Pinecone:\"       list:         - \"**Accelerate your go-to-market**: Reach, educate, convert, and expand more customers who are interested in vector-embedding and vector-search technology to power real-world applications.\"         - \"**Add value to your customers**: Unlock valuable use cases for your customers through compatibility and integration with the leading vector database.\"         - \"**Differentiate yourself**: Offer new and powerful solutions for your customers, leveraging cutting-edge vector database technology.\"         - \"**Take on any challenge**: Get tools, training, and technical guidance from our experts to help you confidently take on and successfully complete exciting projects.\" --- ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2a2"
  },
  "title": "\"Pinecone Technical Support Policy and Service Level Agreement\"",
  "headline": "\"Pinecone Technical Support Policy and Service Level Agreement\"",
  "cta": "true",
  "intro": "\"\"",
  "content": "Last updated: September 15, 2022  This Service Level Agreement (“**SLA**”) describes the terms under which Pinecone Systems, Inc. (“**Pinecone**”) provides technical support for its Platform to customers that have purchased access to the Pinecone Platform pursuant to any agreement providing for access to the Platform between Pinecone and customer (“**Customer**”) that references this SLA (the “**Agreement**”). This SLA will apply only to the extent expressly set forth in the Agreement. All support and other services provided under this SLA are subject to the terms and conditions of the Agreement. Any capitalized terms used but not defined herein will have the meanings prescribed to them in the Agreement.   ## 1. Definitions.  “**Applicable Monthly Service Fees**” means the total consumption fees paid by Customer for the Platform during the calendar month in which Service Downtime occurred.  “**Business Days**” means Monday through Friday, excluding US Federal Holidays.  “**Business Hours**” means 8am to 6pm Eastern US Time on Business Days.  “**Initial Response**” means an initial response to a Support Request, which, at a minimum, is made by a human agent and acknowledges receipt of the request.  “**Issue**” means a failure of the Service to conform to the specifications set forth in the Documentation.  “**Monthly Uptime Percentage**” means with respect to any particular calendar month, the total number of minutes in that month, minus the number of minutes of measured Service Downtime during that month, divided by the total number of minutes in that month. Monthly Uptime Percentage is calculated as:  Monthly Uptime Percentage = total number of minutes in a month - Service Downtime / total number of minutes in that month  When calculating the Monthly Uptime Percentage for any month in which the Platform is deployed for only part of the month, it is assumed that there was no Service Downtime during the portion of the month in which the Platform was not deployed.  “**Production Index**\" means an environment serving your end users or customers.  “**Service Credit**” is the percentage of the Applicable Monthly Service Fees to be credited to Customer if Pinecone approves Customer’s claim, as set forth in Section 3.1.    “**Service Downtime**” is measured at each Pinecone index as the total number of full minutes, outside of scheduled downtime for maintenance and upgrades, where continuous attempts to establish a connection within the minute fail as reflected in minute-by-minute logs.  “**Service Hours**” means twenty-four (24) hours per day, seven (7) days per week, including US Federal Holidays  ## 2. Service levels.  **Availability**. Pinecone will undertake commercially reasonable measures to ensure that the Monthly Uptime Percentage equals or exceeds 99.9% during each calendar month (the “**Service Standard**”).  To achieve the Monthly Uptime standard, you must maintain at least two replicas of your index.  ## 3. Remedy.  **3.1** - **Service Credits**. If Pinecone does not achieve and maintain the Monthly Uptime Percentages set forth in the table below, then Customer may be eligible for Service Credits according to the following table:  <div class=\"responsive-table pricing-table\">  | MONTHLY UPTIME PERCENTAGE |       SERVICE CREDIT       | | :------: | :-----------------: | |    Less than 99.9% and equal or greater than 99.0%   |    10% of Applicable Monthly Service Fees    | |    Less than 99.0% and equal or greater than 95.0%    |    25% of Applicable Monthly Service Fees    |  |    Less than 95.0%   |    50% of Applicable Monthly Service Fees    |   </div>  **3.2** - **Customer Obligations**. As a condition to Pinecone’s obligation to provide Service Credits to Customer, Customer must meet all the following criteria:  **(a)** Have a qualifying index on the Enterprise tier or above, maintained with at least two replicas of that index, throughout the eligible month.    **(b)** Log a support ticket with Pinecone within sixty (60) minutes of the start of a Service Downtime.   **(c)** Submit a claim by emailing Pinecone at support@pinecone.io by the end of the calendar month immediately following the month in which Downtime with occurred, with all the following required information:   1. The words \"Pinecone: Request for SLA Credit\" in the subject line; 2. A detailed description of the events resulting in Service Downtime, including Customer’s request logs that document the errors and corroborate Customer’s claimed outage (with any confidential or sensitive information in the logs removed or replaced with asterisks); 3. The index name, number of pods, number of replicas, the cloud provider, and the cloud region of the affected Pinecone index; 4. A description of your affected customers or end users; 5. Information regarding the dates, time and duration of the Service Downtime; 6. Descriptions of Customer’s attempts to resolve the Service Downtime at the time of occurrence.  **(d)** Reasonably assist Pinecone in investigating the cause of the Service Downtime and processing the claim.  **(e)** Customer must comply with the Pinecone Services Agreement, applicable Pinecone documentation and any advice from Pinecone’s support team.  If Pinecone determines that Customer has satisfied the customer obligations above and that none of the below limitations apply to Customer’s claim, Pinecone will grant Customer a Service Credit. Pinecone will apply any Service Credit to a future invoice or payment for the Pinecone Platform. Service Credits will not entitle you to any cash refund or other payment from Pinecone.  Customer’s rights under this Section 3 (Remedy) are Customer’s sole and exclusive remedy with respect to any Service Downtime or any failure by Pinecone to meet the Service Standard required by Section 2.1 (Availability).  ## 4. Exclusions.  Service Downtime excludes, and Customer will not be eligible for a Service Credit for, any performance or availability issue that results from:  **4.1** - A Force Majeure Event, a network or device failure at Customer’s site or between Customer’s site and the Pinecone Platform, or any other factors outside of Pinecone’s reasonable control;  **4.2** - Services, hardware, or software provided by a third party, such as cloud platform services (e.g., AWS, GCP) on which the Pinecone Platform runs;  **4.3** - Customer’s failure to timely pay Service Fees owed and due to Pinecone for use of the Services;  **4.4** - Customer’s failure to be in full compliance with the terms of the Agreement with Pinecone for use of the Services, as well as applicable Pinecone documentation, best practices, and advice from the Pinecone support team; or Customer’s or any third party’s (a) improper use, scaling or configuration of the Pinecone Platform, (b) attempts at modifications to the Pinecone Platform, or (c) failure to follow appropriate security practices;  **4.5** - Impact to your ability to access or use the Pinecone console, an interface provided to manage services, is excluded. This includes any component and content linked from the Pinecone console, including Documentation, the Customer Support portal, and Monitoring. These components operate independently from database services and do not impact database availability;  **4.6** - Pinecone’s beta services.  ## 5. Pinecone Technical Support Policy.  This Pinecone Technical Support Policy (“**Policy**”).   **5.1** - **Generally**. Technical support is intended to cover the provision of support to Customer’s authorized support contacts for technical issues with the Pinecone Platform that are not covered in the Pinecone documentation. Customer may designate up to two (2) individuals for Standard Support, and four (4) individuals for Premium Support, at any time with appropriate technical expertise that are familiar with the Platform as Customer’s designated support contacts (“**Customer Representative**”).   **5.2** - Technical support does not include, and Pinecone will not be required to provide any assistance related to: (a) use of the Platform other than in accordance with the documentation; (b) use of the Platform in violation of the Agreement; (c) issues that result from a Force Majeure Event or any other factors beyond Pinecone’s reasonable control; (d) the integration or communication of Customer systems with the Pinecone Platform or Services.  **5.3** - **Severity Levels and Response Time**:  Pinecone offers three levels of support policies - Community, Standard and Premium. Each policy is defined by its own Uptime Credit, contact method, support hours, number of allowed customer representatives and the Initial Response time, according to the following table:  <div class=\"responsive-table pricing-table\">  |                   |       Community       |       Standard        |       Premium      | | :---------------: | :-------------------: | :-------------------: | :----------------: | |    **Uptime Credits** |           N/A         |            N/A        | Production Indexes are entitled to service credits as detailed in Section 3.1. | |    **Contact Method** | [Community Forum](https://community.pinecone.io/) | [Web Portal](https://support.pinecone.io/), [Email](mailto:support@pinecone.io) |  [Web Portal](https://support.pinecone.io/), [Email](mailto:support@pinecone.io)| |    **Support Hours**  |           N/A         |  Sev 1: Service Hours<br>Sev 2-4: Business Hours | Sev 1-2: Service Hours<br>Sev 3-4: Business Hours  | |    **Max Customer Representatives** |    N/A  |            2          |          4         | |    **Initial Response Times***   |    N/A    | Sev 1: 4 Hours<br>Sev 2: 8 Business Hours<br>Sev 3: 24 Business Hours<br>Sev 4: 48 Business Hours | Sev 1: 1 Hours<br>Sev 2: 4 Hours<br>Sev 3: 12 Business Hours<br>Sev 4: 24 Business Hours |  </div> *For the avoidance of doubt, nothing in these initial response times or in this Support Policy guarantees full resolution of a reported Issue.  **Severity Level Definitions**  - Sev 1: Production Index is down or is severely impacted such that routine operation is impossible, with no workaround available. - Sev 2: Production Index Issue where system is functional but offers service in degraded or restricted capacity.  - Sev 3: Production Index has minor impact or has Issue in development, but Customer can still access and use most functionality of the Service, or situation may be temporarily circumvented using an available workaround. - Sev 4: No production impact, questions or requests for features.  **5.4** - **Support Requests**. Pinecone will provide 24x7, 365 days support For Sev 1 and Sev 2 requests under Premium Support and Sev 1 requests under Standard Support. All other requests will be responded to during Business Hours. Pinecone provides a variety of ways for Customer to request help or otherwise make inquiries, including:   - Web Portal: an online support module that may be used to report and track issues. Pinecone requests that Customer use this system as the first method of reporting issues and requesting support. It can be accessed through the url https://support.pinecone.io/ or via the Pinecone console by clicking ‘Help Desk’.  - Email: the online support may also be accessed via the email support@pinecone.io. However, requests received via email will be assumed to be Sev 4 at time of creation.  Support Requests should include the following information:  1. A description of the Issue you are experiencing, sufficiently detailed to allow Pinecone Support to effectively assess the Issue, including index name, and any relevant error message, and assumed Severity Level in accordance with the Support Definitions.  2. Information regarding the time and duration of the issue.  3. Descriptions of Customer’s attempts to resolve the issue at the time of occurrence. 4. Give Pinecone any other important Support Case information in a timely manner.  **5.5** - **Measurement**. Support Services response times are measured by Pinecone from the time the Support Request is received by Pinecone via the contact methods above.  **5.6** - While Pinecone will make commercially reasonable efforts to correct defects or other errors in the Platform and respond to reported incidents of Service Downtime in accordance with Pinecone’s policies and procedures for correcting verified, reproducible errors in the Platform, Customer acknowledges that it may not be possible for Pinecone to correct every or any defect, error, or problem reported by Customer or of which Pinecone is otherwise made aware. ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2a4"
  },
  "draft": "true",
  "title": "One of the <span>world's largest social media platforms</span> increased user enagement with Pinecone",
  "intro": "\"Search through billions of items for similar matches to any object, in milliseconds. It’s the next generation of search, an API call away.\"",
  "# preload": "/images/pinecone-product-diagram-wide.png",
  "# preloadsetX1": "/images/pinecone-product-diagram-wide.png",
  "# preloadsetX2": "/images/pinecone-product-diagram-wide-2x.png",
  "description": "uptime",
  "href": "'/pdfs/pinecone-factsheet.pdf'",
  "text": "Download datasheet",
  "src": "/images/pinecone-product-diagram-6.png",
  "alt": "Pinecone Diagram",
  "width": "447",
  "height": "548",
  "eyebrow": "Customer Success",
  "- href": "'/docs/examples/it-threat-detection/'",
  "- eyebrow": "Fully Managed",
  "maxWidth": "50%",
  "left": "true",
  "- title": "99.9%",
  "content": "",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2a6"
  },
  "title": "Newsletter Signup",
  "description": "|",
  "emailSubmit": "true",
  "inputText": "Email Address...",
  "buttonText": "Submit",
  "content": "",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2a8"
  },
  "title": "Perception Point",
  "intro": "\"The #1 vector database. Search through billions of items for similar matches to any object, in milliseconds. It’s the next generation of search, an API call away.\"",
  "announcement": "⭐️ <a href=\"/learn/pinecone-aws-marketplace/\">Now available on the AWS Marketplace</a>",
  "description": "\"Pinecone made it fast and easy to take vector search from research to production, without DevOps. Thanks to their performance and scalability we can provide accurate, reliable, and real-time threat detection to our customers.\"",
  "href": "/contact/",
  "text": "Pinecone is SOC 2 Type II certified, GDPR-ready, and built to keep data secure.",
  "src": "/images2/pinecone-diagram.svg",
  "alt": "\"co:here\"",
  "width": "447",
  "height": "548",
  "eyebrow": "Managed Vector Database",
  "- title": "Secure",
  "icon": "\"/images/secure.svg\"",
  "logo": "/images/perception-point-logo.svg",
  "url": "https://perception-point.io",
  "authorName": "Oded K.",
  "authorTitle": "R&D Group Lead",
  "authorImage": "/images/oded-kalev.jpeg",
  "authorUrl": "https://www.linkedin.com/in/oded-kalev-9a4667ba/",
  "- logo": "/images/cohere.png",
  "content": "",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2aa"
  },
  "title": "\"Trust and Security\"",
  "headline": "\"Trust and Security\"",
  "cta": "true",
  "intro": "\"\"",
  "content": "Some of the largest enterprises trust Pinecone to store their data and power their critical applications. We work hard to earn and maintain that trust by treating security and reliability as a cornerstone of our company and product. We extend this to all our users and customers, regardless of size and industry.  Since its founding in 2019, Pinecone has been developed by engineers and scientists experienced in building secure and reliable systems at companies like Amazon, Yahoo, and Google. Pinecone is based in California, United States.  ---  ## Data Safeguards Pinecone runs on fully managed and secure AWS infrastructure as a multi-tenant Kubernetes cluster. * Customer data is stored in isolated containers. * Customer data is encrypted at rest and in transit. * Customer data is never used for any reason other than servicing API calls. * Pinecone only monitors operational metrics to support the operational health and performance of the system. * Strict role based access control (RBAC) for service engineers.   ## Additional Safeguards for Dedicated-Cloud Deployments Enterprise customers enjoy all the safeguard above and additional security measures  * A dedicated AWS account for complete resource isolation. * A dedicated, single-tenant, Kubernetes cluster. * Complete network isolation from the internet. * AWS CloudTrail is enabled for audit logging. * The above holds for all data including vector data and metadata.   [Contact us](/contact/) for complete deployment options.  ## SOC2 Type II Pinecone is SOC2 Type II certified. The certification is based on the COSO framework and has been audited by an external Big4 CPA firm (EY). The scope of the program includes Information Security, Availability, and Confidentiality.  ## GDPR Pinecone is committed to supporting customers in their GDPR compliance efforts, and has undertaken the necessary steps to be GDPR-ready. The [Website Privacy Policy](/privacy/), [Customer Data Protection Addendum](/dpa/), and [Product Privacy Statement](/product-privacy/) detail how Pinecone collects, uses, and protects information.  ## Penetration Tests Pinecone routinely undergoes third-party security reviews and remediates findings according to their criticality and prioritization. Security personnel can request executive summaries of findings by contacting info@pinecone.io.  ## Policies, Guidelines, and Practices for Protecting Data Pinecone information assets and systems are classified into public and confidential, including a subset of Pinecone Confidential Information which is “Pinecone Third Party Confidential” information. This is confidential information belonging or pertaining to Pinecone customers or another corporation.  The use of these assets is subject to an Acceptable Use Policy which includes user accounts, passwords, media use, email and communication activities, and other such procedures.  Access Control is based on a policy that instructs relevant employees of the company about methods of access control management and user authorizations in the information systems of the company.  HR policies and procedures define the proper ways to address various security issues in Human Resources management, prior to employment (screening, interviewing, background checks), during employment, and at the time of termination of employment (i.e. off-boarding).  Pinecone follows Software Development Lifecycle (SDLC) best practices. Pinecone has a procedure that defines the process for change control in Pinecone's systems and services in its production environment, relating to development, implementation, operations, and IT issues.  ## Employee Access Lifecycle Pinecone addresses various security issues in Human Resources management, prior to employment, during employment, and at the termination of employment. Onboarding includes data security training and adherence to the requirements set in the Information Security Policy, Acceptable Use Policy, and Code of Conduct. The business unit owner and Pinecone IT provide the employee only with the relevant access rights, according to his or her work profile and role.   For an employee or an employee transferred to a new position, the CTO will provide the authorizations accordingly with the Role-Based Access Control Matrix after receiving a trigger from HR. Any change in an employee's position in Pinecone or change in his or her access privileges is reported to HR and documented by HR.   ## Risk Assessment Process Pinecone's Risk Assessment process takes place on an annual basis to identify, assess, and manage risks that affect the company's ability to achieve its objectives. The Risk Assessment process involves identifying, assessing, and minimizing risks through ongoing monitoring and risk assessment procedures that are built into the normal recurring activities and include regular management and supervisory activities. Action plans are tracked by the COO and communicated to appropriate personnel.  ## Incident Notification Pinecone has an incident management policy, including effective identification, repairs, investigation, prevention, and follow-up actions. In case of a security incident, Pinecone’s incident management team will act and make decisions as necessary to appropriately respond to security incidents and breaches of personal data in accordance with the applicable laws and regulations.  The incident management team includes the CEO and COO and all relevant employees as decided by the CEO and COO. Wherever a security incident of either a physical or electronic nature is suspected or confirmed, all parties are expected to follow appropriate procedures and instructions given by the incident management team.  ## Monitoring Pinecone aggregates production environment audit logs from various components such as Kubernetes, storage, and networking. Some of them are analyzed automatically (e.g. GuardDuty) while others are reviewed manually on a regular basis for signs of intrusion.  * Code Vulnerability Scanner: Pinecone performs weekly scans of its code base using a service that provides fix suggestions for any discovered vulnerabilities. Pinecone engineers promptly address any critical issues. * External Vulnerability Scanner: Pinecone uses a service to scan production environments at least once a quarter for network vulnerabilities. * Events Threat Detections: Pinecone’s production environment audit logs are archived and analyzed.  ## Questions [Contact us](/contact/) for more information related to Pinecone trust and security. ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2ac"
  },
  "title": "Rewriting a high performance vector database in Rust",
  "headline": "Rewriting a high performance vector database in Rust",
  "name": "Jack Pertschuk",
  "position": "Engineering Manager",
  "src": "/images/jack-pertschuk.jpg",
  "href": "https://www.linkedin.com/in/jack-pertschuk-833196114/",
  "date": "\"2022-09-14\"",
  "thumbnail": "\"/images/rust-rewrite.jpg\"",
  "description": "Rewriting a high performance vector database in Rust",
  "images": "[\"/images/rust-rewrite.jpg\"]",
  "content": "![August’s Rust NYC Meetup](/images/rust-rewrite.jpg)  I recently spoke at the [Rust NYC meetup group](https://www.meetup.com/rust-nyc/) about the Pinecone engineering team’s experience rewriting our vector database from Python and C++ to Rust. The event was very well attended (178+ registrations), which just goes to show the growing interest in Rust and its applications for real-world products. Below is a recap of what I discussed, but make sure to check out the [full recording](https://www.youtube.com/watch?v=zv6bXXQmMqA) if interested in learning more.   ## Introduction to Pinecone - why are we here?  Data lakes, ML Ops, feature stores - these are all common buzzwords trying to solve similar sorts of problems. For example, let’s say you have a lot of unstructured data, and in order to gain insights you store it in blob storage. Historically, you would use an ML Ops platform, like a hosted Spark pipeline, for this. However, in many ways, we’re seeing the industry start to transition to the concept of vector databases and specifically approximate nearest neighbor (ANN) search to support similar use cases.   ![Vector Databse](/images/rust-rewrite-vector-database.png)  [Pinecone](/) is a fully managed, SaaS solution for this piece of the puzzle - [the vector database](/learn/vector-database/). While the concept of the vector database has been used by many large tech companies for years, these sorts of companies have built their own proprietary, deep learning ANN indexing algorithms to serve news feeds, advertisements, and recommendations. These infrastructures and algorithms require intensive resources and overhead that most companies can’t support. With its strict memory management, efficient multi-threading, and fast, reliable performance, this is where the Pinecone solution comes in.  ## Ramping up with Rust  Pinecone was originally written in C++ with a connectivity wrapper written in Python. While this worked well for a while, we began to run into issues.   First of all, Python is a garbage collected language, which means it can be extremely slow for writing anything high performance at scale. In addition, it’s challenging to find developers with experience in both Python and C++. And so the idea of iterating on the database was born - we wanted to find some way to unify our code base while achieving the performance predictability we needed.  We looked at and compared several languages - Go, Java, C++, and Rust. We knew that C++ was harder to scale and maintain high quality as you build a dev team; that Java doesn't provide the flexibility and systems programming language we needed; and that Go is also a garbage collected language. This left us with Rust. With Rust, the pros around performance, memory management, and ease of use outweighed the cons of it not yet being a very established language.   ## Identifying bottlenecks   ### Continuous benchmarking  As we began ramping up with Rust, we ran into a few bottlenecks. Before shipping the newly rewritten database, we wanted to ensure it continued to scale easily and have predictable performance. How did we test this? With continuous benchmarking.   Continuous benchmarking allowed us to see every commit broken down by the performance of a specific benchmark test. Through HTML reports, we are able to see the exact commit that caused the regression of the debt anytime a code change is merged.   ![Continuous benchmarking](/images/continuous-benchmarking.png)  As you can see in the above graph, a commit was merged that caused a huge spike. However, with Criterion, an open source benchmarking tool, we were easily able to identify it, mitigate it, and push a fix. And over time, we lowered our latency and shipped improvements.  ### Building an observability layer   At this point, we’ve confirmed that the new database is performant, and have benchmarks to run it against. But what happens when you go to production, and things are slower than they should be? This is when you need an observability solution.   Adding an observability layer with Rust can be complicated without the support of a more mature developer community. As a result, we wanted a solution with minimal instrumentation, that’s easy to integrate, and is cloud agnostic. Our end goal was to provide a layer compatible with Datadog, Prometheus or any other metrics provider.   There are two main components to our observability layer - traces and aggregated metrics. With each of these signals, you can see how each part of the code is performing over time.   How did we achieve this? For metrics, we used some macros for histogram and counter metrics. We also used a custom Rust macro that hooks into OpenMetrics, and from there we can push metrics to Prometheus or Datadog. For tracing, we took a similar approach. We implemented an OpenTelemetry protocol that allows us to send traces to any observability solution. This way we’re able to see all of our metrics and trace requests as graphs in a single dashboard (see the below example).   ![Dashboard graphs](/images/dashboard-graphs.png)  ## Optimizing performance with Rust  After identifying and addressing the above bottlenecks, we were able to focus on optimizing performance. With Rust, there are several aspects around achieving high performance that we liked - low level optimized instruction sets, memory layout, and running async tasks.   ### Optimized instruction sets   One of the things we considered when choosing Rust was its access to low level optimized instruction sets, which are critical for optimizing the kind of vector based workloads that Pinecone utilizes. So for example, AVX-512 allows us to utilize parallel dot-product to compute high throughput dot-product queries on anything. And Rust gives us direct access to these compiler optimizations.  ### Memory layout  If you're using a higher level language, you're not going to have access to how the memory is laid out. A simple change, like removing indirection in our list, was an order of magnitude improvement in our latencies since there's memory prefetching in the compiler and the CPU can anticipate which vectors are going to be loaded next in order to improve the memory footprint.  ### Running async tasks   Rust is async, and [Tokio](https://tokio.rs/) is the one of the most popular async providers. It's performant, ergonomic, and has options for running on a single event loop. However, it's not great for running CPU intensive workloads, like with Pinecone.   When it comes to running these tasks, there are many options. For example, because Tokio has different runtime modes, you can run it by itself in this async mode with multiple threads. And in that context, you can block on an individual thread in place, which is called 'block_in_place'. You can also use 'spawn_blocking'.   There are also “smart” work, parallel processing libraries, like [Rayon](https://github.com/rayon-rs/rayon), that maintain a thread pool and implement things like work stealing. And finally there's the option of your own solution. If you want more control, you can use MPSC channels. While you have to write some custom code, they give you the fine grained ability to schedule work and ensure data locality.   ## What’s next for Pinecone?   We are continuing to optimize our codebase to ensure we’re maintaining a highly performant, stable, and fast database. This recap highlights the key points discussed during the meetup, but make sure to [watch the full recording](https://www.youtube.com/watch?v=zv6bXXQmMqA) for more detail.   If you are interested in learning more about Pinecone and vector databases, check out the resources on our [learn page](/learn/) or [try it out](https://app.pinecone.io/) (it's free). Also, if you’re currently using or interested in working with Rust, [we are hiring](/careers/#open-roles).   {{< youtube zv6bXXQmMqA >}} ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2ae"
  },
  "content": "categories:   - Company News toc: >- author:   name: Gibbs Cullen   position: Senior Product Marketing Manager   src: /images/gibbs-cullen.jpg   href: https://www.linkedin.com/in/gibbscullen/ date: \"2022-12-07\" # Open Graph description: With organizations, users and teams can now collaborate across projects in a way that works best for them. images: [\"/images/organizations-announcement-1v2.png\"] thumbnail: \"/images/organizations-announcement-thumbnail.png\" ---  Prior to organizations, projects and billing within Pinecone could only be owned and managed by a single user. But when responsibilities change, you may want more than one user to manage projects and billing so operations continue to run smoothly. Or as new projects arise, you may want additional users to make contributions or test out a new application.  With organizations, teams can easily define roles to manage access and billing for a set of projects. Every project is assigned to an organization with an organization owner, and all projects within an organization have the same billing method. In addition, a project’s users can easily transfer ownership amongst themselves and consolidate usage across projects within an organization for billing purposes. Users and teams can now collaborate across projects in a way that works best for them.  As of today, organizations are available to all users and support four predefined roles: Organization Owner, Organization User, Project Owner, and Project User. Note that all existing users and projects have been assigned to an organization by default.  ## How it works  To get started with organizations, navigate to “Settings” in the left-hand menu of the Pinecone Console, then click on “Organization” to see your current organization. You can then set the desired plan type and billing method using the “Billing” tab under the organization view.  ![Organization Settings](/images/organizations-announcement-2.png)  If you are a [project](https://preview.redoc.ly/pinecone/orgs/manage-projects) or [organization](https://preview.redoc.ly/pinecone/orgs/organizations) owner, you can also add users to organizations and projects with a few easy steps:  1. In the Settings view, click the “Users” tab.  2. Click “Invite User”.    {{< image src=\"/images/organizations-announcement-3.png\" alt=\"Invite user\" width=\"auto\">}}  3. (Organization owner only) Select an [organization role](https://preview.redoc.ly/pinecone/orgs/organizations#organization-roles).  4. Select a project and [project role](https://preview.redoc.ly/pinecone/orgs/projects#project-roles).  5. Enter the user's email address, and “Invite User”.  Each role has a different level of access - at both an organization and project level. Note that everyone within an organization is an organization user by default.  - **Organization owner**: Manages organization settings and billing, and creates and manages all project and index settings. - **Organization user**: Read-only access to organization settings, but can create a project within an organization and manage project permissions. - **Project owner**: Manages project settings, API keys, and the index. - **Project user**: Read-only access to API keys, but can manage the index.  ## Get started  Learn more about this feature and how to get started in our [documentation](/docs/add-users-to-projects-and-organizations/). ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2b0"
  },
  "title": "\"Readers for Question-Answering\"",
  "headline": "\"Reader Models for Open Domain Question-Answering\"",
  "weight": "9",
  "name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "How to fine-tune reader models to identify answers from relevant contexts.",
  "images": "['/images/reader-models-0.jpg']",
  "content": "Open-domain question-answering (ODQA) is a wildly popular *pipeline* of databases and language models that allow us to ask a machine human-like questions and return comprehensible and even intelligent answers.  Despite the outward guise of simplicity, ODQA requires a reasonably advanced set of components placed together to enable the *extractive* Q&A functionality.  We call this *extractive* Q&A because the models are not generating an answer. Instead, the answer already exists but is hidden somewhere within potentially thousands, millions, or even more data sources.  By enabling extractive Q&A, we enable a more *intelligent* and *efficient* way to retrieve information from what can be massive stores of data.  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/-fzCSPsfMic\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ODQA relies on three components: the vector database, for storing encoded vector representations of the data we will search, a retriever to handle context and question encoding, and a reader model that consumes relevant *retrieved* contexts and identifies a shorter, more specific answer.  The reader is the final act in an ODQA pipeline; it takes the contexts returned by the vector database and retriever components and *reads* them. Our reader will then return what it believes to be the *specific answer* to our question.  To be exact, we don't get the 'specific answer'. The model is reading *input IDs*, which are integers representing words or subwords. So, rather than returning a human-readable text answer, it actually returns a *span* of input ID positions.  ![question_context_answer](/images/reader-models-1.jpg) <small>Question (grey), context (cyan), and answer (blue). The model doesn't read the strings. It reads token IDs, and so when outputting a prediction for the answer, it outputs a span of token IDs that it believes represent the answer.</small>  To fine-tune a model, we need two inputs and two labels. The inputs are the question and a relevant context, and the labels are the answer's start and end positions.  ![inputs_and_labels](/images/reader-models-2.jpg) <small>Inputs (cyan) and target labels (answer start and end positions, blue). The start and end positions are the token positions from the encoded question-context *input_ids* tensor that represent the start and end of the answer (extracted from the context).</small>  There isn't much more to fine-tuning a reader model. It’s a relatively straightforward process. The most complex part is pre-processing the training data.  With our overview complete, let's dive into the details and work through an actual training example.    ## Implementation  There are more steps when training a reader model than just *train the model*. As mentioned, these other steps can prove to be the tricky part. In our case, we have three distinct steps.  1. Download and pre-process Q&A dataset 2. Fine-tune the model 3. Evaluation  Without any further ado, let's begin with the data.  ### Download and Pre-process  We will be using the **S**tanford **Q**uestion and **A**nswering **D**ataset (SQuAD) for fine-tuning. We can download it with HuggingFace *Datasets*.  {{< notebook file=\"download-squad\" height=\"full\" >}}  Looking at this, we have *five* features, of which we only care about `question`, `context` for the inputs, and `answers` for the labels.  {{< notebook file=\"squad-0\" height=\"full\" >}}  We must make a few transformations to format the `answers` into the start and end token ID positions we need. We have `answer_start`, but this gives us the position within the context *string* that the answer begins. These positions are not what the model needs. Instead, it requires the start position using token ID indexes.  {{< notebook file=\"squad-ans-extract\" height=\"full\" >}}  That is our main hurdle. To push through it, we will take three steps:  1. Tokenize the context. 2. Convert `answer_start` to a token ID index. 3. Find the end token index using the starting position and answer `text`.  Starting with **tokenize the context**, we first initialize a tokenizer using the HuggingFace *Transformers* library.  {{< notebook file=\"tokenize\" height=\"full\" >}}  Then we tokenize our question-context pairs, and this returns *three* tensors by default:  * `input_ids`, the token ID representation of our text. * `attention_mask` a list of values telling our model whether to apply the attention mechanism to respective token embeddings with `1` or to ignore padding token positions with `0`. * `token_type_ids` indicates sentence A (the question) with the first set of `0` values, sentence B (the context) with `1` values, and remaining padding tokens with the trailing `0` values.  We have added another tensor called `offset_mapping` by setting `return_offsets_mapping=True`. This tensor is very important for finding our label values for training our model.  Earlier, we found the start and end positions for the *character* positions from our *context* string. As mentioned, we cannot use these. We need the token positions, and the `offset_mapping` tensor is essential in finding the token positions.  {{< notebook file=\"decode-example\" height=\"full\" >}}  Another consideration when finding the token position is that when we tokenized, we tokenized both the question *and* context as shown above where we follow the format `[CLS] question [SEP] context [SEP] padding`. To find the answer start and end positions, we must shift the values by the length of the question segment.  To find the question and context segment lengths, we use the `token_type_ids` tensor.  {{< notebook file=\"get-lengths\" height=\"full\" >}}  We need to consider one additional case where the answer has been truncated or never existed (some records have no answer). In both of these scenarios, we set the start and end positions to `0`.  {{< notebook file=\"char-to-id\" height=\"full\" >}}  Once we have the start and end positions, we need to define how we will load the dataset into our model for training. At the moment, our dataset will return lists of dictionaries for each training batch.  We cannot feed lists of dictionaries into our model. Instead, we need to pull these dictionaries into single batch-size tensors. For that, we use the `default_data_collator` function.  {{< notebook file=\"prep-data\" height=\"full\" >}}  We don't need to do anything else with our dataset or data collator for now, so we move on to the next step of fine-tuning.  ### Fine-tuning the Model  As mentioned, we will be fine-tuning the model using the HuggingFace *Transformers* `Trainer` class. To use this, we first need a model to fine-tune, which we load as usual with transformers.  {{< notebook file=\"init-model\" height=\"full\" >}}  Next, we set up the `Trainer` training parameters.  {{< notebook file=\"training-params\" height=\"full\" >}}  We use tried and testing training parameters used in the first BERT for QA with SQuADv2 paper *and* Deepset AI's BERT training parameters, we set a learning rate of `2e-5`, `0.1` weight decay, and train in batches of `24` for `3` epochs [1] [2].  {{< notebook file=\"trainer-train\" height=\"full\" >}}  Like we said, fine-tuning the model is the easy part. We can find our model files in the directory defined in the `args` parameter, in this case, `./bert-base-uncased-squad2`. We will see a set of folders named `checkpoint-x` in this directory. The last of those is the *latest* model checkpoint saved during training.  ![model_dir](/images/reader-models-3.jpg) <small>Model and tokenizer files in the `/bert-reader-squad2` model directory.</small>  By default, a new checkpoint is saved every 500 steps. These checkpoint saves mean the *final* model (at step 27,150) is not the final model but rather the model at step 27,000.  There is unlikely to be a noticeable difference between these two states, so we either take the model files from `./bert-base-uncased-squad2/checkpoint-24000` or we manually save our model with:  {{< notebook file=\"save-model\" height=\"full\" >}}  We can find the model files in the specified directory.  ### Inference  Before moving on to the next step of evaluation, let's take a look at how we can use this model.  First, we initialize a transformers `pipeline`.  {{< notebook file=\"init-pipeline\" height=\"full\" >}}  Next, we prepare the evaluation data. Again we will use the `squad_v2` dataset from HuggingFace, taking the *validation* split.  {{< notebook file=\"val-split\" height=\"full\" >}}  The `pipeline` requires an iterable set of key-value pairs where the only keys are `question` and `context`. We can simply drop the unneeded columns of `id` and `title` to handle this. However, we will need to keep track of the true answers during the next step of *evaluation*, so we store them in a separate `ans` dataset.  {{< notebook file=\"ans-dataset\" height=\"full\" >}}  To make a prediction, we take a single *question* and *context* and feed them into our pipeline `qa`:  {{< notebook file=\"qa-example\" height=\"full\" >}}  We'll process the whole dataset like this in the next section.  ## Evaluation  We've technically finished fine-tuning our model, but it's not of much use if we can't validate its performance. We need confidence in the model's performance.  Evaluation of our reader model is a little tricky as we want to identify matches between true and predicted answer labels. The most straightforward approach is to use an **E**xact **M**atch metric. This metric will simply tell us `1` if the true and predicted answers are *precisely* the same or `0` if not.  There are two reasons we might want to avoid this and try something more flexible. First, we may find that a model predicts the correct answer, but when decoded, the predicted tokens are in a slightly different format.  The second reason is that our model might predict a *partially correct* answer and partially correct is better than nothing, but this *better than nothing* isn't accounted for by the EM metric.  {{< notebook file=\"em-v-rouge\" height=\"full\" >}}  We can solve the first issue in *most cases* by normalizing both the true and predicted answers, meaning we lowercase, remove punctuation, and remove any other potential points of conflict.  The second problem requires a more sophisticated solution, and it is best if we *do not* use the EM metric. Instead, we use *ROUGE*.  There are a few different ROUGE metrics. We will focus on ROUGE-N, which measures the number of matching *n-grams* between the predicted and true answers, where an n-gram is a grouping of tokens/words.  The *N* in ROUGE-*N* stands for the number of tokens/words within a single n-gram. This means that ROUGE-1 compares individual tokens/words (unigrams), ROUGE-2 compares tokens/words in chunks of two (bigrams), and so on.  ![ngrams](/images/reader-models-4.jpg) <small>Example of unigram, bigram, and trigram which are single-token, double-token, and triple-token groupings respectively.</small>  Either way, we return a score of `1` for an exact match, `0` for no match, or any value in between.  {{< notebook file=\"rouge-examples\" height=\"full\" >}}  To apply ROUGE-1 for measuring reader model performance, we first need to *predict* answers using our model. We can then compare these predicted answers to the true answers.  {{< notebook file=\"get-results\" height=\"full\" >}}  Finally, given the two sets of answers, we can call `rouge.get_scores` to return recall `r`, precision `p`, and F1 `f` scores for both uni and bi-grams.  We still need to deal with where there is no answer and that the SQuAD evaluation set contains four possible answers for each sample.  {{< notebook file=\"question-examples\" height=\"full\" >}}  We could check if the model correctly predicted that no answer exists for the ‘no answer’ scenario. If the model correctly identifies that there is no answer, we would return a score of *1.0*. Otherwise, we would return a score of *0.0*.  We will calculate the ROUGE-1 F1 score for every possible answer to deal with the multiple answers and take the best score.  After calculating all scores, we take the average value. This average value is the final ROUGE-1 F1 score for the model.  | Model                              | ROUGE-1 F1 | | ---------------------------------- | ---------- | | `bert-reader-squad2`               | 0.354      | | `deepset/bert-base-uncased-squad2` | 0.450      |  These scores seem surprisingly low. A big reason for this is the *no answer scenarios*. Let's take a look at a few.  {{< notebook file=\"unanswerable\" height=\"full\" >}}  If, like me, you're wondering how these are unanswerable, take note of the particular question and context wording. The first example specifies the 1000s and 1100s, but the context is the 10th and 11th centuries, e.g., 1100s and 1200s. The second example question should be *\"**destructive incursions** devolved into **encampments**\"*. The third should be *\"draining **mines**\"*.  Even by humans, each of these questions is easily mistaken as answerable. If we remove unanswerable examples, the model scores are less surprising.  | Model                              | ROUGE-1 F1 | | ---------------------------------- | ---------- | | `bert-reader-squad2`               | 0.708      | | `deepset/bert-base-uncased-squad2` | 0.901      |  The importance of identifying unanswerable questions varies between use cases. Many will not need to identify unanswerable questions, so question whether your models should prioritize unanswerable question identification or focus on performing well on answerable questions.    That’s it for this walkthrough in fine-tuning reader models for ODQA pipelines. By understanding how to fine-tune a QA reader model, we are able to effectively optimize the final step in the ODQA pipeline for our own specific use cases.  Pairing this with a custom vector database and retriever components allows us to add highly optimized ODQA capabilities to a variety of possible use cases, such as internal document search, e-commerce product discovery, or anything where a more natural information retrieval experience can be beneficial.  ## References  [1] Y. Zhang, Z. Xu, [BERT for Question Answering on SQuAD 2.0](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/default/15848021.pdf) (2019)  [2] [Model Card for *deepset/bert-base-uncased-squad2*](https://huggingface.co/deepset/bert-base-uncased-squad2), HuggingFace Model Hub  ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2b2"
  },
  "title": "\"Spotify's Podcast Search Explained\"",
  "headline": "\"How Spotify Uses Semantic Search for Podcasts\"",
  "weight": "1",
  "name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "How Spotify is reimagining podcast discovery with semantic search.",
  "images": "[\"/images/spotify-podcast-search-0.jpg\"]",
  "thumbnail": "\"https://www.pinecone.io/images/spotify-podcast-search-0.jpg\"",
  "content": "**Want to add audio search to your applications just like Spotify? You'll need a vector database like [Pinecone](/). [Try it now for free.](https://app.pinecone.io)**  The market for podcasts has grown tremendously in recent years, with the number of global listeners having increased by 20% annually in recent years [1].  Driving the charge in podcast adoption is Spotify. In a few short years, they have become the undisputed leaders in podcasting. Despite only entering the game in 2018, by late 2021, Spotify had already usurped Apple, the long-reigning leader in podcasts, with more than 28M monthly podcast listeners [2]  To back their podcast investments, Spotify has worked on making the podcast experience as seamless and accessible as possible. From their all-in-one podcast creation app (Anchor) to podcast APIs and their latest *natural language enabled* podcast search.  Spotify's natural language search for podcasts is a fascinating use case. In the past, users had to rely on keyword/term matching to find the podcast episodes they wanted. Now, they can search in natural language, in much the same way we might ask a real person where to find something.  This technology relies on what we like to call *semantic search*. It enables a more intuitive search experience because we tend to have an *idea* of what we're looking for, but rarely do we know precisely which terms appear in what we want.  Imagine we wanted to find a podcast talking about healthy eating over the holidays. How would we search for that? It might look something like:  ![podcast-search](/images/spotify-podcast-search-1.png)  There is a podcast episode talking about precisely this. Its description is:  ``` \"Alex Straney chats to Dr. Preeya Alexander about how to stay healthy over Christmas and about her letter to patients.\" ```  We have zero overlaps between the query and episode description using term matching, so this result would not be returned using keyword search. To make matters worse, there are undoubtedly thousands of episode descriptions on Spotify containing the words *\"eat\"*, *\"better\"*, and *\"holidays\"*. These episodes likely have nothing to do with our intended search query, but we could return them.  Suppose we were to swap that for a semantic search query. We could see much better results because semantic search looks at the meaning of the words and sentences, *not* specific terms.  Despite sharing no words, our query and episode description would be identified as having very similar meanings. They both describe *being or eating healthier over the winter holidays*.  Enabling meaningful search is not easy, but the impact can be huge if done well. As Spotify has proven, it can lead to a much greater user experience. Let's dive into how Spotify built its natural language podcast search.  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/ok0SDdXdat8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ## Semantic Search  The technology powering Spotify's new podcast search is more widely known as semantic search. Semantic search relies on two pillars, [**N**atural **L**anguage **P**rocessing (NLP)](/learn/nlp/) and [vector search](/learn/vector-search-basics/).  These technologies act as two steps in the search process. Given a natural language query, a particular NLP model can encode it into a [vector embedding](/learn/vector-embeddings/), also known as a [*dense vector*](/learn/dense-vector-embeddings-nlp/). These dense vectors can numerically represent the meaning of the query. We can visualize this behaviour:  <div>                        <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>         <script src=\"https://cdn.plot.ly/plotly-2.11.1.min.js\"></script>                <div id=\"a86fded0-6adc-4826-86d4-c0f215e40c4c\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a86fded0-6adc-4826-86d4-c0f215e40c4c\")) {                    Plotly.newPlot(                        \"a86fded0-6adc-4826-86d4-c0f215e40c4c\",                        [{\"customdata\":[[\"how do I cook great food\"],[\"interview with cookbook author\"],[\"chat with chef who wrote books\"],[\"podcast about cooking and writing\"],[\"the writing show\"],[\"eat better during xmas holidays\"],[\"superhero film analysis\"],[\"how to tell more engaging stories\"],[\"how to keep readers interested\"],[\"how to make money with online content\"],[\"why is technology so addictive\"]],\"hovertemplate\":\"text=%{customdata[0]}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#1C17FF\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"scene\":\"scene\",\"showlegend\":false,\"x\":[0.4778360334823838,0.3877692637150937,0.3124091180014456,0.389321831339922,-0.05578718560486039,0.19325670270367676,-0.3115741989711601,-0.3135352627882797,-0.35675623425504904,-0.29536054403167183,-0.42757952359150037],\"y\":[-0.17568194026164283,0.14845017300523397,0.16943416332590094,0.11031489848600957,0.22348277549895443,-0.42691002814007695,-0.4709285642813247,0.2510819837829344,0.47394553650664173,-0.023354030051592516,-0.27983496787103823],\"z\":[-0.05635501984033207,0.18334881332576225,0.08162569729764292,0.07194730746878393,0.21446302600133849,-0.5250254378046134,0.5492251168919206,-0.17995528106009603,-0.1520339801885487,-0.029103124835297464,-0.1581371172565605],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"x\"}},\"yaxis\":{\"title\":{\"text\":\"y\"}},\"zaxis\":{\"title\":{\"text\":\"z\"}}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    )                };                            </script>        </div>  These vectors have been encoded by one of these special NLP models, called [*sentence transformers*](/learn/sentence-embeddings/). We can see that queries with similar meanings cluster together, whereas unrelated queries do not.  Once we have these vectors, we need a way of comparing them. That is where the *vector search* component is used. Given our new query vector, we perform a vector search and compare it to previously encoded vectors and retrieve those that are nearest or the most similar.  ![podcast-vec-search-distance](/images/spotify-podcast-search-2.png)  <small>Given a query vector *xq*, we could calculate the distance between that and other indexed vectors to identify the top two [nearest \"neighbors\"](/learn/k-nearest-neighbor/).</small>  NLP and vector search have been around for some time, but recent advancements have acted as catalysts in the performance increase and subsequent adoption of semantic search. In NLP, we have seen the introduction of high-performance [transformer models](/learn/transformers/). In vector search, the rise of **A**pproximate **N**earest **N**eighbor (ANN) algorithms.  Transformers and ANN search have powered the growth of semantic search, but *why* is not so clear. So, let's demystify how they work and why they've proven so helpful.  ### Transformers  Transformer models have become the standard in NLP. These models typically have two components: the core, which focuses on *\"understanding\"* the meaning of a language and/or domain, and a head, which adapts the model for a particular use case.  There is just one problem, the core of these models requires vast amounts of data and computing power to pretrain.  ---  *Pretraining refers to the training step applied to the core transformer component. It is followed by a fine-tuning step where the head and/or the core are trained further for a specific use case.*  ---  One of the most popular transformer models is BERT, and BERT costs a reported $2.5k - 50K to train; this shifts to \\$80K - \\$1.6M for the larger BERT model [4].  These costs are prohibitive for most organizations. Fortunately, that doesn't stop us from using them. Despite these models being expensive to pretrain, they are an order of magnitude cheaper to *fine-tune*.  The way that we would typically use these models is:  1. The core of the transformer model is pretrained at great cost by the likes of Google, Microsoft, etc. 2. This core is made publicly available. 3. Other organizations take the core, add a task-specific *\"head\"*, and fine-tune the extended model to their domain-specific task. Fine-tuning is less computationally expensive and therefore cheaper. 4. The model is now ready to be applied to the organization's domain-specific tasks.  In the case of building a podcast search model, we could take a pretrained model like `bert-base-uncased`. This model already \"understands\" general purpose English language.  Given a training dataset of *user query* to *podcast episode* pairs, we could add a *\"mean pooling\"* head onto our pretrained BERT model. With both the core and head, we fine-tune it for a few hours on our pairs data to create a *sentence* transformer trained to identify similar query-episode pairs.  We must choose a suitable pretrained model for our use case. In our example, if our target query-episode pairs were English language only, it would make no sense to take a French pretrained model. It has no base understanding of the English language and could not learn to understand the English query-episode pairs.  Another term we have mentioned is *\"sentence transformer\"*. This term refers to a transformer model that has been fitted with a pooling layer that enables it to output single vector representations of sentences (or longer chunks of text).  ![podcast-pooling](/images/spotify-podcast-search-3.png)  <small>Sentence transformers add a *\"pooling layer\"* to transform the many token embeddings output by a transformer model into a single sentence embedding.</small>  There are different types of pooling layers, but they all consume the same input and produce the same output. They take many token-level embeddings and merge them in some way to build a single embedding that represents *all* of those token-level embeddings. That single output is called a *sentence embedding*.  The sentence embedding is a *dense vector*, a numerical representation of the meaning behind some text. These dense vectors enable the *vector search* component of semantic search.  ### ANN Search  **A**pproximate **N**earest **N**eighbors (ANN) search allows us to quickly compare millions or even billions of vectors. It is called *approximate* because it does not guarantee that we will find the true nearest neighbors (most similar embeddings).  The only way we can guarantee that is by exhaustively comparing every single vector. At scale, that's slow.  Rather than comparing *every* vector, we approximate with ANN search. If done well, this approximation can be incredibly accurate and super fast. But there is often a trade-off. Some algorithms offer speedier search but poorer accuracy, whereas others may be more accurate but increase search times.  ![podcast-search-balance](/images/spotify-podcast-search-4.png)  <small>In vector search, there is often a decision to be made on whether to prioritize latency or accuracy.</small>  In either case, an approximate solution is required to maintain reasonable query times at scale.    ## How Spotify Did It  To build this type of semantic search tool, Spotify needed a language model capable of encoding similar *(query, episode)* pairs into a similar vector space. There are existing sentence transformer [models like SBERT](/learn/train-sentence-transformers-softmax/), but Spotify found two issues with using this model:  * They needed a model capable of supporting multilingual queries; SBERT was trained on English text only. * SBERT's cross-topic performance *without* further fine-tuning is poor [5].  With that in mind, they decided to use a different, multilingual model called the **U**niversal **S**entence **E**ncoder (USE). But this still needed fine-tuning.  To fine-tune their USE model to encode *(query, episode)* pairs in a meaningful way, Spotify needed *(query, episode)* data. They had *four* sources of this:  1. Using their past search logs, they identified *(query, episode)* pairs from successful searches. 2. They identified unsuccessful searches that were followed by a successful search. The idea is that the unsuccessful query is likely to be a more *natural* query, which was then used as a *(query_prior_to_successful_reformulation, episode)* pair. 3. Generating synthetic queries using a query generation model produces *(synthetic_query, episode)* pairs. 4. A small set of curated queries, manually written for episodes.  Sources (1 - 3) fine-tune the USE model, with some samples left for evaluation. Source (4) was used for evaluation only.  Unfortunately, we don't have access to Spotify's past search logs, so there's little we can do in replicating sources (1 - 2). However, we can replicate the approach of the building source (3) using query generation models. And, of course, we can manually write queries as per source (4).  ### Data Preprocessing  Before generating any queries, we need episode data. Spotify describes *episodes* as a concatenation of textual metadata fields, including episode title and description, with the podcast show's title and description.  We can find a [podcast episodes dataset](https://www.kaggle.com/datasets/listennotes/all-podcast-episodes-published-in-december-2017) on Kaggle that contains records for 881k podcast episodes i. Including episode titles and descriptions, with podcast show titles and descriptions.  We use the Kaggle API to download this data, installed in Python with `pip install kaggle`. An account and API key are needed (find the API key in your *Account Settings*). The *kaggle.json* API key should be stored in the location displayed when attempting to `import kaggle`. If no location or error appears, the API key has already been added.  We then authenticate access to Kaggle.  ```python from kaggle.api.kaggle_api_extended import KaggleApi api = KaggleApi() api.authenticate() ```  Once authenticated, we can download the dataset using the `dataset_download_file` function, specifying the dataset location (found from its URL), files to download, and where to save them.  ```python api.dataset_download_file(     'listennotes/all-podcast-episodes-published-in-december-2017',     file_name='podcasts.csv',     path='./' ) api.dataset_download_file(     'listennotes/all-podcast-episodes-published-in-december-2017',     file_name='episodes.csv',     path='./' ) ```  Both *podcasts.csv* and *episodes.csv* will be downloaded as zip files, which we can extract using the `zipfile` library.  ```python with zipfile.ZipFile('podcasts.csv.zip', 'r') as zipref:     zipref.extractall('./') with zipfile.ZipFile('episodes.csv.zip', 'r') as zipref:     zipref.extractall('./') ```  We have two CSV files, *podcasts.csv* details the podcast shows themselves, including titles, descriptions, and hosts. The *episodes.csv* data includes data from specific podcast episodes, including episode title, description, and publication date.  To replicate Spotify's approach of concatenating podcast shows and episode-specific details, we must merge the two datasets. We do this with an inner join on the podcast ID columns.  ```python episodes = episodes.merge(     podcasts,     left_on='podcast_uuid',     right_on='uuid',     suffixes=('_ep', '_pod') ) ```  Before concatenating the features we want, let's clean up the data. We strip excess whitespace and remove rows where *any* of our relevant features contain null values.  {{< notebook file=\"podcast-clean-data\" height=\"full\" >}}  We're ready to concatenate, giving us our *episodes* feature.  {{< notebook file=\"podcast-concat-shuffle\" height=\"full\" >}}  ### Query Generation  We now have episodes but no queries, and we need *(query, episode)* pairs to fine-tune a model. Spotify generated synthetic queries from episode text, which we can do.  To do this, they fine-tuned a query generation BART model using the MS MARCO dataset. We don't need to fine-tune a BART model as plenty of readily available models have been fine-tuned on the exact same dataset. Therefore, we will initialize one of these models using the HuggingFace *transformers* library.  ```python from transformers import T5Tokenizer, T5ForConditionalGeneration  # after testing many BART and T5 query generation models, this seemed best model_name = 'doc2query/all-t5-base-v1'  tokenizer = T5Tokenizer.from_pretrained(model_name) model = T5ForConditionalGeneration.from_pretrained(model_name).cuda() ```  We tested several T5 *and* BART models for query generation on our episodes data; the [results are here](https://github.com/pinecone-io/examples/blob/master/search/semantic-search/spotify-podcast-search/query-gen.md). The `doc2query/all-t5-base-v1` model was chosen as it produced more reasonable queries and has some multilingual support.  It's time for us to generate queries. We will generate three queries per episode, in-line with the approach taken by the [GenQ](/learn/genq/) and [GPL](/learn/gpl/) techniques.  {{< notebook file=\"podcast-query-gen\" height=\"full\" >}}  Query generation can take some time, and we recommend limiting the number of episodes (we used 100k in this example). Looking at the generated queries, we can see some good and some bad. This randomness is the nature of query generation and should be expected.  We now have *(synthetic_query, episode)* pairs that can be used in fine-tuning a sentence transformer model.  ### Models and Fine-tuning  As mentioned, Spotify considered using pretrained models like BERT and SBERT but found the performance unsuitable for their use case. In the end, they opted for a pretrained **U**niversal **S**entence **E**ncoder (USE) model from TFHub.  We will use a similar model called DistilUSE that is supported by the *sentence-transformers* library. By taking this approach, we can use the *sentence-transformers* model fine-tuning utilities. After installing the library with `pip install sentence-transformers`, we can initialize the model like so:  ```python from sentence_transformers import SentenceTransformer  model = SentenceTransformer('distiluse-base-multilingual-cased-v2') ```  When fine-tuning with the sentence-transformers library, we need to reformat our data into a list of `InputExample` objects. The exact format does vary by training task.  We will be using a ranking function (more on that soon), so we must include two text items, the *(query, episode)* pairs.  ```python from sentence_transformers import InputExample  eval_split = int(0.01 * len(pairs)) test_split = int(0.19 * len(pairs))  # we separate a number of these for testing test_pairs = pairs[-test_split:] pairs = pairs[:-test_split]           # and take a small number of samples for evaluation eval_pairs = pairs[-eval_split:] pairs = pairs[:-eval_split]  train = []  for (query, episode) in pairs:     train.append(InputExample(texts=[query, episode])) ```  We also took a small set of evaluation (`eval_pairs`) and test set pairs (`test_pairs`) for later use.  As mentioned, we will be using a ranking optimization function. That means that the model is tasked with learning how to identify the *correct episode* from a batch of episodes when given a specific *query*, e.g., *ranking* the correct pair above all others.  ![podcast-ranking](/images/spotify-podcast-search-5.png)  <small>Given a query and set of episodes, the model must learn how to embed them so that the relevant episode embedding is the most similar to the query embedding.</small>  The model achieves this by embedding similar *(query, episode)* pairs as closely as possible in a vector space. We measure the proximity of these embeddings using *cosine similarity*, which is essentially the angle between embeddings (e.g., vectors).  ![podcast-vec-search-cosine](/images/spotify-podcast-search-6.png)  <small>When using cosine similarity, we are searching for embeddings that have the shortest angular distance, rather than Euclidean distance.</small>  As we are using a ranking optimization function, we must make sure no duplicate queries or episodes are placed in the same training batch. If there are duplicates, this will confuse the training process as the model will be told that despite two queries/episodes being identical, one is correct, and the other is not.  The sentence-transformers library handles the duplication issue using the `NoDuplicatesDataLoader`. As the name would suggest, this data loader ensures no duplicates make their way into a training batch.  We initialize the data loader with a `batch_size` parameter. A larger batch size makes the ranking task harder for the model as it must identify one correct answer from a higher number of options.  It is harder to choose an answer from a hundred samples than from four samples. With that in mind, a higher `batch_size` tends to produce higher performance models.  ```python from sentence_transformers.datasets import NoDuplicatesDataLoader  batch_size = 64  loader = NoDuplicatesDataLoader(train, batch_size=batch_size) ```  Now we initialize the loss function. As we're using ranking, we choose the `MultipleNegativesRankingLoss`, typically called *MNR loss*.  ```python from sentence_transformers.losses import MultipleNegativesRankingLoss  loss = MultipleNegativesRankingLoss(model) ```  #### In-Batch Evaluation  Spotify describes two evaluation steps. The first can be implemented before fine-tuning using in-batch metrics. What they did here was calculate two metrics at the batch level (using `64` samples at a time in our case); those are:  * Recall@k tells us if the correct answer is placed in the top *k* positions. * **M**ean **R**eciprocal **R**ank (MRR) calculates the average reciprocal rank of a correct answer.  We will implement a similar approach to in-batch evaluation. Using the sentence-transformers `RerankingEvaluator`, we can calculate the MRR score at the end of each training epoch using our evaluation data, `eval_pairs`.  Before initializing this evaluator, we need to remove duplicates from the eval data.  {{< notebook file=\"podcast-eval-dedupe\" height=\"full\" >}}  Then, we reformat the data into a list of dictionaries containing a query, its *positive* episode (that it is paired with), and then all other episodes as *negatives*.  ```python from sentence_transformers.evaluation import RerankingEvaluator  # we must format samples into a list of: # {'query': '<query>', 'positive': ['<positive>'], 'negative': [<all negatives>]} eval_set = [] eval_episodes = [pair[1] for pair in eval_pairs]  for i, (query, episode) in enumerate(eval_pairs):     negatives = eval_episodes[:i] + eval_episodes[i+1:]     eval_set.append(         {'query': query, 'positive': [episode], 'negative': negatives}     )      evaluator = RerankingEvaluator(eval_set, mrr_at_k=5, batch_size=batch_size) ```  We set the MRR@5 metric, meaning if the positive episode is returned within the top *five* results, we return a positive score. Otherwise, the score would be zero.  ---  <em>If the correct episode appeared at position *three*, the reciprocal rank of this sample would be calculated as 1/**3**. At position *one* we would return 1/**1**.</em>  <em>As we're calculating the **mean** reciprocal rank, we take all sample scores and compute the mean, giving us our final MRR@5 score.</em>  ---  Using our evaluator, we first calculate the MRR@5 performance without any fine-tuning.  {{< notebook file=\"podcast-mrr-zero\" height=\"full\" >}}  Returning an MRR@5 of *0.68*, we will compare this to the post-training MRR@5 score.  #### Fine-Tuning  With our evaluator ready, we can fine-tune our model. The Spotify article doesn't give any information about the parameters they used, so we will stick with pretty typical training parameters for sentence transformer models using MNR loss. We train for a single epoch and *\"warm up\"* the learning rate for the first 10% of training steps.  {{< notebook file=\"podcast-model-fit\" height=\"full\" >}}  After fine-tuning, the model will be saved into the directory specified by `output_path`. In *distiluse-podcast-nq*, we will see all the required model files and a directory called *eval*. Here, we will find a post-training MRR@5 score of *0.89*, a sizeable 21-point improvement from the previous MRR@5 of *0.68*.  This score looks promising, but there's further evaluation to be performed.  ### Evaluation  We want to emulate a more *real-world* scenario for the final evaluation step. Rather than calculating MRR@5 across small batches of data (as done previously), we should index *many* episodes and recalculate some retrieval metrics.  Spotify details their *full-retrieval setting metrics* as using Recall@30 and MRR@30, performed both on queries from the eval set and on their curated dataset.  Our eval set is small, so we can discard that. Instead, we will use the much larger test set `test_pairs`.  As before, we must deduplicate the episodes from the dataset.  {{< notebook file=\"podcast-test-dedupe\" height=\"full\" >}}  This time, rather than keeping all of our embeddings stored in memory, we use a vector database, Pinecone.  We first [sign up for a free account](https://app.pinecone.io/), enter the default project and retrieve the *default* API key.  Back in Python, we ensure the Pinecone client is installed with `pip install pinecone-client`. Then we initialize our connection to Pinecone and create a new vector index.  ```python import pinecone  pinecone.init(     api_key='YOUR_API_KEY',  # app.pinecone.io     environment='YOUR_ENV'  # find next to API key in console )  # check if an evaluation index already exists, if not, create it if 'evaluation' not in pinecone.list_indexes():     pinecone.create_index(         'evaluation', dimension=model.get_sentence_embedding_dimension(),         metric='cosine'     )      # now connect to the index index = pinecone.Index('evaluation') ```  The vector index is where we will store all of our episode embeddings. We must encode the episode text using our fine-tuned `distiluse-podcast-nq` model and insert the embeddings into our index.  ```python to_upsert = [] queries = [] eps_batch = [] id_batch = [] upsert_batch = 64  for i, (query, episode) in enumerate(tqdm(test_pairs)):     # create batch     queries.append((query, str(i)))     eps_batch.append(episode)     id_batch.append(str(i))     # on reaching batch_size we encode and upsert     if len(eps_batch) == upsert_batch:         embeds = model.encode(eps_batch).tolist()         # insert to index         index.upsert(vectors=list(zip(id_batch, embeds)))         # refresh batch         eps_batch = []         id_batch = [] ```  ---  *Short on time? Download the fine-tuned model using `model = SentenceTransformer('pinecone/distiluse-podcast-nq')`.*  ---  We will calculate the *Recall@K* score, which differs slightly from the *MRR@K* metric as if the match appears in the top K returned results, we score *1*; otherwise, we score *0*. As before, we take all query scores and compute the mean.  {{< notebook file=\"podcast-synthetic-recall\" height=\"full\" >}}  So far, this looks great; 88% of the time, we are returning the exact positive episode within the top 30 results. But this does assume that our synthetic queries are perfect, which they are not.  We should measure model performance on more realistic queries, as Spotify did with their curated dataset. In this example, we have chosen a selection of episodes and manually written queries that fit the episode.  ```python curated = {     \"funny show about after uni party house\": 1,     \"interview with cookbook author\": 8,     \"eat better during xmas holidays\": 14,     \"superhero film analysis\": 27,     \"how to tell more engaging stories\": 33,     \"how to make money with online content\": 34,     \"why is technology so addictive\": 38 } ```  Using these curated samples, we returned a lower score of 0.57. Compared to 0.88, this seems low, but we must remember that there are likely other episodes that fit these queries. Meaning, we're calculating recall assuming there are no other relevant queries.  What we can do is: measure this score against the score of the model before fine-tuning. We create a new Pinecone index and replicate the same steps but using the `distiluse-base-multilingual-cased-v2` sentence transformer. You can find the [full script here](https://github.com/pinecone-io/examples/blob/master/search/semantic-search/spotify-podcast-search/walkthrough.ipynb).  Using this model, we return a score of just 0.29. By fine-tuning the model on this episode data, despite having no query pairs, we have improved episode retrieval performance by 28-points.    The technique we followed, informed by Spotify's very own semantic search implementation, produced significant performance improvements.  Could this performance be better? Of course! Spotify fine-tuned their model using *three* data sources. We can assume that the first two of those, pulled from Spotify's past search logs, are of much higher quality than our synthetic dataset.  Merging the approach we have taken with a real dataset, as done by Spotify, is almost certain to produce a significantly higher-performing model.  The world of semantic search is already huge, but what is perhaps more exciting is the potential of this field. We will continue seeing new examples of semantic search, like Spotify’s podcast search, applied in many interesting and unique ways.  If you’re using Pinecone for semantic search and are interested in [showcasing your project](https://www.pinecone.io/community/), let us know! Comment them below or email them to us at *info@pinecone.io*.    ## Resources  [Full Code Walkthrough](https://github.com/pinecone-io/examples/blob/master/search/semantic-search/spotify-podcast-search/walkthrough.ipynb)  [1] [Podcast Content is Growing Audio Engagement](https://www.nielsen.com/us/en/insights/article/2020/podcast-content-is-growing-audio-engagement/) (2020), Nielsen  [2] S. Lebow, [Spotify Poised to Overtake Apple Podcasts This Year](https://www.emarketer.com/content/spotify-poised-overtake-apple-podcasts-this-year?ecid=NL1001) (2021), eMarketer  [3] A. Tamborrino [Introducing Natural Language Search for Podcast Episodes](https://engineering.atspotify.com/2022/03/introducing-natural-language-search-for-podcast-episodes/) (2022), Engineering at Spotify Blog  [4] O. Sharir, B. Peleg, Y. Shoham, [The Cost of Training NLP Models](https://arxiv.org/abs/2004.08900) (2020)  [5] N. Reimers, I. Gurevych, [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084) (2019), EMNLP ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2b4"
  },
  "title": "\"Unsupervised Training for Sentence Transformers\"",
  "headline": "\"Unsupervised Training for Sentence Transformers\"",
  "weight": "6",
  "name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "How to create sentence transformer models without labelled data.",
  "images": "['/images/unsupervised-training-sentence-transformers-0.jpg']",
  "content": "Language represents a way for us to communicate abstract ideas and concepts. It has evolved as a human-only form of interaction for the best part of the past 100 million years. Translating that into something a machine can understand is (unsurprisingly) difficult.  Modern(ish) computers appeared during and around WW2. The first application of natural language processing (NLP) came soon after with the Georgetown machine translation (MT) experiment in 1954. In the first decade of research, many expected MT to be solvable within a few short years [1] — they were slightly *too optimistic*.  MT is still not 'solved', but that and the field of NLP have become heavily researched in the past few years, and there have been many breakthroughs. We now have some incredible language models for a stunning array of use-cases.  Much of this recent success is thanks to dense vector representations of words and sentences. These vectors are produced by language models that translate the semantic meaning of language into a numerical vector space that a computer can understand and process.  As is the trend with ML models, we need *a lot* of data and *a lot* of compute to build these models.  Sentence transformers are the current-best models for producing information-rich representations of sentences and paragraphs. The training process to create this type of model varies but begins with the unsupervised *pretraining* of a transformer model using methods like masked-language modeling (MLM).  To adapt a pretrained transformer to produce meaningful sentence vectors, we *typically* need a more supervised fine-tuning approach. We can use datasets like natural language inference (NLI) pairs, labeled semantic textual similarity (STS) data, or parallel data (pairs of translations).  For some domains and languages, such as finance and English, this data is fairly easy to find or gather. But many domains and many languages have *very little* labeled data. If you can find semantic similarity pairs for the agriculture industry, please let me know. There are many languages, such as Dhivehi, where unlabelled data is hard to find and labelled data practically non-existent.  This means you either spend a very long time gathering tens of thousands of labeled samples or you can try an unsupervised fine-tuning approach.  Unsupervised training methods for sentence transformers are not as effective as their supervised counterparts, but they *do work*. And if you have no other choice, why not?  In this article, we will introduce the concept of unsupervised fine-tuning for sentence transformers. We will learn to train these models using the unsupervised **T**ransformer-based **S**equential **D**enoising **A**uto-**E**ncoder (TSDAE) approach.  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/pNvujJ1XyeQ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ## Unsupervised Or Not?  The first thing we need to decipher is: *\"Do we really need to use unsupervised fine-tuning?\"* The answer depends on your use case and available data.  First, let's look at a few *supervised* training approaches, their use cases, and required data.  ### Natural Language Inference  Natural language inference (NLI) is the most common approach to fine-tuning a generic monolingual sentence transformer. It uses optimization functions like [softmax loss](https://www.pinecone.io/learn/train-sentence-transformers-softmax/) or [multiple negatives ranking](https://www.pinecone.io/learn/fine-tune-sentence-transformers-mnr/) to learn how to distinguish between similar and dissimilar sentences [2] [3].  An NLI dataset contains sentence pairs labeled as either (1) *entailing/inferring* each other, being (2) *neutral* (e.g., they're not necessarily related), and sometimes (3) *contradictory*.  Some methods like multiple negatives ranking only require positive *entailment* pairs but work better when we add 'hard negatives' (contradictory) pairs.  Suppose you have these 'positive' pairs of sentences. In that case, a good pretrained transformer can usually be fine-tuned to a reasonable performance with just 20,000 pairs. If you have less than this, you may still be able to successfully fine-tune — it depends on the complexity of your domain and language.  ### Semantic Textual Similarity  Semantic textual similarity (STS) is another common approach to fine-tuning generic sentence transformers. An STS dataset contains sentence pairs alongside their 'semantic similarity', given as a numeric value within a set range.  Optimization functions like cosine similarity loss can be used. The model will attempt to optimize the sentence vectors of each pair to be more-or-less similar according to a cosine similarity function.  ### Multilingual Parallel Data  When building multilingual sentence transformers, we take an already trained monolingual sentence transformer and [use a fine-tuning process called multilingual knowledge distillation](https://www.pinecone.io/learn/multilingual-transformers/) which*distills* the monolingual knowledge of the fine-tuned model and adapts it across multiple languages in a pretrained multilingual model.  To do this, you need:  * A pretrained multilingual model (it does not need to be a sentence transformer) * A *fine-tuned* monolingual sentence transformer * Parallel data, which are translation pairs from the monolingual model's language, to your multilingual target language(s).  ### Unsupervised  If your data and use-case don't seem to fit into any of the above, unsupervised fine-tuning may be the answer.  ## How TSDAE Works  There are different options for unsupervised fine-tuning of sentence transformers. One of the best performing is the **T**ransformer(-based) and **S**equential **D**enoising **A**uto-**E**ncoder (TSDAE) pretraining method developed by Kexin Wang, Nils Reimers, and Iryna Gurevych in 2021 [4].  TSDAE introduces *noise* to input sequences by deleting or swapping tokens (e.g., words). These *damaged sentences* are encoded by the transformer model into sentence vectors. Another decoder network then attempts to reconstruct the *original input* from the damaged sentence encoding.  At first glance, this may seem similar to masked-language modeling (MLM). MLM is the most common pretraining approach for transformer models. A random number of tokens are masked using a '*masking token'*, and the transformer must try to guess what is missing,  like a 'fill in the blanks' test in school.  ``` Fill in the missing words:  The miniature pet ________ became the envy of the neighborhood. ```  If you guessed `elephant`, you're correct (and possibly unhinged).  TSDAE differs in that the decoder in MLM has access to full-length word embeddings for *every single token*. The TSDAE decoder only has access to the sentence vector produced by the encoder.  ![mlm_vs_tsdae](/images/unsupervised-training-sentence-transformers-1.jpg) <small>The TSDAE (top) process outputs a *sentence vector*, which the decoder uses to predict the original text. MLM outputs token vectors, providing the decoder with much more information to use in its prediction.</small>  In the K. Wang, et al. (2021) paper, the best performing *noise* used deletion-only, with a *deletion ratio* of 0.6. To translate from token-level representation to sentence-level, the classifier token `[CLS]` embedding was used.  The TSDAE paper described five tested approaches to noise in the input data and noise ratios tested from 0.1 to 0.9. Of those, it was deletion at a ratio of 0.6 that performed best.  If you've read our previous articles on [sentence transformers](https://www.pinecone.io/learn/sentence-embeddings/), you will remember that we usually apply a mean pooling operation across transformer word embeddings to produce a single sentence embedding/vector.  ![mean_vs_cls](/images/unsupervised-training-sentence-transformers-2.jpg) <small>CLS token pooling (top) takes the CLS token vector and uses this as the sentence vector. Mean pooling (bottom) takes the average value across all token vectors to create a sentence vector.</small>  When fine-tuning with TSDAE, the performance difference between the two approaches is tiny.  | CLS | Mean | Max | | ----- | ----- | ----- | | 78.77 | 78.84 | 78.17 |  <small>Validation scores for the different pooling methods, the difference between using the CLS embedding or mean pooling all token embeddings is minimal, source [4].</small>  Rather than averaging vectors, we can take the `[CLS]` vector as is. Usually, this embedding is used to feed information into a classification head of a transformer, which, of course, classifies the *whole* input sequence. So we can see the `[CLS]` embedding as a representation of the whole sequence/sentence.    ## Fine-Tuning Walkthrough  Before we begin fine-tuning, there are a few things we need to set up:  * Training data * A pretrained model prepared for producing sentence vectors * Loss function  Let's work through each of these.  #### Training Data  When fine-tuning a model for producing sentence vectors, TSDAE requires nothing more than text data. One handy source for text data (in *many* languages) is the OSCAR corpus. We will stick with English, for which OSCAR contains 1.8TB (after deduplication),so we have enough data.  We won't be training on the entire dataset. Instead, we will gather just 100K (mostly) short sentences. To download OSCAR (not the whole thing), we will use HuggingFace's `datasets` library.  {{< notebook file=\"oscar-en\" height=\"full\" >}}  Note that we added `streaming=True`; this allows us to iteratively download samples from the OSCAR dataset rather than downloading the full 1.8TB.  {{< notebook file=\"print-oscar\" height=\"full\" >}}  Each sample in OSCAR contains an `id` and `text`. The text can be very long, with several sentences and paragraphs. Ideally, we need to split each of these into single sentences. We can do this by using a regex function that covers both period and newline characters.  {{< notebook file=\"regex-splitter\" height=\"full\" >}}  We will use `splitter` to create a list of 100K sentences to feed into TSDAE fine-tuning.  {{< notebook file=\"splitter-build\" height=\"full\" >}}  The typical PyTorch process requires creating a `Dataset` object then passing it into a `DataLoader`. In PyTorch, we need to create a function to add noise to the data (usually within the `Dataset` class). Fortunately, sentence-transformers handles this for us via the `DenoisingAutoEncoderDataset` object.  {{< notebook file=\"dataset-and-dataloader\" height=\"full\" >}}  By default, the `DenoisingAutoEncoderDataset` deletes tokens with a probability of 60% per token. Now that our training data is ready we can move on to the final pretraining phase.  #### Model and Training  As mentioned, we need a model to fine-tune. This model should already be pretrained, of which there are *plenty* of choices over at [HuggingFace models](https://huggingface.co/models).  Note that BERT seems to outperform other models after fine-tuning with TSDAE. This is possibly thanks to the next sentence prediction (NSP) pretraining task used for BERT, which learns sentence-level contexts [4]. With this in mind, we will go ahead and use `bert-base-uncased`.  {{< notebook file=\"create-model\" height=\"full\" >}}  Alongside the transformer model (BERT), we need a pooling layer to move from the usual output of 512 token vectors to a single sentence vector. The K. Wang, et al. (2021) paper recommends using the `[CLS]` token vector as the sentence vector, which we have used above. The two parts are then merged with `SentenceTransformer`.  After this, we define a loss function. Again, sentence-transformers handles this with the `DenoisingAutoEncoderLoss` class.  {{< notebook file=\"loss-func\" height=\"full\" >}}  We're now ready to begin fine-tuning. We use an Adam optimizer with a constant learning rate (no warm-up) of `3e-5` and *no* weight decay.  {{< notebook file=\"tsdae-train\" height=\"full\" >}}  Fine-tuning should not take too long, after which we can move on to model evaluation.    ### Does it Work?  The training process is undoubtedly easy to set up and run, but does it work? And, if so, how does it compare to other supervised fine-tuning methods?  To evaluate model performance, we will use the **S**emantic **T**extual **S**imilarity **b**enchmark (STSb) dataset. As before, we will use HuggingFace's `datasets` to retrieve the data.  {{< notebook file=\"get-stsb\" height=\"full\" >}}  The `label` feature contains a score from `0 -> 5` that describes how similar `sentence1` and `sentence2` are (higher is more similar). The evaluator we will be using requires scores from `0 -> 1`, so we normalize  then reformat the data to use sentence-transformers `InputExample` class.  {{< notebook file=\"format-stsb\" height=\"full\" >}}  During evaluation, we want to produce sentence vectors for `sentence1`-`sentence2` pairs and calculate their similarity. If the similarity score is close to the `label` value, great! If not, that's not as great. We can use the `EmbeddingSimilarityEvaluator` to do this.  {{< notebook file=\"eval-new-model\" height=\"full\" >}}  A score of 0.73 seems reasonable, but how does it compare to other models? Let's compare it to an untrained `bert-base-uncased`.  {{< notebook file=\"eval-orig-bert\" height=\"full\" >}}  There's clearly an improvement from untrained BERT to a TSDAE fine-tuned BERT, which is great to see. However, we know that an unsupervised approach is unlikely to compete with supervised methods.  The most popular approach (as mentioned earlier) for fine-tuning sentence transformers is with **N**atural **L**anguage **I**nference (NLI) data. The original SBERT `bert-base-nli-mean-tokens` was trained with this, and many of the highest performing models like `flax-sentence-embeddings/all_datasets_v3_mpnet-base` are too. Let's see how these two perform.  {{< notebook file=\"eval-sbert-mpnet\" height=\"full\" >}}  We can see a big difference here. Fine-tuning with TSDAE simply cannot compete in terms of performance against supervised methods.  However, the point and value of TSDAE is that it allows us to fine-tune models for use-cases where we have no data. Specific domains with unique terminology or low resource languages.  For these use-cases, a score of 0.73 from a TSDAE-trained sentence transformer after just 20 minutes of training  is incredible.    That's it for this article on the unsupervised fine-tuning of sentence transformers using TSDAE. We've explained where *not* to use TSDAE and where to use it.  We worked through the logic being TSDAE and its parallels with MLM pretraining. After working through a TSDAE fine-tuning example, we evaluated the performance of a TSDAE-trained model against models trained using common supervised methods.  Despite TSDAE producing lower performing models than other supervised methods, it opens doors for many previously inaccessible domains and languages. With nothing more than unstructured text, we’re able to build effective sentence transformers. As increasingly effective unsupervised methods are developed, we may find that the future of sentence transformers needs nothing more than unstructured text.    ## References  [1] J. Hutchins, [The History of Machine Translation in a Nutshell](https://www.translationdirectory.com/article411.htm) (2005)  [2] N. Reimers, I. Gurevych, [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084) (2019), EMNLP  [3] N. Reimers, [NLI Fine-Tuning: MultipleNegativesRankingLoss](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/nli#multiplenegativesrankingloss), Sentence Transformers on GitHub  [4] K. Wang, et al., [TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning](https://arxiv.org/abs/2104.06979) (2021), EMNLP ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2b6"
  },
  "title": "Bonus Material",
  "description": "Image embeddings for content-based information retrieval.",
  "author": "James Briggs and Laura Carnevali",
  "intro": "|",
  "emailSubmit": "true",
  "socialShare": "true",
  "image": "/images/image-search-ebook.png",
  "images": "['/images/image-search-ebook.png']",
  "introChapter": "",
  "text": "A deep dive on diffuser models",
  "- title": "And more...",
  "url": "/learn/zero-shot-object-detection-clip",
  "bonusSection": "# Bonus content / further materials",
  "content": "",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2b8"
  },
  "title": "\"What are Vector Embeddings?\"",
  "headline": "\"What are <span>Vector Embeddings?</span>\"",
  "weight": "2",
  "name": "Rajat Tripathi",
  "position": "Software Engineer",
  "src": "/images/company-rajat.jpeg",
  "href": "https://www.linkedin.com/in/rajat-tripathi-08/",
  "description": "A gentle introduction to vector embeddings with key terms, use cases, and examples.",
  "images": "['/images/vector_embeddings.jpg']",
  "content": "## Introduction  Vector embeddings are one of the most fascinating and useful concepts in machine learning. They are central to many NLP, recommendation, and search algorithms. If you've ever used things like recommendation engines, voice assistants, language translators, you've come across systems that rely on embeddings.  ML algorithms, like most software algorithms, need numbers to work with. Sometimes we have a dataset with columns of numeric values or values that can be translated into them (ordinal, categorical, etc). Other times we come across something more abstract like an entire document of text. We create vector embeddings, which are just lists of numbers, for data like this to perform various operations with them. A whole paragraph of text or any other object can be reduced to a vector. Even numerical data can be turned into vectors for easier operations.  ![Vector Embeddings are a list of numbers](/images/vector_embeddings.jpg)  But there is something special about vectors that makes them so useful. This representation makes it possible to translate [semantic similarity](https://en.wikipedia.org/wiki/Semantic_similarity) as perceived by humans to proximity in a [vector space](https://en.wikipedia.org/wiki/Vector_space).  In other words, when we represent real-world objects and concepts such as images, audio recordings, news articles, user profiles, weather patterns, and political views as vector embeddings, the semantic similarity of these objects and concepts can be quantified by how close they are to each other as points in vector spaces.  Vector embedding representations are thus suitable for common machine learning tasks such as clustering, recommendation, and classification.  ![Semantic similarity in sentence embeddings.](/images/sentence_embeddings.png) <small>Source: [DeepAI](https://deepai.org/publication/in-search-for-linear-relations-in-sentence-embedding-spaces)</small>  For example, in a clustering task, clustering algorithms assign similar points to the same cluster while keeping points from different clusters as dissimilar as possible. In a recommendation task, when making recommendations for an unseen object, the recommender system would look for objects that are most similar to the object in question, as measured by their similarity as vector embeddings. In a classification task, we classify the label of an unseen object by the major vote over labels of the most similar objects.  ## Creating Vector Embeddings  One way of creating vector embeddings is to engineer the vector values using domain knowledge. This is known as feature engineering. For example, in medical imaging, we use medical expertise to quantify a set of features such as shape, color, and regions in an image that capture the semantics. However, engineering vector embeddings requires domain knowledge, and it is too expensive to scale.  Instead of engineering vector embeddings, we often train models to translate objects to vectors. A deep neural network is a common tool for training such models. The resulting embeddings are usually high dimensional (up to two thousand dimensions) and dense (all values are non-zero). For text data, models such as [Word2Vec](https://en.wikipedia.org/wiki/Word2vec), [GLoVE](https://en.wikipedia.org/wiki/GloVe_(machine_learning)), and [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) transform words, sentences, or paragraphs into vector embeddings.  Images can be embedded using models such as [convolutional neural networks (CNNs)](https://en.wikipedia.org/wiki/Convolutional_neural_network), Examples of CNNs include [VGG](https://arxiv.org/abs/1409.1556), and [Inception](https://arxiv.org/abs/1409.4842). Audio recordings can be transformed into vectors using image embedding transformations over the audio frequencies visual representation (e.g., using its [Spectrogram](https://en.wikipedia.org/wiki/Spectrogram)).  ## Example: Image Embedding with a Convolutional Neural Network  Consider the following example, in which raw images are represented as greyscale pixels. This is equivalent to a matrix (or table) of integer values in the range ``0`` to ``255``. Wherein the value ``0`` corresponds to a black color and ``255`` to white color. The image below depicts a greyscale image and its corresponding matrix.  ![Grayscale image representation](/images/Image_matrix_source_syyeung_cvweb.png) <small>Source: [Serena Young](https://ai.stanford.edu/~syyeung/)</small>  The left sub-image depicts the grayscale pixels, the middle sub-image contains the pixel grayscale values, and the rightmost sub-image defines the matrix. Notice the matrix values define a vector embedding in which its first coordinate is the matrix upper-left cell, then going left-to-right until the last coordinate which corresponds to the lower-right matrix cell.  Such embeddings are great at maintaining the semantic information of a pixel’s neighborhood in an image. However, they are very sensitive to transformations like shifts, scaling, cropping and other image manipulation operations. Therefore they are often used as raw inputs to learn more robust embeddings.  Convolutional Neural Network (CNN or ConvNet) is a class of deep learning architectures that are usually applied to visual data transforming images into embeddings.  CNNs are processing the input via hierarchical small local sub-inputs which are termed receptive fields. Each neuron in each network layer processes a specific receptive field from the former layer. Each layer either applies a [convolution](https://en.wikipedia.org/wiki/Convolution) on the receptive field or reduces the input size, which is called subsampling.  The image below depicts a typical CNN structure. Notice the receptive fields, depicted as sub-squares in each layer, service as an input to a single neuron within the preceding layer. Notice also the subsampling operations reduce the layer size, while the convolution operations extend the layer size. The resulting vector embedding is received via a fully connected layer.  ![Typical CNN architecture](/images/Typical_cnn_source_wikipedia.png \"Image source: https://upload.wikimedia.org/wikipedia/commons/6/63/Typical_cnn.png\") <small>Source: [Aphex34, CC BY-SA 4.0](https://commons.wikimedia.org/wiki/File:Typical_cnn.png)</small>   Learning the network weights (i.e., the embedding model) requires a large set of labeled images. The weights are being optimized in a way that images with the same labels are embedded closer compared to images with different labels. Once we learn the CNN embedding model we can transform the images into vectors and store them with a K-Nearest-Neighbor index. Now, given a new unseen image, we can transform it with the CNN model, retrieve its k-most similar vectors, and thus the corresponding similar images.  Although we used images and CNNs as examples, vector embeddings can be created for any kind of data and there are multiple models/methods that we can use to create them.  ## Using Vector Embeddings  The fact that embeddings can represent an object as a dense vector that contains its semantic information makes them very useful for a wide range of ML applications.  [Similarity search](/learn/what-is-similarity-search/) is one of the most popular uses of vector embeddings. Search algorithms like KNN and ANN require us to calculate distance between vectors to determine similarity. Vector embeddings can be used to calculate these distances. Nearest neighbor search in turn can be used for tasks like de-duplication, recommendations, anomaly detection, reverse image search, etc.  Even if we don't use embeddings directly for an application, many popular ML models and methods internally rely on them. For example in [encoder-decoder architectures](https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346), embeddings produced by encoder contain the necessary information for the decoder to produce a result. This architecture is widely used in applications, such as machine translation and caption generation.  Check out [some applications](/docs/examples/) you can build with vector embeddings and Pinecone.  {{< newsletter text=\"Subscribe for more vector search tutorials!\" inputText=\"Email address...\" buttonText=\"Submit\">}} ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2ba"
  },
  "title": "\"Making YouTube Search Better with NLP\"",
  "headline": "\"Making YouTube Search Better with NLP\"",
  "weight": "1",
  "name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "Learn how to build better video search using NLP semantic search",
  "images": "[\"/images/youtube-search-0.png\"]",
  "thumbnail": "\"https://www.pinecone.io/images/youtube-search-0.png\"",
  "content": "YouTube is a cultural phenomenon. The first video *\"Me at the zoo\"* was uploaded in 2005. It is a 19 second clip of YouTube's co-founder Jawed Karim at the zoo. This was a uniquely ordinary insight into another person's life, and, back then, this type of content had not really been seen before.  Today's world is different. 30,000 hours of video are uploaded to YouTube *every hour*, and more than one *billion* hours of video are watched daily \\[1\\]\\[2\\].  Technology and culture have advanced and become ever more entangled. Some of the most significant technological breakthroughs are integrated so tightly into our culture that we never even notice they’re there.  One of those is AI-powered search. It powers your Google results, Netflix recommendations, and ads you see everywhere. It is being rapidly weaved throughout all aspects of our lives. Further, this is a new technology; its full potential is unknown.  This technology weaves directly into the cultural phenomenon of YouTube. Imagine a search engine like Google that allows you to rapidly access the billions of hours of YouTube content. There is no comparison to that level of highly engaging video content in the world [3].  [*All supporting notebooks and scripts can be found here*](https://github.com/pinecone-io/examples/tree/master/search/semantic-search/yt-search).  ### Data for Search  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/FzLIIwiaXSU\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  To power this technology, we will need data. We will use the [YTTTS Speech Collection dataset from Kaggle](https://www.kaggle.com/datasets/ryanrudes/yttts-speech?resource=download). The dataset is organized into a set of directories containing folders named by video IDs.  Inside each video ID directory, we find more directories where each represents a timestamp start and end. Those timestamp directories contain a *subtitles.txt* file containing the text from that timestamp range.  ![yttts-dataset-structure](/images/youtube-search-1.png) <small>Dataset directory structure. Containing video IDs > timestamps > subtitles.</small>  We can extract the transcriptions, their start/end timestamps, and even the video URL (using the ID).  The original dataset is excellent, but we do need to make some changes for it to better suit our use case. The code for downloading and processing this [dataset can be found here](https://github.com/pinecone-io/examples/blob/master/search/semantic-search/yt-search/00_data_build.ipynb).  ---  *If you prefer, this step can be skipped by downloading the processed dataset with:*  ```python from datasets import load_dataset  # pip install datasets  ytt = load_dataset(     \"pinecone/yt-transcriptions\",     split=\"train\",     revision=\"926a45\" ) ```  ---  First, we need to extract the data from the *subtitles.txt* files. We do this by iterating through the directory names, structured by video IDs and timestamps.  {{< notebook file=\"yt-search-make-data\" height=\"full\" >}}  We now have the *core* data for building our search tool, but it would be nice to include video titles and thumbnails in search results.  Retrieving this data is as simple as scraping the title and thumbnail for each record using the `url` feature and Python's *BeautifulSoup* package.  {{< notebook file=\"yt-search-get-meta\" height=\"full\" >}}  We need to merge the data we pulled from the YTTTS dataset and this metadata.  {{< notebook file=\"yt-search-merge-data\" height=\"full\" >}}  That leaves us with *11298* sentence-to-paragraph length video transcriptions. Using this, we're now ready to move on to developing the video search pipeline.  ## Retrieval Pipeline  Our video search relies on a subdomain of NLP called semantic search. There are many approaches to semantic search, at a high-level this is the retrieval of *contexts* (sentences/paragraphs) that seem to answer a *query*.  ![indexing-querying](/images/youtube-search-2.png) <small>Indexing and querying pipeline with the retriever and vector database components.</small>  Retrieving contexts requires two components, a *vector database* and a *retriever* model, both of which are used for indexing and retrieving data.  ### Vector Database  The vector database acts as our data storage and retrieval component. It stores vector representations of our text data that can be retrieved using another vector. We will use the Pinecone vector database.  Although we use a small sample here, any meaningful coverage of YouTube would require us to scale to billions of records. Pinecone's vector database allows this through **A**pproximate **N**earest **N**eighbors **S**earch (ANNS). Using ANNS, we can restrict our search scope to a small subset of the index, avoiding the excessive complexity of comparing (potentially) billions of vectors.  To initialize the database, we sign up for a [free Pinecone API key](https://app.pinecone.io/) and `pip install pinecone-client`. Once ready, we initialize our index with:  ```python import pinecone  # pip install pinecone-client  # connect to pinecone (get API key and env at app.pinecone.io) pinecone.init(api_key=\"YOUR_API_KEY\", environment=\"YOUR_ENV\") # create index pinecone.create_index( \t'youtube-search',   \tdimension=768, metric='cosine' ) # connect to the new index index = pinecone.Index('youtube-search') ```  When creating the index, we pass:  * The index name, here we use `'youtube-search'` but it can be anything. * Vector `dimension`, the dimensionality of vector embeddings stored in the index, must align with the *retriever* dimensionality (more on this soon). * Retrieval `metric`, describing the method for calculating the proximity of vectors here we use `'cosine'` similarity, which aligns to the retriever output (again, more later).  We have our index, but we're missing a key detail. How do we go from the transcription text we have now to vector representations for our vector database? We need a retriever model.  ### Retriever Model  The retriever is a transformer model specially trained to embed sentences/paragraphs into a meaningful vector space. By meaningful, we expect sentences with similar semantic meaning (like question-answer pairs) to be placed into the model and embedded into a similar vector space.  ![retriever_vecs](/images/youtube-search-3.png) <small>The retriever model encodes semantically related phrases into a similar vector space.</small>  From this, we can place these vectors into our vector database. When we have a query, we use the same retriever model to create a query vector. This query vector is used to retrieve the most similar (already indexed) context vectors.  ![sim_search](/images/youtube-search-4.png) <small>When given a query vector, the vector database handles the search and retrieval of similar context vectors.</small>  We can load a [pre-existing retriever model](https://huggingface.co/flax-sentence-embeddings/all_datasets_v3_mpnet-base) from the *sentence-transformers* library (`pip install sentence-transformers`).  {{< notebook file=\"yt-search-init-retriever\" height=\"full\" >}}  Now we can see the model details, including that it outputs vectors of dimensionality `768`. This does not include the similarity metric that the model is optimized to use. That information can often be found via the [model card] (TK link) (if in doubt, cosine is most common).  ### Indexing  We can begin embedding and inserting our vectors into the vector database with both our vector database and retriever initialized. We will do this in batches of `32`.  {{< notebook file=\"yt-search-upsert\" height=\"full\" >}}  Once we're finished indexing our data, we can check that all records have been added using `index.describe_index_stats()` or via the [Pinecone dashboard](https://app.pinecone.io/).  ![pinecone-dashboard](/images/youtube-search-5.png) <small>We can see the index details from the [Pinecone dashboard](https://app.pinecone.io).</small>  ## Querying  Everything has been initialized and indexed. All that is left to do is query. To do this, we create a query like `\"what is deep learning?\"`, embed it using our retriever, and query via `index.query`.  {{< notebook file=\"yt-search-query\" height=\"full\" >}}  Within the `index.query` method, we pass our query vector `xq`, the *top_k* number of similar context vectors to return, and that we'd like to return metadata.  Inside that metadata, we have several important features: `title`, `url`, `thumbnail`, and `start_second`. We can build a user-friendly interface using these features and a framework like Streamlit with [straightforward code](https://github.com/pinecone-io/examples/blob/master/search/semantic-search/yt-search/app.py).  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/youtube-search-6.mp4\" type=\"video/mp4\"> </video> <small>Streamlit built YouTube search demo, try it yourself <a href=\"https://share.streamlit.io/pinecone-io/playground/yt-search/src/server.py\">here</a>.</small>      The fields of NLP and vector search are experiencing a renaissance as increasing interest and application generate more research, which fuels even greater interest and application of the technology.  In this walkthrough, we have demoed one use case that, despite its simplicity, can be incredibly useful and engaging. As the adoption of NLP and vector search continues to grow, more use cases will appear and embed themselves into our daily lives, just as Google search and Netflix recommendations have done in the past, becoming an ever-greater influence in the world.    ## Resources  [Article Notebooks and Scripts](https://github.com/pinecone-io/examples/tree/master/search/semantic-search/yt-search)  [1] L. Ceci, [Hours of video uploaded to YouTube every minute](https://www.statista.com/statistics/259477/hours-of-video-uploaded-to-youtube-every-minute/) (2022), Statistica  [2] C. Goodrow, [You know what's cool? A billion hours](https://blog.youtube/news-and-events/you-know-whats-cool-billion-hours/) (2017), YouTube Blog  [3] A. Hayes, [State of Video Marketing report](https://www.wyzowl.com/video-marketing-statistics/) (2022), Wyzowl ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2bc"
  },
  "title": "\"Getting Started with Hybrid Search\"",
  "headline": "\"Getting Started with Hybrid Search\"",
  "weight": "16",
  "name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "Getting started with hybrid search in Pinecone.",
  "images": "[\"/images/hybrid-search-intro-0-alt.png\"]",
  "thumbnail": "\"https://www.pinecone.io/images/hybrid-search-intro-0-alt.png\"",
  "content": "Vector search has unlocked the door to another level of relevance and efficiency in information retrieval. In the past year, the number of vector search use cases has exploded, showing no signs of slowing down.  The capabilities of vector search are impressive, but it isn't a perfect technology. In fact, without big domain-specific datasets to fine-tune models on, a traditional search still has some advantages.  ---  ⚠️ *Hybrid search is currently in private preview, [sign up for access here](https://www.pinecone.io/hybrid-search-early-access/)!*  ---  We repeatedly see that vector search unlocks incredible and *intelligent* retrieval but struggles to adapt to new domains. Whereas traditional search can cope with new domains but is fundamentally limited to a set performance level.  Both approaches have pros and cons, but what if we merge them somehow to eliminate a few of those cons? Could we create a *hybrid* search with the heightened performance potential of vector search and the zero-shot adaptability of traditional search?  Today, we will learn how to take our search to a new level. Taking both vector and traditional search and merging them via Pinecone's new hybrid search.  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/0cKtkaR883c\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ---  ## Out of Domain Datasets  Vector search or *dense retrieval* has been shown to significantly outperform traditional methods *when* the embedding models have been fine-tuned on the target domain. However, this changes when we try using these models for *\"out-of-domain\" tasks*.  That means if we have a large amount of data covering a specific domain like \"Medical question-answering\", we can fine-tune an embedding model. With that embedding model, we can create dense vectors and get outstanding vector search performance.  The problem is if we *don't* have data. In this scenario, a pretrained embedding model *may* perform better than traditional BM25, but it is unlikely. Giving us a best-case performance of BM25, an algorithm that we cannot fine-tune and cannot provide intelligent human-like retrieval.  If we want better performance, we're left with two options; (1) annotate a large dataset to fine-tune the embedding model, or (2) use hybrid search.  ## Hybrid Search  Combining dense and sparse search takes work. In the past, engineering teams needed to run different solutions for dense and sparse search engines and another system to combine results in a meaningful way. Typically a dense vector index, sparse inverted index, and reranking step.  The Pinecone approach to hybrid search uses a *single* hybrid index. It enables search across any modality; text, audio, images, etc. Finally, the weighting of dense vs. sparse can be chosen via the `alpha` parameter, making it easy to adjust.  How does a hybrid search pipeline look?  ![hybrid-pipeline](./images/hybrid-search-intro-1.png)  <small>High-level view of a simple hybrid search pipeline.</small>  Everything within the dotted lines is handled by Pinecone's hybrid index. But before we get there, we still need to create dense and sparse vector representations of our input data.  Let's take a look at how we can do that.  ## Implementation of Hybrid Search  The first step in a hybrid search implementation is preparing a dataset. We will use the [`pubmed_qa`](https://huggingface.co/datasets/pubmed_qa) dataset on Hugging Face *Datasets*. We download it like so:  ```python from datasets import load_dataset  # !pip install datasets pubmed = load_dataset(    'pubmed_qa',    'pqa_labeled',    split='train' ) pubmed ```  ``` Dataset({    features: ['pubid', 'question', 'context', 'long_answer', 'final_decision'],    num_rows: 1000 }) ```  The `context` feature is what we will store in Pinecone. Each `context` record contains multiple contexts within a list. Many lack real significance alone, so we will join them to create larger contexts.  ```python contexts = [] # loop through the context passages for record in pubmed['context']:    # join context passages for each question and append to contexts list    contexts.append('\\n'.join(record['contexts'])) # view some of the contexts for context in contexts[:2]:    print(f\"{context[:300]}...\") ```  ``` Programmed cell death (PCD) is the regulated death of cells within an organism. The lace plant (Aponogeton madagascariensis) produces perforations in its leaves through PCD. The leaves of the plant consist of a latticework of longitudinal and transverse veins enclosing areoles. PCD occurs in the cel... Assessment of visual acuity depends on the optotypes used for measurement. The ability to recognize different optotypes differs even if their critical details appear under the same visual angle. Since optotypes are evaluated on individuals with good visual acuity and without eye disorders, differenc... ```  We can see the highly-technical language contained within each context. An out-of-the-box model will typically struggle with this *domain-specific* language, making this an ideal use-case for hybrid search.  Let's move on to building the sparse and dense vectors.  ### Sparse Vectors  Several methods exist for building sparse vector embeddings, from the latest sparse embedding transformer models like SPLADE to rule-based tokenization logic.  We will stick with a more straightforward tokenization approach to keep things simple. Like the BERT tokenizer hosted by Hugging Face *Transformers*.  ```python from transformers import BertTokenizerFast  # !pip install transformers  # load bert tokenizer from huggingface tokenizer = BertTokenizerFast.from_pretrained(    'bert-base-uncased' ) ```  To tokenize a single context, we can do this:  ```python # tokenize the context passage inputs = tokenizer(    contexts[0], padding=True, truncation=True,    max_length=512 ) inputs.keys() ```  ``` dict_keys(['input_ids', 'token_type_ids', 'attention_mask']) ```  The output from this includes a few arrays that are all important when using transformer models. As we're doing tokenization only, we need the `input_ids`.  ```python input_ids = inputs['input_ids'] input_ids ```  ``` [101, 16984, 3526, 2331, 1006, 7473, 2094, ...] ```  These input IDs represent a unique word or sub-word token translated into integer ID values. This transformation is done using the BERT tokenizer's rule-based tokenization logic.  Pinecone expects to receive sparse vectors in dictionary format. For example, the vector:  ``` [0, 2, 9, 2, 5, 5] ```  Would become:  ``` {        \"0\": 1,        \"2\": 2,        \"5\": 2,        \"9\": 1 } ```  Each token is represented by a single *key* in the dictionary, and its frequency is counted by the respective key-*value*. We apply the same transformation to our `input_ids` like so:  ```python from collections import Counter  # convert the input_ids list to a dictionary of key to frequency values sparse_vec = dict(Counter(input_ids)) sparse_vec ```  ``` {101: 1, 16984: 1, 3526: 2, 2331: 2, 1006: 10, ... } ```  We can reformat all of this logic into two functions; `build_dict` to transform input IDs into dictionaries and `generate_sparse_vectors` to handle the tokenization *and* dictionary creation.  ```python def build_dict(input_batch):  # store a batch of sparse embeddings    sparse_emb = []    # iterate through input batch    for token_ids in input_batch:        # convert the input_ids list to a dictionary of key to frequency values        d = dict(Counter(token_ids))        # remove special tokens and append sparse vectors to sparse_emb list        sparse_emb.append({            key: d[key] for key in d if key not in [101, 102, 103, 0]        })    # return sparse_emb list    return sparse_emb  def generate_sparse_vectors(context_batch):    # create batch of input_ids    inputs = tokenizer(            context_batch, padding=True,            truncation=True,            max_length=512    )['input_ids']    # create sparse dictionaries    sparse_embeds = build_dict(inputs)    return sparse_embeds ```  We also remove special tokens `101`, `102`, `103`, and `0`. These are all tokens explicitly required by the BERT transformer model but have no meaning when building our sparse vectors.  This code is all we need to build our sparse vectors, but as usual, we still need to create dense vectors.  ### Dense Vectors  Our dense vectors are comparatively simple to generate. We initialize a `multi-qa-MiniLM-L6-cos-v1` sentence transformer model and encode the same context as before like so:  ```python # !pip install sentence-transformers from sentence_transformers import SentenceTransformer  # load a sentence transformer model from huggingface model = SentenceTransformer(    'multi-qa-MiniLM-L6-cos-v1' )  emb = model.encode(contexts[0]) emb.shape ```  ``` (1, 384) ```  The model gives us a `384` dimensional dense vector. We can move on to upserting the full dataset with both sparse and dense vectors.  ### Upserting  Our upsert operation is almost identical, with the exception that; we are pointing our requests to the `/hybrid/vectors/upsert` endpoint rather than `/vectors/upsert`, and our upsert includes an additional `sparse_values` parameter.  The private preview of hybrid search does not include a Python Pinecone client so we must interact with the Pinecone API directly. To keep things simple we wrote a [temporary Pinecone client class for hybrid search](https://gist.github.com/jamescalam/37d799cf824e16f0a6337b8a3e25fd34).  We then initialize our hybrid index like so:  ```python # choose a name for your index index_name = \"hybrid-test\"   # create the index pinecone.create_index(    index_name = index_name,    dimension = 384,    metric = \"dotproduct\",    pod_type = \"s1h\" ) ```  Note that we use a **h**ybrid **s1** pod type by specifying `s1h` and all hybrid indexes are currently restricted to the `dotproduct` similarity metric.  With all of that ready, we can begin adding all of our data to the hybrid index like so:  {{< notebook file=\"hybrid-upserts\" height=\"full\" >}}  From `describe_index_stats`, we should see that *1000* records have been added. With that, we can move on to querying the new index.  ## Making Queries  Queries remain very similar to pure dense vector queries, with the exception being that we must include a sparse vector version of our query — alongside the typical dense vector representation.  ![hybrid-queries](./images/hybrid-search-intro-2.png)  <small>Queries are made to the `/hybrid/query` endpoint with both dense and sparse vector embeddings.</small>  We can use the earlier `generate_sparse_vectors` function to build the sparse vector. We will wrap the encode and query operations into a single `hybrid_query` function to keep queries simple.  ```python def hybrid_query(question, top_k, alpha):    # convert the question into a sparse vector    sparse_vec = generate_sparse_vectors([question])    # convert the question into a dense vector    dense_vec = model.encode([question]).tolist()    # set the query parameters to send to pinecone    query = {      \"topK\": top_k,      \"vector\": dense_vec,      \"sparseVector\": sparse_vec[0],      \"alpha\": alpha,      \"includeMetadata\": True    }    # query pinecone with the query parameters    result = pinecone.query(query)    # return search results as json    return result ```  Now we query like so:  {{< notebook file=\"hybrid-query-0\" height=\"full\" >}}  How can we assess the impact of hybrid search vs. vector search with these results? We use the new `alpha` parameter that can be used while making queries.  The `alpha` parameter controls the weighting between the dense and sparse vector search scores. By default, this is set to `0.5`, making any results a pure hybrid search.  Above we performed a pure *dense vector search* by using an `alpha` of `1`.  With a full vector search, we get the same ranking of results. However, the \"best\" context (`711`) is currently in position *two*. We can modify the `alpha` parameter to try and improve this result.  {{< notebook file=\"hybrid-query-1\" height=\"full\" >}}  Using an `alpha` of `0.3` improves the results and returns the best context (`711`) as the top result.  ---  That's it for our fast introduction to hybrid search and how we can implement it with Pinecone. With this, we can reap the benefits of dense vector search while sidestepping its *out-of-domain* pitfalls.  If you'd like to get started with hybrid search, it is available via private preview with Pinecone. [Get in touch for early access](https://www.pinecone.io/hybrid-search-early-access )!  ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2be"
  },
  "title": "\"Making Stable Diffusion Faster with Intelligent Caching\"",
  "headline": "\"Making Stable Diffusion Faster with Intelligent Caching\"",
  "weight": "7",
  "- name": "Nima Boscarino",
  "position": "ML Developer Advocate",
  "src": "/images/nima-boscarino.jpeg",
  "href": "\"https://twitter.com/NimaBoscarino\"",
  "description": "How to speed up stable diffusion by caching previous generations",
  "images": "[\"/images/faster-stable-diffusion-0.png\"]",
  "thumbnail": "\"https://www.pinecone.io/images/faster-stable-diffusion-0.png\"",
  "content": "Creative industries are changing. A new wave of *\"AI art\"* tools like DALL-E 2, Imagen, and Midjourney seemed to pop up from nowhere. In a few short months, they have reshaped the way art is made.  The first of these tools, OpenAI's DALL-E 2, was announced in April 2022. It became the first widespread use of \"diffusion\" models. Since then, diffusion has exploded, reaching far beyond the tech and even creative industries, into common knowledge among people without any ties to either industry.  Yet, there is a problem. Diffusion models take a lot of compute to generate images.  The iterative diffusion process means end users generating images on CPU can expect to wait tens of minutes to produce a single image.   Despite the high compute requirements, innovation in the space has blossomed. Several other tools have since been released — one of the most exciting being *stable diffusion*.  Stable diffusion didn't bring any fundamental changes to the technology itself. It is exciting because it was the first *high-performance* diffusion model that was *open sourced*.  The release of stable diffusion alongside the slightly earlier release of Hugging Face *Diffusers* in July 2022 [1] triggered an explosion in new use cases.  Getting high-quality diffusion into the hands of open-source communities quickly produced \"stable diffusion dreams\" that create trippy videos as the model traverses between different prompts [2]. Diffusion has been applied to 3D objects [3] and even to create video and help animators create masterpieces in record time [4].  It's safe to say that diffusers are here to stay. Yet, since diffusion requires many generations and plenty of prompt tweaking to produce good results — stable diffusion is simply out of reach for most people unless we find a way to make it more efficient.  <script type=\"module\" src=\"https://gradio.s3-us-west-2.amazonaws.com/3.9.1/gradio.js\"> </script> <gradio-app space=\"jamescalam/dream-cacher\"></gradio-app>  Fortunately, there is something we can do to make diffusion more efficient and accessible as demoed above. Whether using the latest GPUs or entry-level CPUs. In this article, we discover how to make stable diffusion more efficient through collaboration and caching with a vector database.  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/YMlzhnlSAww\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ---  ## Collaborative Diffusion  Hugging Face Spaces has been flush with stable diffusion apps ever since stable diffusion was released. There are thousands of people using these apps daily. Yet, stable diffusion requires a *massive amount of compute*, a problem for both the host’s costs and the user’s patience. Yet, there is no clear solution.  ![hf-spaces](./images/faster-stable-diffusion-1.png) <small>Hugging Face Spaces allows *a lot* of users to interact with easy-to-build web apps.</small>  Serving so many users adds a lot of pressure to the service, but we can use this to our advantage.  ![hf-spaces-cacher](./images/faster-stable-diffusion-2.png) <small>We can cache user generations in a Pinecone vector database.</small>  Rather than users competing for service resources, we help users collaborate. We can find a way to cache the generations of every user in a way that similar generations are *”grouped”*. In that case, we can serve *relevant* past generations to current users and bypass the compute-heavy diffusion process.  ![hf-spaces-cacher-retriever](./images/faster-stable-diffusion-3.png) <small>After caching past generations, we can use them to quickly serve generated images to new users.</small>  As more generations are cached, fewer generations are required, speeding up retrieval time for current users *and* freeing up service compute.  ## Deciding Between Retrieval or Generation?  We can't only retrieve past generations; that would miss the value of generating new images with diffusion. However, if a user looks for something highly similar to a previously cached generation, why not return those cached generations?  The only problem is identifying what *”similar”* actually means.  Ideally, we should enable a backend search through past images based on their *visual* meaning and their *semantic similarity* to the user's text prompt.  We will handle this through what we call the **dream cacher** component. It consists of two steps:  ![hf-spaces-cacher-components-1-2](./images/faster-stable-diffusion-4.png)  1. Embedding the user’s text prompt to a [*meaningful* dense vector](/learn/dense-vector-embeddings-nlp/). 2. [Vector search](/learn/vector-search-basics/) through past prompt vectors and their respective images.  Let's start with step *1* of creating meaningful embeddings.  ## Meaningful Embeddings  Our embeddings must represent the *\"semantic meaning\"* behind past prompts in vector space. This means prompts with similar meanings should be placed close to one another, whereas dissimilar prompts should be separated.  ![vector-space](./images/faster-stable-diffusion-5.png) <small>Example of a 3D vector space with two clusters: dog-related prompts and surf/wave prompts.</small>  The typical approach for embedding text prompts uses a [sentence transformer](/learn/sentence-embeddings/) (or another encoder) model. These models all take text as input and output vector representations of that text.  ![encoder-vector-space](./images/f6ster-stable-diffusion-6.png) <small>An encoder model transforms prompts into meaningful vector space.</small>  Now we need to find suitable vector embeddings produced within the stable diffusion pipeline. Stable diffusion uses an encoder model named [CLIP](/learn/clip/). OpenAI's CLIP is a multi-modal model that places both images *and* text in a similar vector space.  CLIP is not a sentence transformer, but it's pretty close. However, there is a slight misalignment. Stable diffusion does *not* use single CLIP embeddings — it uses a large tensor containing *77* of these embeddings, which *cannot* be used in a vector search.  We can go ahead and initialize the stable diffusion pipeline from Hugging Face *Diffusers* like so:  ```python # !pip install torch transformers diffusers import torch from diffusers import StableDiffusionPipeline  # set the hardware device device = 'cuda' if torch.cuda.is_available() else 'cpu'  # init all of the pipeline models and move them to a given GPU pipe = StableDiffusionPipeline.from_pretrained(    \"CompVis/stable-diffusion-v1-4\",    use_auth_token=\"<<ACCESS_TOKEN>>\" ) pipe.to(device) ```  ---  *The `StableDiffusionPipeline` requires a Hugging Face user access token. They can be found in user settings. [Follow this guide on Hugging Face access tokens](https://huggingface.co/docs/hub/security-tokens) if you need help.*  ---  From here, we can use the first two components of the pipeline, the `tokenizer` and the `text_encoder`, to create the CLIP *text embeddings*.  {{< notebook file=\"dream-cacher-prompt-embeds\" height=\"full\" >}}  We can see the token-level CLIP embeddings with shape *77x768*. Having *77* of these embeddings is problematic as we need a *single* vector  ***vector* search** to work.  ![stable-diffusion-pipeline-annotated](./images/faster-stable-diffusion-7.png) <small>The stable diffusion pipeline makes use of **77** *768-d* text embeddings output by CLIP.</small>  Thanks to CLIP's contrastive pretraining, we can produce a meaningful *768-d* vector by *\"mean pooling\"* the *77* 768-d vectors.  ![mean-pooling](./images/faster-stable-diffusion-8.png) <small>Mean pooling takes the *mean* value across each dimension in our 2D tensor to create a new 1D tensor (the vector).</small>  When we apply mean pooling to our 2D text embeddings tensor, we average the values across each dimension, outputting a single 1D text embedding vector.  ![vector-extraction](./images/faster-stable-diffusion-9.png) <small>The mean pooling operation occurs before the long stable diffusion steps.</small>  Fortunately, CLIP generates a mean pooled version of these text embeddings by default, so we don't need to perform this operation ourselves.  {{< notebook file=\"dream-cacher-pooled-embeds\" height=\"full\" >}}  These pooled *\"prompt vectors\"* are created before the long diffusion process begins, so they can be built quickly.  ![creating-embeddings](./images/faster-stable-diffusion-10.png) <small>The user can create meaningful vectors, but what's next?</small>  We now have meaningful prompt vectors, but how do we use them? For this, we need a **vector database**.  ## Vector Database  A [*\"vector database\"*](/learn/vector-database/) is a vector storage and retrieval service that we can use to efficiently search through millions or even billions of vectors.  After generating the prompt vectors, we insert them into our vector database. From there, they can be retrieved by querying with *new* prompt vectors.  ![making-queries](./images/faster-stable-diffusion-11.png) <small>When making queries, a \"query prompt\" is encoded into a query vector, and the most similar already indexed vectors are retrieved.</small>  Given the prompt `\"A person surfing\"`, we build a prompt vector, search within the vector database for similar items, and find several images that already fit this description:  ![surf-search-example](./images/faster-stable-diffusion-12.png) <small>Embedding and retrieval of CLIP prompt vectors allow us to quickly return many similar images previously generated.</small>  For this to work, we need to add a second storage location for our images, like GCP's *Cloud Storage*. Here, we simply save the image using the unique ID assigned to it (we will cover this later) and then retrieve it using this same ID.  ### Prompt Recommendation?  An optional metadata field we can include is the plaintext prompt used to generate the vector. This isn't necessary for direct image retrieval, but it enables a second feature, *prompt recommendations*.  ![surf-search-recommendation-example](./images/faster-stable-diffusion-13.png) <small>Prompt recommendations can be useful to inspire users' creativity.</small>  These recommendations are helpful for users struggling with prompt ideas and encourage interesting movement across the vector space.  ### Upserting Vectors and Storing Images  To implement our search component, we will use [Pinecone's vector database](https://www.pinecone.io). To do this, we initialize our connection (using [a free API key](https://app.pinecone.io)) and create a vector *index* (a single instance within the vector database).  {{< notebook file=\"dream-cacher-create-index\" height=\"full\" >}}  We haven't added any vectors yet, so `'total_vector_count'` should be `0`. We need a unique ID, the `prompt_embeds` vector, and related metadata to create our first record. These will be upserted in the format:  ```python \"abcd-abcd-abcd-abcd\",  # unique ID [0.01, 0.64, 0.27, ...],  # 784-d vector {  \"prompt\": \"A person surfing\" }  # metadata dictionary ```  The ID can be created using the `uuid` module like so:  ```python import uuid  _id = str(uuid.uuid4())  # creates format \"xxxx-xxxx-xxxx-xxxx\" ```  The `prompt_embeds` must be reformated into a flat list to satisfy Pinecone client requirements:  ```python vec = prompt_embeds.cpu().tolist()[0] ```  Then we use the original prompt text to create the metadata dictionary:  ```python meta = {    \"prompt\": prompt } ```  Now we upsert everything with `index.upsert`:  {{< notebook file=\"dream-cacher-upsert\" height=\"full\" >}}  That is our vector and prompt metadata indexed. Now we need to generate the image using the `StableDiffusionPipeline` and store it somewhere to be retrieved later. We will use GCP's *Cloud Storage*.  ---  *If you're not familiar with GCP's Cloud Storage and Python's Cloud Storage API, they are explained in [the GCP Cloud Storage docs](https://cloud.google.com/storage/docs).*  ---  We generate the image like so:  {{< notebook file=\"dream-cacher-generate-image\" height=\"full\" >}}  With the image stored as a PIL object, we save it to file and upload it to a GCP *Cloud Storage* bucket using:  ```python # !pip install google-cloud-storage from google.cloud import storage  # set credentials os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'cloud-storage.json' # connect to bucket (we named it 'hf-diffusion-images') storage_client = storage.Client() bucket = storage_client.get_bucket('hf-diffusion-images') # create object blob blob = bucket.blob(f'images/{_id}.png') # save image to file im.save(f'{_id}.png', format='png') # upload to blob blob.upload_from_filename(f'{_id}.png') ```  That covers most of our logic with embeddings, vector search, and the image storage component. The next step is applying this at scale.  ## Stable Diffusion Pipeline  We could build a frontend and let users begin generating images right away. Yet, with just `\"A person surfing\"` currently indexed, the benefits of *retrieval when possible* are unlikely to be used.  For now, we need far more records indexed to increase the probability of a user entering a prompt similar to those already indexed.  To get started with this, we build an index using existing prompt datasets found on Hugging Face *Datasets*, like the `bartman081523/stable-diffusion-discord-prompts` dataset.  {{< notebook file=\"dream-cacher-load-dataset\" height=\"full\" >}}  This dataset alone contains nearly 4M prompts. Many of these can be nonsensical or low-quality — this is particularly noticeable among short prompts. So, to improve quality, we filter for longer prompts.  {{< notebook file=\"dream-cacher-long-prompts\" height=\"full\" >}}  Doing this, we are still left with a huge 3.56M prompts. More than enough to populate our index.  ### Generating Records at Scale  Our code for building the index requires the prompt vectors *and* the generated images. As before, we use the `StableDiffusionPipeline`:  {{< notebook file=\"dream-cacher-gen-images\" height=\"full\" >}}  ![image-examples](./images/faster-stable-diffusion-14.png) <small>Images output by the above code.</small>  Because we already know what prompts to use, we perform this and successive steps in batches. Allowing us to process many prompts in parallel and speeding up the process.  After generating the images, we must give each record a unique ID. This ID is used for storage in both Pinecone and Cloud Storage — as we did for the single `\"a person surfing\"` example earlier.  ![generating-images](./images/faster-stable-diffusion-15.png) <small>High-level pipeline for generating data.</small>  We upload the image files to Cloud Storage:  ```python for _id, im in zip(ids, out.images):        im.save(f\"tmp.png\")    # connect to target blob in cloud storage    blob = bucket.blob(f\"{_id}.png\")    # upload    blob.upload_from_filename(f\"{_id}.png\") ```  Then insert the vectors and respective metadata in Pinecone:  ```python # tokenize prompts text_inputs = pipe.tokenizer(        prompts, padding=True, truncation=True,        return_tensors='pt' ) # get embeddings text_embeds = pipe.text_encoder(**text_inputs) # mean pool text_embeds = text_embeds.pooler_output.cpu().tolist() # create metadata metadata = [{'prompt': prompt} for prompt in prompts] # add to pinecone index.upsert(zip(ids, text_embeds, metadata)) ```  We repeat this over tens of thousands, millions, or more records. After this, new prompt vectors are reasonably likely to collide with existing vectors in the vector space.  All that remains is a way for users to enter prompts.  ## Dream Cacher App  <gradio-app space=\"jamescalam/dream-cacher\"></gradio-app>  The app is built using Gradio blocks and is relatively simple, consisting of four components:  * `gr.TextInput` * `gr.Button` * `gr.Dataframe` * `gr.Gallery`  From these, we have two key features. The prompt recommendation is triggered every time the `gr.TextInput` value changes and is displayed in `gr.Dataframe`. The image search/diffusion is triggered by the search `gr.Button` and displayed in `gr.Gallery`.  Because both images and prompts are attached as metadata to vectors, the prompt recommendation and image retrieval are pulling records from the same Pinecone index. They differ because the image retrieval process also downloads the stored images from GCP's Cloud Storage.  ```python from PIL import Image  # retrieve most similar records xc = index.query(    embeds, top_k=9, include_metadata=True ) # get IDs ids = [match['id'] for match in xc['matches']] images = [] # begin retrieving images and append to 'images' list for _id in ids:    blob = bucket.blob(f'/images/{_id}.png').download_as_string()    # convert to 'in-memory file'    blob_byes = io.BytesIO(blob)    # open image as PIL object    im = Image.open(blob_bytes)    images.append(im) ```  These images are then passed as a list of PIL objects to Gradio's `gr.Gallery` component.  We have everything needed and can deploy the app using Hugging Face *Spaces*.  ![hf-create-space](./images/faster-stable-diffusion-16.png) <small>Click **New Space**, found on the Hugging Face homepage after logging in.</small>  To do so, we sign up for an account at [huggingface.co](https://huggingface.co) > click **Spaces** > enter space details and use *\"Gradio\"* as the *Space SDK*.  From here we add our Gradio app code to an `app.py` file:  ![hf-new-app-file](./images/faster-stable-diffusion-17.png) <small>Creating the `app.py` file in Hugging Face *Spaces*.</small>  We repeat the **Create a new file** process for a `requirements.txt` file and specify the modules that must be installed:  ``` diffusers transformers --extra-index-url https://download.pytorch.org/whl/cu113 torch google-cloud-storage pinecone-client ```  Our app must communicate with our private Pinecone index and private Cloud Storage. To add API keys or secret tokens to Hugging Face *Spaces*, we open the **Settings** tab and add *Repo secrets*.  ![image-20221103121340788](./images/faster-stable-diffusion-18.png)  These are stored as environment variables in our app deployment, accessible via Python with `os.environ[\"<SECRET_NAME>\"]`.  Because *Cloud Storage* requires a local JSON file with connection details and auth codes, we cannot enter this information directly inside *Repo secrets*. Instead, we encrypt the file with the `cryptography` module:  ```python import json from cryptography.fernet import Fernet  # load secrets JSON file with open('cloud-storage-secrets.json', 'r', encoding='utf-8') as fp:    api = json.load(fp) # convert secrets JSON to string keys_str = json.dumps(api, indent=4)  # initialize key to be used for encryption/decryption key = Fernet.generate_key() fernet = Fernet(key)  # create encrypted secrets JSON encrypted = fernet.encrypt(keys_str.encode()) # save to file with open('cloud-storage.encrypted', 'wb') as fp:    fp.write(encrypted) ```  We save the encrypted secrets to file and upload them again to our Hugging Face space. To allow our `app.py` script to decrypt the file, we must add the `key` value to our *Repo secrets* under `DECRYPTION_KEY`.  From there, we decrypt the file during deployment and initialize our Cloud Storage connection like so:  ```python from cryptography.fernet import Fernet from google.cloud import storage  # decrypt Storage Cloud credentials fernet = Fernet(os.environ['DECRYPTION_KEY'])  with open('cloud-storage.encrypted', 'rb') as fp:    encrypted = fp.read()    creds = json.loads(fernet.decrypt(encrypted).decode()) # then save creds to file with open('cloud-storage.json', 'w', encoding='utf-8') as fp:    fp.write(json.dumps(creds, indent=4)) # connect to Cloud Storage os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'cloud-storage.json' storage_client = storage.Client() bucket = storage_client.get_bucket('hf-diffusion-images') ```  From there, everything is fully prepared, and we simply wait for Hugging Face to build and deploy our app...  ![hf-wait-for-build](./images/faster-stable-diffusion-19.png)  <small>The status will start with *Building* and the app is ready-to-go as soon as we see *Running*.</small>  With that, our app is ready-to-go and accessible to the world.  ---  That's it for this walkthrough to building a diffusion generation and retriever app, or *dream cacher* the latest NLP and vector search technology.  ## References  [1] L. Debut, [Diffusers 0.1.2 Release Notes](https://github.com/huggingface/diffusers/releases/tag/0.1.2) (2022), Hugging Face Diffusers Repo  [2] A. Karpathy, [Stable Diffusion dreams of steam punk neural networks](https://www.youtube.com/watch?v=Jv1ayv-04H4) (2022), YouTube  [3] B. Poole, et al., [DreamFusion: Text-to-3D using 2D Diffusion](https://arxiv.org/abs/2209.14988) (2022), ArXiV  [4] Corridor Crew, [Recreating Spiderverse with AI](https://www.youtube.com/watch?v=QBWVHCYZ_Zs) (2022), YouTube ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2c0"
  },
  "title": "\"Pinecone sponsors the 45th annual SIGIR conference\"",
  "headline": "Pinecone sponsors the 45th annual SIGIR conference",
  "name": "Gibbs Cullen",
  "position": "Senior Product Marketing Manager",
  "src": "/images/gibbs-cullen.jpg",
  "href": "https://www.linkedin.com/in/gibbscullen/",
  "date": "\"2022-07-22\"",
  "# Date": "July 22, 2022",
  "description": "Pinecone was a proud sponsor for the conference and for the Reaching Efficiency in Neural Information Retrieval (ReNeuIR) workshop, co-led by our very own Sebastian Bruch, Staff Research Scientist at Pinecone.",
  "thumbnail": "\"/images/sigir-2022-thumbnail.jpg\"",
  "content": "![Sigir 2022](/images/sigir-2022.jpg)  The 45th annual [SIGIR Conference](https://sigir.org/sigir2022/) on research and development in information retrieval (IR) took place in Madrid, Spain and online last week from July 11-15. Attendance was at capacity as this was the first time attendees were able to gather in person since the start of the pandemic. There was a co-located event [ICTIR](https://www.ictir2022.org/) on the theory of IR that took place on July 11-12.   Pinecone was a proud sponsor for the conference and for the [Reaching Efficiency in Neural Information Retrieval (ReNeuIR) workshop](https://reneuir.org/), co-led by our very own Sebastian Bruch, Staff Research Scientist at Pinecone. Below is a recap of some notable themes we took away from the event as well as a summary of the ReNeuIR workshop.   ![Sigir 2022 Conference](/images/sigir-conference.jpg)  ## Conference themes  The five day conference was broken out into tutorials, paper presentations, and workshops. The various sessions focused on the latest research and development in IR including recommendation engines, semantic search, and deep learning. There were also some notable talks introducing newer concepts around reinforcement learning and the use of knowledge graphs alongside typical IR systems.   ### Increasing interest in vector search and vector databases  In addition to more traditional IR topics, this year’s event had a larger focus around vector search and databases. As the industry moves more towards deep learning applications, the need for vector search is growing. This increased interest was notable at SIGIR with a number of papers on dense retrieval and a panel on “[Applications and Future of Dense Retrieval in Industry](https://dl.acm.org/doi/abs/10.1145/3477495.3536324).” In order to support this level of similarity search, vector databases like [Pinecone](/) are needed.  ### Focus on more sustainable research    In general, discussions around sustainability and environmental impact within technology have been on the rise. So naturally, this was a topic of conversation at SIGIR, as well as the motivation behind the ReNeuIR workshop which we’ll cover more below. A notable paper on this topic was “[Reduce, Reuse, Recycle: Green Information Retrieval Research](https://dl.acm.org/doi/abs/10.1145/3477495.3531766)“, which won Best Paper Honorable Mention. Developments in this space have even sparked internal discussions amongst Pinecone’s engineering teams.    ### Retrieval-enhanced machine learning   Finally, there was a focus on a newer notion of retrieval-enhanced machine learning (REML). This theme was highlighted in a conference paper titled “[Retrieval-Enhanced Machine Learning](https://arxiv.org/abs/2205.01230)”. Using the REML framework broadens the scope of conventional IR methods to include task-driven machines, such as machine learning (ML) models. And when a user of a retrieval system is an ML model or system, requirements are introduced such as continual index updates and stricter efficiency constraints, both of which Pinecone strives to deliver. This talk helped to lay the foundation for this new style of information access research with the hope of advancing ML and artificial intelligence (AI) efforts.   ## ReNeuIR Workshop  The ReNeurIR workshop facilitated discussion and collaboration about methods in the new age of neural information retrieval (NIR), specifically around efficiency. NIR models achieve a greater effectiveness than the previous wave of machine learning models (e.g. decision forests on many IR tasks), but with orders of magnitude more learnable parameters and much greater amounts of data.  In a world where large organizations at the forefront of research in ML and IR have enormous amounts of resources, it is easy for them to deprioritize efficiency and sustainability concerns. This workshop focused on ways to promote more sustainable research by identifying best practices in the development and evaluation of neural models for IR.  The full day workshop included [two keynotes and a panel](https://reneuir.org/program.html) by experts from leading IR research organizations such as HuggingFace, Microsoft, Georgetown University, and University of Queensland. The event had roughly fifty in-person and twenty virtual attendees. In terms of next steps, the workshop’s organizing committee will be publishing guidelines on how to measure and report the environmental impact of research within IR. Follow the [workshop’s Twitter](https://twitter.com/ReNeuIRWorkshop) for updates.   **About Pinecone:**  The Pinecone vector database makes it easy to build high-performance vector search applications. Developer-friendly, fully managed, and easily scalable without infrastructure hassles. Visit our [website](/) to create a free account or [contact](/contact) us to learn more. We hope to see you next year in Taipei for SIGIR 2023! ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2c2"
  },
  "title": "\"What is Similarity Search?\"",
  "headline": "\"What is <span>Similarity Search?</span>\"",
  "weight": "1",
  "name": "Rajat Tripathi",
  "position": "Software Engineer",
  "src": "/images/company-rajat.jpeg",
  "href": "https://www.linkedin.com/in/rajat-tripathi-08/",
  "description": "An introduction to similarity search including key terms, use cases, and examples.",
  "images": "['/images/what-is-similarity-search-word2vec.png']",
  "content": "**[Pinecone](/) is a vector database that makes it easy to add similarity search to any application. [Try it free](https://app.pinecone.io), and continue reading to learn what makes similarity search so useful.**  ## Introduction  Searching through data for similar items is a common operation in databases, search engines, and many other applications. Finding similar items based on fixed numeric criteria is very straightforward using a query language when we are dealing with traditional databases. For example, finding employees in a database within a fixed salary range.  But sometimes we have to answer questions like &ldquo;Which objects in our inventory are similar to what a user searched for?&rdquo; The search terms can be vague and can have a lot of variations. For example, a user can search for something generic like &ldquo;shoes&rdquo;,&ldquo;black shoes&rdquo; or something more precise like &ldquo;Nike AF-1 LV8&rdquo;.  ![Queries can be vague and varied](/images/what-is-similarity-search-shoes.png)  Our system must be able to discern between these terms and must understand how a black shoe differs from other shoes. To handle such queries we need a representation that captures the deeper conceptual meaning of the objects. On top of that, in scenarios like these, we might have to work with data to the scale of billions of objects.  When dealing with data in this scale & context, this problem is quite unlike searching through traditional databases containing symbolic object representations. Hence we need something more powerful that can allow us to search through semantic representations efficiently.  With similarity search, we can work with semantic representations of our data and find similar items fast. And in the sections below we will discuss how exactly it works.  ## What Are Vector Representations?  In the passage above, we talked about representing objects in a way that captures their deeper meanings. In machine learning, we often represent real-world objects and concepts as a set of continuous numbers, also known as [vector embeddings](/learn/vector-embeddings). This very neat method allows us to translate the similarity between objects as perceived by us into a vector space.  This means when we represent images or pieces of text as vector embeddings, their semantic similarity is represented by how close their vectors are in the vector space. Hence, what we want to look at is the distance between vectors of the objects.  These vector representations (embeddings) are often created by training models according to the input data and task. Word2Vec,GLoVE, USE etc. are popular models for generating  embeddings from text data while CNN models like VGG are often used to create image embeddings.  ![Word2Vec illustration](/images/what-is-similarity-search-word2vec.png)  The figure above from [this](https://jalammar.github.io/illustrated-word2vec/) great article on word2vec, can help us visualize how the model can generate similar representations of similar words and is able to capture the semantic meaning.  This concept can be extended to more complex objects. We can combine information from features in the dataset to create embeddings for every row in the dataset. This is something leveraged by many search & recommendation based algorithms. The point that I want to make is, training a machine learning model on any data can transform the broad abstract concept it can have to something on which we can perform mathematical operations that can give us the insights we need.  ## Distance Between Vectors  We mentioned earlier that we find similarities between objects by calculating the distance between their vectors. We can calculate the distance between these vectors in the vectors space according to the distance metric that fits our problem the best.  Some of the commonly used distance metrics in ML are Euclidean, Manhattan, Cosine, and Chebyshev. The image below will help us understand the intuition behind each of these methods.  ![Similarity search distance metrics](/images/what-is-similarity-search-distance-metrics.jpeg)  The choice of distance metric depends on the use case. A great guide to learn more about distance metrics is [here](https://towardsdatascience.com/importance-of-distance-metrics-in-machine-learning-modelling-e51395ffe60d).  ## Performing Search  Now we know we can use vector embeddings to represent our objects, and the distances between vectors represent the similarity between the objects themselves.  This is where the similarity search, or [vector search](/learn/vector-search-basics/), kicks in. Given a set of vectors and a query vector, we need to find the most similar items in our set for the query. We call this task nearest neighbor search.  ### K Nearest Neighbors  K nearest neighbors or [k-NN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) is a very popular algorithm to find nearest vectors in a space for a given query vector. The k here is a hyperparameter set by us which denotes how many nearest neighbors we want to retrieve.  We can perform k-NN on the vectors we have for our data and retrieve the nearest neighbors for our query vector depending on the distance between the vectors.  ![Similarity search with k-NN](/images/what-is-similarity-search-knn.jpg)  A major drawback of k-NN is that to find the nearest vectors for our query we will have to calculate its distance with every vector we have in our database. This will be very inefficient if we have to search through millions of vectors.  ### Approximate Neighbor Search  To reduce the computation complexity added by an exhaustive search like kNN we make use of approximate neighbor search.  Instead of checking distances between each vector in the [database](/learn/vector-database/), we retrieve a \"good guess\" of the nearest neighbor. In some use cases, we would rather lose some accuracy in favor of performance gain, thus allowing us to scale our search. ANN allows us to get a massive performance boost on similarity search when dealing with huge datasets.  In approximately nearest neighbors (ANN), we build [index structures](https://www.pinecone.io/learn/what-is-a-vector-index/) that narrow down the search space and improve lookup times. Apart from that, most ML models produce vectors that have high dimensionality which is another [hurdle](https://en.wikipedia.org/wiki/Curse_of_dimensionality) to overcome. Approximate search relies on the fact that even though data is represented in a large number of dimensions, their actual complexity is low. It tries to work with the *true intrinsic dimensionality* of data. Hashing is a good example of a method that allows us to do it and is used widely for many applications. There are various algorithms to solve the approximate search problem and to actually dive into how approximate search works warrants another article of its own. I suggest going through [this three-part series of articles](https://mccormickml.com/2017/10/13/product-quantizer-tutorial-part-1/) to understand ANN better.  As an overview, it is sufficient to understand that ANN algorithms make use of techniques like indexing, clustering, hashing, and quantization to significantly improve computation and storage at the cost of some loss in accuracy.  ## Conclusion  While we have barely scratched the surface of the complexities of similarity search &mdash; also known as [vector search](/learn/vector-search-basics/) &mdash; with this article, the intent was to introduce some basic concepts and provide resources for a detailed reading on the topic. An increasing number of applications make use of similarity search to untangle problems in search & other domains, I highly encourage diving deeper into some of these concepts to learn more!  While you are on the website, why not have a look at how easy it is to build a scalable similarity search system in just a few lines of code? You can find a few [example notebooks](https://www.pinecone.io/learn/) that use Pinecone for solving similarity search and related problems. Grab your free [API key](https://app.pinecone.io/) and we will be happy to help you along the way! ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2c4"
  },
  "title": "\"Getting Started with LLMs Using LangChain\"",
  "headline": "\"Getting Started with LLMs Using LangChain\"",
  "weight": "3",
  "name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "Introduction to the Generative AI and LLM framework for building apps with OpenAI's GPT-3 and open-source alternatives.",
  "images": "['https://www.pinecone.io/images/langchain-intro-0.png']",
  "content": "**L**arge **L**anguage **M**odels (LLMs) entered the world stage with the release of OpenAI's GPT-3 in 2020 [GPT3]. Since then, they've enjoyed a steady growth in popularity.  That is until late 2022. Interest in LLMs and the broader discipline of generative AI has skyrocketed. The reasons for this are likely the continuous upward momentum of significant advances in LLMs.  We saw the dramatic news about Google's _\"sentient\"_ LaMDA chatbot. The first high-performance and *open-source* LLM called BLOOM was released. OpenAI released their next-generation text embedding model and the next generation of *\"GPT-3.5\"* models.  After all these giant leaps forward in the LLM space, OpenAI released *ChatGPT* — thrusting LLMs into the spotlight.  *LangChain* appeared around the same time. Its creator, Harrison Chase, made the first commit in late October 2022. Leaving a short couple of months of development before getting caught in the LLM wave.  Despite being early days for the library, it is already packed full of incredible features for building amazing tools around the core of LLMs. In this article, we'll introduce the library and start with the most straightforward component offered by LangChain — LLMs.  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/nE2skSRWTTs\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ---  # LangChain  At its core, LangChain is a framework built around LLMs. We can use it for chatbots, [**G**enerative **Q**uestion-**A**nswering (GQA)](https://www.pinecone.io/learn/openai-gen-qa/), summarization, and much more.  The core idea of the library is that we can _\"chain\"_ together different components to create more advanced use cases around LLMs. Chains may consist of multiple components from several modules:  * **Prompt templates**: Prompt templates are templates for different types of prompts. Like \"chatbot\" style templates, ELI5 question-answering, etc  * **LLMs**: Large language models like GPT-3, BLOOM, etc  * **Agents**: Agents use LLMs to decide what actions should be taken. Tools like web search or calculators can be used, and all are packaged into a logical loop of operations.  * **Memory**: Short-term memory, long-term memory.  We will dive into each of these in much more detail in upcoming chapters of the LangChain handbook. You can stay updated for each release via our newsletter:  {{< newsletter text=\"Subscribe to stay updated with LangChain releases!\" inputText=\"Email address...\" buttonText=\"Submit\">}}  For now, we'll start with the basics behind **prompt templates** and **LLMs**. We'll also explore two LLM options available from the library, using models from *Hugging Face Hub* or *OpenAI*.  # Our First Prompt Templates  Prompts being input to LLMs are often structured in different ways so that we can get different results. For Q&A, we could take a user's question and reformat it for different Q&A styles, like conventional Q&A, a bullet list of answers, or even a summary of problems relevant to the given question.  ## Creating Prompts in LangChain  Let's put together a simple question-answering prompt template. We first need to install the `langchain` library.  ``` !pip install langchain ```  ---  *Follow along with the code via [Colab](https://colab.research.google.com/github/pinecone-io/examples/blob/master/generation/langchain/handbook/00-langchain-intro.ipynb)!*  ---  From here, we import the `PromptTemplate` class and initialize a template like so:  ```python from langchain import PromptTemplate  template = \"\"\"Question: {question}  Answer: \"\"\" prompt = PromptTemplate(         template=template,     input_variables=['question'] )  # user question question = \"Which NFL team won the Super Bowl in the 2010 season?\" ```  When using these prompt template with the given `question` we will get:  ``` Question: Which NFL team won the Super Bowl in the 2010 season?  Answer:  ```  For now, that's all we need. We'll use the same prompt template across both Hugging Face Hub and OpenAI LLM generations.  # Hugging Face Hub LLM  The Hugging Face Hub endpoint in LangChain connects to the Hugging Face Hub and runs the models via their free inference endpoints. We need a [Hugging Face account and API key](https://huggingface.co/settings/tokens) to use these endpoints.  Once you have an API key, we add it to the `HUGGINGFACEHUB_API_TOKEN` environment variable. We can do this with Python like so:  ```python import os  os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'HF_API_KEY' ```  Next, we must install the `huggingface_hub` library via Pip.  ``` !pip install huggingface_hub ```  Now we can generate text using a Hub model. We'll use [`google/flan-t5-x1`](https://huggingface.co/google/flan-t5-xl).  ---  *The default Hugging Face Hub inference APIs do not use specialized hardware and, therefore, can be slow. They are also not suitable for running larger models like `bigscience/bloom-560m` or `google/flan-t5-xxl` (note `xxl` vs. `xl`).*  ---  {{< notebook file=\"langchain-00-hf-gen\" height=\"full\" >}}  For this question, we get the correct answer of `\"green bay packers\"`.  ## Asking Multiple Questions  If we'd like to ask multiple questions, we can try two approaches:  1. Iterate through all questions using the `generate` method, answering them one at a time. 2. Place all questions into a single prompt for the LLM; this will only work for more advanced LLMs.  Starting with option (1), let's see how to use the `generate` method:  {{< notebook file=\"langchain-00-hf-generate\" height=\"full\" >}}  Here we get bad results except for the first question. This is simply a limitation of the LLM being used.  If the model cannot answer individual questions accurately, grouping all queries into a single prompt is unlikely to work. However, for the sake of experimentation, let's try it.  {{< notebook file=\"langchain-00-hf-multi-query\" height=\"full\" >}}  As expected, the results are not helpful. We'll see later that more powerful LLMs can do this.  # OpenAI LLMs  The OpenAI endpoints in LangChain connect to OpenAI directly or via Azure. We need an [OpenAI account and API key](https://beta.openai.com/account/api-keys) to use these endpoints.  Once you have an API key, we add it to the `OPENAI_API_TOKEN` environment variable. We can do this with Python like so:  ```python import os  os.environ['OPENAI_API_TOKEN'] = 'OPENAI_API_KEY' ```  Next, we must install the `openai` library via Pip.  ``` !pip install openai ```  Now we can generate text using OpenAI's GPT-3 generation (or *completion*) models. We'll use [`text-davinci-003`](https://huggingface.co/google/flan-t5-xl).  ```python from langchain.llms import OpenAI  davinci = OpenAI(model_name='text-davinci-003') ```  ---  *Alternatively, if you're using OpenAI via Azure, you can do:*  ```python from langchain.llms import AzureOpenAI  llm = AzureOpenAI(     deployment_name=\"your-azure-deployment\",      model_name=\"text-davinci-003\" ) ```  ---  We'll use the same simple question-answer prompt template as before with the Hugging Face example. The only change is that we now pass our OpenAI LLM `davinci`:  {{< notebook file=\"langchain-00-openai-gen\" height=\"full\" >}}  As expected, we're getting the correct answer. We can do the same for multiple questions using `generate`:  {{< notebook file=\"langchain-00-openai-generate\" height=\"full\" >}}  Most of our results are correct or have a degree of truth. The model undoubtedly functions better than the `google/flan-t5-xl` model. As before, let's try feeding all questions into the model at once.  {{< notebook file=\"langchain-00-openai-multi-query\" height=\"full\" >}}  As we keep rerunning the query, the model will occasionally make errors, but at other times manage to get all answers correct.  ---  That's it for our introduction to LangChain — a library that allows us to build more advanced apps around LLMs like OpenAI's GPT-3 models or the open-source alternatives available via Hugging Face.  As mentioned, LangChain can do much more than we've demonstrated here. We'll be covering these other features in upcoming articles.  ---  # References  [GPT3] [GPT-3 Archived Repo](https://github.com/openai/gpt-3) (2020), OpenAI GitHub",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2c6"
  },
  "title": "\"Using Semantic Search to Find GIFs\"",
  "headline": "\"Using Semantic Search to Find GIFs\"",
  "weight": "2",
  "name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "Learn how to enhance GIF discovery using NLP semantic search",
  "images": "[\"/images/gif-search-0.png\"]",
  "thumbnail": "\"https://www.pinecone.io/images/gif-search-0.png\"",
  "content": "Vector search powers some of the most popular services in the world. It serves your Google results, delivers the [best podcasts on Spotify](/learn/spotify-podcast-search/), and accounts for *at least* 35% of consumer purchases on Amazon \\[1\\]\\[2\\].  In this article, we will use vector search applied to language, called *semantic* search, to build a GIF search engine. Unlike more traditional search where we rely on *keyword* matching, semantic search enables search based on the *human meaning* behind text and images. That means we can find highly relevant GIFs with natural language prompts.  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/gif-search-1.mp4\" type=\"video/mp4\"> </video> <small>Preview of the GIF search app <a href=\"https://share.streamlit.io/pinecone-io/playground/gif-search/src/server.py\">available here</a>.</small>  The pipeline for a project like this is simple, yet powerful. It can easily be adapted to tasks as diverse as [video search](/learn/youtube-search/) or [answering Super Bowl questions](/learn/question-answering/), or as we'll see, finding GIFs.  [*All supporting notebooks and scripts can be found here*](https://github.com/pinecone-io/examples/tree/master/search/semantic-search/gif-search).  ## GIF Dataset  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/xXsDIK9z_fg\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  We will be using the TGIF dataset found [on GitHub here](https://github.com/raingo/TGIF-Release). To get the dataset we can use `wget` (alternatively, download it manually), and unzip.  ```bash wget https://github.com/raingo/TGIF-Release/archive/master.zip  unzip master.zip ```  In these unzipped files we should be able to find a file named `tgif-v1.0.tsv` inside the `data` directory. We'll use *pandas* to load the file using `\\t` as the field delimiter.  {{< notebook file=\"gif-search-read-csv\" height=\"full\" >}}  The dataset contains GIF URLs and their descriptions in natural language. We can take a look at the first five GIFs.  {{< notebook file=\"gif-search-examples\" height=\"full\" >}}  We will find that there are some duplicate URLs, but these do not necessarily indicate duplicate records as a *single GIF* can be assigned multiple descriptions.  {{< notebook file=\"gif-search-dupes\" height=\"full\" >}}  With our data, we can move on to building the search pipeline.  ## Search  The search pipeline will at a high level take our natural language query like *\"a dog talking on the phone\"* and search through the existing GIF descriptions for anything that has a similar *meaning* to this query.  In this context, we describe *meaning* as *semantic similarity*, both of which are loaded terms and could refer to many things. For example, are the two phrases `\"the dog eats lunch\"` and `\"the dog does not eat lunch\"` similar? In this case, it would depend very much on our use case.  Another example: which of the following two sentences are the most similar?  ``` A: the stock market took a turn for the worse  B: how did the stock market do today?  C: the stock market performed worse than expected ```  If we wanted to find phrases with similar meaning then the obvious choice would be `A` and `C`. Matching those with `B` would make little sense. However, this is not the case if we are searching for similar *question-answer pairs*; in that case, `B` should match very closely with `A` and `C`.  It's important to identify what your use case requires in it's definition of *\"semantic similarity\"*. For us, we really want to identify generic similarity. That is, we want `A` and `C` to match, and `B` to not match either of those.  To do this, we will transform our phrases into [*dense vector embeddings*](/learn/dense-vector-embeddings-nlp/). These dense vectors can be stored in a [*vector database*](https://www.pinecone.io/learn/vector-database/) where we can very quickly compare vectors and identify those that are most similar based on metrics like Euclidean distance and cosine similarity.  ![euclidean-cosine](/images/gif-search-2.png) <small>Both of these metrics identify the similarity (proximity) of vectors, but they do it based on distance (left) or angular similarity (right).</small>  The vector database handles the storage and fast search of our vector embeddings, but we still need a way to create these embeddings. To do that we use NLP transformer models called *retrievers* that are [fine-tuned for creating *sentence embeddings*](https://www.pinecone.io/learn/sentence-embeddings/). These sentence embeddings/vectors are able to *numerically represent* the *meaning* behind the text that they represent.  ![retriever-to-vector-space](/images/gif-search-3.png) <small>Retriever models are able to take two semantically similar phrases and encode them as similar vectors.</small>  Putting these two components together gives us a semantic search pipeline that we can use to retrieve semantically similar GIF descriptions given a query.  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/gif-search-4.mp4\" type=\"video/mp4\"> </video> <small>GIF search pipeline covering the one-time indexing step (left) and querying (right).</small>  Let's take a look at how we can put all of this together.  ### Initializing Components  We will start by initializing our retriever model. Many of the most powerful retrievers use a *sentence transformer* architecture, which are best supported via the `sentence-transformers` library, installed via a `pip install sentence-transformers`.  To find sentence transformer models we go to [*huggingface.co/models*](https://huggingface.co/models) and search for `sentence-transformers` for the *official* sentence transformer models. However, there are other models we can use like the [`all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) sentence transformer trained during a special event on [over 1B training pairs](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354). We will use this model.  {{< notebook file=\"gif-search-sentence-transformers\" height=\"full\" >}}  There are a couple of important details here:  * `max_sequence_length=128` means the model can read up to *128* input tokens. * `word_embedding_size=384` actually refers to the *sentence* embedding size. This means the model will output a 384-dimensional vector representation of the input text.  For the short several-word GIF descriptions of our dataset a maximum sequence length of *128* is *more than enough*.  We need to use the *sentence* embedding size when initializing our vector database, so we store that in the `embed_dim` variable above.  To initialize our vector database we first need to sign up for a [free Pinecone API key](https://app.pinecone.io/) and install the Pinecone Python client via `pip install pinecone-client`. Once ready, we initialize:  {{< notebook file=\"gif-search-init-index\" height=\"full\" >}}  Here, we are specifying an index name of `'gif-search'`; feel free to choose anything you like. It is simply a name. The `metric` is more important and depends on the model being used. For our chosen model we can see in its [*model card*](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) that it has been trained to use *cosine similarity*, hence why we have specified `metric='cosine'`. Alternative metrics include `euclidean` and `dotproduct`.  We've initialized both our vector database and retriever components, so we can move on to embedding and indexing our data.  ### Indexing  The embedding and indexing process is much faster when we perform these steps for multiple records *in parallel*. However, we cannot process all of our records at once as the retriever model must shift everything it is embedding into on-chip memory, which is limited.  To avoid this limit, while keeping indexing times as fast as possible, we process everything in batches of `64`.  {{< notebook file=\"gif-search-indexing\" height=\"full\" >}}  Here we are extracting the `batch` from our data `df`. We encode the descriptions via our retriever model, create metadata (covering both *descriptions* and *url*), and create some string format IDs. From this we have everything we need to create *documents*, which will look like this:  ```json (        \"some-id-value\",    [0.1, 0.2, 0.1, 0.4 ...],    {        'description': \"something descriptive\",        'url': \"https://xyz.com\"    } ) ```  When we `upsert` these *documents* to the Pinecone index, we do so in batches of *64*. After all of this, we use `index.describe_index_stats()` to check that we have inserted all *125,782* documents, which we have.  ### Querying  The final step of querying our data covers:  1. Encoding a query like *\"dogs talking on the phone\"* to create a *query vector*, 2. Retrieval of similar *context vectors* from Pinecone, 3. Getting relevant GIFs from the URLs found in our metadata fields.  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/gif-search-5.mp4\" type=\"video/mp4\"> </video> <small>The querying pipeline.</small>  Steps *one* and *two* will be performed by a function named `search_gif`:  ```python def search_gif(query):     # Generate embeddings for the query     xq = retriever.encode(query).tolist()     # Compute cosine similarity between query and embeddings vectors and return top 10 URls     xc = index.query(xq, top_k=10,                     include_metadata=True)     result = []     for context in xc['matches']:         url = context['metadata']['url']         result.append(url)     return result ```  To display the GIFs we display HTML `<img>` elements using the metadata URLs to point to the correct GIFs. We do this using the `display_gif` function:  ```python def display_gif(urls):     figures = []     for url in urls:         figures.append(f'''             <figure style=\"margin: 5px !important;\">               <img src=\"{url}\" style=\"width: 120px; height: 90px\" >             </figure>         ''')     return HTML(data=f'''         <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">         {''.join(figures)}         </div>     ''') ```  Let's test some queries.  {{< notebook file=\"gif-search-queries\" height=\"full\" >}}  That looks pretty accurate so we've managed to put this GIF search pipeline together very easily. With a [little added effort](https://github.com/pinecone-io/examples/blob/master/search/semantic-search/gif-search/app.py) we can translate these steps in creating a web app using something like [*Streamlit*](https://www.youtube.com/watch?v=QpISF8gMsjQ).    We've successfully built a GIF search tool using a simple semantic search pipeline with out-of-the-box models and Pinecone. This same pipeline can be applied across a variety of domains with very little tweaking.  The ease-of-use and potential of both vector and semantic search have led to a lot of research and applications of both technologies beyond the world of big tech. If you're interested in seeing other applications of this technology, or would like to share your own, considering joining the [Pinecone community](https://community.pinecone.io).    ## Resources  [Article Notebooks and Scripts](https://github.com/pinecone-io/examples/tree/master/search/semantic-search/gif-search)  [1] M. Osborne, [How Retail Brands Can Compete And Win Using Amazon's Tactics](https://www.forbes.com/sites/forbesagencycouncil/2017/12/21/how-retail-brands-can-compete-and-win-using-amazons-tactics/) (2017), Forbes  [2] L. Hardesty, [The history of Amazon's recommendation algorithm](https://www.amazon.science/the-history-of-amazons-recommendation-algorithm) (2019), Amazon Science ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2c8"
  },
  "title": "\"Time Series Analysis Through Vectorization\"",
  "headline": "\"Time Series Analysis Through Vectorization\"",
  "weight": "4",
  "name": "Diego Lopez Yse",
  "position": "Data Scientist",
  "src": "/images/diego-lopez-yse.jpeg",
  "href": "\"https://lopezyse.medium.com/\"",
  "description": "Components and complexities of time series, and how vectorization deals with them.",
  "images": "['/images/time-series-vectors-1.png']",
  "content": "**The components and complexities of time series, and how vectorization can deal with them.**  Time series data is all around us. The daily closing price of JP Morgan’s stock, the monthly sales of your company, the annual GDP value of Spain, or the daily maximum temperature values in a given region, are all examples of times series.  A time series is a sequence of observations of data points measured over a time interval. The concept is not new, but we are witnessing an explosion of this type of data as the world gets increasingly measured. Today, sensors and systems are continuously growing the universe of time series datasets. From wearables to cell phones and self-driving cars, the number of [connected devices worldwide is set to hit 46 billion](https://techjury.net/blog/how-many-iot-devices-are-there/#gref).  Depending on the frequency of observations, a time series may typically be hourly, daily, weekly, monthly, quarterly or annual — the data is in order, with a fixed time difference between the occurrence of successive data points.  ![Time series example](/images/time-series-vectors-1.png) <small>Example of time series: temperature records. Source: [Influxdata](https://www.influxdata.com/what-is-time-series-data/)</small>  ![Time series example](/images/time-series-vectors-2.png) <small>Example of time series: stock price evolution. Source: Influxdata</small>  ![Time series example](/images/time-series-vectors-3.png) <small>Example of time series: health monitoring records. Source: Influxdata</small>  The concept and applications of time series have become so important, that several tech giants had taken the lead by developing state of the art solutions to ingest, process and analyse them like never before:  1. [Prophet](https://facebook.github.io/prophet/) is open-source software released by Facebook’s Core Data Science team. It’s used in many applications across Facebook for producing reliable forecasts for planning and goal setting, and it includes many possibilities for users to tweak and adjust forecasts. The solution is robust to outliers, missing data, and dramatic changes in the time series. 2. [Amazon Forecast](https://aws.amazon.com/forecast/) is a fully managed service that uses Machine Learning to deliver highly accurate forecasts based on the same technology that powers Amazon.com. It builds precise forecasts for virtually any business condition, including cash flow projections, product demand and sales, infrastructure requirements, energy needs, and staffing levels. 3. Uber’s Forecasting Platform team created [Omphalos](https://eng.uber.com/omphalos/), which is a time series back testing framework that generates efficient and accurate comparisons of forecasting models across languages and streamlines the model development process, thereby improving the customer experience. 4. [SAP Analytics Cloud](https://www.sapanalytics.cloud/resources-your-guide-to-time-series-forecasting/) offers an automatic time series forecasting solution to perform advanced statistical analysis and generate forecasts by analyzing trends, fluctuations and seasonality. The algorithm works by analyzing the historical data to identify the existing patterns in the data and then using those patterns, projects the future values.  ## Time Series is Different  The “time component” in a time series provides an internal structure that must be accounted for, which makes it very different to any other data type, and sometimes more difficult to handle than traditional datasets.  This is the **main difference with sequential data**, where the order of the data matters, but the timestamp is irrelevant or doesn’t matter (as in the case of a DNA sequence, where the sequence is important but the concept of time is irrelevant).  This means that, unlike other Machine Learning challenges, you can’t just plug in an algorithm at a time series dataset and expect to have a proper result. Time series data can be transformed into supervised learning problems, but the key step is to consider their temporal structure like trends, seasonality, and forecast horizon.  Since there are so many prediction problems that involve time series, first we need to understand their main components.  ## Anatomy of Time Series  Unlike other data types, time series have a strong identity on their own. This means that we can’t use the usual strategies to analyze and predict them, since traditional analytical tools fail at capturing their temporal component.  A good way to get a feel of how a time series pattern behaves is to break the time series into its many distinct components. The **decomposition** of a time series is a task that deconstructs a time series into several pieces, each representing one of the underlying categories of the pattern. Time series decomposition is built on the assumption that data arises as the result of the combination of some underlying components:  * **Base Level**: This represents the average value in the series. * **Trend**: is observed when there is a sustained increasing or decreasing slope observed in the time series. * **Seasonality**: Occurs when there is a distinct repeated pattern observed between regular intervals due to seasonal factors, whether it is the month of the year, the day of the month, weekdays, or even times of the day. For example, retail stores sales will be high during weekends and festival seasons. * **Error**: The random variation in the series.  All series have a base level and error, while the trend and seasonality may or may not exist. This way, a time series may be imagined as a combination of the base level, trend, seasonality, and error terms.  Another aspect to consider is **cyclic behavior**. This happens when the rise and fall pattern in the series does not happen in fixed calendar-based intervals, like increases in retail sales that occur around December in response to Christmas or increases in water consumption in summer due to warmer weather.  Care should be taken to not confuse the ‘cyclic’ effect with the ‘seasonal’ effect. So, how to differentiate between a ‘cyclic’ vs ‘seasonal’ pattern?  If patterns are not of fixed calendar-based frequencies, then it is cyclic. Because, unlike seasonality, cyclic effects are typically influenced by the business and other socio-economic factors.  ![Time series components](/images/time-series-vectors-4.png) <small>A number of components can be extracted from a time series: Seasonality, Trend, Cycle and Error (Irregular). Source: [QuantDare](https://quantdare.com/decomposition-to-improve-time-series-prediction/)</small>  ## Forecasting  What if besides analyzing a time series, we could predict it? Forecasting is the process of predicting future behaviors based on current and past data.  A time series represents the relationship between two variables: time is one of them, and the measured value is the second one. From a statistical point of view, we can think of the value we want to forecast as a “random variable”.   A random variable is a variable that is subject to random variations so it can take on multiple different values, each with an associated probability.  A random variable doesn’t have a specific value, but rather a collection of potential values. After a measurement is taken and the specific value is revealed, then the random variable ceases to be a random variable and becomes data.  The set of values that this random variable could take, along with their relative probabilities, is known as the “probability distribution”. When forecasting, we call this the [forecast distribution](https://otexts.com/fpp2/perspective.html). This way, when referring to the “forecast,” we usually mean the average value of the forecast distribution.  ![Time series forecast](/images/time-series-vectors-5.png) <small>A forecast example: the black line after the real data (in green) represents the forecasted value, and the shaded grey area the confidence interval of the forecast. Source: [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2020/07/time-series-forecasting-using-microsoft-power-bi/)</small>  The example on the image above highlights the importance of considering uncertainty. Can we assure how the future will unfold? Of course not, and for this reason it’s important to define prediction intervals (lower and upper boundaries) in which the forecast value is expected to fall.  Statistical time series methods have dominated the forecasting landscape because they are heavily studied and understood, robust, and effective on many problems. Some popular examples of these are **ARIMA** (autoregressive integrated moving average), **Exponential Smoothing** methods such as Holt-Winters, and **Theta**.  However, recent impressive results of Machine Learning methods on time series forecasting tasks triggered a big shift towards these types of models. The year 2018 marked a crucial year when the [M4 forecasting competition](https://mofc.unic.ac.cy/m4/) was won for the first time with a model using Machine Learning techniques. Using this kind of approach, models are able to extract patterns not just from a single time series, but from collections of them. Machine Learning models like Artificial Neural Networks can ingest multiple time series and produce tremendous performances. Nevertheless, these models are “black boxes” that become challenging when interpretability is required.  Decomposing a time series into its different elements allows us to perform unbiased forecasting and brings insights into what might happen in the future. Forecasting is a key activity through different industries and sectors, and those who get it right have a competitive advantage over those who don’t.  By now it becomes clear that we need more than standard techniques to deal with time series. **Time series embeddings** represent a novel way to uncover insights and perform Machine Learning tasks.  ## Embeddings to the Rescue  Distance measurement between data examples is a key component of many classification, regression, clustering, and anomaly detection algorithms for time series. For this reason, it’s critical to develop time series representations that can be used to improve the results over these tasks.  **Time series embeddings are a representation of time data in the form of vector embeddings** that can be used by different models, improving their performance. Vector embeddings are well known and pretty successful in domains like Natural Language Processing and Graphs, but uncommon within time series. Why? Because time series can be challenging to vectorize. Sequential data elude a straightforward definition of similarity because of the necessity of alignment between examples, but [time series similarity is also dependent on the task at hand](https://dspace.mit.edu/bitstream/handle/1721.1/119575/1076345253-MIT.pdf?sequence=1), further complicating the matter.  Fortunately, there are methods that make time-series vectorization a straightforward process. For example, [Time2Vec](https://www.arxiv-vanity.com/papers/1907.05321/) serves as a vector representation for time series that can be used by many models and Artificial Neural Networks architectures like Long Short Term Memory (LSTM), which excel at time series challenges. Time2Vec can be reproduced and used [with Python](https://ojus1.github.io/posts/time2vec/).  ![Time series embedding with time2vec](/images/time-series-vectors-6.png) <small>The red dots on the figure represent multiples of 7. In this example, it can be observed that Time2Vec successfully learns the correct period of the time series and oscillates every 7 days. The phase-shifts have been learned in a way that all multiples of 7 are placed on the positive peaks of the signal to facilitate separating them from the other days. Source: [DeepAI](https://deepai.org/publication/time2vec-learning-a-vector-representation-of-time)</small>  Once your time series are vectorized, you can use [Pinecone](/) to store and search for them in an easy-to-use and efficient environment. Check out [this example](https://www.pinecone.io/docs/examples/time-series/) showing how to perform time-series “pattern” matching to find out the most similar stock trends. ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2ca"
  },
  "content": "categories:   - Company News toc: >- author:   name: Edo Liberty   position: Founder and CEO   src: /images/company-edo.png   href: https://edoliberty.github.io/ date: \"2021-09-14\" # Date: September 14, 2021 #Open Graph description: Pinecone 2.0 helps companies move vector similarity search from R&D labs to production applications. # No image in article, default will be used thumbnail: \"/images/vector-search-thumbnail.jpg\" ---  Pinecone 2.0 helps companies move [vector similarity search](/learn/what-is-similarity-search/) from R&D labs to production applications. The fully managed vector database now comes with metadata filtering for **greater control over search results** and **hybrid storage for up to 10x lower costs**.  This update also includes a new REST API for ease of use, a completely new architecture for maximum reliability and availability, and a completed SOC2 Type II audit for enterprise-grade security.  ## Single-Stage Filtering  Store metadata with your vector embeddings, and limit the vector similarity search to embeddings that meet your metadata filters.  In many cases, you want to combine a vector similarity search with some arbitrary filter to provide more relevant results. For example, doing a semantic search on a corpus of documents but only from certain categories, or excluding certain authors.  In the past, you had two options: The first was pre-filtering, which first filters records by metadata and then must use an inefficient brute-force search through the remaining vectors. The second was post-filtering, where you would first retrieve a large set of nearest neighbors and then apply metadata filters on the results. In that case there is a high latency penalty for retrieving more items than needed, and there is no guarantee the result set would include all the items you actually want.  For the many companies that require filtering in their search, there was no good option. It’s no wonder vector search has been stuck in R&D labs.  The metadata filtering introduced in Pinecone v2.0 provides the fine-grained control over vector search results that many search and recommendation applications require, at the ultra-low latencies their users expect. Get the power of vector search with the control of traditional search. It accepts arbitrary filters on metadata and retrieves exactly the number of nearest-neighbor results that match the filters. For most cases the search latency will be even lower than unfiltered searches.  For example, suppose you want to search through vector embeddings of documents (i.e., semantic search), but only want to include documents labeled as “finance” from this year. You can add the metadata to those document embeddings within Pinecone, and then filter for those criteria when sending the query. Pinecone will search for similar vector embeddings only among those items that match the filter.  <iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/r5CsJ_S9_w4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>  [See documentation for metadata filtering.](/docs/metadata-filtering/)  ## Hybrid Storage  Vector searches typically run completely in-memory (RAM). For many companies with over a billion items in their catalog, the memory costs alone could make vector search too expensive to consider. Some vector search libraries have the option to store everything on disk, but this could come at the expense of search latencies becoming unacceptably high.  Pinecone 2.0 introduces a hybrid configuration, in which a compressed [vector index](/learn/vector-indexes/) is stored in memory and the original, full-resolution vector index is stored on disk. The in-memory index is used to locate a small set of candidates to search within the full index on disk. This method provides the same fast and accurate search results yet **cuts infrastructure costs by up to 10x**.  ## Other Updates ### New Architecture  Pinecone now provides fault tolerance, data persistence, and high availability for customers with billions of items and many thousands of operations per second.  Before, enterprises with strict reliability requirements either had to build and maintain complex infrastructure around vector search libraries to meet those requirements or relax their standards and risk downgraded performance for their users.  Now, the Pinecone platform has been re-architected to use Kafka ingestion and Kubernetes orchestration, in a cloud-native paradigm which separates the read and write paths and disassociates storage and compute. This makes Pinecone’s vector database as reliable, flexible, and performant as top-tier enterprise-grade cloud databases.   ### REST API and Python Client  Pinecone now uses a new [REST API](/docs/api/overview/) based on the OpenAPI spec. This makes Pinecone more flexible and even easier to use for developers from any system and in any language.  Upsert and query vectors using HTTPS and JSON without the need to install anything. The REST API gives you maximum flexibility to use the Pinecone service from any environment that can make HTTPS calls. No need to be familiar with Python.  For users who prefer Python, the Python client has been rebuilt to use the new API and to use fewer dependencies. Clients for Go and Java are coming soon.  This update also comes with a completely revamped [documentation portal](/docs/) to make developing with Pinecone even easier.  ### SOC2  Pinecone is now SOC2 Type II audited, with certification expected soon. Enterprises with even the strictest security requirements can deploy Pinecone to production with confidence and assurance that their data is safe.  [Learn how we keep your data secure](/security/), such as regularly performing third-party penetration tests, keeping data in isolated containers, encryption, and more.  ---  Whether you’re already experimenting with vector search or just learning about it, Pinecone 2.0 makes it quicker, easier, and more cost-effective to bring vector search into production applications than ever before.  Pinecone 2.0 is available now by request. [Contact us](/contact/) with questions or [request a free trial](/start/) today. It will be generally available to all users within a few weeks. Learn all about the new features and ask us questions in a live webinar, [Introduction to Pinecone 2.0](https://pinecone-io.zoom.us/webinar/register/WN_IIs_xe7NTV2fC5t1nbCUXg). ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2cc"
  },
  "title": "\"Multilingual Sentence Transformers\"",
  "content": "  - NLP for Semantic Search toc: >- weight: 5 author:   name: James Briggs   position: Developer Advocate   src: /images/james-briggs.jpeg   href: \"https://www.youtube.com/c/jamesbriggs\" description: How to create multilingual sentence transformers with knowledge distillation. #Open Graph images: ['/images/multilingual-transformers-4.jpg'] ---  We've learned about how [sentence transformers](/learn/sentence-embeddings/) can be used to create high-quality [vector representations](/learn/dense-vector-embeddings-nlp/) of text. We can then use these vectors to find similar vectors, which can be used for many applications such as semantic search or topic modeling.  These models are *very* good at producing meaningful, information-dense vectors. But they don't allow us to compare sentences across different languages.  Often this may not be a problem. However, the world is becoming increasingly interconnected, and many companies span across multiple borders and languages. Naturally, there is a need for sentence vectors that are language agnostic.  Unfortunately, very few textual similarity datasets span multiple languages, particularly for less common languages. And the standard training methods used for sentence transformers would require these types of datasets.  Different approaches need to be used. Fortunately, some techniques allow us to extend models to other languages using more easily obtained language translations.  In this article, we will cover how multilingual models work and are built. We'll learn how to develop our own multilingual sentence transformers, the datasets to look for, and how to use high-performing pretrained multilingual models.  <p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/NNS5pOpjvAQ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>  ## Multilingual Models  By using multilingual sentence transformers, we can map similar sentences from different languages to similar vector spaces.  If we took the sentence `\"I love plants\"` and the Italian equivalent `\"amo le piante\"`, the ideal multilingual sentence transformer would view both of these as exactly the same.  ![Multilingual language vectors represented in a 3D space](/images/multilingual-transformers-1.jpg) <small>A multilingual model will map sentences from different languages into the same vector space. Similar sentences to similar vectors, creating ‘language-agnostic’ vectors (as highlighted).</small>  The model should identify `\"mi piacciono le piante\"` (*I like plants*) as more similar to `\"I love plants\"` than `\"ho un cane arancione\"` (*I have an orange dog*).  Why would we need a model like this? For any scenario we might find usual sentence transformers applied; identifying similar documents, finding plagiarism, topic modeling, and so on. But now, used across borders or extended to previously inaccessible populations.  The lack of suitable datasets means that many languages have limited access to language models. By starting with existing, high-performance monolingual models trained in high resource languages (such as English), we can use multilingual training techniques to extend the performance of these models to other languages using significantly less data.    ## Training approaches  Typical training methods for sentence transformer models use some sort of contrastive training function. Given a high similarity sentence pair, models are optimized to produce high similarity sentence vectors.  Training data for this is not hard to come by as long as you stick to common languages, mainly English. But it can be hard to find data like this in other languages.  Both examples below rely *in-part or in full* on having translation pairs rather than similarity pairs, which are easier to find. There are *many* materials in the world that have been translated, but far fewer that compare similar same-language sentences.    ### Translation-based Bridge Tasks  Using a multi-task training setup, we train on two alternate datasets:  1. An English dataset containing question-answer or anchor-positive) pairs (anchor-positive meaning two high-similarity sentences). 2. *Parallel data* containing cross-language pairs (English_sentence, German_sentence).  The idea here is that the model learns monolingual sentence-pair relationships via a (more common) source language dataset. Then learns how to translate that knowledge into a multilingual scope using *parallel data* [2].  This approach works, but we have chosen to focus on the *next* multilingual training approach for a few reasons:  * The amount of training data required is high. The multilingual universal sentence encoder (mUSE) model was trained on over a billion sentence pairs [3]. * It uses a multi-task dual-encoder architecture. Optimizing two models in parallel is harder as both training tasks must be balanced (optimizing one is hard enough…). * Results can be mediocre without the use of hard negatives [1]. Hard negatives are sentences that *seem similar* (often on a related topic) but are irrelevant/or contradict the *anchor* sentence. Because they're *harder* for a model to identify as dissimilar, by training on these, the model becomes better.  Let’s move on to our preferred approach and the focus of the remainder of the article.    ### Multilingual Knowledge Distillation  Another approach is to use **multilingual knowledge distillation** — a more recent method introduced by Nils Reimers and Iryna Gurevych in 2020 [1]. With this, we use two models during fine-tuning, the *teacher* and *student* models.  The teacher model is an already fine-tuned sentence transformer used for creating embeddings in a single language (most likely English). The student model is a transformer that has been *pretrained* on a multilingual corpus.  ---  *There are two stages to training a transformer model. Pretraining refers to the initial training of the core model using techniques such as masked-language modeling (MLM), producing a 'language engine'. Fine-tuning comes after — where the core model is trained for a specific task like semantic similarity, Q&A, or classification.*  *However, it is also common to refer to previously fine-tuned models as pretrained.*  ---  We then need a parallel dataset (translation pairs) containing translations of our sentences. These translation pairs are fed into the teacher and student models.  ![Fine-tuning process with multilingual knowledge distillation](/images/multilingual-transformers-2.jpg) <small>Chart showing the flow of information from parallel pairs through the teacher and student models and the optimization performed using MSE loss. Adapted from [1].</small>  Let's assume we have English-Italian pairs. The English sentence is fed into our teacher and student models, producing two English sentence vectors. Then we feed the Italian sentence into the student model. We calculate the mean squared error (MSE) loss between the one teacher vector and the two student vectors. The student model is optimized using this loss.  The student model will learn to mimic the monolingual teacher model — but for multiple languages.  Using multilingual knowledge distillation is an excellent way to extend language options using already trained models. It requires much less data than training from scratch, and the data it uses is widely available — translated pairs of sentences.    ## Fine-tuning with Multilingual Sentence Transformers  The final question is, how do we build one of these models? We covered multilingual knowledge distillation *conceptually*, but translating concepts into code is never as straightforward as it seems.  Luckily for us, the `sentence-transformers` library makes this process *much* easier. Let's see how we can use the library to build our very own multilingual models.    ### Data Preparation  As always, we start with data. We need a data source that contains multilingual pairs, split into our *source* language and *target* language(s).  Note that we wrote language(s) — we can fine-tune a model on *many* languages. In fact, some of the multilingual models in [sentence-transformers](https://sbert.net/docs/pretrained_models.html#multi-lingual-models) support more than *50* languages. All of these are trained with multilingual knowledge distillation.  In the paper from Reimers and Gurevych, one dataset uses translated subtitles from thousands of TED talks. These subtitles also cover a wide range of languages (as we will see). We can access a similar dataset using HF `datasets`.  {{< notebook file=\"load-dataset\" height=\"full\" >}}  This dataset contains a list of language labels, the translated sentences, and the talk they came from. We only really care about the labels and sentences.  {{< notebook file=\"ted-out\" height=\"full\" >}}  We need to transform this dataset into a friendlier format. The data we feed into training will consist of nothing more than pairs of *source* sentences and their respective *translations*.  To create this format, we need to use the language labels to (1) identify the position of our *source* sentence and (2) extract translations of languages we want to fine-tune on. Which will look something like this:  {{< notebook file=\"transform-example\" height=\"full\" >}}  Here we returned *27* pairs from a single row of data. We don't *have* to limit the languages we fine-tune on. Still, unless you plan on using and evaluating every possible language, it's likely a good idea to restrict the range.  We will use English `en` as our source language. For target languages, we will use Italian `it`, Spanish `es`, Arabic `ar`, French `fr`, and German `de`. *These are ISO language codes, which you can find [here](http://www.mathguide.de/info/tools/languagecode.html)*.  Later we will be using a `ParallelSentencesDataset` class, which expects our pairs to be separated by a tab character `\\t`, and each language pair in a different dataset — so we add that in too.  {{< notebook file=\"pair-build\" height=\"full\" >}}  Hopefully, all TED talk subtitles end with `'( Applause )'`. With that, let's save our training data to file ready for the `ParallelSentencesDataset` class to pick it up again later.  {{< notebook file=\"save-pairs\" height=\"full\" >}}  That's it for data preparation. Now let's move on to set everything up for fine-tuning.    ### Set-up and Training  Before training, we need *four* things:  * Our `teacher` model. * The new `student` model. * A loaded `DataLoader` to feed the *(source, translation)* pairs into our model during training. * The loss function.  Let's start with our *teacher* and *student* models.  #### Model Selection  We already know we need a *teacher* and a *student*, but how do we choose a *teacher* and *student*? Well, the *teacher* must be a competent model in producing sentence embeddings, just as we'd like our teachers to be competent in the topic they are teaching us.  The ideal student can take what the teacher teaches and extend that knowledge beyond the teacher's capabilities. We want the same from our student model. That means that it must be capable of functioning with different languages.  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/multilingual-transformers-3.mp4\" type=\"video/mp4\"> </video>  <small class=\"video\">The knowledge of a monolingual teacher capable of performing in a single language is distilled to a new model capable of working across multiple languages.</small>  Not all models can do this, and of the models that can — some are better than others.  The first check for a capable student model is its tokenizer. Can the student's tokenizer deal with a variety of languages?  BERT uses a WordPiece tokenizer. That means that it encodes either word-level or sub-word-level chunks of text. The vocabulary of a pretrained BERT tokenizer is already set and limited to (mostly) English tokens. If we begin introducing unrecognizable words/word pieces, the tokenizer will convert them into 'unknown' tokens or small character sequences.  When BERT sees the occasional unknown token, it's not a problem. But if we feed many unknowns to BERT — it becomes unmanageable. If I gave you the sentence:  ``` I went to the [UNK] today to buy some milk. ```  You could probably fill in the 'unknown' `[UNK]` with an accurate guess of 'shop' or 'store'. What if I gave you:  ``` [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ```  Can you fill in the blanks? In this sentence, I said *\"I went for a walk in the forest yesterday\"* — if you guessed correct, well done! If not, well, that's to be expected.  BERT works in the same way. It can fill in the occasional blank, but too many, and the task is unsolvable. Let's take a look at how BERTs tokenizer copes with different languages.  {{< notebook file=\"bert-tokenize\" height=\"full\" >}}  The tokenizer misses most of our Chinese text and all of the Georgian text. Greek is split into character-level tokens, limiting the length of input sequences to just 512 characters. Additionally, character-level tokens carry limited meaning.  A BERT tokenizer is therefore not ideal. There is another transformer model built for multilingual comprehension called XLM-RoBERTa (XLMR).   XLMR uses a *SentencePiece*-based tokenizer with a vocabulary of 250K tokens. This means XLMR already *knows* many more words/characters than BERT. *SentencePiece* also handles new languages much better thanks to language-agnostic preprocessing (it treats all sentences as sequences of Unicode characters) [4].  {{< notebook file=\"xlmr-tokenizer\" height=\"full\" >}}  We can see straight away that our XLMR tokenizer handles these other languages *much* better. Naturally, we'll use XLMR as our student.  The student model will be initialized from Hugging Face's transformers. It has *not* been fine-tuned to produce sentence vectors, and we need to initialize a *mean pooling* to convert the 512 token vectors into a single sentence vector.  To put these two components together, we will use `sentence-transformers` transformer and pooling modules.  {{< notebook file=\"student\" height=\"full\" >}}  That's our student. Our teacher must be an already fine-tuned monolingual sentence transformer model. We could try the `all-mpnet-base-v2` model:  {{< notebook file=\"teacher-all\" height=\"full\" >}}  But here, there is a final *normalization* layer. We need to avoid outputting normalized embeddings for our student to mimic. So we either remove that normalization layer or use a model without it. The `paraphrase` models do *not* use normalization. We'll use one of those.  {{< notebook file=\"teacher-paraphrase\" height=\"full\" >}}  And with that, we're ready to set everything up for fine-tuning.   #### Fine-Tuning  For fine-tuning, we now need to initialize our data loader and loss function. Starting with the data loader, we first need to initialize a `ParallelSentencesDataset` object.  {{< notebook file=\"dataset-object\" height=\"full\" >}}  And once we have initialized the dataset object, we load in our data.  {{< notebook file=\"load-data\" height=\"full\" >}}  With our dataset ready, all we do is pass it to a PyTorch data loader.  {{< notebook file=\"dataloader\" height=\"full\" >}}  The final thing we need for fine-tuning is our loss function. As we saw before, we will be calculating the MSE loss, which we initialize like so:  {{< notebook file=\"mseloss\" height=\"full\" >}}  It's that simple! Now we're onto the fine-tuning itself. As usual with `sentence-transformers` we call the `.fit` method on our student model.  {{< notebook file=\"model-fit\" height=\"full\" >}}  And we wait. Once fine-tuning is complete, we find the new model in the `./xlmr-ted` directory. The model can be loaded using the `SentenceTransformer` class as we would any other sentence transformer.  It would be helpful to understand how our model is performing, so let's take a look at model evaluation.    ### Evaluation  To evaluate our model, we need a multilingual textual similarity dataset. That is a dataset containing multilingual pairs and their respective similarity scores. A great one is the Sentence Textual Similarity benchmark (STSb) multilingual dataset.  We can find this dataset on HF `datasets`, named `stsb_multi_mt`. It includes *a lot* of different languages, but we will stick to evaluating two, English and Italian. First, we download both of those.  {{< notebook file=\"load-data-eval\" height=\"full\" >}}  Each row of the different language sets aligns with the same row in the other language sets. Meaning *sentence1* in row *0* of the English dataset is translated to *sentence1* in row *0* of the Italian dataset.  {{< notebook file=\"eval-align\" height=\"full\" >}}  Here the Italian dataset *sentence1* means *'A girl is styling her hair'*. This alignment also applies to *sentence2* and the *similarity_score*.  One thing we do need to change in this dataset is the *similarity_score*. When we calculate the *positive* cosine similarity between sentence vectors, we will output a zero (no similarity) to one (exact matches) value. The *similarity_score* varies between zero to five. We must normalize this to bring it within the correct range.  {{< notebook file=\"norm\" height=\"full\" >}}  Before feeding our data into a similarity evaluator, we need to reformat it to use an `InputExample` format. While we do this, we will also merge English and Italian sets to create a new English-Italian dataset for evaluation.  {{< notebook file=\"input-example\" height=\"full\" >}}  We can use an `EmbeddingSimilarityEvaluator` class to evaluate the performance of our model. First, we need to initialize one of these evaluators for each of our sets.  {{< notebook file=\"eval-init\" height=\"full\" >}}  And with that, we just pass our student model through each evaluator to return its performance.  {{< notebook file=\"trained-eval\" height=\"full\" >}}  That looks pretty good. Let's see how it compares to our untrained student.  {{< notebook file=\"untrained-eval\" height=\"full\" >}}  Some really great results. We can now take the new model and use it with  English `en`, Italian `it`, Spanish `es`, Arabic `ar`, French `fr`, and German `de`.    ### Sentence Transformer Models  Fortunately, we rarely need to fine-tune our own model. We can load many high-performing multilingual models as quickly as we initialized our teacher model earlier.  We can find a list of these multilingual models on the [Pretrained Models](https://sbert.net/docs/pretrained_models.html#multi-lingual-models) page of the `sentence-transformers` docs. A few of which support more than 50 languages.  To initialize one of these, all we need is:  ```python from sentence_transformers import SentenceTransformer  model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2') ```  And that's it, encode your sentences with `model.encode`, and you're good to go.  ---  That's all for this article on multilingual sentence transformers. We've taken a look at the two most common approaches taken to train multilingual sentence transformers; multi-task translation-based bridging and multilingual knowledge distillation.  From there, we dived into the tune-tuning process of a multilingual model using multilingual knowledge distillation, covering the required data, loss functions, fine-tuning, and evaluation.  We've also looked at how to use the existing pretrained multilingual sentence transformers.  ## References  [1] N. Reimers, I. Gurevych, [Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation](https://arxiv.org/abs/2004.09813#) (2020), EMNLP  [2] M. Chidambaram, [Learning Cross-Lingual Sentence Representations vis a Multi-task Dual-Encoder Model](https://arxiv.org/abs/1810.12836) (2019), RepL4NLP  [3] Y. Yang, et al., [Multilingual Universal Sentence Encoder for Semantic Retrieval](https://arxiv.org/abs/1907.04307) (2020), ACL  [4] Google, [SentencePiece Repo](https://github.com/google/sentencepiece), GitHub ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2ce"
  },
  "title": "How Nyckel Built An API for Semantic Image Search",
  "headline": "How Nyckel Built An API for Semantic Image Search",
  "weight": "2",
  "name": "George Mathew",
  "position": "Founder, Nyckel",
  "src": "/images/george-mathew.png",
  "href": "\"https://www.linkedin.com/in/georgemathew\"",
  "description": "How Nyckel automates ML workflows for image and text classification, object detection, and visual search with the help of vector search.",
  "images": "[\"/images/nyckel-1.jpg\"]",
  "content": "*Written by [George Mathew](https://www.linkedin.com/in/georgemathew), Co-Founder at Nyckel.*  Businesses that accumulate large amounts of data eventually run into problems with information retrieval – especially if they’re working with image data. While information retrieval for text is mostly a solved problem, the same cannot be said for images. Recent advances in deep neural networks and in vector search databases make it possible to search through large sets of images. Nyckel implements a [simple API](https://www.nyckel.com/semantic-image-search) for semantic image search – let’s look deeper at how it is implemented.   ## What is semantic image search?  Semantic search is the ability to search based on user intent and on the contextual meaning of both the search query and the target content. For images, this means understanding the meaning of search queries (whether in the form of text or images), and then mapping them to images in the search set. This is exactly what Google’s image search engine does. I could use a picture of my dog catching a frisbee on the beach, paste it into Google reverse image search, and it would retrieve public web images that are similar to my picture. Alternatively, I could use the more common means of Google searching by typing in “dog catching a frisbee on the beach”, and Google will retrieve public web images that match those search terms. The latter search case is called a cross-modal search because the search term and the result item are in different modalities: in this case, text and image respectively.  ![Dog catching a frisbee](/images/nyckel-1.jpg)  ## Applications of Image Search  As a platform, we are constantly surprised and encouraged by the variety of use-cases we see. Here are a few examples of applications that our customers are using our image search API for:  - Detecting the unauthorized use of images, e.g., copyright infringement or fraud; - De-duplicating images in a collection when the duplicates are not exact matches; - Image classification into hundreds of thousands of classes where traditional classification mechanisms are difficult; - Enabling search functionality over their images for their end-customers or employees. To showcase this, we built a [demo app](https://www.nyckel.com/nft-finder/) to search over the wild world of NFT images:  ![NFT image search](/images/nyckel-2.png)  ## Nyckel's Image Search Service  Nyckel is the lightning fast ML-platform for non-experts. We make ML functionality, such as [image](https://www.nyckel.com/docs/image-classification-quickstart) / [text](https://www.nyckel.com/docs/quickstart) classification, [object detection](https://www.nyckel.com/docs/detection-quickstart), and [image search](https://www.nyckel.com/docs/image-search-quickstart), accessible to developers and companies without any ML expertise.   For image search, our customers don’t want to deal with model selection, tuning, deployment, and infrastructure management and scaling. They use the following simple API, and we take care of the rest:  ![Image search API](/images/nyckel-3.jpg)  ## What goes into implementing the API?  Searching within any medium requires two capabilities:  - Extracting semantic information from the data – both the search set and query data; - Indexing the semantic information for fast and accurate retrieval.  ### Extracting Semantic Information from Images  Neural networks, such as Resnet, are good at extracting a semantic representation of an image in the form of “embeddings”: fixed-length vectors of floating point numbers. Once you have a set of images that have been mapped in this way, then you can use your vector database to index the vector embeddings. If two images have similar embeddings, meaning that they are close to one another in vector space, then they are semantically similar. Conversely, the embeddings for two dissimilar images will be far apart in vector space. You can read more about embeddings and how useful they are in [Peter Gao’s blog post](https://medium.com/aquarium-learning/the-unreasonable-effectiveness-of-neural-network-embeddings-93891acad097).  But what about cross-modal semantic search, for example when queries for image data come in the form of textual descriptions? What do we do in these cases? We could use a natural language model like BERT to get text embeddings, but these embeddings will be unrelated to the image embeddings. What this means is that the embedding for \"dog catching a frisbee on the beach\" is not guaranteed to be close to the embedding for an image of a dog catching a frisbee on the beach. In the next section we'll talk about how to solve this problem.  ### Cross-Modal Embeddings  In order to support searching for images with a text query, we need a neural network that can provide embeddings for images and text in the same \"embedding space\". OpenAI's [CLIP](https://openai.com/blog/clip/) is one such network. It’s actually two networks: one for images and one for text, trained together to minimize \"[contrastive loss](https://towardsdatascience.com/contrastive-loss-explaned-159f2d4a87ec)\". CLIP is trained on a large corpus of (image, text) pairs from the internet. With the idea that (image, text) pairs that accompany each other are likely to be semantically similar, the training objective was to minimize the distance between vector embeddings for pairs that accompany each other and maximize the distance between embeddings for unrelated pairs. You can read more about CLIP [here](/learn/clip/).  The result is that CLIP’s embeddings for an image of a dog catching a frisbee on the beach, and the text “dog catching frisbee on beach” will be close together. This gives us the embeddings we need for cross-modal search.   ### Indexing  Once you have embeddings, searching for images becomes a problem of finding the closest vectors to the embedding for that particular query. Finding the closest vectors is a computationally challenging problem for large numbers of high-dimensional vectors. But there are relatively efficient mechanisms for finding approximate closest matches (commonly known as approximate nearest neighbors, or ANN) and several vector-database products that offer this capability. We evaluated several vector databases and picked pinecone because they are a fully managed service with low ingest latency, low query latency, on-demand and scalable provisioning and pricing, and an easy-to-use API.   ## Putting it together  Now that we've talked about all the pieces involved in semantic image search, let's look at how Nyckel puts them together for its image search service. The sequence diagram below ignores some of the complexities and asynchronous processing, but provides a general overview of how things work:  ![Nyckel's image search service](/images/nyckel-4.png)  ## Future work  What we’ve described above is just the beginning and we have an exciting roadmap for image search. Here are a couple of items items on our to-do list:  - Support multilingual text queries. CLIP currently only does well with English queries, and we want to support many more languages; - Model selection and fine-tuning for specialized use-cases based on search feedback. CLIP does surprisingly well for a wide range of subject matter, but in cases where the set of images is in a very narrow or specialized domain, model fine-tuning will help provide significantly better search results.  ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2d0"
  },
  "title": "\"Vision Transformers (ViT) Explained\"",
  "headline": "\"Vision Transformers (ViT) Explained\"",
  "weight": "5",
  "- name": "Laura Carnevali",
  "position": "Developer",
  "src": "/images/laura-carnevali.jpeg",
  "href": "\"https://www.linkedin.com/in/laura-carnevali-071a01b7/\"",
  "description": "A deep dive into the Vision Transformer (ViT) and practical implementation",
  "images": "['https://www.pinecone.io/images/vision-transformer-0.png']",
  "content": "**[Pinecone](/) lets you implement semantic, audio, or visual search into your applications using vector search. But first you need to convert your data into vector embeddings, and vision transformers do that for images. This article introduces vision transformers, how they work, and how to use them.**  Vision and language are the two big domains in machine learning. Two distinct disciplines with their own problems, best practices, and model architectures. At least, that *was* the case.  The **Vi**sion **T**ransformer (ViT)<sup>[1]</sup> marks the first step towards the merger of these two fields into a single unified discipline. For the first time in the history of ML, a single model architecture has come to dominate both language *and vision*.  Before ViT, transformers were *\"those language models\"* and nothing more. Since then, ViT and further work has solidified them as a likely contender for the architecture that merges the two disciplines.  This article will dive into ViT, explaining and visualizing the intuition behind how and why it works. Later, we'll look at how to implement it ourselves.  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/qU7wO02urYU\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ---  # Transformers  Transformers were introduced in 2017 by Vaswani et al. in the now-famous paper *\"Attention is All You Need\"* <sup>[2]</sup>. The primary function powering these models is the *attention mechanism*.  ## Attention 101  In NLP, attention allows us to consider the context of words and *focus attention* on the key relationships between different tokens (represented as word or sub-word tokens).  It works by comparing \"token embeddings\" and calculating an *\"alignment\"* score that describes how similar two tokens are based on their semantic and contextual meaning.  In the layers preceding the attention layer, each word embedding is encoded into a *\"vector space\"*.  In this vector space, similar tokens share a similar location. Therefore, when we calculate the dot product between token embeddings (inside the attention mechanism), we return a high *alignment* score when embeddings are aligned in vector space. When embeddings are *not aligned*, we produce a low alignment score.  <img src=\"./images/vision-transformers-1.png\" alt=\"alignment of vectors\" style=\"width:60%;\" center /> <small>Alignment between vectors is higher where vectors share similar direction and magnitude.</small>  Before applying attention, our tokens' initial positions are based purely on a \"general meaning\" of a particular word or sub-word token.  As we go through several encoder blocks (these include the attention mechanism), the position of these embeddings is updated to better reflect the meaning of a token *with respect* to its context. The context being all of the other words within that specific sentence.  So, given three phrases:  * A plane **banks** * The grassy **bank** * The **Bank** of England  The initial embedding for the token *bank* is equal. Yet, the token is pushed towards its context-based meaning through many attention encoder blocks. These blocks might push *bank* towards tokens like [plane, airport, flight], [nature, fields, outdoors], or [finance, England, money].  ![contextual-meaning-of-embeddings](./images/vision-transformers-10.png) <small>An encoder with attention layers can add contextual meaning to embeddings.</small>  Attention has been used in **C**onvolutional **N**eural **N**etworks (CNNs) over the years. Generally speaking, this has been shown to produce *some benefit* but is often computationally limited.  Attention is a heavy operation and does not scale to large sequences. Therefore, attention can *only* be used in later CNN layers — where the number of pixels has been reduced. This limits the potential benefit of attention as it cannot be applied across the complete set of network layers <sup>[1] [3]</sup>.  Transformer models do not have this limitation and instead apply attention over many layers.  BERT, a well-known transformer architecture, uses several \"encoder\" blocks. Each of these blocks consists of normalization layers, *multi-head* attention (i.e., several parallel attention operations) layers, and a multilayer perceptron (MLP) component.  Each of these encoder *\"blocks\"* encodes *more information* into the token (or patch) embeddings using their context. This operation produces a *deeper* semantic representation of each token.  At the end of this process, we get super information-rich embeddings. These embeddings are the ultimate output of the *core* of a transformer, including ViT.  Another set of layers is used to transform these rich embeddings into useful predictions. These final few layers are called the *\"head\"* and a different *head* is used for each task, such as for classification, [**N**amed **E**ntity **R**ecognition (NER)](/docs/examples/ner-search/), [question-answering](/learn/question-answering/), etc.  ![model-heads](./images/vision-transformers-2.png) <small>Example of a transformer encoder building information-rich final embeddings before passing these on to a task-specific *\"head\"*.</small>  ViT works similarly, but rather than consuming *word tokens*, ViT consumes *image patches*. The remainder of the transformer functions in the same way.  # Images to Patch Embeddings  The new procedure introduced by ViT is limited to the first few processing steps. These first steps take us from images to a set of *patch embeddings*.  If we didn't split the images into patches, we could alternatively feed in pixel values of the image directly. However, this causes problems with the attention mechanism.  Attention requires the comparison of every input to all other inputs. If we perform that on a *224x224* pixel image, we must perform $224^4$ ($2.5E^9$) comparisons. That's for a single attention layer, of which transformers contain several.  Doing this would be a computational nightmare far beyond the capabilities of even the latest GPUs and TPUs within a reasonable timeframe.  Therefore, we create image patches and embed those as patch embeddings. Our high-level process for doing this is as follows:  ![model-prep-steps](./images/vision-transformers-3.png)  1. Split the image into image patches. 2. Process patches through the linear projection layer to get initial patch embeddings. 3. Preappend trainable *\"class\"* embedding to patch embeddings. 4. Sum patch embeddings and *learned positional embeddings*.  After these steps, we process the patch embeddings like token embeddings in a typical transformer. Let's dive into each of these components in more detail.  ## Image Patches  Our first step is the transformation of images into image patches. In NLP, we do the same thing. Images are sentences and patches are word or sub-word tokens.  ![nlp-vs-cv-tokens](./images/vision-transformers-4.png) <small>NLP transformers and ViT both split larger sequences (sentences or images) into tokens or patches.</small>  Recall that a *224x224* pixel image requires $2.5E9$ comparisons. If, instead, we split a *224x224* pixel image into 256 *14x14* pixel image patches, a single attention layer requires a more manageable $256 * 14^4$ ($9.8e6$) comparisons.  ![image-patches](./images/vision-transformers-5.png) <small>Conversion of *224x224* pixel image into *256* *14x14* pixel image patches.</small>  Through this, these image patches act as a form of *much needed* quantization required for effective use of attention.  ## Linear Projection  After building the image patches, a *linear projection* layer is used to map the image patch *\"arrays\"* to *patch embedding \"vectors\"*.  ![linear-projection](./images/vision-transformers-6.png) <small>The linear projection layer attempts to transform arrays into vectors while maintaining their \"physical dimensions\". Meaning similar image patches should be mapped to similar patch embeddings.</small>  By mapping the patches to embeddings, we now have the correct dimensionality for input into the transformer. However, two more steps remain before the embeddings are fully prepared.  ## Learnable Embeddings  One *feature* introduced to transformers with the popular BERT models was the use of a `[CLS]` (or *\"classification\"*) token. The `[CLS]` token was a *\"special token\"* prepended to every sentence fed into BERT<sup>[4]</sup>.  ![bert-cls](./images/vision-transformers-7.png) <small>The BERT `[CLS]` token is preappended to every sequence.</small>  This `[CLS]` token is converted into a token embedding and passed through several encoding layers.  Two things make `[CLS]` embeddings special. First, it does *not* represent an actual token, meaning it begins as a \"blank slate\" for each sentence. Second, the final output from the `[CLS]` embedding is used as the input into a classification head during pretraining.  Using a \"blank slate\" token as the sole input to a classification head pushes the transformer to learn to encode a *\"general representation\"* of the entire sentence into that embedding. The model *must* do this to enable accurate classifier predictions.  ViT applies the same logic by adding a *\"learnable embedding\"*. This learnable embedding is the same as the `[CLS]` token used by BERT.  ![model-class-highlighted](./images/vision-transformers-8.png) <small>ViT process with the learnable *class embedding* highlight (left).</small>  The preferred pretraining function of ViT is based solely on classification, unlike BERT, which uses masked language modeling. Based on that, this learning embedding is *even more* important to the successful pretraining of ViT.  ## Positional Embeddings  Transformers do *not* have any default mechanism that considers the \"order\" of token or patch embeddings. Yet, *order* is essential. In language, the order of words can completely change their meaning.  The same is true for images. If given a jumbled jigsaw set, it's hard-to-impossible for a person to accurately predict what the complete puzzle represents. This applies to transformers too. We need a way of enabling the model to infer the *order* or *position* of the puzzle pieces.  We enable order with *positional embeddings*. For ViT, these positional embeddings are learned vectors with the same dimensionality as our patch embeddings.  After creating the patch embeddings and prepending the \"class\" embedding, we sum them all with positional embeddings.  These positional embeddings are learned during pretraining and (sometimes) during fine-tuning. During training, these embeddings converge into vector spaces where they show *high similarity* to their neighboring position embeddings — particularly those sharing the same column and row:  <img src=\"./images/vision-transformers-9.png\" alt=\"position embedding similarity\" style=\"width:80%;\" center /> <small>Cosine similarity between trained positional embeddings. Adapted from [1].</small>  After adding the positional embeddings, our *patch embeddings* are complete. From here, we pass the embeddings to the ViT model, which processes them as a typical transformer model.  # Implementation  <div class=\"source\">   <a href=\"https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/image-retrieval/vision-transformers/vit.ipynb\" class=\"source-link\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a> </div>  We've worked through the logic and innovations introduced by ViT. Let's now work through an example of implementing the model. We start by installing all of the libraries that we'll be using:  ``` !pip install datasets transformers torch ```  We will fine-tune with a well-known image classification dataset called CIFAR-10. It can be downloaded via Hugging Face's *Datasets* library, and we'll download both the training *and* validation/test datasets.  {{< notebook file=\"vit-datasets\" height=\"full\" >}}  The training dataset contains 50K images across 10 classes. To find the human-readable class labels, we can do the following:  {{< notebook file=\"vit-class-labels\" height=\"full\" >}}  Every record in the dataset contains an `img` and `label` feature. The `img` values are all Python PIL objects with *32x32* pixel resolution and three color channels, **r**ed, **g**reen, and **b**lue (RGB).  {{< notebook file=\"vit-single-record\" height=\"full\" >}}  ## Feature Extractor  Preceding the ViT model, we use something called a *feature extractor*. The feature extractor is used to *preprocess* images into normalized and resized image *\"pixel_values\"* tensors. We initialize it from the Hugging Face *Transformers* library like so:  {{< notebook file=\"vit-feature-extractor\" height=\"full\" >}}  The feature extractor configuration shows that normalization and resizing are set to true. Normalization is performed across the three color channels using the mean and standard deviation values stored in `\"image_mean\"` and `\"image_std\"` respectively. The output size is set by `\"size\"` at *224x224* pixels.  To process an image with the feature extractor, we do the following:  {{< notebook file=\"vit-feature-extractor-process\" height=\"full\" >}}  Later we'll be fine-tuning our ViT model with these tensors. Although fine-tuning is *not* as computationally heavy as pretraining, it still takes time. Therefore we *ideally* should be running everything on GPU rather than CPU. So, we move these tensors to a CUDA-enabled GPU *if* it is available.  ```python import torch #  if cuda enabled GPU is available, use it device = torch.device(   \t'cuda' if torch.cuda.is_available() else 'cpu' ) patches = patches.to(device) ```  Fortunately, the `Trainer` utility we will use for fine-tuning later *does handle* this move for our data by default. Still, we will need to later repeat this step for the model.  To apply this preprocessing step across the entire dataset more efficiently, we will package into a function called `preprocess` and apply the transformations using the `with_transform` method, like so:  ```python def preprocess(batch):     # take a list of PIL images and turn them to pixel values     inputs = feature_extractor(         batch['img'],         return_tensors='pt'     )     # include the labels     inputs['label'] = batch['label']     return inputs  # apply to train-test datasets prepared_train = dataset_train.with_transform(preprocess) prepared_test = dataset_test.with_transform(preprocess) ```  ## Loading ViT  The next step is downloading and initializing ViT. Again, we're using Hugging Face *Transformers* with the same `from_pretrained` method used to load the feature extractor.  ```python from transformers import ViTForImageClassification  labels = dataset_train.features['label'].names  model = ViTForImageClassification.from_pretrained(     model_name_or_path,     num_labels=len(labels)  # classification head ) # move to GPU (if available) model.to(device) ```  Because we are fine-tuning ViT for classification, we use the `ViTForImageClassification` class. By default, this will initialize a classification head with just two outputs.  We have *10* classes in CIFAR-10, so we must specify that we'd like to initialize the head with *10* outputs. We do this via the `num_labels` parameter.  Now we're ready to move on to fine-tuning.  ## Fine-Tuning  We will implement fine-tuning using Hugging Face's `Trainer` function. `Trainer` is an abstracted training and evaluation loop implemented in PyTorch for transformer models.  There are several variables that we must define beforehand. First, we start with the collate function. Collate helps us handle the collation of our dataset into batches of tensors that we will be fed into the model during training.  ```python def collate_fn(batch):     return {         'pixel_values': torch.stack([x['pixel_values'] for x in batch]),         'labels': torch.tensor([x['label'] for x in batch])     } ```  Another important variable is the *evaluation metric* to measure our model performance over time. We will use a simple *accuracy* metric calculated as: $$ Accuracy = \\frac{TP+TN}{TP+TN+FP+FN} $$ Where:  $TP:$ *True Positives*  $TN: $ *True Negatives*  $FP:$ *False Positives*  $FN:$ *False Negatives*  We implement this using *Datasets* metrics, defined in the `compute_metrics` function:  ```python import numpy as np from datasets import load_metric  # accuracy metric metric = load_metric(\"accuracy\") def compute_metrics(p):     return metric.compute(         predictions=np.argmax(p.predictions, axis=1),         references=p.label_ids     ) ```  The final variable required by `Trainer` is the `TrainingArguments` configuration. These are simply the training parameters, save settings, and logging settings.  ```python from transformers import TrainingArguments  training_args = TrainingArguments(   output_dir=\"./cifar\",   per_device_train_batch_size=16,   evaluation_strategy=\"steps\",   num_train_epochs=4,   save_steps=100,   eval_steps=100,   logging_steps=10,   learning_rate=2e-4,   save_total_limit=2,   remove_unused_columns=False,   push_to_hub=False,   load_best_model_at_end=True, ) ```  With all this, we're ready to initialize `Trainer` and begin the training loop.  ```python from transformers import Trainer  trainer = Trainer(     model=model,     args=training_args,     data_collator=collate_fn,     compute_metrics=compute_metrics,     train_dataset=prepared_train,     eval_dataset=prepared_test,     tokenizer=feature_extractor, ) # begin training results = trainer.train() ```  Training will take some time, even on GPU. Once complete, the best version of the model will be saved in the `output_dir` we set in the `TrainingArguments` config object.  ## Evaluation and Prediction  The `Trainer` performs evaluation during training but we can also perform a more qualitative check (or make a prediction) by passing a single image through the `feature_extractor` and `model`. We will use this image:  {{< notebook file=\"vit-show-image\" height=\"full\" >}}  The image isn't very clear, and most people would struggle to correctly classify the image. However, we can see from the label that this is a cat. Let's see what the model predicts.  {{< notebook file=\"vit-inference\" height=\"full\" >}}  Looks like the model is correct!  ---  That concludes our introduction to the Vision Transformer and how to use it via Hugging Face *Transformers*. It's worth noting how quickly transformers have come to dominate NLP and, increasingly likely, computer vision in the near future.  Before 2021, transformers being used in anything but NLP was unheard of. Yet, despite being known as *\"those language models\"*, they have already found use in some of the most advanced computer vision applications. Transformers are a crucial component of diffusion models<sup>[5]</sup> and even Tesla's Full Self Driving<sup>[6]</sup>.  As time progresses, we will undoubtedly see both fields continue to merge and more real-world applications of transformers in both domains.  {{< newsletter text=\"Subscribe for the latest ML news!\" inputText=\"Email address...\" buttonText=\"Submit\">}}  ---  # Resources  [Vision Transformers in Examples Repo](https://github.com/pinecone-io/examples/blob/master/learn/image-retrieval/vision-transformers/vit.ipynb)  [1] A. Dosovitskiy et al., [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) (2021), ICLR  [2] A. Vaswani et al., [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (2017), NeurIPS  [3] L. Beyer, [Transformers in Vision: Tackling problems in Computer Vision](https://www.youtube.com/watch?v=BP5CM0YxbP8) (2022), Stanford Seminar  [4] J. Devlin et al., [BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) (2019), ACL  [5] [Stable Diffusion](https://github.com/CompVis/stable-diffusion) (2022), CompVis GitHub Repo  [6] A. Kaparthy, [Tesla AI Day 2021 on Transformers in Vision](https://youtu.be/j0z4FweCy4M?t=3554) (2021), Tesla ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2d2"
  },
  "title": "Introduction",
  "description": "Natural Language Processing (NLP) for Semantic Search.",
  "author": "James Briggs",
  "intro": "|",
  "emailSubmit": "true",
  "socialShare": "true",
  "image": "/images/nlp-ebook.png",
  "images": "['/images/nlp-ebook.png']",
  "introChapter": "",
  "text": "How to create sentence transformers by fine-tuning with MNR loss.",
  "- title": "Training Sentence Transformers with Multiple Negatives Ranking Loss",
  "url": "/learn/fine-tune-sentence-transformers-mnr/",
  "bonusSection": "# Bonus content / further materials",
  "content": "        - title: \"Using Semantic Search to Find GIFs\"           url: /learn/gif-search/         - title: \"Making YouTube Search Better with NLP\"           url: /learn/youtube-search/    - title: Multilingual Sentence Transformers     text: How to create multilingual sentence transformers with knowledge distillation.     url: /learn/multilingual-transformers/    - title: Unsupervised Training for Sentence Transformers     text: How to create sentence transformer models without labelled data.     url: /learn/unsupervised-training-sentence-transformers/    - title: An Introduction to Open Domain Question-Answering     text: The illustrated overview to open domain question-answering.     url: /learn/question-answering      - title: Retrievers for Question-Answering     text: How to fine-tune retriever models to find relevant contexts in vector databases.     url: /learn/retriever-models    - title: Readers for Question-Answering     text: How to fine-tune reader models to identify answers from relevant contexts.     url: /learn/reader-models     bonusSection:       title: \"Bonus Materials: Question-Answering\"       links:         - title: Generative Question-Answering with OpenAI           url: /learn/openai-gen-qa         - title: Long-Form Question-Answering with Haystack           url: /learn/haystack-lfqa    - title: Data Augmentation with BERT     text: Augmented SBERT (AugSBERT) is a training strategy to enhance domain-specific datasets.     url: /learn/data-augmentation/    - title: Domain Transfer with BERT     text: Transfer information from an out-of-domain (or source) dataset to a target domain.     url: /learn/domain-transfer/    - title: Unsupervised Training with Query Generation (GenQ)     text: Fine-tune retrievers for asymmetric semantic search using GenQ     url: /learn/genq/    - title: Generative Pseudo-Labeling (GPL)     text: A powerful technique for domain adaptation using unstructured text data.     url: /learn/gpl/     bonusSection: # Bonus content / further materials       title: Further Resources       links:         - title: \"Searching Freely: Using GPL for Semantic Search ft. Nils Reimers\"           url: https://www.youtube.com/watch?v=OQhoi1CabWw         - title: \"Faiss: The Missing Manual\"           url: /learn/faiss    - title: EMAIL_SUBMIT #email submit form    - title: Training Sentence Transformers     text: The most popular methods for training sentence transformers, and tips for each.    - title: And more... ---  ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2d4"
  },
  "title": "\"Nearest Neighbor Indexes for Similarity Search\"",
  "headline": "\"Nearest Neighbor Indexes for Similarity Search\"",
  "- \"Faiss": "The Missing Manual\"",
  "weight": "2",
  "name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "An overview and comparison of similarity search indexes.",
  "images": "['/images/similarity-search-indexes26.png']",
  "content": "<video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/similarity-search-indexes.mp4\" type=\"video/mp4\"> </video>  Vector similarity search is a game-changer in the world of search. It allows us to efficiently search a huge range of media, from GIFs to articles — with incredible accuracy in sub-second timescales for billion+ size datasets.  One of the key components to efficient search is flexibility. And for that we have a wide range of search indexes available to us — there is no 'one-size-fits-all' in similarity search.  However, this great flexibility produces a question — how do we know which size fits our use case?  Which index do we choose? Should we use multiple indexes, or is one enough?  This article will explore the pros and cons of some of the most important indexes — Flat, LSH, [HNSW](/learn/hnsw/), and IVF. We will learn how we decide which to use and the impact of parameters in each index.  ---  *Note: [Pinecone](/) lets you add vector search to your production applications without knowing anything about vector indexes. However, we know you like seeing how things work, so enjoy learning about these popular vector index types!*  ---  Check out the video walkthrough:  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/B7wmo_NImgM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ## Indexes in Search  Before jumping into the different indexes available, let's take a look at why we care about similarity search — and how we can use indexes for efficient similarity search.  ### Why Similarity Search  Let's start with the most fundamental question to the relevance of this article — why do we care about [similarity search](/learn/what-is-similarity-search/)?  Similarity search can be used to compare data quickly. Given a query (which could be in any format — text, audio, video, GIFs — you name it), we can use similarity search to return relevant results.  This is key to a huge number of companies and applications spanning across industries. It's used to identify similar genes in genome databases, deduplication of datasets, or search billions of results to search queries every day.  Search seems like an easy process — we take one item and compare it to another. But when we have millions (or even billions) of 'other' items to compare against — it begins to get tricky.  For search to be useful, it needs to be accurate and fast. And it is this more efficient search that we are interested in.  ### Indexes For Efficient Search  In vector similarity search, we use an index to store [vector representations](/learn/vector-embeddings/) of the data we intend to search.  Through either [statistical methods or machine learning](/learn/semantic-search/) — we can build vectors that encode useful, meaningful information about our original data.  ![Using densely encoded vectors, we can show that the equivalent man-King semantic relationship for woman is Queen.](/images/similarity-search-indexes1.jpg)  <small>Using densely encoded vectors, we can show that the equivalent <b>man-King</b> semantic relationship for <b>woman</b> is <b>Queen</b>.</small>  We take these 'meaningful' vectors and store them inside an index to use for intelligent similarity search.  There are many index solutions available; one, in particular, is called [Faiss (Facebook AI Similarity Search)](/learn/faiss/). We store our vectors in Faiss and query our new Faiss index using a *'query' vector*. This query vector is compared to other index vectors to find the nearest matches — typically with Euclidean (L2) or inner-product (IP) metrics.  So, with that introduction to the why and how of similarity search. But what is this about Faiss — and choosing the right indexes?  ### Faiss And Indexes  Faiss comes with many different index types — many of which can be mixed and matched to produce multiple layers of indexes.  We will be focused on a few indexes that prioritize search speed, quality, or index memory.  Now, which one of these indexes we use depends very much on our use case. We must consider factors such as dataset size, search frequency, or search-quality vs. search-speed.  ---  ## Flat And Accurate  ![Flat indexes come with perfect search-quality at the cost of slow search speeds. Memory utilization of flat indexes is reasonable.](/images/similarity-search-indexes2.jpg)  <small>Flat indexes come with perfect search-quality at the cost of slow search speeds. Memory utilization of flat indexes is reasonable.</small>  The very first indexes we should look at are the simplest — flat indexes.  Flat indexes are 'flat' because we do not modify the vectors that we feed into them.  Because there is no approximation or clustering of our vectors — these indexes produce the most accurate results. We have perfect search quality, but this comes at the cost of significant search times.  With flat indexes, we introduce our query vector `xq`and compare it against every other full-size vector in our index — calculating the distance to each.  ![With flat indexes, we compare our search query xq to every other vector in the index.](/images/similarity-search-indexes3.png)  <small>With flat indexes, we compare our search query <b>xq</b> to every other vector in the index.</small>  After calculating all of these distances, we will return the nearest k of those as our nearest matches. A k-nearest neighbors (kNN) search.  ![Once we have calculated all of the distances, we return the k nearest vectors.](/images/similarity-search-indexes4.png)  <small>Once we have calculated all of the distances, we return the <b>k</b> nearest vectors.</small>  ### When To Use  So when should we use a flat index? Well, when search quality is an unquestionably high priority — and search speed is less important.  Search speed can be an irrelevant factor for smaller datasets — especially when using more powerful hardware.  ![Euclidean (L2) and Inner Product (IP) flat index search times using faiss-cpu on an M1 chip. Both using vector dimensionality of 100. IndexFlatIP is shown to be slightly faster than IndexFlatL2.](/images/similarity-search-indexes5.jpg)  <small>Euclidean (L2) and Inner Product (IP) flat index search times using <b>faiss-cpu</b> on an M1 chip. Both using vector dimensionality of 100. <b>IndexFlatIP</b> is shown to be slightly faster than <b>IndexFlatL2</b>.</small>  The above chart demonstrates Faiss CPU speeds on an M1-chip. Faiss is optimized to run on GPU at significantly higher speeds when paired with CUDA-enabled GPUs on Linux to improve search times significantly.  In short, use flat indexes when:  -   Search quality is a *very* high priority. -   Search time does *not* matter OR when using a small index (<10K).  ### Implementing a Flat Index  To initialize a flat index, we need our data, Faiss, and one of the two flat indexes — `IndexFlatL2` if using Euclidean/L2 distance, or `IndexFlatIP` if using inner product distance.  First, we need data. We will be using the [Sift1M dataset](http://corpus-texmex.irisa.fr/), which we can download and load into a notebook with:  {{< notebook file=\"download-sift1m\" height=\"full\" >}}  Now we can index our new data using one of our two flat indexes. We saw that IndexFlatIP is slightly faster, so let’s use that.  ```python d = 128  # dimensionality of Sift1M data k = 10  # number of nearest neighbors to return  index = faiss.IndexFlatIP(d) index.add(data) D, I = index.search(xq, k) ```  And for flat indexes, that is all we need to do — there is no training (as we have no parameters to optimize when storing vectors without transformations or clustering).  ### Balancing Search Time  Flat indexes are brilliantly accurate but terribly slow. In similarity search, there is always a trade-off between search-speed and search-quality (accuracy).  What we must do is identify where our use-case sweet spot lies. With flat indexes, we are here:  ![Flat indexes are 100% search-quality, 0% search-speed.](/images/similarity-search-indexes6.png)  <small>Flat indexes are 100% search-quality, 0% search-speed.</small>  Here we have completely unoptimized search-speeds, which will fit many smaller index use-cases — or scenarios where search-time is irrelevant. But, other use-cases require a better balance speed and quality.  So, how can we make our search faster? There are two primary approaches:  1. Reduce vector size — through dimensionality reduction or reducing the number of bits representing our vectors values.  2. Reduce search scope — we can do this by clustering or organizing vectors into tree structures based on certain attributes, similarity, or distance — and restricting our search to closest clusters or filter through most similar branches.  Using either of these approaches means that we are no longer performing an exhaustive nearest-neighbors search but an approximate nearest-neighbors (ANN) search — as we no longer search the entire, full-resolution dataset.  So, what we produce is a more balanced mix that prioritizes both search-speed and search-time:  ![Often, we will want a more balanced mix of both search-speed and search-quality.](/images/similarity-search-indexes7.jpg)  <small>Often, we will want a more balanced mix of both search-speed and search-quality.</small>  ---  ## Locality Sensitive Hashing  ![LSH — a wide range of performances heavily dependent on the parameters set. Good quality results in slower search, and fast search results in worse quality. Poor performance for high-dimensional data. The ‘half-filled’ segments of the bars represent the range in performance encountered while modifying index parameters.](/images/similarity-search-indexes8.jpg)  <small>LSH — a wide range of performances heavily dependent on the parameters set. Good quality results in slower search, and fast search results in worse quality. Poor performance for high-dimensional data. The ‘half-filled’ segments of the bars represent the range in performance encountered while modifying index parameters.</small>  Locality Sensitive Hashing (LSH) works by grouping vectors into buckets by processing each vector through a hash function that maximizes hashing collisions — rather than minimizing as is usual with hashing functions.  What does that mean? Imagine we have a Python dictionary. When we create a new key-value pair in our dictionary, we use a hashing function to hash the key. This hash value of the key determines the 'bucket' where we store its respective value:  ![A typical hash function for a dictionary-like object will attempt to minimize hash collisions, aiming to assign only one value to each bucket.](/images/similarity-search-indexes9.jpg)  <small>A typical hash function for a dictionary-like object will attempt to minimize hash collisions, aiming to assign only one value to each bucket.</small>  A Python dictionary is an example of a hash table using a typical hashing function that *minimizes* hashing collisions, a hashing collision where two different objects (keys) produce the same hash.  In our dictionary, we want to avoid these collisions as it means that we would have multiple objects mapped to a single key — but for LSH, we want to *maximize* hashing collisions.  Why would we want to maximize collisions? Well, for search, we use LSH to group similar objects together. When we introduce a new query object (or vector), our LSH algorithm can be used to find the closest matching groups:  ![Our hash function for LSH attempts to maximize hash collisions, producing groupings of vectors.](/images/similarity-search-indexes10.jpg)  <small>Our hash function for LSH attempts to maximize hash collisions, producing groupings of vectors.</small>  ### Implementing LSH  Implementing our LSH index in Faiss is easy. We initialize a IndexLSH object, using the vector dimensions d and the nbits argument — and add our vectors like so:  ```python nbits = d*4  # resolution of bucketed vectors # initialize index and add vectors index = faiss.IndexLSH(d, nbits) index.add(wb) # and search D, I = index.search(xq, k) ```  Our `nbits` argument refers to the 'resolution' of the hashed vectors. A higher value means greater accuracy at the cost of more memory and slower search speeds.  ![Recall score of IndexLSH with d of 128. Note that to get higher recall performance, we need to increase the num_bits value dramatically. For ~90% recall we use 64*d, which is 64*128 = 8192.](/images/similarity-search-indexes11.jpg)  <small>Recall score of <b>IndexLSH</b> with <b>d</b> of <b>128</b>. Note that to get higher recall performance, we need to increase the <b>num_bits</b> value dramatically. For 90% recall we use 64*d, which is 64*128 = <b>8192</b>.</small>  Our baseline `IndexFlatIP` index is our 100% recall performance, using `IndexLSH` we can achieve 90% using a very high `nbits` value.  This is a strong result — 90% of the performance could certainly be a reasonable sacrifice to performance if we get improved search-times.  However, LSH is highly sensitive to the curse of dimensionality when using a larger `d` value we also need to increase `nbits` to maintain search-quality.  So our stored vectors become increasingly larger as our original vector dimensionality `d` increases. This quickly leads to excessive search times:  ![Search time for IndexLSH with varying nbits values, compared against a flat IP index.](/images/similarity-search-indexes12.jpg)  <small>Search time for <b>IndexLSH</b> with varying <b>nbits</b> values, compared against a flat IP index.</small>  Which is mirrored by our index memory size:  ![Index size for IndexLSH with varying nbits values, compared against a flat IP index.](/images/similarity-search-indexes13.jpg)  <small>Index size for <b>IndexLSH</b> with varying <b>nbits</b> values, compared against a flat IP index.</small>  So `IndexLSH` is *not* suitable if we have large vector dimensionality (`128` is already *too* large). Instead, it is best suited to low-dimensionality vectors — and small indexes.  If we find ourselves with large `d` values or large indexes — we avoid LSH completely, instead focusing on our next index, HNSW.  ---  ## Hierarchical Navigable Small World Graphs  ![HNSW — great search-quality, good search-speed, but substantial index sizes. The ‘half-filled’ segments of the bars represent the range in performance encountered while modifying index parameters.](/images/similarity-search-indexes14.jpg)  <small>HNSW — great search-quality, good search-speed, but substantial index sizes. The ‘half-filled’ segments of the bars represent the range in performance encountered while modifying index parameters.</small>  Hierarchical Navigable Small World (HNSW) graphs are another, more recent development in search. HNSW-based ANNS consistently top out as the highest performing indexes [1].  HNSW is a further adaption of navigable small world (NSW) graphs — where an NSW graph is a graph structure containing vertices connected by edges to their nearest neighbors.  The 'NSW' part is due to vertices within these graphs all having a very short average path length to all other vertices within the graph — despite not being directly connected.  Using the example of Facebook — in 2016, we could connect every user (a vertex) to their Facebook friends (their nearest neighbors). And despite the 1.59B active users, the average number of steps (or *hops*) needed to traverse the graph from one user to another was just 3.57 [2].  ![Visualization of an NSW graph. Notice that each point is no more than four hops away from another.](/images/similarity-search-indexes15.png)  <small>Visualization of an NSW graph. Notice that each point is no more than four hops away from another.</small>  Facebook is just one example of high connectivity in vast networks — otherwise known as an NSW graph.  At a high level, HNSW graphs are built by taking NSW graphs and breaking them apart into multiple layers. With each incremental layer eliminating intermediate connections between vertices.  ![With HNSW, we break networks into several layers, which are traversed during the search.](/images/similarity-search-indexes16.jpg)  <small>With HNSW, we break networks into several layers, which are traversed during the search.</small>  For bigger datasets with higher-dimensionality — HNSW graphs are some of the best performing indexes we can use. We'll be covering using the HNSW index alone, but by layering other quantization steps, we can improve search-times even further.  ### HNSW Implementation  To build and search a flat HNSW index in Faiss, all we need is `IndexHNSWFlat`:  ```python # set HNSW index parameters M = 64  # number of connections each vertex will have ef_search = 32  # depth of layers explored during search ef_construction = 64  # depth of layers explored during index construction  # initialize index (d == 128) index = faiss.IndexHNSWFlat(d, M) # set efConstruction and efSearch parameters index.hnsw.efConstruction = ef_construction index.hnsw.efSearch = ef_search # add data to index index.add(wb)  # search as usual D, I = index.search(wb, k) ```  Here, we have three key parameters for modifying our index performance.  - `M` — the number of nearest neighbors that each vertex will connect to. - `efSearch` — how many entry points will be explored between layers during the search. - `efConstruction` — how many entry points will be explored when building the index.  Each of these parameters can be increased to improve search-quality:  ![Recall values for different efConstruction, efSearch, and M values on the Sift1M dataset.](/images/similarity-search-indexes17.jpg)  <small>Recall values for different <b>efConstruction</b>, <b>efSearch</b>, and <b>M</b> values on the Sift1M dataset.</small>  `M` and `efSearch` have a larger impact on search-time — `efConstruction` primarily increases index *construction time* (meaning a slower `index.add`), but at higher `M` values and higher query volume we do see an impact from `efConstruction` on search-time too.  ![Search time for different M and efSearch values on the full Sift1M dataset.](/images/similarity-search-indexes18.jpg)  <small>Search time for different <b>M</b> and <b>efSearch</b> values on the full Sift1M dataset.</small>  HNSW gives us great search-quality at very fast search-speeds — but there's always a catch — HNSW indexes take up a significant amount of memory. Using an `M` value of `128` for the Sift1M dataset requires upwards of 1.6GB of memory.  ![Index memory usage for different M values on the Sift1M dataset.](/images/similarity-search-indexes19.jpg)  <small>Index memory usage for different <b>M</b> values on the Sift1M dataset.</small>  However, we can increase our other two parameters — `efSearch` and `efConstruction` with no effect on the index memory footprint.  So, where RAM is not a limiting factor HNSW is great as a well-balanced index that we can push to focus more towards quality by increasing our three parameters.  ![We can use the lower set of parameters to balance prioritize a slightly faster search-speed with good search-quality — or we use the higher set of parameters for slightly slower search-speed with high search-quality.](/images/similarity-search-indexes20.jpg)  <small>We can use the lower set of parameters to balance prioritize a slightly faster search-speed with good search-quality — or we use the higher set of parameters for slightly slower search-speed with high search-quality.</small>  That's HNSW in Faiss — an incredibly powerful and efficient index. Now let's move onto our final index — IVF.  ---  ## Inverted File Index  ![IVF — great search-quality, good search-speed, and reasonable memory usage. The ‘half-filled’ segments of the bars represent the range in performance encountered while modifying index parameters.](/images/similarity-search-indexes21.jpg)  <small>IVF — great search-quality, good search-speed, and reasonable memory usage. The ‘half-filled’ segments of the bars represent the range in performance encountered while modifying index parameters.</small>  The Inverted File Index (IVF) index consists of search scope reduction through clustering. It's a very popular index as it's easy to use, with high search-quality and reasonable search-speed.  It works on the concept of Voronoi diagrams — also called Dirichlet tessellation (a much cooler name).  To understand Voronoi diagrams, we need to imagine our highly-dimensional vectors placed into a 2D space. We then place a few additional points in our 2D space, which will become our 'cluster' (Voronoi cells in our case) centroids.  We then extend an equal radius out from each of our centroids. At some point, the circumferences of each cell circle will collide with another — creating our cell edges:   <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/similarity-search-indexes2.mp4\" type=\"video/mp4\"> </video>  <small class=\"video\">How Voronoi cells are constructed — here, we have three centroids, resulting in three Voronoi cells. Each datapoint within a given cell is then assigned to that respective centroid.</small>  Now, every datapoint will be contained within a cell — and be assigned to that respective centroid.  Just as with our other indexes, we introduce a query vector `xq` — this query vector must land within one of our cells, at which point we restrict our search scope to that single cell.  But there is a problem if our query vector lands near the edge of a cell — there's a good chance that its closest other datapoint is contained within a neighboring cell. We call this the *edge problem*:  ![Our query vector xq lands on the edge of the magenta cell. Despite being closer to datapoints in the teal cell, we will not compare these if nprobe == 1 — as this means we would restrict search scope to the magenta cell only.](/images/similarity-search-indexes22.jpg)  <small>Our query vector <b>xq</b> lands on the edge of the magenta cell. Despite being closer to datapoints in the teal cell, we will not compare these if <b>nprobe == 1</b> — as this means we would restrict search scope to the magenta cell only.</small>  Now, what we can do to mitigate this issue and increase search-quality is increase an index parameter known as the `nprobe` value. With `nprobe` we can set the number of cells to search.  ![Increasing nprobe increases our search scope.](/images/similarity-search-indexes23.png) ![Increasing nprobe increases our search scope.](/images/similarity-search-indexes26.png)  <small>Increasing <b>nprobe</b> increases our search scope.</small>  ### IVF Implementation  To implement an IVF index and use it in search, we can use `IndexIVFFlat`:  ```python nlist = 128  # number of cells/clusters to partition data into  quantizer = faiss.IndexFlatIP(d)  # how the vectors will be stored/compared index = faiss.IndexIVFFlat(quantizer, d, nlist) index.train(data)  # we must train the index to cluster into cells index.add(data)  index.nprobe = 8  # set how many of nearest cells to search D, I = index.search(xq, k) ```  There are two parameters here that we can adjust.  -   `nprobe` — the number of cells to search -   `nlist` — the number of cells to create  A higher `nlist` means that we must compare our vector to more centroid vectors — but after selecting the nearest centroid's cells to search, there will be fewer vectors within each cell. So, *increase* `nlist` to prioritize *search-speed*.  As for `nprobe`, we find the opposite. *Increasing* `nprobe` increases the search scope — thus prioritizing *search-quality*.  ![Search-time and recall for IVF using different nprobe and nlist values.](/images/similarity-search-indexes24.jpg)  <small>Search-time and recall for IVF using different <b>nprobe</b> and <b>nlist</b> values.</small>  In terms of memory, `IndexIVFFlat` is reasonably efficient — and modifying `nprobe` will not affect this. The effect of `nlist` on memory usage is small too — higher `nlist` means a marginally larger memory requirement.  ![Memory usage of the index is affected only by the nlist parameter. However, for our Sift1M dataset, the index size changed only very slightly.](/images/similarity-search-indexes25.jpg)  <small>Memory usage of the index is affected only by the nlist parameter. However, for our Sift1M dataset, the index size changed only very slightly.</small>  So, we must decide between greater search-quality with `nprobe` and faster search-speed with `nlist`.  ---  We’ve covered a lot in this article , so let’s wrap everything up with a quick summary of each index's memory, speed, and search-quality performance.  <script src=\"https://gist.github.com/jamescalam/c752e0b5c7a7c8136de5b6167afb7de3.js\"></script>  So we have four indexes, with equally potent pros and cons — depending on the use-case. Hopefully, with this article, you will now be much better prepared to decide which of these indexes best fits your own use case.  Beyond these metrics, it is possible to improve memory usage and search-speed even further by adding other quantization or compression steps — but that is for another article.  {{< newsletter text=\"Subscribe for the latest in similarity search!\" inputText=\"Email address...\" buttonText=\"Submit\">}}  ---  ### References  [1] E Bernhardsson, [ANN-benchmarks repo](https://github.com/erikbern/ann-benchmarks), Github  [2] S Edunov, et al., [Three and a half degrees of separation](https://research.fb.com/blog/2016/02/three-and-a-half-degrees-of-separation/) (2016), Facebook Research  ---  *All images are by the author except where stated otherwise* ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2d6"
  },
  "title": "\"Introduction to Transfer Learning\"",
  "headline": "\"Introduction to Transfer Learning\"",
  "weight": "1",
  "name": "Diego Lopez Yse",
  "position": "Data Scientist",
  "src": "/images/diego-lopez-yse.jpeg",
  "href": "\"https://lopezyse.medium.com/\"",
  "description": "An introduction to transfer learning.",
  "images": "[\"/images/tradition-vs-transfer-learning.jpg\"]",
  "content": "As humans, we don’t learn tasks in isolation. As we move our knowledge path forward, we build on previously learned and related challenges, which accelerates the learning process.  If you already know a particular musical, cooking, or dance style, you will not forget that knowledge and start from scratch when learning a different style; you will reuse the knowledge you already have and finetune it to absorb the new features. This is an incredible human capability, which can also be applied to machines through the use of Transfer Learning (TL).  This way, what has been learned in one setting is built upon to improve generalization on a related setting, which is a huge advantage over traditional approaches that don’t capture this benefit. Traditional Machine Learning models require training from scratch, which is computationally expensive and requires a large amount of data to achieve high performance. They also involve isolated training, where each model is independently trained for a specific purpose without any dependency on past knowledge.  <div class=\"centered-text-section mt\">  **Transfer Learning (TL)** is a Machine Learning method where a model developed for a task is reused as the starting point for a model in another task.   </div>  ![Tradition vs transfer learning](/images/tradition-vs-transfer-learning.jpg) <small>Traditional Machine Learning vs. Transfer Learning. Source: [datascience.aero](https://datascience.aero/)</small>  TL has become highly popular given the enormous resources required to train Deep Learning models, especially in the areas of:  * **Natural Language Processing (NLP)** to solve problems that use text as input or output. In this discipline, there are several effective algorithms that generate distributed representations or text, such as words or sentences. These solutions are commonly released as **pretrained models** which are trained on a [very large corpus of text documents](https://machinelearningmastery.com/transfer-learning-for-deep-learning/).  * **Computer Vision (CV)** in which it’s very unusual to train an entire [Convolutional Neural Network (CNN)](https://en.wikipedia.org/wiki/Convolutional_neural_network) from scratch (with random initialization), since it’s relatively rare to have a dataset of sufficient size. Instead, [it is common to pretrain a CNN on a very large dataset](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) and then use that model either as an initialization or a fixed feature extractor for the task of interest. This way, the majority of the pretrained CNN can be employed on new models through TL, just retraining a section of it.  ## Transfer Learning in action  How does the transfer learning process work? Although there are [different approaches](https://arxiv.org/abs/1606.09282), two of the main ones are:  * **Feature-based**, which does not modify the original model and allows new tasks to benefit from complex features learned from previous tasks. However, these features are not specialized for the new task and can often be improved by fine-tuning.  * **Fine-tuning**, which modifies the parameters of an existing model to train a new task. The original model is “unfrozen” and retrained on new data, increasing the performance for the new task.  ![Transfer Learning in action](/images/transfer-learning-in-action.png)  [These approaches can be integrated in TL](https://keras.io/guides/transfer_learning/), producing the following workflow:  1. Take layers from a previously trained model. 2. Freeze them to avoid altering any of the information they contain during future training rounds. 3. Add some new, trainable layers on top of the frozen layers. They will learn to turn old features into predictions on a new dataset. 4. Train the new layers on your dataset.  As a last step, one can perform a **fine-tuning of the model**: unfreezing the entire model obtained above (or part of it) and re-training it on the new data with a very low learning rate. This can potentially achieve meaningful improvements by incrementally adapting the pretrained features to the new data.  ![The Transfer Learning process](/images/transfer-learning-process.png) <small>The Transfer Learning process. Source: [V7 Labs](https://www.v7labs.com/blog/transfer-learning-guide)</small>  Using this workflow as a generic approach to perform TL, let’s see how specific pretrained models can be used to solve different challenges.  ## Computer Vision  In Computer Vision, the intuition is that if a model is trained on a large and general enough dataset, it will effectively serve as a generic model of the visual world. Some of the main pretrained models used in Computer Vision for TL are:  * Visual Geometry Group **(VGG)** and all its variants like the VGG16 model, which is a CNN of 16 layers trained on a subset of the [ImageNet](https://www.image-net.org/) dataset, a collection of over 14 million images belonging to 22,000 categories. Characterized by its simplicity, VGG was developed to increase the depth of such CNNs in order to increase the model performance. The VGG model is the basis of [ground-breaking object recognition models](https://viso.ai/deep-learning/vgg-very-deep-convolutional-networks/) and became one of the most popular architectures.  ![VGG16 Macro Architecture](/images/macro-architecture-vgg16.png) <small>Macro Architecture of VGG16. Source: [VGG in TensorFlow](http://www.cs.toronto.edu/~frossard/post/vgg16/)</small>  * Residual Network **(ResNet)**, which relies on the fact that as the number of layers increases in a CNN, the ability of the model to fit more complex functions also increases. ResNet introduces a so-called “*identity shortcut connection*” that skips one or more layers in the CNN architecture, gaining accuracy from considerably increases in depth. This way, the model ensures that the higher layers of the model do not perform any worse than the lower layers, improving the efficiency while minimizing the percentage of errors. Models like Resnet150 can work with more than 150 neural network layers, as deep models generalize well to different datasets.  ![Identity short connection](/images/identity-short-connection.png) <small>The advantage of adding the “identity shortcut connection” is that if any layer hurts the performance of the model, then it will be skipped. Source: [Geeks for Geeks](https://www.geeksforgeeks.org/residual-networks-resnet-deep-learning/)</small>  ## Natural Language Processing (NLP)  Like in Computer Vision, you can take advantage of the learned features from [NLP](/learn/nlp/) pretrained models without having to start from scratch by training a large model on a large dataset. Some of the main pretrained models used in NLP for TL are:  * Universal Language Model Fine-tuning for Text Classification **(ULMFiT)**, an [AWD-LSTM](https://paperswithcode.com/method/awd-lstm#:~:text=ASGD%20Weight%2DDropped%20LSTM%2C%20or,of%20last%20iterations%20of%20weights.) model that was trained on the Wikitext-103 dataset and showed incredible performance on [tasks](https://nlp.fast.ai/classification/2018/05/15/introducing-ulmfit.html) like finding documents relevant to a legal case, identifying spam and offensive comments, classifying reviews of a product, grouping articles by political orientation, and much more. ULMFiT introduces [different techniques](https://arxiv.org/abs/1801.06146) like discriminative fine-tuning (which allows us to tune each layer with different learning rates), *slanted triangular learning rates* (a learning rate schedule that first linearly increases the learning rate and then linearly decays it), and *gradual unfreezing* (unfreezing one layer per epoch) to retain previous knowledge.  ![ULMFiT](/images/ulmfit.png) <small>ULMFiT requires orders of magnitude less data than previous approaches. Source: [Fast.ai](https://nlp.fast.ai/)</small>  * Bidirectional Encoder Representations from Transformers **(BERT)**, a model that significantly altered the NLP landscape, became a new benchmark useful for almost any task. BERT is a [transformer-based model](/learn/transformers/) trained on 2.5 billion words with the ability to consider the context from both the left and right sides of each word. Its popularity came after [breaking several records](https://jalammar.github.io/illustrated-bert/) for how well it can handle language-based tasks like *question answering, text summarization, sentence prediction, word sense disambiguation, natural language inference, and sentiment classification*, among others.  ![BERT](/images/bert.png) <small>BERT proved to become the most popular of the modern NLP models. BERT was made possible by two major trends. Firstly, hardware has gotten exponentially better, especially GPUs. Second, the web, mostly composed of text, provided a large, open and diverse dataset. Source: [Techcrunch](https://techcrunch.com/sponsor/nvidia/how-the-revolution-of-natural-language-processing-is-changing-the-way-companies-understand-text/)</small>  ## The good and the bad  Transfer learning is a shortcut to saving time, resources and getting better performance. Although it’s not obvious that there will be a benefit to using TL in the domain until after the model has been developed and evaluated, there are some [possible advantages](https://machinelearningmastery.com/transfer-learning-for-deep-learning/) like:  1. **Higher start**. The initial skill (before refining the model) on the source model is higher than it otherwise would be. 2. **Higher slope**. The rate of improvement of skill during training of the source model is steeper than it otherwise would be. 3. **Higher asymptote**. The converged skill of the trained model is better than it otherwise would be.  ![Transfer learning](/images/transfer-learning.png) <small>Three ways in which transfer might improve learning. Source: [Machine Learning Mastery](https://machinelearningmastery.com/transfer-learning-for-deep-learning/)</small>  Ideally, you would see all three benefits from a successful application of TL, which in practical terms, you can try to seek if:  * You identify a related task with abundant data and you have the resources to develop a model for that task and reuse it on your problem, or * There is a pre-trained model available that you can use as a starting point for your own model.  This all sounds great, but although the goal of TL is to improve a model’s performance, it can also degrade it. If the TL task ends up with a decrease in the performance or accuracy of the new model, then it produces an effect called **negative transfer**. Negative transfer can be caused by too [high a dissimilarity of the problem domains](https://developer.ibm.com/articles/transfer-learning-for-deep-learning/) or the inability of the model to train for the new domain’s data set (in addition to the new data set itself).  Transfer learning can also induce **overfitting**, which happens when the new model learns details and noises from the training data that negatively impact its outputs.  Transfer learning can come with a variety of serious problems, including [distributional biases](https://arxiv.org/abs/1911.03064), [social stereotypes](https://arxiv.org/abs/2004.09456), potentially revealing [training samples](https://arxiv.org/abs/2012.07805), and other [possible harms](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922). In the case of NLP language pretrained models, one particular [type of harm](https://deepmind.com/research/publications/2021/Challenges-in-Detoxifying-Language-Models) is the generation of [toxic language](https://arxiv.org/abs/2009.11462), which includes hate speech, insults, profanities, and threats.  There’s no such thing as free lunch. Although transfer learning has several limitations, it allows for repurposing models for new problems with less data for training, gaining efficiencies in time and resources.   Besides the obvious economic benefits of using TL, some people believe it might also be the gateway to more impactful advancements in AI. As Demis Hassabis (CEO and co-founder of DeepMind) said:  “*I think Transfer Learning is the key to General Intelligence. And I think the key to doing Transfer Learning will be the acquisition of conceptual knowledge that is abstracted away from perceptual details of where you learned it from.*”  ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2d8"
  },
  "title": "\"Hierarchical Navigable Small Worlds (HNSW)\"",
  "headline": "\"Hierarchical Navigable Small Worlds (HNSW)\"",
  "- \"Faiss": "The Missing Manual\"",
  "weight": "6",
  "name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "Demystifying and experimenting with HNSW for vector search.",
  "images": "['/images/hnsw-0.jpg']",
  "content": "![Hierarchical Navigable Small Worlds](/images/hnsw-0.jpg)  Hierarchical Navigable Small World (HNSW) graphs are among the top-performing indexes for [vector similarity search](/learn/what-is-similarity-search/)<sup>[1]</sup>. HNSW is a hugely popular technology that time and time again produces state-of-the-art performance with super fast search speeds and fantastic recall.  Yet despite being a popular and robust algorithm for approximate nearest neighbors (ANN) searches, understanding how it works is far from easy.  ---  **Note: [Pinecone](/) lets you build scalable, performant vector search into applications without knowing anything about HNSW or vector indexing libraries. But we know you like seeing how things work, so enjoy the guide!**  ---  This article helps demystify HNSW and explains this intelligent algorithm in an easy-to-understand way. Towards the end of the article, we'll look at how to implement HNSW using [Faiss](/learn/faiss/) and which parameter settings give us the performance we need.  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/QvKMwLjdK-s\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ## Foundations of HNSW  We can split [ANN algorithms](/learn/what-is-similarity-search/) into three distinct categories; trees, hashes, and graphs. HNSW slots into the *graph* category. More specifically, it is a *proximity graph*, in which two vertices are linked based on their proximity (closer vertices are linked) — often defined in Euclidean distance.  There is a significant leap in complexity from a *'proximity'* graph to *'hierarchical navigable small world'* graph. We will describe two fundamental techniques that contributed most heavily to HNSW: the probability skip list, and navigable small world graphs.  ### Probability Skip List  The probability skip list was introduced *way back* in 1990 by *William Pugh* <sup>[2]</sup>. It allows fast search like a sorted array, while using a linked list structure for easy (and fast) insertion of new elements (something that is not possible with sorted arrays).  Skip lists work by building several layers of linked lists. On the first layer, we find links that *skip* many intermediate nodes/vertices. As we move down the layers, the number of *'skips'* by each link is decreased.  To search a skip list, we start at the highest layer with the longest *'skips'* and move along the edges towards the right (below). If we find that the current node 'key' is *greater than* the key we are searching for — we know we have overshot our target, so we move down to previous node in the *next* level.   ![prob_skip_list](/images/hnsw-1.jpg) <small>A probability skip list structure, we start on the top layer. If our current key is greater than the key we are searching for (or we reach `end`), we drop to the next layer.</small>  HNSW inherits the same layered format with longer edges in the highest layers (for fast search) and shorter edges in the lower layers (for accurate search).  ### Navigable Small World Graphs  Vector search using *Navigable Small World* (NSW) graphs was introduced over the course of several papers from 2011-14 <sup>[4, 5, 6]</sup>. The idea is that if we take a proximity graph but build it so that we have both long-range and short-range links, then search times are reduced to (poly/)logarithmic complexity.  Each vertex in the graph connects to several other vertices. We call these connected vertices *friends*, and each vertex keeps a *friend list*, creating our graph.  When searching an NSW graph, we begin at a pre-defined *entry-point*. This entry point connects to several nearby vertices. We identify which of these vertices is the closest to our query vector and move there.   ![nsw search](/images/hnsw-2.jpg) <small>The search process through a NSW graph. Starting at a pre-defined entry point, the algorithm greedily traverses to connected vertices that are nearer to the query vector.</small>  We repeat the *greedy-routing search* process of moving from vertex to vertex by identifying the nearest neighboring vertices in each friend list. Eventually, we will find no nearer vertices than our current vertex — this is a local minimum and acts as our stopping condition.  ---  *Navigable small world models are defined as any network with (poly/)logarithmic complexity using greedy routing. The efficiency of greedy routing breaks down for larger networks (1-10K+ vertices) when a graph is not navigable <sup>[7]</sup>.*  ---  The *routing* (literally the route we take through the graph) consists of two phases. We start with the \"zoom-out\" phase where we pass through low-degree vertices (degree is the number of links a vertex has) — and the later \"zoom-in\" phase where we pass through higher-degree vertices <sup>[8]</sup>.  ![vertex degrees](/images/hnsw-3.jpg) <small>High-degree vertices have *many* links, whereas low-degree vertices have very *few* links.</small>  Our *stopping condition* is finding no nearer vertices in our current vertex's friend list. Because of this, we are more likely to hit a local minimum and stop too early when in the *zoom-out* phase (fewer links, less likely to find a nearer vertex).  To minimize the probability of stopping early (and increase recall), we can increase the average degree of vertices, but this increases network complexity (and search time). So we need to balance the average degree of vertices between recall and search speed.  Another approach is to start the search on high-degree vertices (*zoom-in* first). For NSW, this *does* improve performance on low-dimensional data. We will see that this is also a significant factor in the structure of HNSW.  ### Creating HNSW  HNSW is a natural evolution of NSW, which borrows inspiration from hierarchical multi-layers from Pugh's probability skip list structure.  Adding hierarchy to NSW produces a graph where links are separated across different layers. At the top layer, we have the longest links, and at the bottom layer, we have the shortest.  ![hnsw](/images/hnsw-4.jpg) <small>Layered graph of HNSW, the top layer is our entry point and contains only the *longest links*, as we move down the layers, the link lengths become shorter and more numerous.</small>  During the search, we enter the top layer, where we find the longest links. These vertices will tend to be higher-degree vertices (with links separated across multiple layers), meaning that we, by default, start in the *zoom-in* phase described for NSW.  We traverse edges in each layer just as we did for NSW, greedily moving to the nearest vertex until we find a local minimum. Unlike NSW, at this point, we shift to the current vertex in a lower layer and begin searching again. We repeat this process until finding the local minimum of our bottom layer — *layer 0*.   ![hnsw search](/images/hnsw-5.jpg) <small>The search process through the multi-layer structure of an HNSW graph.</small>  ## Graph Construction  During graph construction, vectors are iteratively inserted one-by-one. The number of layers is represented by parameter *L*. The probability of a vector insertion at a given layer is given by a probability function normalized by the *'level multiplier' m_L*, where *m_L = ~0* means vectors are inserted at *layer 0* only.   ![hnsw insertion](/images/hnsw-6.jpg) <small>The probability function is repeated for each layer (other than *layer 0*). The vector is added to its insertion layer *and* every layer below it.</small>  The creators of HNSW found that the best performance is achieved when we minimize the overlap of shared neighbors across layers. *Decreasing m_L* can help minimize overlap (pushing more vectors to *layer 0*), but this increases the average number of traversals during search. So, we use an *m_L* value which balances both. *A rule of thumb for this optimal value is `1/ln(M)` <sup>[1]</sup>*.  Graph construction starts at the top layer. After entering the graph the algorithm greedily traverse across edges, finding the *ef* nearest neighbors to our inserted vector *q* — at this point *ef = 1*.  After finding the local minimum, it moves down to the next layer (just as is done during search). This process is repeated until reaching our chosen *insertion layer*. Here begins phase two of construction.  The *ef* value is increased to `efConstruction` (a parameter we set), meaning more nearest neighbors will be returned. In phase two, these nearest neighbors are candidates for the links to the new inserted element *q* *and* as entry points to the next layer.  *M* neighbors are added as links from these candidates — the most straightforward selection criteria are to choose the closest vectors.  After working through multiple iterations, there are two more parameters that are considered when adding links. *M_max*, which defines the maximum number of links a vertex can have, and *M_max0*, which defines the same but for vertices in *layer 0*.   ![hnsw insert params](/images/hnsw-7.jpg) <small>Explanation of the number of links assigned to each vertex and the effect of `M`, `M_max`, and `M_max0`.</small>  The stopping condition for insertion is reaching the local minimum in *layer 0*.  ## Implementation of HNSW  We will implement HNSW using the Facebook AI Similarity Search (Faiss) library, and test different construction and search parameters and see how these affect index performance.  To initialize the HNSW index we write:  {{< notebook file=\"hnsw-init\" height=\"full\" >}}  With this, we have set our `M` parameter — the number of neighbors we add to each vertex on insertion, but we're missing *M_max* and *M_max0*.  In Faiss, these two parameters are set automatically in the `set_default_probas` method, called at index initialization. The *M_max* value is set to `M`, and *M_max0* set to `M*2` (find further detail in the [notebook](https://github.com/pinecone-io/examples/blob/master/hnsw_faiss/hnsw_faiss.ipynb)).  Before building our `index` with `index.add(xb)`, we will find that the number of layers (or *levels* in Faiss) are not set:  {{< notebook file=\"hnsw-no-levels\" height=\"full\" >}}  If we go ahead and build the index, we'll find that both of these parameters are now set.  {{< notebook file=\"hnsw-with-levels\" height=\"full\" >}}  Here we have the number of levels in our graph, *0* -> *4* as described by `max_level`. And we have `levels`, which shows the distribution of vertices on each level from *0* to *4* (ignoring the first `0` value). We can even find which vector is our entry point:  {{< notebook file=\"get-entry-point\" height=\"full\" >}}  That's a high-level view of our Faiss-flavored HNSW graph, but before we test the index, let's dive a little deeper into how Faiss is building this structure.  #### Graph Structure  When we initialize our index we pass our vector dimensionality `d` and number of neighbors for each vertex `M`. This calls the method '`set_default_probas`', passing `M` and `1 / log(M)` in the place of `levelMult` (equivalent to *m_L* above). A Python equivalent of this method looks like:  ```python def set_default_probas(M: int, m_L: float):     nn = 0  # set nearest neighbors count = 0     cum_nneighbor_per_level = []     level = 0  # we start at level 0     assign_probas = []     while True:         # calculate probability for current level         proba = np.exp(-level / m_L) * (1 - np.exp(-1 / m_L))         # once we reach low prob threshold, we've created enough levels         if proba < 1e-9: break         assign_probas.append(proba)         # neighbors is == M on every level except level 0 where == M*2         nn += M*2 if level == 0 else M         cum_nneighbor_per_level.append(nn)         level += 1     return assign_probas, cum_nneighbor_per_level ```  Here we are building two vectors — `assign_probas`, the *probability* of insertion at a given layer, and `cum_nneighbor_per_level`, the cumulative total of nearest neighbors assigned to a vertex at different insertion levels.  {{< notebook file=\"levels\" height=\"full\" >}}  From this, we can see the significantly higher probability of inserting a vector at *level 0* than higher levels (although, as we will explain below, the probability is not exactly as defined here). This function means higher levels are more sparse, reducing the likelihood of 'getting stuck', and ensuring we start with longer range traversals.  Our `assign_probas` vector is used by another method called `random_level` — it is in this function that each vertex is assigned an insertion level.  ```python def random_level(assign_probas: list, rng):     # get random float from 'r'andom 'n'umber 'g'enerator     f = rng.uniform()      for level in range(len(assign_probas)):         # if the random float is less than level probability...         if f < assign_probas[level]:             # ... we assert at this level             return level         # otherwise subtract level probability and try again         f -= assign_probas[level]     # below happens with very low probability     return len(assign_probas) - 1 ```  We generate a random float using Numpy's random number generator `rng` (initialized below) in `f`. For each `level`, we check if `f` is less than the assigned probability for that level in `assign_probas` — if so, that is our insertion layer.  If `f` is too high, we subtract the `assign_probas` value from `f` and try again for the next level. The result of this logic is that vectors are *most likely* going to be inserted at *level 0*. Still, if not, there is a decreasing probability of insertion at ease increment level.  Finally, if no levels satisfy the probability condition, we insert the vector at the highest level with `return len(assign_probas) - 1`. If we compare the distribution between our Python implementation and that of Faiss, we see very similar results:  {{< notebook file=\"insertion-run\" height=\"full\" >}}   ![insertion distribution](/images/hnsw-8.jpg) <small>Distribution of vertices across layers in both the Faiss implementation (left) and the Python implementation (right).</small>  The Faiss implementation also ensures that we *always* have at least one vertex in the highest layer to act as the entry point to our graph.  ### HNSW Performance  Now that we've explored all there is to explore on the theory behind HNSW and how this is implemented in Faiss — let's look at the effect of different parameters on our recall, search and build times, and the memory usage of each.  We will be modifying three parameters: `M`, `efSearch`, and `efConstruction`. And we will be indexing the Sift1M dataset, which you can download and prepare using [this script](https://gist.github.com/jamescalam/a09a16c17b677f2cf9c019114711f3bf).  As we did before, we initialize our index like so:  ```python index = faiss.IndexHNSWFlat(d, M) ```  The two other parameters, `efConstruction` and `efSearch` can be modified *after* we have initialized our `index`.  ```python index.hnsw.efConstruction = efConstruction index.add(xb)  # build the index index.hnsw.efSearch = efSearch # and now we can search index.search(xq[:1000], k=1) ```  Our `efConstruction` value *must* be set before we construct the index via `index.add(xb)`, but `efSearch` can be set anytime before searching.  Let's take a look at the recall performance first.   ![recall viz](/images/hnsw-9.jpg) <small>Recall@1 performance for various `M`, `efConstruction`, and `efSearch` parameters.</small>  High `M` and `efSearch` values can make a big difference in recall performance — and it's also evident that a reasonable `efConstruction` value is needed. We can also increase `efConstruction` to achieve higher recall at lower `M` and `efSearch` values.  However, this performance does not come for free. As always, we have a balancing act between recall and search time — let's take a look.  ![search time viz](/images/hnsw-10.jpg) <small>Search time in µs for various `M`, `efConstruction`, and `efSearch` parameters when searching for `1000` queries. Note that the y-axis is using a log scale.</small>  Although higher parameter values provide us with better recall, the effect on search times can be dramatic. Here we search for `1000` similar vectors (`xq[:1000]`), and our recall/search-time can vary from 80%-1ms to 100%-50ms. If we're happy with a rather terrible recall, search times can even reach 0.1ms.  If you've been following our [articles on vector similarity search](/learn/), you may recall that `efConstruction` has a [negligible effect on search-time](/learn/vector-indexes/) — but that is not the case here...  When we search using a few queries, it *is* true that `efConstruction` has little effect on search time. But with the `1000` queries used here, the small effect of `efConstruction` becomes much more significant.  If you believe your queries will mostly be low volume, `efConstruction` is a great parameter to increase. It can improve recall with little effect on *search time*, particularly when using lower `M` values.   ![efConstruction search time](/images/hnsw-11.jpg) <small>`efConstruction` and search time when searching for only one query. When using lower `M` values, the search time remains almost unchanged for different `efConstruction` values.</small>  That all looks great, but what about the memory usage of the HNSW index? Here things can get slightly *less* appealing.  ![memory viz](/images/hnsw-12.jpg) <small>Memory usage with increasing values of `M` using our Sift1M dataset. `efSearch` and `efConstruction` have no effect on the memory usage.</small>  Both `efConstruction` and `efSearch` do not affect index memory usage, leaving us only with `M`. Even with `M` at a low value of `2`, our index size is already above 0.5GB, reaching almost 5GB with an `M` of `512`.  So although HNSW produces incredible performance, we need to weigh that against high memory usage and the inevitable high infrastructure costs that this can produce.  #### Improving Memory Usage and Search Speeds  HNSW is not the best index in terms of memory utilization. However, if this is important and using [another index](/learn/vector-indexes/) isn’t an option, we can improve it by compressing our vectors using [product quantization (PQ)](/learn/product-quantization/). Using PQ will reduce recall and increase search time — but as always, much of ANN is a case of balancing these three factors.  If, instead, we'd like to improve our search speeds — we can do that too! All we do is add an IVF component to our index. There is plenty to discuss when adding IVF or PQ to our index, so we wrote an [entire article on mixing-and-matching of indexes](/learn/composite-indexes/).  That's it for this article covering the Hierarchical Navigable Small World graph for vector similarity search! Now that you’ve learned the intuition behind HNSW and how to implement it in Faiss, you’re ready to go ahead and test HNSW indexes in your own vector search applications, or use a managed solution like [Pinecone](/) or OpenSearch that has vector search ready-to-go!  If you’d like to continue learning more about vector search and how you can use it to supercharge your own applications, we have a [whole set of learning materials](/learn/) aiming to bring you up to speed with the world of vector search.  {{< newsletter text=\"Subscribe for the latest in similarity search!\" inputText=\"Email address...\" buttonText=\"Submit\">}}  ## References  [1] E. Bernhardsson, [ANN Benchmarks](https://github.com/erikbern/ann-benchmarks) (2021), GitHub  [2] W. Pugh, [Skip lists: a probabilistic alternative to balanced trees](https://15721.courses.cs.cmu.edu/spring2018/papers/08-oltpindexes1/pugh-skiplists-cacm1990.pdf) (1990), Communications of the ACM, vol. 33, no.6, pp. 668-676  [3] Y. Malkov, D. Yashunin, [Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs](https://arxiv.org/abs/1603.09320) (2016), IEEE Transactions on Pattern Analysis and Machine Intelligence  [4] Y. Malkov et al., [Approximate Nearest Neighbor Search Small World Approach](https://www.iiis.org/CDs2011/CD2011IDI/ICTA_2011/PapersPdf/CT175ON.pdf) (2011), International Conference on Information and Communication Technologies & Applications  [5] Y. Malkov et al., [Scalable Distributed Algorithm for Approximate Nearest Neighbor Search Problem in High Dimensional General Metric Spaces](https://www.researchgate.net/publication/262334462_Scalable_Distributed_Algorithm_for_Approximate_Nearest_Neighbor_Search_Problem_in_High_Dimensional_General_Metric_Spaces) (2012), Similarity Search and Applications, pp. 132-147  [6] Y. Malkov et al., [Approximate nearest neighbor algorithm based on navigable small world graphs](https://publications.hse.ru/mirror/pubs/share/folder/x5p6h7thif/direct/128296059) (2014), Information Systems, vol. 45, pp. 61-68  [7] M. Boguna et al., [Navigability of complex networks](https://arxiv.org/abs/0709.0303) (2009), Nature Physics, vol. 5, no. 1, pp. 74-80  [8] Y. Malkov, A. Ponomarenko, [Growing homophilic networks are natural navigable small worlds](https://arxiv.org/abs/1507.06529) (2015), PloS one  Facebook Research, [Faiss HNSW Implementation](https://github.com/facebookresearch/faiss/blob/main/faiss/impl/HNSW.cpp), GitHub",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2da"
  },
  "title": "\"Generative Question-Answering with Long-Term Memory\"",
  "headline": "\"Generative Question-Answering with Long-Term Memory\"",
  "weight": "2",
  "- name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "Building better generative AI models with long-term memory components.",
  "images": "[\"/images/openai-gen-qa-0.png\"]",
  "thumbnail": "\"https://www.pinecone.io/images/openai-gen-qa-0.jpg\"",
  "content": "Generative AI sparked several _\"wow\"_ moments in 2022. From generative art tools like OpenAI's DALL-E 2, Midjourney, and Stable Diffusion, to the next generation of **L**arge **L**anguage **M**odels like OpenAI's GPT-3.5 generation models, BLOOM, and chatbots like LaMDA and ChatGPT.  It’s hardly surprising that Generative AI is experiencing a boom in interest and innovation [1]. Yet, this marks the *just* first year of generative AI's widespread adoption. The early days of a new field poised to disrupt how we interact with machines.  One of the most thought-provoking use cases belongs to **G**enerative **Q**uestion-**A**nswering (GQA). Using GQA, we can sculpt human-like interaction with machines for information retrieval (IR).  We all use IR systems every day. Google search indexes the web and retrieves relevant information to your search terms. Netflix uses your behavior and history on the platform to recommend new TV shows and movies, and Amazon does the same with products [2].  These applications of IR are world-changing. Yet, they may be little more than a faint echo of what we will see in the coming months and years with the combination of IR and GQA.  Imagine a Google that can answer your queries with an intelligent and insightful summary based on the top 20 pages — highlighting key points and information sources.  The technology available today already makes this possible and surprisingly easy. This article will look at retrieval-enhanced GQA and how to implement it with Pinecone and OpenAI.  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/dRUIGgNBvVk\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ---  # Generative Question-Answering  The most straightforward GQA system requires nothing more than a user text query and a large language model (LLM).  ![gqa-simple](./images/openai-gen-qa-1.png)  <small>Simplest GQA system.</small>  We can access one of the most advanced LLMs in the world via OpenAI. To start, we sign up for an [API key](https://beta.openai.com).  ![openai-api-key](./images/openai-gen-qa-2.png)  <small>After signing up for an account, API keys can be created by clicking on your account (top-right) > View API keys > Create new secret key.</small>  Then we switch to a Python file or notebook, install some prerequisites, and initialize our connection to OpenAI.  {{< notebook file=\"openai-transcriptions\" height=\"full\" >}}  From here, we can use the OpenAI completion endpoint to ask a question like _\"who was the 12th person on the moon and when did they land?\" _:  {{< notebook file=\"openai-transcripts-complete-easy\" height=\"full\" >}}  We get an accurate answer immediately. Yet, this question is relatively easy, what happens if we ask about a lesser-known topic?  {{< notebook file=\"openai-transcripts-complete-harder\" height=\"full\" >}}  Although this answer is technically correct, it isn't an answer. It tells us to use a supervised training method and learn the relationship between sentences. Both of these facts are true but do not answer the original question.  There are two options for allowing our LLM to better understand the topic and, more precisely, answer the question.  1. We fine-tune the LLM on text data covering the domain of fine-tuning sentence transformers. 2. We use *retrieval-enhanced ML*, meaning we add an information retrieval component to our GQA process. Adding a retrieval step allows us to retrieve relevant information and feed this into the LLM as a _secondary source_ of information.  In the following sections, we will outline how to implement option **two**.  ---  # Building a Knowledge Base  With option **two** of implementing retrieval, we need an external _\"knowledge base \"_. A knowledge base acts as the place where we store information and as the system that effectively retrieves this information.  A knowledge base is a store of information that can act as an external reference for GQA models. We can think of it as the _\"long-term memory\"_ for AI systems.  We refer to knowledge bases that can enable the retrieval of semantically relevant information as *vector databases*.  A vector database stores vector representations of information encoded using specific ML models. These models have an \"understanding\" of language and can encode passages with similar meanings into a similar vector space and dissimilar passages into a dissimilar vector space.  ![similar-v-dissimilar](./images/openai-gen-qa-3.png)  We can achieve this with OpenAI via the embed endpoint:  {{< notebook file=\"openai-transcripts-embed\" height=\"full\" >}}  We'll need to repeat this embedding process over many records that will act as our pipeline's external source of information. These records still need to be downloaded and prepared for embedding.  ## Data Preparation  The dataset we will use in our knowledge base is the `jamescalam/youtube-transcriptions` dataset hosted on Hugging Face _Datasets_. It contains transcribed audio from several ML and tech YouTube channels. We download it with the following:  {{< notebook file=\"openai-transcripts-dataset\" height=\"full\" >}}  The dataset contains many small snippets of text data. We need to merge several snippets to create more substantial chunks of text that contain more meaningful information.  {{< notebook file=\"openai-transcripts-merge-snippets\" height=\"full\" >}}  With the text chunks created, we can begin initializing our knowledge base and populating it with our data.  ## Creating the Vector Database  The vector database is the storage and retrieval component in our pipeline. We use Pinecone as our vector database. For this, we need to sign up for a [free API key](https://app.pinecone.io) and enter it below, where we create the index for storing our data.  {{< notebook file=\"openai-transcripts-create-index\" height=\"full\" >}}  Then we embed and index a dataset like so:  {{< notebook file=\"openai-transcripts-indexing\" height=\"full\" >}}  We're ready to combine OpenAI's `Completion` and `Embedding` endpoints with our Pinecone vector database to create a retrieval-enhanced GQA system.  ---  # OP Stack  The OpenAI Pinecone (OP) stack is an increasingly popular choice for building high-performance AI apps, including retrieval-enhanced GQA.  Our pipeline during *query time* consists of the following:  1. OpenAI `Embedding` endpoint to create vector representations of each query. 2. Pinecone vector database to search for relevant passages from the database of previously indexed contexts. 3. OpenAI `Completion` endpoint to generate a natural language answer considering the retrieved contexts.  ![retrieval-enhanced-gqa-query](./images/openai-gen-qa-4.png)  We start by encoding queries using the same encoder model to create a query vector `xq`.  {{< notebook file=\"openai-transcripts-xq\" height=\"full\" >}}  The query vector `xq` is used to query Pinecone via `index.query`, and previously indexed passage vectors are compared to find the most similar matches — returned in `res` above.  Using these returned contexts, we can construct a prompt instructing the generative LLM to answer the question based on the retrieved contexts. To keep things simple, we will do all this in a function called `retrieve`.  {{< notebook file=\"openai-transcripts-retrieve\" height=\"full\" >}}  <small>Note that the generated *expanded query* (`query_with_contexts`) has been shortened for readability.</small>  From `retrieve`, we produce a longer prompt (`query_with_contexts`) containing some instructions, the contexts, and the original question.  The prompt is then fed into the generative LLM via OpenAI's `Completion` endpoint. As before, we use the `complete` function to handle everything.  {{< notebook file=\"openai-transcripts-complete-final\" height=\"full\" >}}  Because of the additional _\"source knowledge\"_ (information fed directly into the model), we have eliminated the hallucinations of the LLM — producing accurate answers to our question.  Beyond providing more factual answers, we also have the *sources* of information from Pinecone used to generate our answer. Adding this to downstream tools or apps can help improve user trust in the system. Allowing users to confirm the reliability of the information being presented to them.  ---  That's it for this walkthrough of retrieval-enhanced **G**enerative **Q**uestion **A**nswering (GQA) systems.  As demonstrated, LLMs alone work incredibly well but struggle with more niche or specific questions. This often leads to *hallucinations* that are rarely obvious and likely to go undetected by system users.  By adding a _\"long-term memory\"_ component to our GQA system, we benefit from an external knowledge base to improve system factuality and user trust in generated outputs.  Naturally, there is vast potential for this type of technology. Despite being a new technology, we are already seeing its use in [YouChat](https://blog.you.com/introducing-youchat-the-ai-search-assistant-that-lives-in-your-search-engine-eff7badcd655), several [podcast search apps](https://huberman.rile.yt), and rumors of its upcoming use as a challenger to Google itself [3].  There is potential for disruption in any place where the need for information exists, and retrieval-enhanced GQA represents one of the best opportunities for taking advantage of the outdated information retrieval systems in use today.  # References  [1] E. Griffith, C. Metz, [A New Area of A.I. Booms, Even Amid the Tech Gloom](https://www.nytimes.com/2023/01/07/technology/generative-ai-chatgpt-investments.html) (2023), NYTimes  [2] G. Linden, B. Smith, J. York, [Amazon.com Recommendations: Item-to-Item Collaborative Filtering](https://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf) (2003), IEEE  [3] T. Warren, [Microsoft to challenge Google by integrating ChatGPT with Bing search](https://www.theverge.com/2023/1/4/23538552/microsoft-bing-chatgpt-search-google-competition) (2023), The Verge ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2dc"
  },
  "content": "categories:   - Algorithms & Libraries toc: >- weight: 1 author:   name: James Briggs   position: Developer Advocate   src: /images/james-briggs.jpeg   href: \"https://www.youtube.com/c/jamesbriggs\" description: Demystifying BERTopic and how it works with transformers, UMAP, HDBSCAN, and c-TF-IDF. # Open graph images: [\"/images/bertopic-0.png\"] thumbnail: \"https://www.pinecone.io/images/bertopic-0.png\" ---  90% of the world's data is unstructured. It is built by humans, for humans. That's great for human consumption, but it is _very_ hard to organize when we begin dealing with the massive amounts of data abundant in today's information age.  Organization is complicated because unstructured text data is not intended to be understood by machines, and having humans process this abundance of data is wildly expensive and _very slow_.  Fortunately, there is light at the end of the tunnel. More and more of this unstructured text is becoming accessible and understood by machines. We can now [search text based on _meaning_](/learn/dense-vector-embeddings-nlp/), identify the sentiment of text, extract entities, and much more.  [Transformers](/learn/transformers/) are behind much of this. These transformers are (unfortunately) not Michael Bay's Autobots and Decepticons and (fortunately) not buzzing electrical boxes. Our NLP transformers lie somewhere in the middle, they're not sentient Autobots (yet), but they can understand language in a way that existed only in sci-fi until a short few years ago.  Machines with a human-like comprehension of language are pretty helpful for organizing masses of unstructured text data. In machine learning, we refer to this task as _topic modeling_, the automatic clustering of data into particular topics.  BERTopic takes advantage of the superior language capabilities of these (not yet sentient) transformer models and uses some other ML magic like UMAP and HDBSCAN (more on these later) to produce what is one of the most advanced techniques in language topic modeling today.  ---  ## BERTopic at a Glance  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/fb7LENb9eag\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  We will dive into the details behind BERTopic [1], but before we do, let us see how we can use it and take a first glance at its components.  To begin, we need a dataset. We can download the dataset from HuggingFace datasets with:  ```python from datasets import load_dataset  data = load_dataset('jamescalam/python-reddit') ```  The dataset contains data extracted using the Reddit API from the _/r/python_ subreddit. The code used for this (and all other examples) can [be found here](https://github.com/pinecone-io/examples/tree/master/learn/algos-and-libraries/bertopic).  Reddit thread contents are found in the `selftext` feature. Some are empty or short, so we remove them with:  ```python data = data.filter(     lambda x: True if len(x['selftext']) > 30 else 0 ) ```  We perform topic modeling using the `BERTopic` library. The _\"basic\"_ approach requires just a few lines of code.  ```python from bertopic import BERTopic from sklearn.feature_extraction.text import CountVectorizer  # we add this to remove stopwords vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\")  model = BERTopic(     vectorizer_model=vectorizer_model,     language='english', calculate_probabilities=True,     verbose=True ) topics, probs = model.fit_transform(text) ```  From `model.fit_transform` we return two lists:  - `topics` contains a one-to-one mapping of inputs to their modeled _topic_ (or cluster). - `probs` contains a list of probabilities that an input belongs to their assigned topic.  We can then view the topics using `get_topic_info`.  {{< notebook file=\"bertopic-glance-topics-table\" height=\"full\" >}}  The top `-1` topic is typically assumed to be irrelevant, and it usually contains stop words like _\"the\"_, _\"a\"_, and _\"and\"_. However, we removed stop words via the `vectorizer_model` argument, and so it shows us the _\"most generic\"_ of topics like _\"Python\"_, _\"code\"_, and _\"data\"_.  The library has several built-in visualization methods like `visualize_topics`, `visualize_hierarchy`, and `visualize_barchart`.  ![bertopic-viz-hierarchy](/images/bertopic-1.png) <small>BERTopic's `visualize_hierarchy` visualization allows us to view the \"hierarchy\" of topics.</small>  These represent the surface level of the BERTopic library, which has excellent documentation, so we will not rehash that here. Instead, let's try and understand _how_ BERTopic works.  ## Overview  There are _four_ key components used in BERTopic [2], those are:  - A transformer embedding model - UMAP dimensionality reduction - HDBSCAN clustering - Cluster tagging using c-TF-IDF  We already did _all_ of this in those few lines of BERTopic code; everything is just abstracted away. However, we can optimize the process by understanding the essentials of each component. This section will work through each component _without_ BERTopic, and learn how they work before returning to BERTopic at the end.  ### Transformer Embedding  BERTopic supports several libraries for encoding our text to dense vector embeddings. If we build poor quality embeddings, nothing we do in the other steps will be able to help us, so it is _very important_ that we choose a suitable embedding model from one of the supported libraries, which include:  - Sentence Transformers - Flair - SpaCy - Gensim - USE (from TF Hub)  Of the above, the _Sentence Transformers_ library provides the most extensive library of high-performing [sentence embedding models](/learn/sentence-embeddings/). They can be found on HuggingFace Hub by searching for _\"sentence-transformers\"_.  ![hf-hub-screenshot](/images/bertopic-2.jpg)  <small>We can find _official_ sentence transformer models by searching for _\"sentence-transformers\"_ on HuggingFace Hub.</small>  The first result of this search is `sentence-transformers/all-MiniLM-L6-v2`, this is a popular high-performing model that creates _384_-dimensional sentence embeddings.  To initialize the model and encode our Reddit topics data, we first `pip install sentence-transformers` and then write:  {{< notebook file=\"bertopic-embedding\" height=\"full\" >}}  Here we have encoded our text in batches of `16`. Each batch is added to the `embeds` array. Once we have all of the [sentence embeddings](/learn/sentence-embeddings/) in `embeds` we're ready to move on to the next step.  ### Dimensionality Reduction  After building our embeddings, BERTopic compresses them into a lower-dimensional space. This means that our 384-dimensional vectors are transformed into two/three-dimensional vectors.  We can do this because 384 dimensions are _a lot_, and it is unlikely that we really need that many dimensions to represent our text [4]. Instead, we attempt to _compress_ that information into two or three dimensions.  We do this so that the following HDBSCAN clustering step can be done more efficiently. Performing the clustering step with 384-dimensions would be desperately slow [5].  Another benefit is that we can visualize our data; this is incredibly helpful when assessing whether our data can be clustered. Visualization also helps when tuning the dimensionality reduction parameters.  To help us understand dimensionality reduction, we will start with a 3D representation of the world. You can find [the code for this part here](https://github.com/pinecone-io/examples/tree/master/learn/algos-and-libraries/bertopic).  {{< plotly obj=\"bertopic-1\" >}}  <small>3D scatter plot of points from the [`jamescalam/world-cities-geo`](https://huggingface.co/datasets/jamescalam/world-cities-geo) dataset.</small>  We can apply many dimensionality reduction techniques to this data; two of the most popular choices are PCA and t-SNE.  {{< plotly obj=\"bertopic-2\" >}}  <small>Our 2D world reduced using PCA.</small>  PCA works by preserving _larger distances_ (using mean squared error). The result is that the _global structure_ of data is usually preserved [6]. We can see that behavior above as each continent is grouped with its neighboring continent(s). When we have easily distinguishable clusters in datasets, this can be good, but it performs poorly for more nuanced data where _local structures_ are important.  {{< plotly obj=\"bertopic-3\" >}}  <small>2D Earth reduced using t-SNE.</small>  t-SNE is the opposite; it preserves _local structures_ rather than _global_. This localized focus results from t-SNE building a graph, connecting all of the nearest points. These local structures can indirectly suggest the global structure, but they are not strongly captured.  ---  PCA focuses on preserving _dissimilarity_ whereas t-SNE focuses on preserving _similarity_.  ---  Fortunately, we can capture the best of both using a lesser-known technique called **U**niform **M**anifold **A**pproximation and **P**roduction (UMAP).  We can apply UMAP in Python using the UMAP library, installed using `pip install umap-learn`. To map to a 3D or 2D space using the default UMAP parameters, all we write is:  ```python import umap  fit = umap.UMAP(n_components=3)  # by default this is 2 u = fit.fit_transform(data) ```  The UMAP algorithm can be fine-tuned using several parameters. Still, the simplest and most effective tuning can be achieved with just the `n_neighbors` parameter.  For each datapoint, UMAP searches through other points and identifies the **k**th nearest neighbors [3]. It is **k**, controlled by the `n_neighbors` parameter.  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/bertopic-9.mp4\" type=\"video/mp4\"> </video>  <small>**k** and `n_neighbors` are synonymous here. As we increase `n_neighbors` the graph built by UMAP can consider more distant points and better represent the global structure.</small>  Where we have many points (high-density regions), the distance between our point and its **k**th nearest neighbor is usually smaller. In low-density regions with fewer points, the distance will be much greater.  ![bertopic-umap-density](/images/bertopic-3.png)  <small>Density is measured indirectly using the distances between **k**th nearest neighbors in different regions.</small>  UMAP will attempt to preserve distances to the **k**th nearest point is what UMAP attempts to preserve when shifting to a lower dimension.  By increasing `n_neighbors` we can preserve more global structures, whereas a lower `n_neighbors` better preserves local structures.  ![bertopic-n_neighbors-global](/images/bertopic-4.png) <small>Higher `n_neighbors` (**k**) means we preserve larger distances and thus maintain more of the global structure.</small>  Compared to other dimensionality reduction techniques like PCA or t-SNE, finding a good `n_neighbours` value allows us to preserve _both_ local and global structures relatively well.  Applying it to our 3D globe, we can see neighboring countries remain neighbors. At the same time, continents are placed correctly (with North-South inverted), and islands are separated from continents. We even have what seems to be the Spanish Peninsula in \"western Europe\".  {{< plotly obj=\"bertopic-4\" >}}  <small>The UMAP-reduced Earth.</small>  UMAP maintains distinguishable features that are not preserved by PCA and a better global structure than t-SNE. This is a great overall example of where the benefit of UMAP lies.  UMAP can also be used as a supervised dimensionality reduction method by passing labels to the `target` argument if we have labeled data. It is possible to produce even more meaningful structures using this supervised approach.  With all that in mind, let us apply UMAP to our Reddit topics data. Using `n_neighbors` of `3`-`5` seems to work best. We can add `min_dist=0.05` to allow UMAP to place points closer together (the default value is `1.0`); this helps us separate the three _similar_ topics from _r/Python_, _r/LanguageTechnology_, and _r/pytorch_.  ```python fit = umap.UMAP(n_neighbors=3, n_components=3, min_dist=0.05) u = fit.fit_transform(embeds) ```  {{< plotly obj=\"bertopic-5\" >}}  <small>Reddit topics data reduced to 3D space using UMAP.</small>  With our data reduced to a lower-dimensional space and topics easily visually identifiable, we're in an excellent spot to move on to clustering.  ### HDBSCAN Clustering  We have visualized the UMAP reduced data using the existing _sub_ feature to color our clusters. It looks pretty, but we don't usually perform topic modeling to label already labeled data. If we assume that we have no existing labels, our UMAP visual will look like this:  {{< plotly obj=\"bertopic-6\" >}}  <small>UMAP reduced cities data, we can distinguish many clusters/continents, but it is much more difficult without label coloring.</small>  Now let us look at how HDBSCAN is used to cluster the (now) low-dimensional vectors.  Clustering methods can be broken into flat or hierarchical _and_ centroid or density-based techniques [5]. Each of which has its own benefits and drawbacks.  Flat or hierarchical focuses simply on whether there is (or is not) a hierarchy in the clustering method. For example, we may (ideally) view our graph hierarchy as moving from continents to countries to cities. These methods allow us to view a given hierarchy and try to identify a logical \"cut\" along the tree.  ![bertopic-hierarchy](/images/bertopic-5.png) <small>Hierarchical techniques begin from one large cluster and split this cluster into smaller and smaller parts and try to find the *ideal* number of clusters in the hierarchy.</small>  The other split is between centroid-based or density-based clustering. That is clustering based on proximity to a centroid or clustering based on the density of points. Centroid-based clustering is ideal for _\"spherical\"_ clusters, whereas density-based clustering can handle more irregular shapes _and_ identify outliers.  ![bertopic-density-centroid](/images/bertopic-6.png) <small>Centroid-based clustering (left) vs density-based clustering (right).</small>  HDBSCAN is a hierarchical, density-based method. Meaning we can benefit from the easier tuning and visualization of hierarchical data, handle irregular cluster shapes, _and_ identify outliers.  When we first apply HDBSCAN clustering to our data, we will return _many_ tiny clusters, identified by the red _circles_ in the _condensed tree plot_ below.  {{< notebook file=\"bertopic-hdbscan-tree\" height=\"full\" >}}  ---  _The condensed tree plot shows the drop-off of points into outliers and the splitting of clusters as the algorithm scans by increasing lambda values._  _HDBSCAN chooses the final clusters based on their size and persistence over varying lambda values. The tree's thickest, most persistent \"branches\" are viewed as the most stable and, therefore, best candidates for clusters._  ---  These clusters are not very useful because the _default_ minimum number of points needed to \"create\" a cluster is just `5`. Given our ~3K points dataset where we aim to produce ~4 subreddit clusters, this is small. Fortunately, we can increase this threshold using the `min_cluster_size` parameter.  {{< notebook file=\"bertopic-hdbscan-tree-2\" height=\"full\" >}}  Better, but not quite there, we can try to reduce the `min_cluster_size` to `60` to pull in the three clusters below the green block.  {{< notebook file=\"bertopic-hdbscan-tree-3\" height=\"full\" >}}  Unfortunately, this still pulls in the green block and even allows _too small_ clusters (as on the left). Another option is to keep `min_cluster_size=80` but add `min_samples=40`, to allow for more sparse core points.  {{< notebook file=\"bertopic-hdbscan-tree-4\" height=\"full\" >}}  Now we have four clusters, and we can visualize them using the data in `clusterer.labels_`.  {{< plotly obj=\"bertopic-7\" >}}  <small>HDBSCAN clustered Reddit topics, accurately identifying the different subreddit clusters. The sparse blue points are outliers and are not identified as belonging to any cluster.</small>  A few outliers are marked in blue, some of which make sense (pinned daily discussion threads) and others that do not. However, overall, these clusters are very accurate. With that, we can try to identify the meaning of these clusters.  ### Topic Extraction with c-TF-IDF  The final step in BERTopic is extracting topics for each of our clusters. To do this, BERTopic uses a modified version of TF-IDF called c-TF-IDF.  TF-IDF is a popular technique for identifying the most relevant _\"documents\"_ given a term or set of terms. c-TF-IDF turns this on its head by finding the most relevant _terms_ given all of the \"documents\" within a cluster.  ![bertopic-ctfidf-intuition](/images/bertopic-7.png) <small>c-TF-IDF looks at the most relevant terms from each class (cluster) to create topics.</small>  In our Reddit topics dataset, we have been able to identify very distinct clusters. However, we still need to determine what these clusters talk about. We start by preprocessing the `selftext` to create tokens.  {{< notebook file=\"bertopic-ctfidf-tokens\" height=\"full\" >}}  Part of c-TF-IDF requires calculating the frequency of term _t_ in class _c_. For that, we need to see which tokens belong in each class. We first add the cluster/class labels to `data`.  ```python # add the cluster labels to our dataset data = data.add_column('class', clusterer.labels_) ```  Now create class-specific lists of tokens.  ```python classes = {label: {'tokens': []} for label in set(clusterer.labels_)} # add tokenized sentences to respective class for row in data:     classes[row['class']]['tokens'].extend(row['tokens']) ```  We can calculate **T**erm **F**requency (TF) per class.  {{< notebook file=\"bertopic-ctfidf-tf\" height=\"full\" >}}  Note that this can take some time; our TF process prioritizes readability over any notion of efficiency. Once complete, we're ready to calculate **I**nverse **D**ocument **F**requency (IDF), which tells us how common a term is. Rare terms signify greater relevance than common terms (and will output a greater IDF score).  {{< notebook file=\"bertopic-ctfidf-idf\" height=\"full\" >}}  We now have TF and IDF scores for every term, and we can calculate the c-TF-IDF score by simply multiplying both.  {{< notebook file=\"bertopic-ctfidf-tfidf\" height=\"full\" >}}  In `tf_idf`, we have a `vocab` sized list of TF-IDF scores for each class. We can use Numpy's `argpartition` function to retrieve the index positions containing the greatest TF-IDF scores per class.  {{< notebook file=\"bertopic-ctfidf-top-idx\" height=\"full\" >}}  Now we map those index positions back to the original words in the `vocab`.  {{< notebook file=\"bertopic-ctfidf-top-terms\" height=\"full\" >}}  Here we have the top five most relevant words for each cluster, each identifying the most relevant topics in each subreddit.  ## Back to BERTopic  We've covered a considerable amount, but can we apply what we have learned to the BERTopic library?  Fortunately, all we need are a few lines of code. As before, we initialize our custom embedding, UMAP, and HDBSCAN components.  ```python from sentence_transformers import SentenceTransformer from umap import UMAP from hdbscan import HDBSCAN  embedding_model = SentenceTransformer('all-MiniLM-L6-v2') umap_model = UMAP(n_neighbors=3, n_components=3, min_dist=0.05) hdbscan_model = HDBSCAN(min_cluster_size=80, min_samples=40,                         gen_min_span_tree=True,                         prediction_data=True) ```  You might notice that we have added `prediction_data=True` as a new parameter to `HDBSCAN`. We need this to avoid an **AttributeError** when integrating our custom HDBSCAN step with BERTopic. Adding `gen_min_span_tree` adds another step to HDBSCAN that can improve the resultant clusters.  We must also initialize a `vectorizer_model` to handle stopword removal during the c-TF-IDF step. We will use the list of stopwords from NLTK but add a few more tokens that seem to pollute the results.  ```python from sklearn.feature_extraction.text import CountVectorizer from nltk.corpus import stopwords  stopwords = list(stopwords.words('english')) + ['http', 'https', 'amp', 'com']  # we add this to remove stopwords that can pollute topcs vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=stopwords) ```  We're now ready to pass all of these to a `BERTopic` instance and process our Reddit topics data.  ```python from bertopic import BERTopic  model = BERTopic(     umap_model=umap_model,     hdbscan_model=hdbscan_model,     embedding_model=embedding_model,     vectorizer_model=vectorizer_model,     top_n_words=5,     language='english',     calculate_probabilities=True,     verbose=True ) topics, probs = model.fit_transform(data['selftext']) ```  We can visualize the new topics with `model.visualize_barchart()`  ![bertopic-final-barchart](/images/bertopic-8.png)  <small>Our final topics produced using the BERTopic library with the tuned UMAP and HDBSCAN parameters.</small>  We can see the topics align perfectly with _r/investing_, _r/pytorch_, _r/LanguageTechnology_, and _r/Python_.  Transformers, UMAP, HDBSCAN, and c-TF-IDF are clearly powerful components that have huge applications when working with unstructured text data. BERTopic has abstracted away much of the complexity of this stack, allowing us to apply these technologies with nothing more than a few lines of code.  Although BERTopic can be simple, you have seen that it is possible to dive quite deeply into the individual components. With a high-level understanding of those components, we can greatly improve our topic modeling performance.  We have covered the essentials here, but we genuinely are just scratching the surface of topic modeling in this article. There is much more to BERTopic and each component than we could ever hope to cover in a single article.  So go and apply what you have learned here, and remember that despite showing the incredible performance of BERTopic shown here, there is even more that it can do.  {{< newsletter text=\"Subscribe for the latest in NLP!\" inputText=\"Email address...\" buttonText=\"Submit\">}}  ## Resources  🔗 [All Notebook Scripts](https://github.com/pinecone-io/examples/tree/master/learn/algos-and-libraries/bertopic)  [1] M. Grootendorst, [BERTopic Repo](https://github.com/MaartenGr/BERTopic), GitHub  [2] M. Grootendorst, [BERTopic: Neural Topic Modeling with a Class-based TF-IDF Procedure](https://arxiv.org/abs/2203.05794) (2022)  [3] L. McInnes, J. Healy, J. Melville, [UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction](https://arxiv.org/abs/1802.03426) (2018)  [4] L. McInnes, [Talk on UMAP for Dimension Reduction](https://www.youtube.com/watch?v=nq6iPZVUxZU) (2018), SciPy 2018  [5] J. Healy, [HDBSCAN, Fast Density Based Clustering, the How and the Why](https://www.youtube.com/watch?v=dGsxd67IFiU) (2018), PyData NYC 2018  [6] L. McInnes, [A Bluffer's Guide to Dimension Reduction](https://www.youtube.com/watch?v=9iol3Lk6kyU) (2018), PyData NYC 2018  [UMAP Explained](https://www.youtube.com/watch?v=6BPl81wGGP8), AI Coffee Break with Letitia, YouTube ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2de"
  },
  "title": "\"Embeddings to Identify Fake News\"",
  "headline": "\"Embeddings to Identify Fake News\"",
  "weight": "4",
  "name": "Diego Lopez Yse",
  "position": "Data Scientist",
  "src": "/images/diego-lopez-yse.jpeg",
  "href": "\"https://lopezyse.medium.com/\"",
  "description": "Learn how Machine Learning can be used to identify fake news.",
  "images": "['/images/embeddings-to-identify-fake-news-4.png']",
  "content": " **How to fight misinformation in the digital era.**  Times of crisis are perfect for controversial statements. Do you think COVID-19 was genetically engineered as a biological weapon? And that climate policies will hurt working families and ruin the economy? What about Bitcoin? Is it the latest economic revolution, or a massive fraudulent system? Hard to tell since we’re living in the era of fake news.  **Fake news** consists of deliberate misinformation under the guise of being authentic news, spread via some communication channel, and produced with a particular objective like generating revenue, promoting or discrediting a public figure, a political movement, an organization, etc.  ![Total facebook engagements for top 20 election stories](/images/embeddings-to-identify-fake-news-1.png) <small>Analysis shows how Viral Fake Election News Stories outperformed Real News on Facebook during the US presidential elections in 2016. Source: [University of Windsor](https://scholar.uwindsor.ca/cgi/viewcontent.cgi?article=9401&context=etd)</small>  Fake news travels [six times faster](https://www.wbur.org/hereandnow/2018/03/09/social-media-buzz-study-false-news) on Twitter and reaches significantly more people than actual news. And from the big universe of fake news, false political news travels [farther, faster, deeper, and more broadly](https://news.mit.edu/news-clip/wbur-136) than any other type.  During the [2018 national elections in **Brazil**](https://www.nytimes.com/2018/10/17/opinion/brazil-election-fake-news-whatsapp.html), WhatsApp was used to spread alarming amounts of misinformation, rumours and false news favouring Jair Bolsonaro. In 2019, the two main Indian political parties took these tactics to a new scale by [trying to influence India’s 900 million eligible voters](https://www.hindustantimes.com/india-news/bjp-plans-a-whatsapp-campaign-for-2019-lok-sabha-election/story-lHQBYbxwXHaChc7Akk6hcI.html) through creating content on Facebook and spreading it on WhatsApp.  But these tactics aren’t only applied in the political arena: They also involve activities from [manipulating share prices](https://www.ft.com/content/32013b6a-202f-11ea-b8a1-584213ee7b2b) to attacking commercial rivals with fake customer critics.  How can we deal with this problem? Are we supposed to just live with it? Fortunately we don’t have to, and we can use Machine Learning to identify fake news. Let’s see how.  ### Embeddings  Fake documents and articles contain attributes and linguistic signs that can be used to reveal patterns. Considering their textual components, features like [author, context, and writing style](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8053013/) can help in identifying fake news.  But before applying any Machine Learning technique to text, we need to format the input data into a numerical representation that can be understood by the model we’re building. Here’s where the problem begins: Traditional techniques like Term frequency–inverse document frequency ([TF-IDF](https://lopezyse.medium.com/nlp-text-vectorization-e472a3a9983a)) result in high-dimensional representations of linguistic information, which lead to some negative effects like the [Curse of Dimensionality](https://analyticsindiamag.com/curse-of-dimensionality-and-what-beginners-should-do-to-overcome-it/), increasing the error of our model as the number of features increases.  One way to overcome these problems is through word and [sentence embeddings](https://www.pinecone.io/learn/sentence-embeddings/), which give us low dimensional, distributed representations of the data. **Embeddings are representations of words or sentences** in multidimensional spaces such that words or sentences with similar meanings have similar embeddings. It means that each word or sentence is [mapped to the vector of real numbers](https://orangedatamining.com/blog/2020/2020-10-15-document-embedders/) that represents those words or sentences.  This is what an embedding for the word “king” looks like:  ![Vector embedding representation example](/images/embeddings-to-identify-fake-news-2.png) <small>Example of a vector embedding representation (GloVe trained on Wikipedia). Source: [Jay Alammar](https://jalammar.github.io/)</small>   [Embeddings](https://www.pinecone.io/learn/vector-embeddings/) not only convert the word or text to a numerical representation, but also identify and incorporate their semantic and syntax information.  ![Visualizations of real embeddings](/images/embeddings-to-identify-fake-news-3.png) <small>Position (distance and direction) in the vector space can encode semantics in a good embedding. For example, the following visualizations of real embeddings show geometrical relationships that capture semantic relations like the relation between a country and its capital. This sort of meaningful space gives your machine learning system opportunities to detect patterns that may help with the learning task. Source: [Google](https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space)</small>  Embeddings are distributed representations of words and text in a continuous vector space and can facilitate tasks like [semantic search](https://www.pinecone.io/learn/semantic-search/), clustering, recommendation, sentiment analysis, [question-answering](https://www.pinecone.io/learn/question-answering/), or deduplication.  ### How to create embeddings  There are a number of ways to create embeddings and many techniques for capturing the important structure of a high dimensional space in a low dimensional space. [Although methods like principal component analysis (PCA) have been used to create embedding](https://developers.google.com/machine-learning/crash-course/embeddings/obtaining-embeddings), newer techniques have shown better performance. Let’s begin with word embeddings.  #### Word embeddings  Word embeddings are vectorized representations of words and perhaps one of the most important innovations in the [Natural Language Processing](https://www.pinecone.io/learn/dense-vector-embeddings-nlp/) (NLP) discipline. Let’s see look at of the main algorithms already in use:  #### Word2vec  Since its inception in 2013, Word2vec has become widely used both in research and commercial areas. The idea behind it is that it’s possible to **predict a word based on its context** (neighbouring words) under the assumption that the [meaning of a word can be inferred by the company it keeps](https://kavita-ganesan.com/comparison-between-cbow-skipgram-subword/#.YYwHZ2DMI2w). Word2vec can use two architectures to produce a distributed representation of words: continuous bag-of-words or **CBOW** (where we predict the current word from a window of surrounding context words) and **Skip-gram** (where we try to predict the context words using the main word).  ![Word2vec algorithm](/images/embeddings-to-identify-fake-news-4.png) <small>The Word2vec algorithm implements both CBOW and Skip-gram. The basic idea behind the two training models is that either a word is used to predict the context of it (Skip-gram) or the other way around, to use the context to predict a current word (CBOW). This task is iterated word by word over the corpus. Source: [ResearchGate](https://www.researchgate.net/figure/Continuous-Bag-of-words-CBOW-CB-and-Skip-gram-SG-training-model-illustrations_fig1_326588219)</small>  #### GloVe  Developed by Stanford, GloVe (Global Vectors for Word Representation) is another method to create word embeddings. Its advantage over Word2Vec is that it doesn’t rely just on local statistics (local context information of the words), but incorporates global statistics (word co-occurrence) from the whole text corpus.  GloVe uses **co-occurrence** (how frequently two words appear together) statistics at a global text level to model the vector representations of words. This is an important aspect since word-word co-occurrences may carry rich semantic information. [For example](https://d2l.ai/chapter_natural-language-processing-pretraining/glove.html), in a large corpus the word “solid” is more likely to co-occur with “ice” than with “steam”, but the word “gas” probably co-occurs with “steam” more frequently than with “ice”.  #### Fasttext  Fasttext was developed by Facebook with the idea to use the **internal structure of words** to improve vector representations (creating “sub-words” from words), which is a huge advantage over other models.  Both in Word2Vec and GloVe an embedding is created for each word, and, as such, they can’t handle any words they haven’t encountered during their training. Alternatively, Fasttext can [derive word vectors for unknown words](https://medium.com/analytics-vidhya/word-embeddings-in-nlp-word2vec-glove-fasttext-24d4d4286a73), creating embeddings for **words that weren’t seen before**.  ![Fasttext embeddings](/images/embeddings-to-identify-fake-news-5.png) <small>Fasttext generates character n-grams of length n. For example, for the word “eating”, character n-grams of length 3 can be generated by sliding a window of 3 characters from the start of the angular bracket till the ending angular bracket is reached. Source: [A Visual Guide to FastText Word Embeddings](https://amitness.com/2020/06/fasttext-embeddings/)</small>   ### Sentence embeddings  Word embeddings are highly effective for tasks that don’t require comparisons between sentences or documents. But using word embeddings over large pieces of text can limit our understanding if we need to compute the semantic similarity of different texts, analyse intention, sentiment, or cluster them.  ![Sentence embeddings](/images/embeddings-to-identify-fake-news-6.png) <small>Pairwise semantic similarity comparison via outputs from TensorFlow Hub Universal Sentence Encoder. Source: [Google AI Blog](https://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html)</small>  What if, instead of embedding individual words, we could embed sentences? Sentence embeddings are the extension of the key ideas behind word embeddings.  #### Doc2vec  Based on the Word2vec algorithm, Doc2vec represents pieces of texts (ranging from sentences to documents) as fixed-length, low dimensional embeddings. In this architecture, the two learning algorithms are Distributed Memory version of Paragraph Vector (**PV-DM**), and Distributed Bag of Words version of Paragraph Vector (**PV-DBOW**).  In PV-DM, [a **paragraph “id”** is inserted as another word](https://gab41.lab41.org/doc2vec-to-assess-semantic-similarity-in-source-code-667acb3e62d7) in an ordered sequence of words. PV-DM attempts to predict a word in the ordered sequence based on the other surrounding words in the sentence and the context provided by the paragraph “id”. On the other hand, PV-DBOW takes a given paragraph “id” and uses it to predict words in the window without any restriction on word order.  #### SentenceBERT  BERT is a word embedding algorithm ([Transformer](https://www.pinecone.io/learn/transformers/) based) that can **encode the context of words**. This means that while standard word embedding algorithms would produce the same vector for “bank” whether it was “a grassy bank” or “the bank of England”, BERT would instead [modify the encoding for “bank” based on the surrounding context](https://www.pinecone.io/learn/dense-vector-embeddings-nlp/).  This capability was extended from words to sentences through SentenceBERT (SBERT), which outputs sentence embeddings with an impressive **speed** performance. While finding the most similar sentence pair from 10K sentences took 65 hours with BERT, [SBERT embeddings](https://www.pinecone.io/learn/sentence-embeddings/) are created in around 5 seconds and compared with cosine similarity in around 0.01 seconds.  #### Universal Sentence Encoder (USE)  One technique that proved superior performance is the Universal Sentence Encoder (USE) developed by Google.  The idea is to design [an embedding to solve multiple tasks](https://amitness.com/2020/06/universal-sentence-encoder/) and, based on the mistakes it makes on those, **update** the sentence embedding. Since the same embedding has to work on multiple generic tasks, it captures only the most informative features and discards noise. The intuition is that this will result in a generic embedding that transfers universally to a wide variety of NLP tasks such as relatedness, clustering, paraphrase detection and text classification.  ![Universal Sentence Encoder representation](/images/embeddings-to-identify-fake-news-7.png) <small>A representation of the Universal Sentence Encoder (USE). Source: [Universal Sentence Encoder Visually Explained](https://amitness.com/2020/06/universal-sentence-encoder/)</small>  USE comes with two variations: one trained with Transformers and the other trained with Deep Averaging Network (DAN). While the one [using Transformers has higher accuracy](https://www.analyticsinsight.net/googles-universal-sentence-encoder-would-revolutionize-the-application-of-neural-network/), it is computationally more intensive. Alternatively, the DAN variant aims at [efficient inference](https://analyticsindiamag.com/guide-to-universal-sentence-encoder-with-tensorflow/) despite a [slightly reduced accuracy](https://amitness.com/2020/06/universal-sentence-encoder/).  ### A classification problem  From a Machine Learning perspective, the challenge of identifying fake news can be translated into a **classification problem**: Given a text input, we want to classify it correctly as fake or not fake. Using embeddings in a classification task, we can classify the label of an unknown object (e.g. an unseen article) based on the labels of the most similar known objects (e.g. articles already labelled as “fake” or “not fake”).  Embedding models like [Word2vec (for words) and Doc2vec (for sentences) can serve as feature extraction methods for the creation of various classifications tasks](https://www.researchgate.net/publication/351285564_Fake_news_detection_based_on_word_and_document_embedding_using_machine_learning_classifiers). After creating embeddings for our texts, we can train our model on these embeddings so it can learn to differentiate between misleading facts and real news. The classifier model is then evaluated and adjusted based on the results.  ![Data processing and classification](/images/embeddings-to-identify-fake-news-8.png) <small>Starting with pre-processing the dataset by removing unnecessary characters and words from the data, a matrix of features is formed representing the documents involved. The next step in the process is to train the classifier, so it can distinguish between fake and real news. Source: [ResearchGate](https://www.researchgate.net/publication/322128415_Detecting_opinion_spams_and_fake_news_using_text_classification)</small>  The process of converting text into embeddings results in fixed-length vector representations that attempt to encode each sentence key information. Next, we can compare the similarity (e.g. [cosine similarity](https://www.pinecone.io/learn/what-is-similarity-search/)) between sentence embeddings in our dataset to identify potential references to fake news.  ### The future of misinformation  Today, machines are playing a key role in the fake news arena. Text-generation systems like [GPT-3](https://singularityhub.com/2020/06/18/openais-new-text-generator-writes-even-more-like-a-human/) have the ability to produce coherent text from minimal prompts: Feed it a title, and it will write a story. But can these systems also spot other model-generated outputs?  Machine language models like GPT-3 produce sentences by predicting the next word in a sequence of text. So, if they can predict most of the words in a given passage, it’s likely it was written by one of their own. [This idea](https://www.technologyreview.com/2019/03/12/136668/an-ai-for-generating-fake-news-could-also-help-detect-it/) was tested using the [Giant Language model Test Room](http://gltr.io/dist/index.html) (GLTR), where human-generated text showed a higher fraction of red and purple words (meaning a decreasing ease of predictability) in contrast to machine-generated text that showed a greater share of green and yellow words (meaning it was likely written by a language model).  ![Passage written by a human](/images/embeddings-to-identify-fake-news-9.png) <small>A reading comprehension passage from a US standardized test, written by a human. Source: [MIT Technology Review](https://www.technologyreview.com/2019/03/12/136668/an-ai-for-generating-fake-news-could-also-help-detect-it/)</small>  ![Passage written by GPT-2](/images/embeddings-to-identify-fake-news-10.png) <small>A passage written by OpenAI’s downgraded GPT-2. Source: [MIT Technology Review](https://www.technologyreview.com/2019/03/12/136668/an-ai-for-generating-fake-news-could-also-help-detect-it/)</small>   But here’s the catch: a machine language model might be good at detecting its own output, but [not necessarily the output of others](https://www.aiweirdness.com/it-takes-a-bot-to-know-one-19-03-08/).  With machines able to mass produce content, the future of fake news is a challenging one. But technology can also be used to solve this problem. Improving the way machines represent and process content can lead the way to new solutions. Specifically, **embeddings** can serve as an effective method to represent massive amounts of texts (or other types of content) in order to improve our understanding about fake news.  ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2e0"
  },
  "title": "\"Retrievers for Question-Answering\"",
  "headline": "\"Retriever Models for Open Domain Question-Answering\"",
  "weight": "8",
  "name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "How to fine-tune retriever models to find relevant contexts in vector databases.",
  "images": "['/images/train-retriever-0.jpg']",
  "content": "It's a sci-fi staple. A vital component of the legendary Turing test. The dream of many across the world. And, until recently, impossible.  We are talking about the ability to ask a machine a question and receive a genuinely intelligent, insightful answer.  Until recently, technology like this existed only in books, Hollywood, and our collective imagination. Now, it is everywhere. Most of us use this technology every day, and we often don't even notice it.  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/w1dMEWm7jBc\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  Google is just one example. Over the last few years, Google has gradually introduced an intelligent question-answering angle to search. When we now ask *how do I tie my shoelaces?\"* Google gives us the 'exact answer' alongside the *context* or video this answer came from:  ![image-20220104094649638](/images/train-retriever-1.png) <small>In response to our question, Google finds the exact (audio-to-text) answer to be *\"Start by taking the first lace. And place it behind the second one...\"*, and highlights the exact part of the video that contains this extracted answer.</small>  We can ask other questions like *\"Is Google Skynet?\"* and this time return an even more precise answer **\"Yes\"**.  ![image-20220104095121529](/images/train-retriever-2.png) <small>At least Google is honest.</small>  In this example, Google returns an exact answer and the *context* (paragraph) from where the answer is extracted.  How does Google do this? And more importantly, why should we care?  This search style emulates a human-like interaction. We're asking a question in natural language as if we were speaking to another person. This natural language Q&A creates a very different search experience to traditional search.  Imagine you find yourself in the world's biggest warehouse. You have no idea how the place is organized. All you know is that your task is to find some round marble-like objects.  Where do you start? Well, we need to figure out how the warehouse is organized. Maybe everything is stored alphabetically, categorized by industry, or intended use. The traditional search interface requires that we understand how the warehouse is structured before we begin searching. Often, there is a specific 'query language' such as:  ```sql SELECT * WHERE 'material' == 'marble'  or  (\"marble\" | \"stone\") & \"product\" ```  Our first task is to learn this query language so we can search. Once we understand how the warehouse is structured, we use that knowledge to begin our search. How do we find *\"round marble-like objects\"*? We can narrow our search down using similar *queries* to those above, but we are in the world's biggest warehouse, so this will take a *very* long time.  Without a natural Q&A-style interface, this is your search. Unless your users know the ins and outs of the warehouse and its contents, they're going to struggle.  What happens if we add a natural Q&A-style interface to the warehouse? Imagine we now have people in the warehouse whose entire purpose is to guide us through the warehouse. These people know exactly where everything is.  Those people can understand our question of *\"where can I find the round marble-like objects?\"*. It may take a few tries until we find the exact object we're looking for, but we now have a guide that understands our question. There is no longer the need to understand how the warehouse is organized nor to *know* the exact name of what it is we're trying to find.  With this natural Q&A-style interface, your users now have a guide. They just need to be able to ask a question.   ## Answering Questions  How can we design these natural, human-like Q&A interfaces? The answer is **o**pen-**d**omain **q**uestion-**a**nswering (ODQA). ODQA allows us to use natural language to query a database.  That means that, given a dataset like a set of internal company documents, online documentation, or as is the case with Google, everything on the world's internet, we can retrieve relevant information in a natural, more human way.  However, ODQA is not a single model. It is more complex and requires three primary components.  * A **[vector database](/learn/vector-database/)** to store information-rich vectors that numerically represent the *meaning* of *contexts* (paragraphs that we use to extract answers to our questions). * The **retriever** model encodes questions and contexts into the same vector space. It is these context vectors that we later store in the vector database. The retriever also encodes questions to be compared to the context vectors in a *vector database* to *retrieve* the most relevant contexts. * A **reader** model takes a question and context and attempts to identify a *span* (sub-section) from the context which answers the question.  Building a retriever model is our focus here. Without it, there is no ODQA; it is arguably the most critical component in the whole process. We *need* our retriever model to return relevant results; otherwise, the reader model will receive and output garbage.  If we instead had a mediocre reader model, it may still return garbage to us, but it has a much smaller negative impact on the ODQA pipeline. A good retriever means we can at least retrieve relevant contexts, therefore successfully returning relevant information to the user. A paragraph-long context isn't as clean-cut as a perfectly framed two or three-word answer, but it's better than nothing.  Our focus in this article is on building a *retriever* model, of which the *vector database* is a crucial component, as we will see later.  ### Train or Not?  Do we need to fine-tune our retriever models? Or can we use pretrained models like those in the [HuggingFace model hub](https://huggingface.co/models)?  The answer is: *It depends*. An excellent concept from Nils Reimers describes the difficulty of benchmarking models where the use case is within a niché domain that very few people would understand. The idea is that most benchmarks and datasets focus on this short head of knowledge (where most people understand), whereas the most exciting use cases belong in the long-tail portion of the graph [1].  ![long_tailed_semantic_relatedness_nils](/images/train-retriever-3.jpg)  <small>Nils Reimer's *long tail of semantic relatedness* [1]. The more people that know about something (y-axis), the easier it is to find benchmarks and labeled data (x-axis), but the most interesting use cases belong in the long-tail region.</small>  We can take the same idea and modify the x-axis to indicate whether we should be able to take a pretrained model or fine-tune our own.  ![long_tailed_semantic_relatedness](/images/train-retriever-4.jpg)  <small>The more something is common knowledge (y-axis), the easier it is to find pretrained models that excel in the broader, more general scope. However, as before, most interesting use cases belong in the long-tail, and here is where we would need to fine-tune our own model.</small>  Imagine you are walking down your local high street. You pick a stranger at random and ask them the sort of question that you would expect from your use case. Do you think they would get the answer? If there's a good chance they will, you might be able to get away with a pretrained model.  On the other hand, if you ask this stranger what the difference is between RoBERTa and DeBERTa, there is a very high chance that they will have no idea what you're asking. In this case, you will probably need to fine-tune a retriever model.    ## Fine-Tuning a Retriever  Let's assume the strangers on the street have no chance of answering our questions. Most likely, a custom retriever model is our best bet. But, how do we train/fine-tune a custom retriever model?  The very first ingredient is *data*. Our retriever consumes a question and returns relevant contexts to us. For it to do this, it must learn to encode similar question-context pairs into the same vector space.  ![encode_question_context](/images/train-retriever-5.jpg)<small>The retriever model must learn to encode similar question-context pairs into a similar vector space.</small>  Our first task is to find and create a set of question-context pairs. One of the best-known datasets for this is the **S**tanford **Q**uestion **A**nswering **D**ataset (SQuAD).  ### Step One: Data  SQuAD is a reading comprehension dataset built from question, context, and answers with information from Wikipedia articles. Let's take a look at an example.  {{< notebook file=\"download-squad-1\" height=\"full\" >}}  We first download the `squad_v2` dataset via 🤗 *Datasets*. In the first sample, we can see:  * the `title` (or topic) of *Beyoncé* * the `context`, a short paragraph from Wikipedia about Beyoncé * a `question`, *\"When did Beyonce start becoming popular?\"* * the answer `text`, *\"in the late 1990s\"*, which is extracted from the *context* * the `answer_start`, which is the starting position of the answer within the *context* string.  The SQuAD v2 dataset contains *130,319* of these samples, more than enough for us to train a good retriever model.  We will be using the *Sentence Transformers* library to train our retriever model. When using this library, we must format our training data into a list of `InputExample` objects.  {{< notebook file=\"input-examples\" height=\"full\" >}}  After creating this list of `InputExample` objects, we need to load them into a data loader. A data loader is commonly used with PyTorch, which Sentence Transformers uses under the hood. Because of this, we can often use the PyTorch `DataLoader` class.  However, we need to do something slightly different. Our training data consists of positive question-context pairs; positive meaning that every sample in our dataset can be viewed as having a positive or *high* similarity. There are no negative or dissimilar pairs.  When our data looks like this, one of the most effective training techniques we can use uses the **M**ultiple **N**egatives **R**anking (MNR) loss function. We will not explain MNR loss in this article, but you [can learn about it here](/learn/fine-tune-sentence-transformers-mnr/).  One crucial property of training with MNR loss is that each training batch does *not* contain duplicate questions or contexts. This is a problem, as the SQuAD data includes several questions for each context. Because of this, if we used the standard `DataLoader`, there is a high probability that we would find duplicate contexts in our batches.  ![image-20220104185439001](/images/train-retriever-6.png) <small>Screenshot from [HuggingFace's dataset viewer](https://huggingface.co/datasets/squad_v2/viewer/squad_v2/train) for the *squad_v2* dataset. Each row represents a different question, but they all map to the same *context*.</small>  Fortunately, there is an easy solution to this. *Sentence Transformers* provides a set of modified data loaders. One of those is the `NoDuplicatesDataLoader`, which ensures our batches contain *no* duplicates.  {{< notebook file=\"no-dupes-loader\" height=\"full\" >}}  With that, our training data is fully prepared, and we can move on to initializing and training our retriever model.  ### Step Two: Initialize and Train  Before training our model, we need to initialize it. For this, we begin with a pretrained transformer model from the [HuggingFace model hub](https://huggingface.co/models). A popular choice for sentence transformers is Microsoft's MPNet model, which we access via `microsoft/mpnet-base`.  There is one problem with our pretrained transformer model. It outputs many word/token-level vector embeddings. We don't want token vectors; we need *sentence vectors*.  We need a way to transform the many token vectors output by the model into a *single* sentence vector.  ![token_to_sentence_vecs](/images/train-retriever-7.jpg)<small>Transformation of the many token vectors output by a transformer model into a single sentence vector.</small>  To perform this transformation, we add a *mean pooling layer* to process the outputs of the transformer model. There are a few different pooling techniques. The one that we will use is *mean pooling*. This approach will take the many token vectors output by the model and average the activations across each vector dimension to create a single sentence vector.  We can do this via `models` and `SentenceTransformer` utilities of the *Sentence Transformers* library.  {{< notebook file=\"init-model-1\" height=\"full\" >}}  We have a `SentenceTransformer` object; a pretrained `microsoft/mpnet-base` model followed by a mean pooling layer.  With our model defined, we can initialize our MNR loss function.  {{< notebook file=\"mnr-loss\" height=\"full\" >}}  That is everything we need for fine-tuning the model. We set the number of training epochs to `1`; anything more for sentence transformers often leads to overfitting. Another method to reduce the likelihood of overfitting is adding a learning rate warmup. Here, we warmup for the first 10% of our training steps (10% is the *go to* % for warmup steps; if you find the model is overfitting, try increasing the number).  {{< notebook file=\"fit-model\" height=\"full\" >}}  We now have an ODQA retriever model saved to the local `./mpnet-mnr-squad2` directory. That's great, but we have no idea how well the model performs, so our next step is to evaluate model performance.  ## Retriever Evaluation  Evaluation of retriever models is slightly different from the evaluation of most language models. Typically, we input some text and calculate the error between clearly defined predicted and true values.  For information retrieval (IR), we need a metric that measures the rate of successful vs. unsuccessful retrievals. A popular metric for this is [mAP@K](https://sdsawtelle.github.io/blog/output/mean-average-precision-MAP-for-recommender-systems.html). In short, this is an averaged precision value (fraction of retrieved contexts that are relevant) that considers the top *K* of retrieved results.  The setup for IR evaluation is a little more involved than with other evaluators in the *Sentence Transformers* library. We will be using the `InformationRetrievalEvaluator`, and this requires three inputs:  * `ir_queries` is a dictionary mapping question IDs to question text * `ir_corpus` maps context IDs to context text * `ir_relevant_docs` maps question IDs to their relevant context IDs  Before we initialize the evaluator, we need to download a new set of samples that our model has not seen before and format them into the three dictionaries above. We will use the SQuAD *validation* set.  {{< notebook file=\"squad-dev\" height=\"full\" >}}  To create the dictionary objects required by the `InformationRetrievalEvaluator`, we must assign unique IDs to both contexts and questions. And we need to ensure that duplicate contexts are *not* assigned different IDs. To handle these, we will first convert our dataset object into a Pandas dataframe.  {{< notebook file=\"dev-to-df\" height=\"full\" >}}  From here, we can quickly drop duplicate contexts with the `drop_duplicates` method. As we no longer have duplicates, we can append `'con'` to each context ID, giving each *unique* context a unique ID different from any question IDs.  {{< notebook file=\"no-dupes\" height=\"full\" >}}  We now have unique question IDs in the `squad_df` dataframe and unique context IDs in the `no_dupe` dataframe. Next, we perform an inner join on the `context` feature to bring these two sets of IDs together and find our question ID to context ID mappings.  {{< notebook file=\"context-join\" height=\"full\" >}}  We're now ready to build the three mapping dictionaries for the `InformationRetrievalEvaluator`. First, we map question/context IDs to questions/contexts.  {{< notebook file=\"ir-queries-corpus\" height=\"full\" >}}  And then map question IDs to a *set* of relevant context IDs. For the SQuAD data, we only have *many-to-one* or *one-to-one* question ID to context ID mappings, but we will write our code to *additionally* handle *one-to-many* mappings (so we can handle other, non-SQuAD datasets).  {{< notebook file=\"ir-relevant-docs\" height=\"full\" >}}  Our evaluator inputs are ready, so we initialize the evaluator and then evaluate our `model`.  {{< notebook file=\"ir-eval-our-model\" height=\"full\" >}}  We return a mAP@K score of 0.74, where @K is *100* by default. This performance is comparable to other state-of-the-art retriever models. Performing the same evaluation with the `multi-qa-mpnet-base-cos-v1` returns a mAP@K score of 0.76, just two percentage points greater than our custom model.  {{< notebook file=\"ir-eval-other-model\" height=\"full\" >}}  Of course, if your target domain was SQuAD data, the pretrained `multi-qa-mpnet-base-cos-v1` model would be the better model. But if you have your own unique dataset and domain. A custom model fine-tuned on that domain will *very likely* outperform existing models like `multi-qa-mpnet-base-cos-v1` *in that domain*.  ## Storing the Vectors  We have our retriever model, we've evaluated it, and we're happy with its performance. But we don't know how to use it.  When you perform a Google search, Google does *not* look at the whole internet, encode all of that information into vector embeddings, and then compare all of those vectors to your query vector. We would be waiting a *very* long time to return results if that were the case.  Instead, Google has already searched for, collected, and encoded all of that data. Google then stores those encoded vectors in some sort of vector database. When you query now, the only thing Google needs to encode is your question.  Taking this a step further, comparing your query vector to *all* vectors indexed by Google (which represent the entire Google-accessible internet) would still take an incredibly long time. We refer to this accurate but inefficient comparison of every single vector as an *exhaustive search*.  For big datasets, an exhaustive search is too slow. The solution to this is to perform an *approximate search*. An approximate search allows us to massively reduce our search scope to a smaller but (hopefully) more relevant sub-section of the index. Making our search times much more manageable.  The [Pinecone vector database](/) is a straightforward and robust solution that allows us to (1) store our context vectors and (2) perform an *accurate and fast* approximate search. These are the two elements we need for a promising ODQA pipeline.  Again, we need to work through a few steps to set up our vector database.  ![vector_db_setup](/images/train-retriever-8.jpg)<small>Steps from retriever and context preparation (top-right) that allow us to *encode contexts* into *context vectors*. After initializing a vector database index, we can populate the index with the *context vectors*.</small>  After working through each of those steps, we will be ready to begin retrieving relevant contexts.  ### Encoding Contexts  We have already created our retriever model, and during the earlier evaluation step, we downloaded the SQuAD validation data. We can use this same validation data and encode all *unique* contexts.  {{< notebook file=\"encode-contexts\" height=\"full\" >}}  After removing duplicate contexts, we're left with 1,204 samples. It is a tiny dataset but large enough for our example.  ### Initializing the Index  Before adding the context vectors to our index, we need to initialize it. Fortunately, Pinecone makes this very easy. We start by installing the Pinecone client if required:  ``` !pip install pinecone-client ```  Then we initialize a connection to Pinecone. For this, we need a [free API key](https://app.pinecone.io/).  {{< notebook file=\"init-pinecone\" height=\"full\" >}}  We then create a new index with `pinecone.create_index`. Before initializing the index, we should check that the index name does not already exist (which it will not if this is your first time creating the index).  {{< notebook file=\"create-index\" height=\"full\" >}}  When creating a new index, we need to specify the index `name`, and the dimensionality of vectors to be added. We either check our encoded context vectors’ dimensions directly or find the dimension attribute within the retriever model (as shown above).  ### Populating the Index  After creating both our index and the context vectors, we can go ahead and *upsert* (upload) the vectors into our index.  {{< notebook file=\"upsert-vectors\" height=\"full\" >}}  Pinecone expects us to [upsert data](https://www.pinecone.io/docs/insert-data/) in the format:  ```python vectors = [ (id_0, vector_0, metadata_0), (id_1, vector_1, metadata_1) ] ```  Our IDs are the unique alphanumeric identifiers that we saw earlier in the SQuAD data. The vectors are our encoded context vectors formatted as lists; the metadata is a dictionary that allows us to store extra information in a key-value format.  ---  *Using the metadata field, Pinecone allows us to [create complex or straightforward metadata filters](https://www.pinecone.io/learn/vector-search-filtering/) to target our search scope to specific numeric ranges, categories, and more.*  ---  Once the upsert is complete, the retrieval components of our ODQA pipeline are ready to go, and we can begin asking questions.  ## Making Queries  With everything set up, querying our retriever-vector database pipeline is pretty straightforward. We first define a question and encode it as we did for our context vectors before.  {{< notebook file=\"encode-query\" height=\"full\" >}}  After creating our query vector, we pass it to Pinecone via the `index.query` method, specify how many results we'd like to return with `top_k`, and `include_metadata` so that we can see the text associated with each returned vector.  {{< notebook file=\"query1\" height=\"full\" >}}  We return the correct context as our second top result in this example. The first result is relevant in the context of Normans and Normandy, but it does not answer the specific question of *when* the Normans were in Normandy.  Let's try a couple more questions.  {{< notebook file=\"query2\" height=\"full\" >}}  For this question, we return the correct context as the highest result with a much higher score than the remaining samples.  {{< notebook file=\"query3\" height=\"full\" >}}  We return the correct context in the first position. Again, there is a good separation between sample scores of the correct context and other contexts.    That's it for this guide to fine-tuning and implementing a custom retriever model in an ODQA pipeline. Now we can implement two of the most crucial components in ODQA: enabling a more human and natural approach to information retrieval.  One of the most incredible things about ODQA is how widely applicable it is. Organizations across almost every industry have the opportunity to benefit from more intelligent and efficient information retrieval.  Any organization that handles unstructured information such as word documents, PDFs, emails, and more has a clear use case: freeing this information and enabling easy and natural access through QA systems.  Although this is the most apparent use case, there are many more, whether it be an internal efficiency speedup or a key component in a product (as with Google search). The opportunities are both broad and highly impactful.   ## References  [1] N. Reimers, [Neural Search for Low Resource Scenarios](https://www.youtube.com/watch?v=XNJThigyvos) (2021), YouTube  S. Sawtelle, [Mean Average Precision (MAP) For Recommender Systems](https://sdsawtelle.github.io/blog/output/mean-average-precision-MAP-for-recommender-systems.html) (2016), GitHub ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2e2"
  },
  "title": "\"Sentence Transformers and Embeddings\"",
  "content": "  - NLP for Semantic Search toc: >- weight: 2 author:   name: James Briggs   position: Developer Advocate   src: /images/james-briggs.jpeg   href: \"https://www.youtube.com/c/jamesbriggs\" description: How sentence transformers and embeddings can be used for a range of semantic similarity applications. # Open Graph images: ['/images/sentence-embeddings-1.jpg'] ---  **Once you learn about and generate sentence embeddings, combine them with the [Pinecone vector database](/) to easily build applications like semantic search, deduplication, and multi-modal search. [Try it now for free.](https://app.pinecone.io)**  Transformers have wholly rebuilt the landscape of natural language processing (NLP). Before transformers, we had *okay* translation and language classification thanks to recurrent neural nets (RNNs) — their language comprehension was limited and led to many minor mistakes, and coherence over larger chunks of text was practically impossible.  Since the introduction of the first transformer model in the 2017 paper *'Attention is all you need'* [1], NLP has moved from RNNs to models like BERT and GPT. These new models can answer questions, write articles *(maybe GPT-3 wrote this)*, enable incredibly intuitive semantic search — and much more.  The funny thing is, for many tasks, the latter parts of these models are the same as those in RNNs — often a couple of feedforward NNs that output model predictions.  It's the *input* to these layers that changed. The [dense embeddings](https://www.pinecone.io/learn/dense-vector-embeddings-nlp/) created by transformer models are so much richer in information that we get massive performance benefits despite using the same final outward layers.  These increasingly rich sentence embeddings can be used to quickly compare sentence similarity for various use cases. Such as:  * **Semantic textual similarity (STS)** — comparison of sentence pairs. We may want to identify patterns in datasets, but this is most often used for benchmarking. * **Semantic search** — information retrieval (IR) using semantic meaning. Given a set of sentences, we can search using a *'query'* sentence and identify the most similar records. Enables search to be performed on concepts (rather than specific words). * **Clustering** — we can cluster our sentences, useful for topic modeling.  In this article, we will explore how these embeddings have been adapted and applied to a range of semantic similarity applications by using a new breed of transformers called *'sentence transformers'*.  <p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/WS1uVMGhlWQ\" title=\"Sentence Embeddings\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>  ## Some “Context”  Before we dive into sentence transformers, it might help to piece together why transformer embeddings are so much richer — and where the difference lies between a vanilla *transformer* and a *sentence transformer*.  Transformers are indirect descendants of the previous RNN models. These old recurrent models were typically built from many recurrent *units* like [LSTMs or GRUs](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21).  In *machine translation*, we would find [encoder-decoder networks](https://machinelearningmastery.com/encoder-decoder-recurrent-neural-network-models-neural-machine-translation/). The first model for *encoding* the original language to a *context vector*, and a second model for *decoding* this into the target language.  ![Encoder decoder bottleneck](/images/sentence-embeddings-2.jpg) <small>Encoder-decoder architecture with the single context vector shared between the two models, this acts as an information bottleneck as *all* information must be passed through this point.</small>  The problem here is that we create an *information bottleneck* between the two models. We're creating a massive amount of information over multiple time steps and trying to squeeze it all through a single connection. This limits the encoder-decoder performance because much of the information produced by the encoder is lost before reaching the decoder.  The *attention mechanism* provided a solution to the bottleneck issue. It offered another route for information to pass through. Still, it didn't overwhelm the process because it focused *attention* only on the most relevant information.  By passing a *context vector* from each timestep into the attention mechanism (producing *annotation* vectors), the information bottleneck is removed, and there is better information retention across longer sequences.  ![Encoder decoder attention](/images/sentence-embeddings-3.jpg) <small>Encoder-decoder with the attention mechanism. The attention mechanism considered all encoder output activations and each timestep's activation in the decoder, which modifies the decoder outputs.</small>  During decoding, the model decodes one word/timestep at a time. An alignment (e.g., similarity) between the word and all encoder annotations is calculated for each step.  Higher alignment resulted in greater weighting to the encoder annotation on the output of the decoder step. Meaning the mechanism calculated which encoder words to pay *attention* to.  ![Attention example](/images/sentence-embeddings-4.png) <small>Attention between an English-French encoder and decoder, source [2].</small>  The best-performing RNN encoder-decoders all used this attention mechanism.  ### Attention is All You Need  In 2017, a paper titled *Attention Is All You Need* was published. This marked a turning point in NLP. The authors demonstrated that we could remove the RNN networks and get superior performance using *just* the attention mechanism — with a few changes.  This new attention-based model was named a *'transformer'*. Since then, the NLP ecosystem has entirely shifted from RNNs to transformers thanks to their vastly superior performance and incredible capability for generalization.  The first transformer removed the need for RNNs through the use of *three* key components:  * Positional Encoding * Self-attention * Multi-head attention  **Positional encoding** replaced the key advantage of RNNs in NLP — the ability to consider the order of a sequence (they were *recurrent*). It worked by adding a set of varying sine wave activations to each input embedding based on position.  **Self-attention** is where the attention mechanism is applied between a word and all of the other words in its own context (sentence/paragraph). This is different from vanilla attention which specifically focused on attention between encoders and decoders.  **Multi-head attention** can be seen as several *parallel* attention mechanisms working together. Using several attention *heads* allowed the representation of several sets of relationships (rather than a single set).  ### Pretrained Models  The new transformer models generalized much better than previous RNNs, which were often built specifically for each use-case.  With transformer models, it is possible to use the same *'core'* of a model and simply swap the last few layers for different use cases (without retraining the *core*).  This new property resulted in the rise of *pretrained* models for NLP. Pretrained transformer models are trained on vast amounts of training data — often at high costs by the likes of Google or OpenAI, then released for the public to use for free.  One of the most widely used of these pretrained models is BERT, or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers by Google AI.  BERT spawned a whole host of further models and derivations such as distilBERT, RoBERTa, and ALBERT, covering tasks such as classification, Q&A, POS-tagging, and more.  ### BERT for Sentence Similarity  So far, so good, but these transformer models had one issue when building sentence vectors: Transformers work using word or *token*-level embeddings, *not* sentence-level embeddings.  Before sentence transformers, the approach to calculating *accurate* sentence similarity with BERT was to use a cross-encoder structure. This meant that we would pass two sentences to BERT, add a classification head to the top of BERT — and use this to output a similarity score.  ![Cross encoder](/images/sentence-embeddings-5.jpg) <small>The BERT cross-encoder architecture consists of a BERT model which consumes sentences A and B. Both are processed in the same sequence, separated by a `[SEP]` token. All of this is followed by a feedforward NN classifier that outputs a similarity score.</small>  The cross-encoder network does produce very accurate similarity scores (better than SBERT), but it's *not scalable*. If we wanted to perform a similarity search through a small 100K sentence dataset, we would need to complete the cross-encoder inference computation 100K times.  To cluster sentences, we would need to compare all sentences in our 100K dataset, resulting in just under 500M comparisons — this is simply not realistic.  Ideally, we need to pre-compute sentence vectors that can be stored and then used whenever required. If these vector representations are good, all we need to do is calculate the cosine similarity between each.  With the original BERT (and other transformers), we can build a sentence embedding by averaging the values across all token embeddings output by BERT (if we input 512 tokens, we output 512 embeddings). Alternatively, we can use the output of the first `[CLS]` token (a BERT-specific token whose output embedding is used in classification tasks).  Using one of these two approaches gives us our sentence embeddings that can be stored and compared much faster, shifting search times from 65 hours to around 5 seconds (see below). However, the accuracy is not good, and is worse than using averaged GloVe embeddings (which were developed in 2014).  **The solution** to this lack of an accurate model *with* reasonable latency was designed by Nils Reimers and Iryna Gurevych in 2019 with the introduction of sentence-BERT (SBERT) and the `sentence-transformers` library.  SBERT outperformed the previous state-of-the-art (SOTA) models for all common semantic textual similarity (STS) tasks — more on these later — except a single dataset (SICK-R).  Thankfully for scalability, SBERT produces sentence embeddings — so we do *not* need to perform a whole inference computation for every sentence-pair comparison.  Reimers and Gurevych demonstrated the dramatic speed increase in 2019. Finding the most similar sentence pair from 10K sentences took 65 hours with BERT. With SBERT, embeddings are created in ~5 seconds and compared with cosine similarity in ~0.01 seconds.  Since the SBERT paper, many more sentence transformer models have been built using similar concepts that went into training the original SBERT. They’re all trained on many similar and dissimilar sentence pairs.  Using a loss function such as softmax loss, multiple negatives ranking loss, or MSE margin loss, these models are optimized to produce similar embeddings for similar sentences, and dissimilar embeddings otherwise.  Now you have some context behind sentence transformers, where they come from, and why they're needed. Let's dive into how they work.  *[3] The SBERT paper covers many of the statements, techniques, and numbers from this section.*  ## Sentence Transformers  We explained the cross-encoder architecture for sentence similarity with BERT. SBERT is similar but drops the final classification head, and processes one sentence at a time. SBERT then uses mean pooling on the final output layer to produce a sentence embedding.  Unlike BERT, SBERT is fine-tuned on sentence pairs using a *siamese* architecture. We can think of this as having two identical BERTs in parallel that share the exact same network weights.  ![Start sbert](/images/sentence-embeddings-6.jpg) <small>An SBERT model applied to a sentence pair *sentence A* and *sentence B*. Note that the BERT model outputs token embeddings (consisting of 512 768-dimensional vectors). We then compress that data into a single 768-dimensional sentence vector using a pooling function.</small>  In reality, we are using a single BERT model. However, because we process sentence A followed by sentence B as *pairs* during training, it is easier to think of this as two models with tied weights.  ### Siamese BERT Pre-Training  There are different approaches to training sentence transformers. We will describe the original process featured most prominently in the original SBERT that optimizes on *softmax-loss*. Note that this is a high-level explanation, we will save the in-depth walkthrough for another article.  The softmax-loss approach used the *'siamese'* architecture fine-tuned on the Stanford Natural Language Inference (SNLI) and Multi-Genre NLI (MNLI) corpora.  SNLI contains 570K sentence pairs, and MNLI contains 430K. The pairs in both corpora include a `premise` and a `hypothesis`. Each pair is assigned one of three labels:  * **0** — *entailment*, e.g. the `premise` suggests the `hypothesis`. * **1** — *neutral*, the `premise` and `hypothesis` could both be true, but they are not necessarily related. * **2** — *contradiction*, the `premise` and `hypothesis` contradict each other.  Given this data, we feed sentence A (let's say the `premise`) into siamese BERT A and sentence B (`hypothesis`) into siamese BERT B.  The siamese BERT outputs our pooled sentence embeddings. There were the results of *three* different pooling methods in the SBERT paper. Those are *mean*, *max*, and *[CLS]*-pooling. The *mean*-pooling approach was best performing for both NLI and STSb datasets.  There are now two sentence embeddings. We will call embeddings A `u` and embeddings B `v`. The next step is to concatenate `u` and `v`. Again, several concatenation approaches were tested, but the highest performing was a `(u, v, |u-v|)` operation:  ![UV Vectors](/images/sentence-embeddings-7.jpg) <small>We concatenate the embeddings **u**, **v**, and **|u - v|**.</small>  `|u-v|` is calculated to give us the element-wise difference between the two vectors. Alongside the original two embeddings (`u` and `v`), these are all fed into a feedforward neural net (FFNN) that has *three* outputs.  These three outputs align to our NLI similarity labels **0**, **1**, and **2**. We need to calculate the softmax from our FFNN, which is done within the [cross-entropy loss function](/learn/cross-entropy-loss/). The softmax and labels are used to optimize on this *'softmax-loss'*.  ![SBERT Training](/images/sentence-embeddings-8.jpg) <small>The operations were performed during training on two sentence embeddings, `u` and `v`. Note that *softmax-loss* refers cross-entropy loss (which contains a softmax function by default).</small>  This results in our pooled sentence embeddings for similar sentences (label **0**) becoming *more similar*, and embeddings for dissimilar sentences (label **2**) becoming *less similar*.  Remember we are using *siamese* BERTs **not** *dual* BERTs. Meaning we don't use two independent BERT models but a single BERT that processes sentence A followed by sentence B.  This means that when we optimize the model weights, they are pushed in a direction that allows the model to output more similar vectors where we see an *entailment* label and more dissimilar vectors where we see a *contradiction* label.  ---  *We are working on a step-by-step guide to training a siamese BERT model with the SNLI and MNLI corpora described above using both the softmax-loss and multiple-negatives-ranking-loss approaches. You can get an email as soon as we release the article by [clicking here](https://www.pinecone.io/learn/) (the form is at the bottom of the page).*  ---  The fact that this training approach works is not particularly intuitive and indeed has been described by Reimers as *coincidentally* producing good sentence embeddings [5].  Since the original paper, further work has been done in this area. Many more models such as the [latest MPNet and RoBERTa models trained on 1B+ samples](https://huggingface.co/spaces/flax-sentence-embeddings/sentence-embeddings) (producing much better performance) have been built. We will be exploring some of these in future articles, and the superior training approaches they use.  For now, let's look at how we can initialize and use some of these sentence-transformer models.  ### Getting Started with Sentence Transformers  The fastest and easiest way to begin working with sentence transformers is through the `sentence-transformers` library created by the creators of SBERT. We can install it with `pip`.  ```bash !pip install sentence-transformers ```  We will start with the original SBERT model `bert-base-nli-mean-tokens`. First, we download and initialize the model.  {{< notebook file=\"sbert-init\" height=\"full\" >}}  The output we can see here is the `SentenceTransformer` object which contains *three* components:  * The **transformer** itself, here we can see the max sequence length of `128` tokens and whether to lowercase any input (in this case, the model does *not*). We can also see the model class, `BertModel`.  * The **pooling** operation, here we can see that we are producing a `768`-dimensional sentence embedding. We are doing this using the *mean pooling* method.  Once we have the model, building sentence embeddings is quickly done using the `encode` method.  {{< notebook file=\"encoding-sentences\" height=\"full\" >}}  We now have sentence embeddings that we can use to quickly compare sentence similarity for the use cases introduced at the start of the article; STS, semantic search, and clustering.  We can put together a fast STS example using nothing more than a cosine similarity function and Numpy.  {{< notebook file=\"cos-sim\" height=\"full\" >}}  ![SBERT heatmap](/images/sentence-embeddings-9.jpg) <small>Heatmap showing cosine similarity values between all sentence-pairs.</small>  Here we have calculated the cosine similarity between every combination of our five sentence embeddings. Which are:  | Index | Sentence | | ----- | ------------------------------------------------------------ | | 0 | the fifty mannequin heads floating in the pool kind of freaked them out | | 1 | she swore she just saw her sushi move | | 2 | he embraced his new life as an eggplant | | 3 | my dentist tells me that chewing bricks is very bad for your teeth | | 4 | the dental specialist recommended an immediate stop to flossing with construction materials |  We can see the highest similarity score in the bottom-right corner with `0.64`. As we would hope, this is for sentences `4` and `3`, which both describe poor dental practices using construction materials.  ## Other sentence-transformers  Although we returned good results from the SBERT model, many more sentence transformer models have since been built. Many of which we can find in the `sentence-transformers` library.  These newer models can significantly outperform the original SBERT. In fact, SBERT is no longer listed as an available model on the [SBERT.net models page](https://www.sbert.net/docs/pretrained_models.html).  | Model | Avg. Performance | Speed | Size (MB) | | ---------------------- | ---------------- | ----- | --------- | | `all-mpnet-base-v2` | 63.30 | 2800 | 418 | | `all-roberta-large-v1` | 53.05 | 800 | 1355 | | `all-MiniLM-L12-v1` | 59.80 | 7500 | 118 |  <small>A few of the top-performing models on the sentence transformers model page.</small>  We will cover some of these later models in more detail in future articles. For now, let's compare one of the highest performers and run through our STS task.  {{< notebook file=\"sentence-transformer-init\" height=\"full\" >}}  Here we have the `SentenceTransformer` model for `all-mpnet-base-v2`. The components are very similar to the `bert-base-nli-mean-tokens` model, with some small differences:  * `max_seq_length` has increased from `128` to `384`. Meaning we can process sequences that are *three* times longer than we could with SBERT. * The base model is now `MPNetModel` [4] not `BertModel`. * There is an additional normalization layer applied to sentence embeddings.  Let's compare the STS results of `all-mpnet-base-v2` against SBERT.  {{< notebook file=\"mpnet-sts\" height=\"full\" >}}  ![SBERT and MPNet heatmaps](/images/sentence-embeddings-10.jpg) <small>Heatmaps for both SBERT and the MPNet sentence transformer.</small>  The semantic representation of later models is apparent. Although SBERT correctly identifies `4` and `3` as the most similar pair, it also assigns reasonably high similarity to other sentence pairs.  On the other hand, the MPNet model makes a *very* clear distinction between similar and dissimilar pairs, with most pairs scoring less than 0.1 and the `4`-`3` pair scored at *0.52*.  By increasing the separation between dissimilar and similar pairs, we’re:  1. Making it easier to automatically identify relevant pairs. 2. Pushing predictions closer to the *0* and *1* target scores for *dissimilar* and *similar* pairs used during training. This is something we will see more of in our future articles on fine-tuning these models.  ---  That's it for this article introducing sentence embeddings and the current SOTA sentence transformer models for building these incredibly useful embeddings.  Sentence embeddings, although only recently popularized, were produced from a long range of fantastic innovations. We described some of the mechanics applied to create the first sentence transformer, SBERT.  We also demonstrated that despite SBERT's very recent introduction in 2019, other sentence transformers already outperform the model. Fortunately for us, it's easy to switch out SBERT for one of these newer models with the `sentence-transformers` library.  In future articles, we will dive deeper into some of these newer models and how to train our own sentence transformers.  {{< newsletter text=\"Subscribe for more semantic search material!\" inputText=\"Email address...\" buttonText=\"Submit\">}}  ## References  [1] A. Vashwani, et al., [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (2017), NeurIPS  [2] D. Bahdanau, et al., [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) (2015), ICLR  [3] N. Reimers, I. Gurevych, [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084) (2019), ACL  [4] [MPNet Model](https://huggingface.co/transformers/model_doc/mpnet.html), Hugging Face Docs  [5] N. Reimers, [Natural Language Inference](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/nli/README.md), sentence-transformers on GitHub",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2e4"
  },
  "title": "\"Meet Pinecone’s New VP of Engineering, Dr. Ram Sriharsha\"",
  "headline": "\"Meet Pinecone’s New VP of Engineering, Dr. Ram Sriharsha\"",
  "name": "Greg Kogan",
  "position": "VP Marketing",
  "src": "/images/company-greg.png",
  "href": "https://www.linkedin.com/in/gkogan/",
  "description": "Our new VP of Engineering, Dr. Ram Sriharsha, brings a wealth of knowledge to our team and is uniquely positioned to help execute on our lofty vision.",
  "images": "['/images/team-ram-sriharsha.jpg']",
  "date": "\"2021-11-22\"",
  "thumbnail": "\"/images/team-ram-sriharsha-resized.jpg\"",
  "# Date": "November 22, 2021",
  "content": "Pinecone is rapidly growing, which means we need people driving our teams that know how to not only lead at the helm but support us from the bottom up, so we can grow from a strong foundation. With experience in engineering, product management, and VP roles at the likes of Yahoo, Databricks, and Splunk, our new VP of Engineering, [Dr. Ram Sriharsha](https://www.linkedin.com/in/harsha340/), brings a wealth of knowledge to our team and is uniquely positioned to help execute on our lofty vision.   At Yahoo, he was both a principal software engineer and then research scientist; at Databricks, he was the product and engineering lead for the unified analytics platform for genomics; and, in his three years at Splunk, he played multiple roles including Sr Principal Scientist, VP Engineering and Distinguished Engineer.  With this wealth of experience, he could have done anything. So, why did he choose to join us? The decision was easy, Ram said.   “I tend to choose companies and technologies that are very cutting edge,” he said. “I try to place my bet on technologies that are going to be the way things are done. ... From my own personal experience, I knew that semantic search over [vector embeddings](/learn/vector-embeddings/) wasn’t a solved problem, and I recognized the significance of such a technology to unlocking new use cases around unstructured data. ... Pinecone is working on exactly this.”  Jumping onto the Databricks and Splunk teams after seeing their potential, Ram’s intuition is keen and impressive. But, again, we were curious as to why a seemingly small, scrappy company was so attractive to him.   You can make more moves working on cutting edge technologies at a company of this size, where everyone’s contributions are ground-shaking, than you could even as VP at a big company, he said. Pinecone is the place to be, in his eyes.   “I really want to build systems and interface with users,” he said. “At Pinecone, you’re continually iterating and learning. In a big company, Version 0 can take many years to get out, and by then you’re not connected to the product anymore. At Pinecone, that’s not a problem. For example, one of our scientists fixed something and got the fix into production the very same night. This rarely happens at a big company.”  In addition, the technology Pinecone is creating was a no brainer for Ram, someone whose finger is always on the pulse. He doesn’t just want to work at a company that is pursuing greatness; he wants to get his hands dirty too. He watched as CEO Edo Liberty was on the forefront of Machine Learning research and knew he had to be involved. He wasn’t making the old stuff better; he was building a different type of database in a new domain.   The team is innovating on science, as well, Ram said.   “We are focused on building the world’s best [vector database](/learn/vector-database/),” Ram said. “The use cases you can build on top of it are fascinating to me. We are writing our entire database in Rust, which is interesting — if we can shave off 20% memory usage, that's savings in cost for customers. It's very valuable for us to do that. What we are building is pushing the limits of databases themselves. How do we build algorithms that are accurate but allow us to scale? These are really amazing challenges in redesigning databases.”  After working on teams as the first employee, as well as stepping into high-level leadership roles right away, Ram knows what it takes to create a strong team and maintain that team for years to come. From the outset of his role, he wants to ensure he builds the best team of scientists possible and is looking for people who are just as excited as he is about transforming search using machine learning and vector databases.   When thinking about hiring, it’s not just about coding skill, either. Having an open culture and strong teamwork is vital, and that’s why he chose Pinecone, he said. Everyone has the opportunity to build and contribute, and he wants all engineers to feel like they’re getting their hands dirty and owning their own projects.   Even with a strong culture coming in, though, the team cannot remain complacent, especially as it begins to think about expansion and reaching out to different audiences for hiring. This includes working with a recruiting company focused on diversity, building relationships with organizations that support women in coding, like Grace Hopper, and constantly evolving to be welcoming and inclusive. The team has a long way to go with this, he said, but he’s excited to work on it.    Ram is set to take Pinecone into its next phase of greatness, as we work to build out and up. He hopes everyone is as excited about this journey as he is, and he invites anyone who resonates with it to come along.   “We are truly trying to build something fundamentally new here,” Ram said. “I really want that to resonate with people reading this. If you resonate with this, this is the place to be.”  [We are hiring. Come work with Ram!](https://www.pinecone.io/careers/) ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2e6"
  },
  "title": "\"Introduction to K-Means Clustering\"",
  "headline": "\"Introduction to K-Means Clustering\"",
  "weight": "4",
  "name": "Diego Lopez Yse",
  "position": "Data Scientist",
  "src": "/images/diego-lopez-yse.jpeg",
  "href": "\"https://lopezyse.medium.com/\"",
  "description": "An introduction to K-Means Clustering.",
  "images": "['/images/global-data-creation.png']",
  "content": "With massive data volumes growing at exponential rates, we need to find scalable methods to process them and find insights. The world of data entered the Zettabyte era several years ago. What’s a Zettabyte? Well, it is [enough storage](https://www.pcmag.com/news/seagate-is-the-first-company-to-ship-3-zettabytes-of-hard-drive-storage) for 30 billion 4K movies, or 60 billion video games, or 7.5 trillion MP3 songs.  Today, the total amount of data created, captured, copied, and consumed globally is in the order of 100 Zettabytes and just keeps growing.  ![Global data creation](/images/global-data-creation.png) <small>Through 2035, global data creation is projected to grow to more than 2,142 Zettabytes. From 2020, the growth was higher than previously expected caused by increased demand due to the COVID-19 pandemic, as more people worked and learned from home and used more home entertainment options. Source: [Statista](https://www.statista.com)</small>  Although this might seem overwhelming, the good news is that we can turn to machines for help: There are many different Machine Learning algorithms to discover patterns in big data that lead to actionable insights.  Depending on the way the algorithm “learns” about data to make predictions, we classify them into two groups, each proving one different type of learning:  - **Supervised learning:** existing data is already labeled and you know which behavior you want to predict in the new data you obtain.  - **Unsupervised learning:**  there is no output variable to guide the learning process,and data is explored by algorithms to find patterns. Since the data has no labels, the algorithm identifies similarities on the data points and groups them into clusters.  Under unsupervised learning, all the objects in the same group (cluster) should be more similar to each other than to those in other clusters; data points from different clusters should be as different as possible. Clustering allows you to find and organize data into groups that have been formed organically, rather than defining groups before looking at the data.  While this article will focus most closely on K-means, there are other powerful types of clustering that can be used as well. Let’s take a look at the main ones like hierarchical, density-based, and partitional clustering.  {{< newsletter text=\"Subscribe for more ML articles!\" inputText=\"Email address...\" buttonText=\"Submit\">}}  ## Hierarchical Clustering  Cluster assignments are determined by building a hierarchy. This is implemented by either a bottom-up or a top-down approach:  - The bottom-up approach is called **agglomerative** clustering and merges the two points that are the most similar until all points have been merged into a single cluster.  - The top-down approach is **divisive** clustering and starts with all points as one cluster and splits the least similar clusters at each step until only single data points remain.  ![Hierarchical clustering](/images/hierarchical-clustering.png) <small>The agglomerative case starts with every object being a cluster and, in the next steps, merges them with the two closest clusters. The process finishes with every object in one cluster. The divisive algorithm, contrastingly, starts with every object in one cluster and ends with every object in individual clusters. Source: [QuantDare](https://quantdare.com/hierarchical-clustering/)</small>   These methods produce a tree-based hierarchy of points called a dendrogram. The number of clusters “k” is often predetermined by the user, and clusters are assigned by cutting the dendrogram at a specified depth that results in “k” groups of smaller dendrograms.   <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/hierarchical-clustering-dendrogram.mp4\" type=\"video/mp4\"> </video>  <small class=\"video\">Hierarchical Clustering returns an output (typically as a dendrogram like the right figure) from which the user can decide the appropriate number of clusters (either manually or algorithmically). If done manually, the user may cut the dendrogram where the merged clusters are too far apart (represented by long lines in the dendrogram). Alternatively, the user can just return a specific number of clusters. Source: [Dashee87](https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/)</small>  Unlike other clustering techniques, hierarchical clustering is a **deterministic process**, which means that assignments won’t change when you run an algorithm multiple times on the same input data. Hierarchical clustering methods often reveal the finer details about the relationships between data objects and provide interpretable dendrograms. On the other hand, they’re computationally expensive with respect to algorithm complexity and sensitive to noise and outliers.  ## Density-Based Clustering  Under this category, cluster assignments are determined based on the density of data points in a region and assigned where there are high densities of data points separated by low-density regions.  Unlike other clustering categories, this approach doesn’t require the user to specify the number of clusters. Instead, there is a distance-based parameter that acts as a tunable threshold and determines how close points must be to be considered a cluster member.  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/density-based-clustering-algorithm.mp4\" type=\"video/mp4\"> </video>  <small class=\"video\">Density-Based Clustering algorithms like DBSCAN don’t require a preset number of clusters. It also identifies outliers as noises unlike others that simply throws them into a cluster even if the data point is very different. Additionally, it is able to find arbitrarily sized and arbitrarily shaped clusters quite well. Source: [Primo.ai](http://primo.ai/index.php?title=Density-Based_Spatial_Clustering_of_Applications_with_Noise_(DBSCAN))</small>  Density-based clustering methods excel at identifying clusters of nonspherical shapes, and they are resistant to outliers. Nevertheless, they aren’t well suited for clustering in high-dimensional spaces (since [density of data points is very low](https://scialert.net/fulltext/?doi=itj.2011.1092.1105) in those spaces), and they are not able to produce [clusters of differing density](http://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/IEEE_SSCI_2016/pdf/SSCI16_paper_256.pdf).  ## Partitional clustering  With this method, data objects are divided into non-overlapping groups: No object can be a member of more than one cluster, and every cluster must have at least one object.  Like in hierarchical clustering, the user needs to define the number of clusters “k”, which ultimately produces **non-deterministic** results: Partitional clustering produces different results from two or more separate runs even if the runs were based on the same input.  This clustering method works very well when clusters have a spherical shape (due to its [fixed distance norm](https://academic.oup.com/bioinformatics/article/21/9/1927/408943)), and they’re scalable with respect to algorithm complexity. However, they’re not well suited for clusters with complex shapes and different sizes, and they break down when used with clusters of different densities, since it doesn’t employ density parameters.  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/partitional-clustering-algorithm.mp4\" type=\"video/mp4\"> </video>  <small class=\"video\">Partitional clustering algorithms deal with the data space and focus on creating a certain number of divisions of the space. Source: [What Matrix](https://www.whatmatrix.com/portal/clustering-algorithms-from-start-to-state-of-the-art-2/)</small>  **K-means** is an example of a partitional clustering algorithm. Once the algorithm has been run and the groups are defined, any new data can be easily assigned to the existing groups. K-means is an extremely popular clustering algorithm, widely used in tasks like behavioral segmentation, inventory categorization, sorting sensor measurements, and detecting bots or anomalies.  ## K-means clustering  From the universe of unsupervised learning algorithms, K-means is probably the most recognized one. This algorithm has a clear objective: partition the data space in such a way so that data points within the same cluster are as similar as possible (intra-class similarity), while data points from different clusters are as dissimilar as possible (inter-class similarity).  ![Intercluster and intracluster distance](/images/intercluster-intracluster-distance.png) <small>An illustration of inter-cluster and intra-cluster distance. Source: [dinhanhthi.com](https://dinhanhthi.com/) </small>  In K-means, each cluster is represented by its center (called a “centroid”), which corresponds to the arithmetic mean of data points assigned to the cluster. A **centroid** is a data point that represents the center of the cluster (the mean), and it might not necessarily be a member of the dataset. This way, the algorithm works through an iterative process until each data point is closer to its own cluster’s centroid than to other clusters’ centroids, minimizing intra-cluster distance at each step. But how?  K-means searches for a predetermined number of clusters within an unlabelled dataset by using an iterative method to produce a final clustering based on the number of clusters defined by the user (represented by the variable K). For example, by setting “k” equal to 2, your dataset will be grouped in 2 clusters, while if you set “k” equal to 4 you will group the data in 4 clusters.  K-means triggers its process with arbitrarily chosen data points as proposed centroids of the groups and iteratively recalculates new centroids in order to converge to a final clustering of the data points. Specifically, the process works as follows:  1. The algorithm randomly chooses a centroid for each cluster. For example, if we choose a “k” of 3, the algorithm randomly picks 3 centroids.  2. K-means assigns every data point in the dataset to the nearest centroid, meaning that a data point is considered to be in a particular cluster if it is closer to that cluster’s centroid than any other centroid.  3. For every cluster, the algorithm recomputes the centroid by taking the average of all points in the cluster, reducing the total intra-cluster variance in relation to the previous step. Since the centroids change, the algorithm re-assigns the points to the closest centroid.  4. The algorithm repeats the calculation of centroids and assignment of points until the sum of distances between the data points and their corresponding centroid is minimized, a maximum number of iterations is reached, or no changes in centroids value are produced.  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/k-means-algorithm.mp4\" type=\"video/mp4\"> </video>  <small class=\"video\">The figure shows the centroids updating through the first five iterations from two different runs of the K-means algorithm on the same dataset. The purpose of this figure is to show that the initialization of the centroids is an important step. Source: [Real Python](https://realpython.com/k-means-clustering-python/)</small>  ### Finding the value of K  How do you choose the right value of “k”? When you define “k” you are telling the algorithm how many centroids you want, but how do you know how many clusters to produce?  One popular approach is testing different numbers of clusters and measuring the resulting Sum of Squared Errors (SSE), choosing the “k” value at which an increase will cause a very small decrease in the error sum, while a decrease will sharply increase the error sum. This point that defines the optimal number of clusters is known as the “elbow point”.  ![Sum of squared errors](/images/sum-squared-errors.png) <small>As the number of clusters increases, the Sum of Squared Errors within clusters will start to decrease. The error value is largest when “k” = 1. We can see that the graph will rapidly change at a point,thus creating an elbow shape. From this point, the graph starts to move almost parallel to the X-axis. The “k”  value corresponding to this point is the optimal number of clusters. Source: [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/01/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning/)</small>  Another alternative is to use the Silhouette Coefficient metric. This coefficient is a measure of cluster cohesion and separation, frequently used in unsupervised learning problems. It quantifies how well a data point fits into its assigned cluster based on two factors:  - How close the data point is to other points in the cluster - How far away the data point is from points in other clusters  Silhouette coefficient values range between -1 and 1, meaning that well-defined clusters result in positive values of this coefficient, while incorrect clusters will result in negative values.  We can use a Silhouette plot to display a [measure of how close each point in one cluster is to a point in the neighboring clusters](https://neptune.ai/blog/k-means-clustering) and thus provide a way to assess parameters like the number of clusters visually.  ![Silhouette analysis](/images/silhouette-analysis.png) <small>Using the above Silhouette analysis, we can choose an optimal k value as 3 because the average silhouette score is higher and indicates that the data points are optimally positioned. Source: [Neptune Blog](https://neptune.ai/blog/k-means-clustering)</small>  ## When to Use K-Means Clustering  K-means presents huge [advantages](https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages), since it scales to large data sets, is relatively simple to implement, guarantees convergence, can warm-start the positions of centroids, it easily adapts to new examples, and generalizes to clusters of different shapes and sizes, such as elliptical clusters.  But as any other Machine Learning method, it also presents downsides. The most obvious one is that you need to define the number of clusters manually, and, although we showed some ways to find the optimal “k”, this is a decision that will deeply affect the results.  Also, K-means is highly dependent on initial values. For low values of “k”, you can mitigate this dependence by running K-means several times with different initial values and picking the best result. As “k” increases, you need [advanced versions of K-means](https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages) to pick better values of the initial centroids (called K-means seeding). K-means produces clusters with uniform sizes (in terms of density and quantity of observations), even though the underlying data might behave in a very different way. Finally, K-means is very sensitive to outliers, since centroids can be dragged in the presence of noisy data.  K-means is highly flexible and can be used to cluster data in lots of different domains. It also can be modified to adapt it to specific challenges, making it extremely powerful. Whether you’re dealing with structured data, [embeddings](/learn/vector-embeddings/), or any other data type, you should definitely consider using K-means. ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2e8"
  },
  "title": "\"Dense Vectors\"",
  "content": "  - NLP for Semantic Search toc: >- weight: 1 author:   name: James Briggs   position: Developer Advocate   src: /images/james-briggs.jpeg   href: \"https://www.youtube.com/c/jamesbriggs\" description: An overview of dense vector embeddings with NLP. # Open Graph images: ['/images/nlp-embedding-methods-8.jpg'] ---  **[Pinecone](/) is a vector database for storing and searching through dense vectors. Why would you ever want to do that? Keep reading to find out, then [try Pinecone for free](https://app.pinecone.io).**  There is perhaps no greater contributor to the success of modern Natural Language Processing (NLP) technology than vector representations of language. The meteoric rise of NLP was ignited with the introduction of word2vec in 2013 [1].  Word2vec is one of the most iconic and earliest examples of dense vectors representing text. But since the days of word2vec, developments in representing language have advanced at ludicrous speeds.  This article will explore *why* we use dense vectors — and some of the best approaches to building dense vectors available today.  <p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/bVZJ_O_-0RE\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>  ## Dense vs Sparse Vectors  The first question we should ask is *why should we represent text using vectors?* The straightforward answer is that for a computer to understand human-readable text, we need to convert our text into a machine-readable format.  Language is inherently full of information, so we need a reasonably large amount of data to represent even small amounts of text. [Vectors](/learn/vector-embeddings/) are naturally good candidates for this format.  We also have two options for vector representation; *sparse* vectors or *dense* vectors.  Sparse vectors can be stored more efficiently and allow us to perform syntax-based comparisons of two sequences. For example, given two sentences; `\"Bill ran from the giraffe toward the dolphin\"`, and `\"Bill ran from the dolphin toward the giraffe\"` we would get a perfect (or near-perfect) match.  Why? Because despite the meaning of the sentences being different, they are composed of the same syntax (e.g., words). And so, sparse vectors would be closely or even perfectly matched (depending on the construction approach).  Where sparse vectors represent text syntax, we could view dense vectors as *numerical representations of semantic meaning*. Typically, we are taking words and encoding them into very dense, high-dimensional vectors. The abstract meaning and relationship of words are numerically encoded.  ---  *Sparse vectors are called sparse because vectors are sparsely populated with information. Typically we would be looking at thousands of zeros to find a few ones (our relevant information). Consequently, these vectors can contain many dimensions, often in the tens of thousands.*  ![clustered](/images/nlp-embedding-methods-2.jpg) <small>Sparse and dense vector comparison. Sparse vectors contain *sparsely* distributed bits of information, whereas dense vectors are much more information-rich with densely-packed information in every dimension.</small>  ---  *Dense vectors are still highly dimensional (784-dimensions are common, but it can be more or less). However, each dimension contains relevant information, determined by a neural net — compressing these vectors is more complex, so they typically use more memory.*  ---  Imagine we create dense vectors for every word in a book, reduce the dimensionality of those vectors and then visualize them in 3D — we will be able to identify relationships. For example, days of the week may be clustered together:  ![clustered](/images/nlp-embedding-methods-3.jpg)  <small>Example of the clustering of related keywords as is typical with word embeddings such as *word2vec* or *GLoVe*.</small>  Or we could perform 'word-based' arithmetic:  ![vector_arithmetic](/images/nlp-embedding-methods-4.jpg)  <small>A classic example of arithmetic performed on word vectors from another Mikolov paper [2].</small>  And all of this is achieved using equally complex neural nets, which identify patterns from *massive* amounts of text data and translate them into dense vectors.  Therefore we can view the difference between sparse and dense vectors as representing *syntax* in language versus representing *semantics* in language.  ## Generating Dense Vectors  Many technologies exist for building dense vectors, ranging from vector representations of words or sentences, Major League Baseball players [3], or even cross-media text and images.  ---  *We usually take an existing public model to generate vectors. For almost every scenario there is a high-performance model out there and it is easier, faster, and *often* much more accurate to use them. There are cases, for example for industry or language-specific embeddings where you sometimes need to fine-tune or even train a new model from scratch, but it isn’t common.*  ---  We will explore a few of the most exciting and valuable of these technologies, including:  * The '2vec' methods * Sentence Transformers * Dense Passage Retrievers (DPR) * Vision Transformers (ViT)  ### Word2Vec  Although we now have superior technologies for building embeddings, no overview on dense vectors would be complete without word2vec. Although *not* the first, it was the first widely used dense embedding model thanks to (1) being *very good*, and (2) the release of the [word2vec toolkit](https://code.google.com/archive/p/word2vec/) — allowing easy training or usage of pre-trained word2vec embeddings.  Given a sentence, word embeddings are created by taking a specific word (translated to a one-hot encoded vector) and mapping it to surrounding words through an encoder-decoder neural net.  ![skip_gram](/images/nlp-embedding-methods-5.jpg)  <small>The skip-gram approach to building dense vectors embeddings in word2vec.</small>  This is the *skip-gram* version of word2vec which, given a word `fox`, attempts to predict surrounding words (its context). After training we discard the left and right blocks, keeping only the middle dense vector. This vector represents the word to the left of the diagram and can be used to embed this word for downstream language models.  We also have the *continuous bag of words (CBOW)*, which switches the direction and aims to predict a word based on its context. This time we produce an embedding for the word on the right (in this case, still `fox`).  ![cbow](/images/nlp-embedding-methods-6.jpg)  <small>The continuous bag of words (CBOW) approach to building dense vector embeddings in word2vec.</small>  Both skip-gram and CBOW are alike in that they produce a dense embedding vector from the middle *hidden layer* of the encoder-decoder network.  From this, Mikolov et al. produced the infamous `King - Man + Woman == Queen` example of vector arithmetic applied to language we saw earlier [2].  Word2vec spurred a flurry of advances in NLP. Still, when it came to representing longer chunks of text using single vectors — word2vec was useless. It allowed us to encode single words (or n-grams) but nothing more, meaning long chunks of text could only be represented by *many* vectors.  To compare longer chunks of text effectively we need it to be represented by a single vector. Because of this limitation, several *extended* embedding methods quickly cropped up, such as sentence2vec and doc2vec.  Whether word2vec, sentence2vec, or even (batter|pitcher)2vec (representations of Major League Baseball players [3]), we now have vastly superior technologies for building these dense vectors. So although *'2vec'* is where it started, we don't often see them in use today.  ### Sentence Similarity  We've explored the beginnings of word-based embedding with word2vec and briefly touched on the other *2vecs* that popped up, aiming to apply this vector embedding approach to longer chunks of text.  We see this same evolution with transformer models. These models produce incredibly information-rich dense vectors, which can be used for a variety of applications from sentiment analysis to question-answering. Thanks to these rich embeddings, transformers have become the dominant modern-day language models.  [BERT](/learn/semantic-search/) is perhaps the most famous of these transformer architectures (although the following applies to *most* transformer models).  Within BERT, we produce vector embeddings for each word (or *token*) similar to word2vec. However, embeddings are much richer thanks to much deeper networks — and we can even encode the *context* of words thanks to the attention mechanism.  The attention mechanism allows BERT to prioritize which context words should have the biggest impact on a specific embedding by considering the *alignment* of said context words (we can imagine it as BERT literally *paying attention* to specific words depending on the context).  What we mean by 'context' is, where word2vec would produce the same vector for 'bank' whether it was *\"a grassy bank\"* or *\"the bank of England\"* — BERT would instead modify the encoding for *bank* based on the surrounding context, thanks to the attention mechanism.  However, there is a problem here. We want to focus on comparing *sentences*, not words. And BERT embeddings are produced for each token. So this doesn't help us in sentence-pair comparisons. What we need is a single vector that represents our sentences or paragraphs like sentence2vec.  The first transformer explicitly built for this was *Sentence-BERT (SBERT)*, a modified version of BERT [4].  BERT (and SBERT) use a *WordPiece* tokenizer — meaning that every word is equal to one *or more* tokens. SBERT allows us to create a single vector embedding for sequences containing no more than 128 tokens. Anything beyond this limit is cut.  This limit isn't ideal for *long* pieces of text, but more than enough when comparing sentences or small-average length paragraphs. And many of the latest models allow for longer sequence lengths too!  #### Embedding With Sentence Transformers  Let's look at how we can quickly pull together some sentence embeddings using the `sentence-transformers` library [5]. First, we import the library and initialize a sentence transformer model from Microsoft called `all-mpnet-base-v2` (maximum sequence length of `384`).  {{< notebook file=\"sentence-transformers-init\" height=\"full\" >}}   Then we can go ahead and encode a few sentences, some more similar than others — while sharing *very few* matching words.  {{< notebook file=\"encoding-sentences-1\" height=\"full\" >}}   And what does our sentence transformer produce from these sentences? A 768-dimensional dense representation of our sentence. The performance of these embeddings when compared using a similarity metric such as cosine similarity is, in most cases — excellent.  {{< notebook file=\"sentence-cos-sim\" height=\"full\" >}}   Despite our most semantically similar sentences about bees and their queen sharing *zero* descriptive words, our model correctly embeds these sentences in the closest vector space when measured with cosine similarity!  ### Question-Answering  Another widespread use of transformer models is for questions and answers (Q&A). Within Q&A, there are several different *architectures* we can use. One of the most common is *open domain Q&A (ODQA)*.  ODQA allows us to take a big set of sentences/paragraphs that contain answers to our questions (such as paragraphs from Wikipedia pages). We then ask a question to return a small chunk of one (or more) of those paragraphs which best answers our question.  When doing this, we are making use of three components or models:  * Some sort of **database** to store our sentence/paragraphs (called *contexts*). * A **retriever** retrieves contexts that it sees as similar to our question. * A **reader** model which extracts the *answer* from our related context(s).  ![odqa](/images/nlp-embedding-methods-7.jpg)  <small>An example open domain question-answering (ODQA) architecture.</small>  The *retriever* portion of this architecture is our focus here. Imagine we use a sentence-transformer model. Given a question, the retriever would return sentences most similar to our question — but we want answers *not* questions.  Instead, we want a model that can map question-answers pairs to the same point in vector space. So given the two sentences:  ``` \"What is the capital of France?\" AND \"The capital of France is Paris.\" ```  We want a model that maps these two sentences to the same (or *very close*) vectors. And so when we receive a question `\"What is the capital of France?\"`, we want the output vector to have very high similarity to the vector representation of `\"The capital of France is Paris.\"` in our [vector database](/learn/vector-database/).  The most popular model for this is Facebook AI's *Dense Passage Retriever (DPR)*.  DPR consists of two smaller models — a *context* encoder and a *query* encoder. Again they're both using the BERT architecture and are trained in parallel on question-answer pairs. We use a contrastive loss function, calculated as the difference between the two vectors output by each encoder [6].  ![dpr](/images/nlp-embedding-methods-8.jpg)  <small>Bi-encoder structure of DPR, we have both a *question encoder* and a *context encoder* — both are optimized to output the same (or close) embeddings for each question-context pair.</small>  So when we give our question encoder `\"What is the capital of France?\"`, we would hope that the output vector would be similar to the vector output by our context encoder for `\"The capital of France is Paris.\"`.  We can't rely on all of the question-answer relationships on having been seen during training. So when we input a new question such as `\"What is the capital of Australia?\"` our model might output a vector that we could think of as similar to `\"The capital of Australia is ___\"`. When we compare that to context embeddings in our database, this *should* be similar to `\"The capital of Australia is Canberra\"` (or so we hope).  #### Fast DPR Setup  Let's take a quick look at building some context and query embeddings with DPR. We'll be using the `transformers` library from Hugging Face.  First, we initialize tokenizers and models for both our context (`ctx`) model and `question` model.  {{< notebook file=\"init-qa\" height=\"full\" >}}   Given a question and several contexts we tokenize and encode like so:  {{< notebook file=\"qa-encode\" height=\"full\" >}}   *Note that we have included the questions within our contexts to confirm that the bi-encoder architecture is not just producing a straightforward semantic similarity operation as with sentence-transformers.*  Now we can compare our query embeddings `xq` against all of our context embeddings `xb` to see which are the most similar with *cosine similarity*.  {{< notebook file=\"qa-cos-sim\" height=\"full\" >}}   Out of our three questions, we returned two correct answers as the *very top* answer. It’s clear that DPR is not the *perfect* model, particularly when considering the simple nature of our questions and small dataset for DPR to retrieve from.  On the positive side however, in ODQA we would return many more contexts and allow a *reader* model to identify the best answers. Reader models can ‘re-rank’ contexts, so retrieving the top context immediately is not required to return the correct answer. If we were to retrieve the most relevant result 66% of the time, it would likely be a good result.  We can also see that despite hiding *exact matches* to our questions in the contexts, they interfered with only our last question, being correctly ignored by the first two questions.  ### Vision Transformers  Computer vision (CV) has become the stage for some exciting advances from transformer models — which have historically been restricted to NLP.  These advances look to make transformers the first widely adopted ML models that excel in both NLP *and* CV. And in the same way that we've been creating dense vectors representing language. We can do the same for images — and even encode images and text into the same vector space.  ![same_vector_space](/images/nlp-embedding-methods-9.jpg)  <small>Using specific text and image encoders, we can encode text and images to the same vector space. Photo credit [Alvan Nee](https://unsplash.com/photos/T-0EW-SEbsE).</small>  The *Vision Transformer (ViT)* was the first transformer applied to CV without the assistance of any upstream CNNs (as with VisualBERT [7]). The authors found that ViT can *sometimes* outperform state-of-the-art (SOTA) CNNs (the long-reigning masters of CV) [8].  These ViT transformers have been used alongside the more traditional language transformers to produce fascinating image and text encoders, as with OpenAI's CLIP model [9].  The CLIP model uses two encoders like DPR, but this time we use a ViT model as our image encoder and a *masked self-attention* transformer like BERT for text [10]. As with DPR, these two models are trained in parallel and optimized via a contrastive loss function — producing *high similarity* vectors for image-text pairs.  That means that we can encode a set of images and then match those images to a caption of our choosing. And we can use the same encoding and cosine similarity logic we have used throughout the article. Let's go ahead and try.  #### Image-Text Embedding  Let's first get a few images to test. We will be using three images of dogs doing different things from Unsplash (links in the caption below).  {{< notebook file=\"get-images\" height=\"full\" >}}   ![dog_pics](/images/nlp-embedding-methods-10.jpg)  <small>Images downloaded from Unsplash (captions have been manually added — they are not included with the images), photo credits to Cristian Castillo \\[[1](https://unsplash.com/photos/73pyV0JJOmE), [2](https://unsplash.com/photos/qA9wk6SDuVw)\\] and [Alvan Nee](https://unsplash.com/photos/T-0EW-SEbsE).</small>  We can initialize the CLIP `model` and `processor` using `transformers` from Hugging Face.  {{< notebook file=\"CLIP-init\" height=\"full\" >}}   Now let's create three true captions (plus some random) to describe our images and preprocess them through our `processor` before passing them on to our `model`. We will get output logits and use an `argmax` function to get our predictions.  {{< notebook file=\"CLIP-predictions\" height=\"full\" >}}   And there, we have flawless image-to-text matching with CLIP! Of course, it is not perfect (our examples here are reasonably straightforward), but it produces some awe-inspiring results in no time at all.  Our model has dealt with comparing text and image embeddings. Still, if we wanted to extract those same embeddings used in the comparison, we access `outputs.text_embeds` and `outputs.image_embeds`.  {{< notebook file=\"CLIP-embeddings\" height=\"full\" >}}   And again, we can follow the same logic as we previously used with cosine similarity to find the closest matches. Let's compare the embedding for `'a dog hiding behind a tree'` with our three images with this alternative approach.  {{< notebook file=\"CLIP-embeds-cos-sim\" height=\"full\" >}}   As expected, we return the dog hiding behind a tree!  ---  That's it for this overview of both the early days of dense vector embeddings in NLP and the current SOTA. We've covered some of the most exciting applications of both text and image embeddings, such as:  * Semantic Similarity with `sentence-transformers`. * Q&A retrieval using Facebook AI's DPR model. * Image-text matching with OpenAI's CLIP.  We hope you learned something from this article.  {{< newsletter text=\"Subscribe for more vector search tutorials!\" inputText=\"Email address...\" buttonText=\"Submit\">}}  ## References  [1] T. Mikolov, et al., [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781) (2013)  [2] T. Mikolov, et al., [Linguistic Regularities in Continuous Space Word Representations](https://aclanthology.org/N13-1090/) (2013), NAACL HLT  [3] M. Alcorn, [(batter|pitcher)2vec: Statistic-Free Talent Modeling With Neural Player Embeddings](https://www.sloansportsconference.com/research-papers/batter-pitcher-2vec-statistic-free-talent-modeling-with-neural-player-embeddings) (2017), MIT Sloan: Sports Analytics Conference  [4] N. Reimers, I. Girevych, [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084) (2019), EMNLP  [5] N. Reimers, [SentenceTransformers Documentation](https://www.sbert.net/index.html), sbert.net  [6] V. Karpukhin, et al., [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) (2020), EMNLP  [7] L. H. Li, et al., [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/abs/1908.03557) (2019), arXiv  [8] A. Dosovitskiy, et al., [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) (2020), arXiv  [9] A. Radford, et al., [CLIP: Connecting Text and Images](https://openai.com/blog/clip/) (2021), OpenAI Blog  [10] [CLIP Model Card](https://huggingface.co/openai/clip-vit-base-patch32), Hugging Face",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2ea"
  },
  "title": "\"Product Quantization\"",
  "content": "  - \"Faiss: The Missing Manual\" toc: >- weight: 5 author:   name: James Briggs   position: Developer Advocate   src: /images/james-briggs.jpeg   href: \"https://www.youtube.com/c/jamesbriggs\" description: Breaking down one of the most important developments in vector similarity search # Open Graph images: ['/images/product-quantization-5.jpg'] ---  ![Product Quantization 101](/images/product-quantization-1.gif)  ---  **[Pinecone](/) lets you add vector search to applications without knowing anything about algorithm optimizations, and [it's free to try](https://app.pinecone.io). However, we know you like seeing how things work, so enjoy learning about memory-efficient search with product quantization!**  ---  Vector similarity search can require huge amounts of memory. Indexes containing 1M dense vectors (a small dataset in today’s world) will often require several GBs of memory to store.  The problem of excessive memory usage is exasperated by high-dimensional data, and with ever-increasing dataset sizes, this can *very* quickly become unmanageable.  Product quantization (PQ) is a popular method for dramatically compressing high-dimensional vectors to use 97% less memory, and for making nearest-neighbor search speeds 5.5x faster in our tests.  A composite IVF+PQ index speeds up the search by another 16.5x without affecting accuracy, for a whopping total speed increase of 92x compared to non-quantized indexes.  In this article, we will cover all you need to know about PQ: How it works, pros and cons, implementation in Faiss, composite IVFPQ indexes, and how to achieve the speed and memory optimizations mentioned above.  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/t9mRf2S5vDI\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ---  ## What is Quantization  Quantization is a generic method that refers to the compression of data into a smaller space. I know that might not make much sense — let me explain.  First, let's talk about dimensionality reduction — which is *not* the same as quantization.  Let’s say we have a high-dimensional vector, it has a dimensionality of `128`. These values are 32-bit floats in the range of *0.0 -> 157.0* (our scope `S`). Through dimensionality reduction, we aim to produce another, lower-dimensionality vector.  ![Product Quantization](/images/product-quantization-2.jpg) <small>Dimensionality reduction reduces the dimensionality `D` of vectors, but not the scope S.</small>  On the other hand, we have quantization. Quantization does not care about dimensionality `D`. Instead, it targets the potential scope of values. Rather than reducing `D`, we reduce `S`.  ![Product Quantization](/images/product-quantization-4.jpg) <small>Quantization reduces the scope S of possible vectors. Note that with pre-quantization the scope is typically infinite.</small>  There are many ways of doing this. For example, we have *clustering*. When we cluster a set of vectors we replace the larger scope of potential values (all possible vectors), with a smaller *discrete and symbolic* set of centroids.  And this is really how we can define a quantization operation. The transformation of a vector into a space with a finite number of possible values, where those values are *symbolic* representations of the original vector.  Just to make it very clear, these symbolic representations vary in form. They can be centroids as is the case for PQ, or [binary codes like those produced by LSH](/learn/locality-sensitive-hashing-random-projection/).  ### Why Product Quantization?  Quantization is primarily used to reduce the memory footprint of indexes — an important task when comparing large arrays of vectors as they must all be loaded in memory to be compared.  PQ is not the only quantization method that does this, however — but other methods do not manage to reduce memory size as effectively as PQ. We can actually calculate memory usage and quantization operation complexity for PQ and other methods like so:  ``` kmeans = kD PQ = mk^*D^* = k^{1/m}D ```  We know that `D` represents the dimensionality of our input vectors, but `k` and `m` may be new. `k` represents the *total* number of centroids (or *codes*) that will be used to represent our vectors. And `m` represents the number of *subvectors* that we will split our vectors into (more on that later).  *(A 'code' refers to the quantized representation of our vectors)*  ![Product Quantization](/images/product-quantization-3.jpg) <small>Memory usage (and complexity) vs dimensionality using k=2048 and m=8.</small>  The problem here is that for good results, the recommended `k` value is of `2048` (2<sup>11</sup>) or more[1]. Given a vector dimensionality of `D`, clustering without PQ leaves us with *very high* memory requirements and complexity:  <script src=\"https://gist.github.com/jamescalam/2fc852bd28aa6ae140ce3f9ce0326936.js\"></script>  Given an m value of `8`, the equivalent memory usage and *assignment* complexity for PQ is significantly lower — thanks to the *chunking* of vectors into subvectors and the subquantization process being applied to those smaller dimensionalities `k*` and `D*`, equal to `k/m` and `D/m` respectively.  A second important factor is quantizer training. Quantizers require datasets that are several times larger than k for effective training, that is *without product subquantization*.  Using subquantizers, we only need several multiples of `k* (which is k/m)` — this can still be a large number — but it can be significantly reduced.  ## How Product Quantization Works  Let's work through the logic of PQ. We would usually have many vectors (all of equal length) — but for the sake of simplicity, we will use a single vector in our examples.  In short, PQ is the process of:  * Taking a big, high-dimensional vector, * Splitting it into equally sized chunks — our subvectors, * Assigning each of these subvectors to its nearest *centroid* (also called reproduction/reconstruction values), * Replacing these centroid values with unique IDs — each ID represents a centroid  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/product-quantization-6.mp4\" type=\"video/mp4\"> </video>  <small class=\"video\">High-level view of PQ: Take a big vector, split it into subvectors, assign each to its nearest centroid value, and replace the centroid value with its unique ID — producing a tiny vector of IDs.</small>  At the end of the process, we've reduced our highly dimensional vector — that requires *a lot of memory* — to a tiny vector of IDs that require *very little memory*.  Our vector is of length D 12. We start by splitting this vector into `m` subvectors like so:  ![Product Quantization](/images/product-quantization-5.jpg) <small>Here we are splitting our high-dimensional vector x into several subvectors u_j.</small>  {{< notebook file=\"make-subvec\" height=\"full\" >}}  We can refer to each subvector by its position `j`.  For the next part, think about clustering. As a random example, given a large number of vectors we can say *\"I want three clusters\"* — and we then optimize these cluster centroids to split our vectors into *three* categories based on each vector’s nearest centroid.  For PQ we do the same thing with one minor difference. Each subvector space (subspace) is assigned its own set of clusters — and so what we produce is a set of clustering algorithms across multiple subspaces.  {{< notebook file=\"build-clusters\" height=\"full\" >}}  Each of our subvectors will be assigned to one of these centroids. In PQ terminology these centroids are called *reproduction values* and are represented by `cⱼ,ᵢ` where `j` is our subvector identifier, and `i` identifies the chosen centroid *(there are `k*` centroids for each subvector space j)*.  ![Product Quantization](/images/product-quantization-8.jpg) <small>Our subvectors are replaced with a specific centroid vector — which can then be replaced with a unique ID specific to that centroid vector.</small>  {{< notebook file=\"clustering\" height=\"full\" >}}  When we process a vector with PQ, it is split into our subvectors, those subvectors are then processed and assigned to their nearest (sub)cluster centroids (reproduction values).  Rather than storing our quantized vector to be represented by the `D*`-dimensional centroids, we replace it with a centroid ID. Every centroid `cⱼ,ᵢ` has its own ID, which can later be used to map those ID values back to the full centroids via our codebook `c`.  {{< notebook file=\"ids-to-centroid\" height=\"full\" >}}  With that, we have compressed a 12-dimensional vector into a 4-dimensional vector of IDs. We have used a small dimensionality here for the sake of simplicity, and so the benefits of such a technique may not be inherently clear.  Let’s switch from our original 12-dimensional vector of 8-bit integers to a more realistic 128-dimensional vector of 32-bit floats (as we will be using throughout the next section). We can find a good balance in performance after compression to an 8-bit integer vector containing just *eight* dimensions.  ``` Original: 128×32 = 4096 Quantized: 8×8 = 64 ``` That's a big difference — 64x!  ---  ## PQ Implementation in Faiss  So far we've worked through the logic behind a simple, readable implementation of PQ in Python. Realistically we wouldn't use this because it is not optimized and we already have excellent implementations elsewhere. Instead, we would use a library like [Faiss](/learn/faiss/) — or a production-ready service like [Pinecone](/).  We'll take a look at how we can build a PQ index in Faiss, and we'll even take a look at combining PQ with an Inverted File (IVF) step to improve search speed.  <p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/BMYBwbkbVec\" title=\"Product Quantization Implementation in Faiss\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>  Before we start, we need to get data. We will be using the Sift1M dataset. It can be downloaded and opened using this script:  {{< notebook file=\"get-sift1m\" height=\"full\" >}}  Now let's move onto our first index: IndexPQ.  ### IndexPQ  Our first index is a pure PQ implementation using IndexPQ. To initialize the index we need to define three parameters.  {{< notebook file=\"index-pq\" height=\"full\" >}}  We have our vector dimensionality `D`, the number of subvectors we'd like to split our full vectors into (we must `assert` that `D` is divisible by `m`).  Finally, we include the nbits parameter. This defines the number of bits that each subquantizer can use, we can translate this into the number of centroids assigned to each subspace as `k_ = 2**nbits`. An `nbits` of 11 leaves us with `2048` centroids per subspace.  Because we are using PQ, which uses clustering — we must train our index using our xb dataset.  {{< notebook file=\"pq-train\" height=\"full\" >}}  And with that, we can go ahead and add our vectors to the index and `search`.  {{< notebook file=\"pq-search\" height=\"full\" >}}  From our search, we will return the top `k` closest matches (not the same `k` used in earlier notation). Returning our distances in `dist`, and the indices in `I`.  We can compare the *recall* performance of our `IndexPQ` against that of a flat index — which has 'perfect' recall (thanks to not compressing vectors and performing an exhaustive search).  {{< notebook file=\"l2-pq-recall\" height=\"full\" >}}  We're getting 50% which is a reasonable recall *if* we are happy to sacrifice the perfect results for the reduced memory usage of PQ. There's also a reduction to just 18% of the flat search time — something that we can improve *even further* using IVF later.  Lower recall rates are a major drawback of PQ. This can be counteracted *somewhat* by using larger `nbits` values at the cost of slower search times and *very* slow index construction times. However, very high recall is out of reach for both PQ and IVFPQ indexes. If higher recall is required another index should be considered.  How does IndexPQ compare to our flat index in terms of memory usage?  {{< notebook file=\"flat-pq-memory\" height=\"full\" >}}  Memory usage using IndexPQ is — put simply — fantastic, with a memory reduction of 98.4%. It is possible to translate some of these preposterous performance benefits into search speeds too by using an IVF+PQ index.  ### IndexIVFPQ  To speed up our search time we can add another step, using an IVF index, which will act as the initial broad stroke in reducing the scope of vectors in our search.  After this, we continue our PQ search as we did before — but with a significantly reduced number of vectors. Thanks to minimizing our search scope, we should find we get vastly improved search speeds.  Let's see how that works. First, we initialize our IVF+PQ index like so:  {{< notebook file=\"index-ivfpq\" height=\"full\" >}}  We have a new parameter here, `nlist` defines how many Voronoi cells we use to cluster our *already* quantized PQ vectors ([learn more about IndexIVF here](/learn/vector-indexes/)). You may be asking, what on earth is a Voronoi cell — what does any of this even mean? Let's visualize some 2D 'PQ vectors':  ![PQ Vectors in 2D](/images/product-quantization-7.jpg) <small>2D chart showing our reconstructed 'PQ' vectors. However, in reality, we would never use PQ for 2D vectors as there is simply not enough dimensionality for us to split into subvectors and subquantization.</small>  Let's add some Voronoi cells:  ![Product Quantization: Voroni Cells](/images/product-quantization-10.jpg) <small>2D chart showing our quantized 'PQ' vectors that have now been assigned to different Voronoi cells via IVF.</small>  At a high level, they're simply a set of partitions. Similar vectors are assigned to different partitions (or *cells*), and when it comes to search — we introduce our query vector xq and restrict our search to the nearest cell:  ![Product Quantization: Voroni Cells](/images/product-quantization-9.jpg) <small>IVF allows us to restrict our search to only vectors that have been assigned nearby cells. The magenta point is our query vector xq. Now let's go ahead with our `train` and `search` — and see how our search speed and recall are doing.</small>  {{< notebook file=\"ivfpq-train-search\" height=\"full\" >}}  A lightning-fast search time of 86.3μs, but the recall has decreased from our IndexPQ significantly (50% to 34%). Given equivalent parameters, both IndexPQ and IndexIVFPQ *should* be able to attain equal recall performance.  The secret to improving our recall, in this case, is bumping up the `nprobe` parameter — which tells us *how many* of the nearest Voronoi cells to include in our search scope.  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/product-quantization-11-ivf-nprobe.mp4\" type=\"video/mp4\"> </video>  <small class=\"video\">2D chart showing our quantized 'PQ' vectors that have now been assigned to different Voronoi cells via IVF.</small>  At one extreme, we can include all cells by setting nprobe to our `nlist` value — this will return the maximum possible recall.  Of course, we don't want to include all cells. It would make our IVF index pointless as we would then not be restricting our search scope (making this equivalent to a flat index). Instead, we find the lowest `nprobe` value that achieves this recall performance.  {{< notebook file=\"nprobe-ivfpq\" height=\"full\" >}}  With a nprobe of 48, we achieve the best possible recall score of 52% (as demonstrated with `nprobe == 2048`), while minimizing our search scope (and therefore maximizing search speeds).  By adding our IVF step, we've dramatically reduced our search time from 1.49ms for IndexPQ to 0.09ms for IndexIVFPQ. And thanks to our PQ vectors we've then paired that with minuscule memory usage that’s 96% lower than the Flat index.  All in all, IndexIVFPQ gave us a *huge reduction* in memory usage — albeit slightly larger than IndexPQ at 9.2MB vs 6.5MB — and lightning-fast search speeds, all while maintaining a reasonable recall of around 50%.  ---  That's it for this article! We've covered the intuition behind product quantization (PQ), and how it manages to compress our index and enable incredibly efficient memory usage.  We put together the Faiss IndexPQ implementation and tested search times, recall, and memory usage — then optimized the index even further by pairing it with an IVF index using IndexIVFPQ.  <table class=\"table table-responsive\"> <thead>   <tr>     <th></th>     <th>FlatL2</th>     <th>PQ</th>     <th>IVFPQ</th>   </tr> </thead> <tbody>   <tr>     <td><strong>Recall (%)</strong></td>     <td>100</td>     <td>50</td>     <td>52</td>   </tr>   <tr>     <td><strong>Speed (ms)</strong></td>     <td>8.26</td>     <td>1.49</td>     <td>0.09</td>   </tr>   <tr>     <td><strong>Memory (MB)</strong></td>     <td>256</td>     <td>6.5</td>     <td>9.2</td>   </tr> </tbody> </table>  The results of our tests show impressive memory compression and search speeds, with reasonable recall scores.  If you're interested in learning more about the variety of indexes available in search, including more detail on the IVF index, read our [article about the best indexes for similarity search](/learn/vector-indexes/).  {{< newsletter text=\"Subscribe for the latest in similarity search!\" inputText=\"Email address...\" buttonText=\"Submit\">}}  ---  ## References  * [1] H Jégou, et al., [Product quantization for nearest neighbor search](https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf) (2010) * [Jupyter Notebooks](https://github.com/pinecone-io/examples/tree/master/product_quantization) ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2ec"
  },
  "title": "\"Domain Adaptation with Generative Pseudo-Labeling (GPL)\"",
  "headline": "\"Domain Adaptation with Generative Pseudo-Labeling (GPL)\"",
  "weight": "13",
  "name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "Unsupervised Domain adaptation with Generative Pseudo-Labeling",
  "images": "['https://www.pinecone.io/images/gpl-0.jpg']",
  "content": "In 1999, a concept known as the *semantic web* was described by the creator of the *World Wide Web*, Tim Berners-Lee. This dream of Berners-Lee was the internet of today that we know and love but deeply understood by machines [1].  This futuristic vision had seemed to be utterly infeasible, but, in recent years, has become much more than a dream. Thanks to techniques like **G**enerative **P**seudo-**L**abeling (GPL) that allow us to fine-tune new or existing models in previously inaccessible domains, machines are ever closer to understanding the meaning behind the content on the web.  <div style=\"padding: 1rem;\">     <em>     I have a dream for the Web [in which computers] become capable of analyzing all the data on the Web – the content, links, and transactions between people and computers. A \"Semantic Web\", which makes this possible, has yet to emerge, but when it does, the day-to-day mechanisms of trade, bureaucracy and our daily lives will be handled by machines talking to machines. The \"intelligent agents\" people have touted for ages will finally materialize.     </em>     <div style=\"padding-top: 1rem; text-align: right;\">         <b><em>- Tim Berners-Lee, 1999</em></b>     </div> </div>  Berners-Lee's vision is fascinating, in particular the *\"intelligent agents\"* referred to in his quote above. Generally speaking, these intelligent agents (IAs) perceive  their environment, take actions based on that environment to achieve a particular goal, and improve their performance with self-learning.  These IAs sound very much like the ML models we see today. Some models can look at a web page's content (its environment), scrape and classify the meaning of this content (takes actions to achieve goals), and do this through an iterative learning process.  A model that can read and comprehend the *meaning* of language from the internet is a vital component of the semantic web. There are already models that can do this within a limited scope.  However, there is a problem. These **L**anguage **M**odels (LMs) need to learn before becoming these autonomous, language-comprehending IAs. They must be *trained*.  Training LMs is hard; they need vast amounts of data. In particular, bi-encoder models (as explored in earlier chapters on [AugSBERT](/learn/data-augmentation/) and [GenQ](/learn/genq/)) that can enable a large chunk of this semantic web are notoriously data-hungry.  Sometimes this is okay. We can fine-tune a model easily in places where we have massive amounts of relevant and labeled data. We can use a simple [supervised fine-tuning approach](/learn/fine-tune-sentence-transformers-mnr/). Unfortunately, these scenarios are few and far between. It is for this reason that existing models have this limited scope. So, what can we do?  We should first consider why it is hard to get data to train these models. On one hand, the internet is full of data, and, on the other, this data is *not* in the format we need. We usually need to use a supervised training method to train a high-performance bi-encoder model.  Supervised training methods require labeled data. The problem with labeled data is that a human must (almost always) manually create it.  Currently, we have data-hungry models that require supervised training methods. We must find a way to train a model with *little* labeled data or use *unsupervised* methods that need nothing more than unstructured text data.  Fortunately, there are *some* unsupervised (or supervised using very little data) approaches, such as:  * [Multilingual Knowledge Distillation](/learn/multilingual-transformers/#training-approaches) for low-resource languages. * [TSDAE](/learn/unsupervised-training-sentence-transformers/) for building simple similarity models without labeled data. * Data augmentation with AugSBERT for [in-domain](/learn/data-augmentation/) and [out-of-domain](/learn/domain-transfer/) tasks. * [GenQ](/learn/genq/) for asymmetric semantic search without labeled data.  We can apply these approaches in different scenarios with varying degrees of success. As we've seen, there is a lot of potential for models being trained using unsupervised techniques. These no (or low) resource scenarios cover the vast majority of use-cases, many of which are the most unique and interesting.  ![datasets_and_uniqueness](/images/gpl-1.png) <small>As the domain (eg topic, language) becomes more niche, the number of available labeled datasets decreases. The vast majority of domains have no labeled datasets.</small>  For example, we may identify an opportunity to introduce semantic search on internal financial documents with highly technical language specific to our organization. Or a specific use-case using a less common language such as Swahili or Dhivehi.  It is infeasible for the semantic web to find labeled data for every topic, language, or format of information found on the internet. Because of this, the dream only becomes a reality once there are techniques that can train or adapt *high-performance* IAs with nothing more than the text found on the internet, without human-made labels or curation.  There is research producing techniques placing us ever closer to this reality. One of the most promising is GPL [2]. GPL is almost a culmination of the techniques listed above. At its core, it allows us to take unstructured text data and use it to build models that can understand this text. These models can then intelligently respond to natural language queries regarding this same text data.  It is a fascinating approach, with massive potential across innumerous use cases spanning all industries and borders. With that in mind, let's dive into the details of GPL and how we can implement it to build high-performance LMs with nothing more than plain text.  ---  Watch our webinar [Searching Freely: Using GPL for Semantic Search](https://www.youtube.com/watch?v=OQhoi1CabWw) for a rundown of GPL presented by Nils Reimers, the creator of *sentence-transformers*.  ---  ## GPL Overview  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/uEbCXwInnPs\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  GPL can be used in two ways, as a technique used in fine-tuning a pretrained model (such as the BERT base model), or as a technique for domain adaptation of an already fine-tuned bi-encoder model (such as SBERT).  By *domain adaptation* we mean the adaptation of an existing [sentence transformer](/learn/sentence-embeddings/) to new topics (domains). Effective domain adaptation is incredibly helpful in taking models pretrained on large, existing datasets, and helping them understand a new domain that lacks any labeled datasets.  For example, any model trained on data from before 2019 will be blissfully unaware of COVID-19 and everything that comes with it. If we query any of these pre-2019 models about COVID, they will struggle to return relevant information as they simply do not know what it is, and what is relevant.  {{< notebook file=\"gpl-old-model-examples\" height=\"full\" >}}  GPL hopes to solve this problem by allowing us to take existing models and *adapt* them to new domains using nothing more than unlabeled data. By using unlabeled data we greatly enhance the ease of finding relevant data, all we need is unstructured text.  ---  Just looking for a fast implementation of GPL? Skip ahead to *[\"A Simpler Approach\"](/learn/gpl/#a-simpler-approach)*.  ---  As you may have guessed, the same applies to the first scenario of *fine-tuning a pretrained model*. It can be hard to find relevant, labeled data. With GPL we don't need to. Unstructured text is all you need.  ### How it Works  At a high level, GPL consists of *three* data preparation steps and *one* fine-tuning step. We will begin by looking at the data preparation portion of GPL. The three steps are:  * Query generation, creating queries from passages. * Negative mining, retrieving similar passages that do not match (negatives). * Pseudo labeling, using a cross-encoder model to assign similarity scores to pairs.  ![gpl_overview](/images/gpl-2.jpg) <small>Overview of the GPL process. Beginning with passages *P<sup>+</sup>*, we generate queries *Q*. The passages are indexed and a dense retrieval step is used to find high similarity *'negative'* passages *P<sup>-</sup>*. We then use a cross encoder to produce margin scores.</small>  Each of these steps requires the use of a pre-existing model fine-tuned for each task. The team that introduced GPL also provided models that handle each task. We will discuss these models as we introduce each step and note alternative models where relevant.  #### 1. Query Generation  GPL is perfect for scenarios where we have no labeled data. However, it does require a *large amount* of unstructured text. That could be text data scraped from web pages, PDF documents, etc. The only requirement is that this text data is *in-domain*, meaning it is relevant to our particular use case.  ![in_and_out_domain](/images/gpl-3.jpg) <small>For a target domain of *German financial documents*, any data that fits the topic and we would expect our model to encounter is *in-domain*. Anything else is *out-of-domain*.</small>  In our examples, we will use the *CORD-19* dataset. CORD-19 can be downloaded using the script [found here](https://gist.github.com/jamescalam/06882ea05a307420c1354481325f08ad#file-00_download_cord_19-ipynb). The script will leave many JSON files in a directory called *document_parses/pdf_json* that we will be using in our query generation step. We will use a generator function called `get_text` to read in those files.  {{< notebook file=\"gpl-passage-generator\" height=\"full\" >}}  If you've read our previous [chapter on GenQ](/learn/genq/), this step follows the same query generation process. However, we will outline the process for new readers.  We're starting with passages of data (the unstructured text data). Generally, these are reasonably long chunks of text, but not always.  {{< notebook file=\"gpl-passage-gen-test\" height=\"full\" >}}  Given a passage, we pass it through a *query generation* T5 model. We can initialize this T5 model using *HuggingFace Transformers*.  ---  *T5 refers to Google’s Text-to-Text Transfer Transformer. We discuss it in more detail in [the chapter on GenQ](/learn/genq/).*  ---   {{< notebook file=\"gpl-gen-model-init\" height=\"full\" >}}  We are using the `doc2query/msmarco-t5-base-v1` model that was trained on a pre-COVID dataset. Nonetheless, when generating queries for COVID-related text the model can produce sensible questions by copying words from the passage text.  With this T5 model, we can begin generating queries that we use to produce synthetic *(query, passage) pairs*.  {{< notebook file=\"gpl-query-gen-example\" height=\"full\" >}}  Query generation is *not perfect*. It can generate noisy, sometimes nonsensical queries. And this is where GPL improved upon GenQ. GenQ relies heavily on these synthetic queries being high-quality with little noise. With GPL, this is not the case as the final cross-encoder step labels the similarity of pairs. Meaning dissimilar pairs are likely to be labeled as such. GenQ does not have any such labeling step.  We now have *(query, passage) pairs* and can move onto the next step of identifying *negative* passages.  *[Full script](https://gist.github.com/jamescalam/8498a03b66b0317d21575c0a0ac50f66#file-01_query_gen-ipynb)*  ### 2. Negative Mining  The (query, passage) pairs we have now are assumed to be positively similar, written as *(Q, P<sup>+</sup>)* where the query is *Q*, and the positive passage is *P<sup>+</sup>*.  Suppose we fine-tune our bi-encoder on only these positive matches. In that case, our model will struggle to learn more nuanced differences. A good model must learn to distinguish between similar and dissimilar pairs even where the content of these different pairs is very similar.  To fix this, we perform a *negative mining* step to find highly similar passages to existing *P<sup>+</sup>* passages. As these new passages will be highly similar but *not* matches to our query *Q*, our model will need to learn how to distinguish them from genuine matches *P<sup>+</sup>*. We refer to these non-matches as *negative passages* and are written as *P<sup>-</sup>*.  The negative mining process is a retrieval step where, given a query, we return the *top_k* most similar results. Excluding the positive passage (if returned), we assume all other returned passages are negatives. We then select one of these *negative passages* at random to become the negative pair for our query.  It may seem counterintuitive at first. Why would we return the most similar passages and train a model to view these as dissimilar?  Yes, those returned results are the most similar passages to our query, but they are *not* the correct passage for our query. We are, in essence, increasing the similarity gap between *the* correct passage and all other passages, no matter how similar they may be.  Adding these *'negative'* training examples *(Q, P<sup>-</sup>)* is a common approach used in many bi-encoder fine-tuning methods, including multiple negatives ranking *and* margin MSE loss (the latter of which we will be using). Using hard negatives in-particular can significantly improve the performance of our models [3].  ![gpl_random_negatives](/images/gpl-4.png) <small>The impact on model performance trained on MSMARCO with and without hard negatives. Model training used margin MSE loss. Adapted from [3].</small>  When we later tune our model to identify the difference between these positive and negative passages, we are teaching it to determine what are often very nuanced differences.  With all of that in mind, we do need to understand that only some of the returned passages will be relevant. We will explain how that is handled in the Pseudo-labeling step later.  Moving on to the implementation of negative mining. As before, we need an existing model to embed our passages and create searchable [dense vectors](/learn/vector-database/). We use the `msmarco-distilbert-base-tas-b` bi-encoder which was fine-tuned on pre-COVID datasets.  {{< notebook file=\"gpl-neg-mine-model\" height=\"full\" >}}  In the GPL paper, two retrieval models are used and their results compared. To keep things simple, we will stick with a single model.  We need a [vector database](/learn/vector-database/) to store the passage embeddings. We will use Pinecone as an incredibly easy-to-use service that can scale to the millions of passage embeddings we'd like to search.  {{< notebook file=\"gpl-neg-mine-pinecone\" height=\"full\" >}}  We encode our passages, assign unique IDs to each, and then upload the record to Pinecone. As we later need to match these returned vectors back to their original plaintext format, we will create an ID-to-passage mapping to be stored locally.  {{< notebook file=\"gpl-neg-mine-encode-store\" height=\"full\" >}} <small>[Full version here](https://gist.github.com/jamescalam/9b84408d6c7f1fe4bf7eda2ab410c086#file-02_negative_mining-ipynb).</small>  The vector database is set up for us to begin *negative mining*. We loop through each query, returning *10* of the most similar passages by setting `top_k=10`.  {{< notebook file=\"gpl-neg-mine-query\" height=\"full\" >}}  We then loop through each set of queries, *P<sup>+</sup>* passages, and their negatively mined results. Next, we shuffle those results and return the first that does *not* match to our *P<sup>+</sup>* passage, this becomes the *P<sup>-</sup>* passage. We write each record to file in the format *(Q, P<sup>+</sup>, P<sup>-</sup>)*, ready for the next step.  *[Full script](https://gist.github.com/jamescalam/9b84408d6c7f1fe4bf7eda2ab410c086#file-02_negative_mining-ipynb)*  #### 3. Pseudo-labeling  Pseudo-labeling is the final step in *preparing* our training data. In this step, we use a cross encoder model to generate similarity scores for both positive and negative pairs.  ![sim(Q, P+)](/images/gpl-8.png)  <center>and</center>  ![sim(Q, P-)](/images/gpl-9.png)  <br>  {{< notebook file=\"gpl-ce-scoring\" height=\"full\" >}}  Given a *positive* and *negative* query-passage similarity score (GPL uses dot-product similarity), we then take the difference between both scores to give the *margin* between both.  ![margin = sim(Q, P+) - sim(Q, P-)](/images/gpl-10.png)  We calculate the margin between the two similarity scores to train our bi-encoder model using *margin MSE loss*, which requires the margin score. After generating these scores, our final data format contains the query, both passages, and the margin score.  ![(Q, P+, P-)](/images/gpl-11.png)  This final Pseudo-labeling step is very important in ensuring we have high quality training data. Without it, we would need to assume that all passages returned in the negative mining step are irrelevant to our query and must share the same dissimilarity when contrasted against our positive passages.  In reality this is never the case. Some negative passages are more relevant than others. The authors of GPL split these negative passages into three categories [2].  ![why_pseudo_label](/images/gpl-5.png) <small>Three categories of negative passages. Whereas previous methods like GenQ that lack the pseudo-labeling step would view passages as either positive `1` or negative `0`, GPL can score passages on a more meaningful scale.</small>  We are likely to return a mix of negative passages, from highly relevant to completely irrelevant. Pseudo-labeling allows us to score passages accordingly. Above we can see three negative categories:  * **False negatives**: we haven't returned the *exact* match to our positive passage, but that does not mean we will not return relevant passages (that are in fact *not* negatives). In this case our cross-encoder will label the passage as relevant, without a cross-encoder this would be marked as irrelevant. * **Easy negatives**: these are passages that are loosely connected to the query (such as containing matching keywords) but are *not* relevant. The cross-encoder should mark these as having low relevance. * **Hard negatives**: in this case the passages may be tightly connected to the query, or even contain a partial answer, but still not answer the query. Our cross-encoder should mark these as being more relevant than *easy negatives* but less so than any positive or false negative passages.  Now that we have our fully prepared data, we can move on to the training portion of GPL.  *[Full script](https://gist.github.com/jamescalam/f9609f0e47937c3545364cfbef3ea1b8#file-03_ce_scoring-ipynb)*  ## Training with Margin MSE  The fine-tuning/training portion of GPL is not anything unique or new. It is a tried and tested bi-encoder training process that optimizes with *margin MSE loss*.  ![margin MSE loss function](/images/gpl-12.png)  We are looking at the sum of squared errors between the predicted margin *𝛿<sup>^</sup><sub>i</sub>* and the true margin *𝛿<sub>i</sub>* for *all samples* in the training set (from *i=0* to *i=M-1*). We make it a *mean* squared error by dividing the summed error by the number of samples in the training set *M*.  Looking back at the generated training data, we have the format *(Q, P<sup>+</sup>, P<sup>-</sup>, margin)*. How do these fit into the *margin MSE loss* function above?  ![gpl_margin_mse_loss](/images/gpl-6.png) <small>High-level view of *(Q, P<sup>+</sup>, P<sup>-</sup>)* triplets and how they fit into the margin MSE loss function.</small>  The bi-encoder model creates embeddings for the query *Q*, positive passage *P<sup>+</sup>, and negative passage *P<sup>-</sup>*. We then calculate the dot-product similarity between embeddings for both *sim(Q, P<sup>+</sup>)* and *sim(Q, P<sup>-</sup>)*. These give us the predicted margin:  ![delta hat = sim(Q, P+) - sim(Q, P-)](/images/gpl-13.png)  The true margin *𝛿<sub>i</sub>* has already been calculated by our cross-encoder, it is simply *𝛿<sub>i</sub> = margin*.  We can use the default *sentence-transformers* methods for fine-tuning models with margin MSE loss. We begin by loading our pairs into a list of `InputExample` objects.  {{< notebook file=\"gpl-train-input-examples\" height=\"full\" >}}  We can see the contents of one of our `InputExample` objects:  {{< notebook file=\"gpl-train-example\" height=\"full\" >}}  We use a generic PyTorch `DataLoader` to load the data into the model during training. One crucial detail is that margin MSE loss works best with large batch sizes. A batch size of *32* or even *64* is a good target, but this does require significant GPU memory and may not be feasible. If that is the case, reduce the batch size until it fits within your hardware restraints.  {{< notebook file=\"gpl-train-dataloader\" height=\"full\" >}}  Next, we initialize a bi-encoder model using the pre-COVID DistilBERT bi-encoder that we used in the negative mining step. It is this model that we are adapting to better understand COVID-19 related language.  {{< notebook file=\"gpl-train-model-init\" height=\"full\" >}}  We're ready to initialize the margin MSE loss that will optimize the model later.  {{< notebook file=\"gpl-train-loss\" height=\"full\" >}}  With that, we're finally ready to begin fine-tuning our model. We used a single epoch, with a training set of 600K samples this is a large number of steps. It was found that GPL performance tends to stop improving after around 100K steps [2]. However, this will vary by dataset.  ![gpl_perf_by_steps](/images/gpl-7.png) <small>NDCG@10% performance for zero-shot (not adapted), GPL fine-tuned, and GPL fine-tuned + [TSDAE pre-trained models](/learn/unsupervised-training-sentence-transformers/). GPL fine-tuning using a model that had previously been pretrained using TSDAE demonstrates consistently better performance. Model performance seems to level-out after 100K training steps. Visual adapted from [2].</small>  <br>  {{< notebook file=\"gpl-train-fit-1\" height=\"full\" >}}  Once training is complete, we will find all of our model files in the *msmarco-distilbert-base-tas-b-covid* directory. To use our model in the future, we simply load it from the same directory using *sentence-transformers*.  {{< notebook file=\"gpl-train-fit-1\" height=\"full\" >}}  If you'd like to use the model trained in this article, you can specify the model name `pinecone/msmarco-distilbert-base-tas-b-covid`.  Now let's return to the COVID-19 queries we asked the initial model (without GPL adaptation).  {{< notebook file=\"gpl-new-model-1epoch\" height=\"full\" >}}  As before we are asking four questions, each of which has three possible passages. Our model is tasked with scoring the similarity of each passage, the goal is to return COVID-19 related sentences higher than any other sentences.  We can see that this has worked for two of our queries. For the two queries it performs worse on, it looks like our GPL trained model is confusing the drink Corona with *\"corona\"* in the context of COVID-19.  What we can do is try and fine-tune our model for more epochs, if we try again with a model trained for 10 epochs we get more promising results.  {{< notebook file=\"gpl-new-model10\" height=\"full\" >}}  Now we see much better results and our model is more easily differentiating between the Corona beer, and COVID-19.  *[Full script](https://gist.github.com/jamescalam/15d968bed79b884bf50090a34093f508#file-04_finetune-ipynb)*  ## A Simpler Approach  We've worked through a lot of theory and code to understand GPL, and hopefully, it is now much clearer. However, we don't need to work through all of that to apply GPL. It is much easier when using the [official GPL library](https://github.com/UKPLab/gpl).  Doing the same as we did before requires little more than a few lines of code. To start, we first `pip install gpl`. Our input data must use the BeIR data format, a single JSON lines (`.jsonl`) file called *corpus.jsonl*. Each sample in the file will look like this:  <script src=\"https://gist.github.com/jamescalam/99a8f865f2fc75db2e834507572d169b.js\"></script>  Our CORD-19 dataset is not initially in the correct format, so we must first reformat it.  ```python from tqdm.auto import tqdm import json import os  # create directory if needed if not os.path.exists('./cord_data'): os.mkdir('./cord_data')  id_count = 0  with open('./cord_data/corpus.jsonl', 'w') as jsonl:     for path in tqdm(paths):         # read each json file in the CORD-19 pdf_json directory         with open(path, 'r') as fp:             doc = json.load(fp)         # extract the passages of text from each document         for line in doc['body_text']:             line = {                 '_id': str(id_count),                 'title': \"\",                 'text': line['text'].replace('\\n', ' '),                 'metadata': doc['metadata']             }             id_count += 1     # iteratively write lines to the JSON lines corpus.jsonl file     jsonl.write(json.dumps(line)+'\\n') ```  Now we will have a new *corpus.jsonl* file in the *cord_data* directory. The first sample from the newly formatted CORD-19 dataset looks similar to this:  <script src=\"https://gist.github.com/jamescalam/77b050c3f9c3029ad5e3ed813b2273a3.js\"></script>  With that newly formatted dataset, we can run the whole GPL data generation and fine-tuning process with a single, slightly lengthy function call.  ```python import gpl  gpl.train(     path_to_generated_data='./cord_data',     base_ckpt='distilbert-base-uncased',     batch_size_gpl=16,     gpl_steps=140_000,     output_dir='./output/cord_model',     generator='BeIR/query-gen-msmarco-t5-base-v1',     retrievers=[         'msmarco-distilbert-base-v3',         'msmarco-MiniLM-L-6-v3'     ],     cross_encoder='cross-encoder/ms-marco-MiniLM-L-6-v2',     qgen_prefix='qgen',     do_evaluation=False ) ```  Let's break all of this down. We have:  * `path_to_generated_data` - the directory containing *corpus.jsonl*. * `base_ckpt` - the starting point of the bi-encoder model that we will be fine-tuning. * `batch_size_gpl` - batch size for the margin MSE loss fine-tuning step. * `gpl_steps` - number of training steps to run for the MSE margin loss fine-tuning. * `output_dir` - where to save the fine-tuned bi-encoder model. * `generator` - the query generation model. * `retrievers` - a list of retriever models to use in the negative mining step. * `cross_encoder` - the cross encoder model used for pseudo-labeling. * `qgen_prefix` - the query generation data files prefix. * `do_evaluation` - whether to evaluate the model on an evaluation dataset requires an evaluation set to be provided.  After running this, which can take some time, we have a bi-encoder fine-tuned using GPL on nothing more than the passages of text passed from the *./cord_data/corpus.jsonl* file.  Using the GPL library is a great way to apply unsupervised learning. When compared to our more in-depth process, it is *much* simpler. The one downside is that the negative mining step uses [exhaustive search](/learn/faiss-tutorial/). This type of search is no problem for smaller corpora but becomes slow for larger datasets (100K–1M+) and, depending on your hardware, impossible for anything too large to be stored in memory.  That's it for this chapter on **G**enerative **P**seudo-**L**abeling (GPL). Using this impressive approach, we can fine-tune new or existing models in domains that were previously inaccessible due to little or no labeled data.  The research on unsupervised training methods for bi-encoder models continues to progress. GPL is the latest in a series of techniques that extends the performance of these exciting models trained without labeled data.  What is possible with GPL is impressive. Perhaps even more exciting is the possibility of further improvements to GPL or completely new methods that take the performance of these unsupervised training methods to even greater heights.   ## References  [Code Notebooks](https://github.com/pinecone-io/examples/tree/master/analytics-and-ml/model-training/gpl)  [1] T. Berners-Lee, M. Fischetti, [Weaving the Web, The Original Design and Ultimate Destiny of the World Wide Web by Its Inventor](https://www.w3.org/People/Berners-Lee/Weaving/) (1999)  [2] K. Wang, et al., [GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval](https://arxiv.org/abs/2112.07577)  [3] Y. Qu, et al., [RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2010.08191) (2021), NAACL",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2ee"
  },
  "title": "\"How Machine Learning is Accelerating Life Sciences\"",
  "headline": "\"How Machine Learning is Accelerating Life Sciences\"",
  "weight": "6",
  "name": "Diego Lopez Yse",
  "position": "Data Scientist",
  "src": "/images/diego-lopez-yse.jpeg",
  "href": "\"https://lopezyse.medium.com/\"",
  "description": "Machine learning (ML) is rapidly transforming the life sciences. Applied ML is improving diagnostics with computer vision (CV) and accelerating drug discovery.",
  "images": "['/images/ml-life-sciences-0.jpg']",
  "content": "Moore’s Law predicts that computing will dramatically increase in power and decrease in relative cost at an [exponential pace](https://www.intel.com/content/www/us/en/silicon-innovations/moores-law-technology.html). Although this principle mainly applies to computing hardware, DNA sequencing cost has followed a similar pattern for many years, approximately halving every two years. But since January 2008 there has been a break in that trend, with sequencing costs dropping much faster than the cost of processing data on computers. The cost of getting DNA data has never been cheaper, and it will continue to decrease.  ![Cost per human genome](/images/ml-life-sciences-0.jpg) <small>The sudden and profound out-pacing of Moore's Law beginning in January 2008. Source: [National Human Genome Research Institute](https://www.genome.gov/about-genomics/fact-sheets/DNA-Sequencing-Costs-Data)</small>  The result of this is that a data tsunami is coming into Life Sciences, with estimations that over [60 million patients](https://www.biorxiv.org/content/10.1101/203554v1.full) will have their genome sequenced in a healthcare context by 2025, and that [genomics research will generate](https://www.genome.gov/about-genomics/educational-resources/fact-sheets/artificial-intelligence-machine-learning-and-genomics) between 2 and 40 **exabytes** of data within the next decade.  ![Number of datasets](/images/ml-life-sciences-1.png) <small>DNA sequencing and other biological techniques will continue to increase the number and complexity of genomic data sets. Source: [National Human Genome Research Institute](https://www.genome.gov/about-genomics/fact-sheets/DNA-Sequencing-Costs-Data)</small>  But it’s not just about data volume. Massive computing power has enabled researchers from the University of Illinois to develop a [software to simulate a 2-billion-atom cell](https://blogs.nvidia.com/blog/2022/01/20/living-cell-simulation/) that metabolizes and grows like a living cell. Cell simulation provides insights into the physical and chemical processes that form the foundation of living cells, where fundamental behaviors emerge not because they were programmed in, but because the model contained the correct parameters and mechanisms.  Life Sciences is going through an accelerated transformation, and Machine Learning (ML) is responsible for it. On top of that, during the Covid-19 pandemic Life Sciences companies were forced to mobilize their resources to respond quickly to public health demands, which caused a spike of new computational methods and ways of thinking.  ## Applied Machine Learning in Life Sciences  Massive data volumes plus improved computation have eased the path to ML models that can solve new challenges in Life Sciences. From the big universe of potential applications, some that deserve special attention are:  - **Improved diagnostics**, since diagnosis is the most fundamental step in the treatment of any patient. - **Drug discovery**, as the biggest goal of the Life Sciences industry is to advance the research and innovation for new products and treatments.  ### Improving diagnostics with Computer Vision  Computer Vision (CV) focuses on image and video understanding, involving [tasks such as](https://viso.ai/applications/computer-vision-in-healthcare/) object detection, image classification, and segmentation. In Life Sciences, CV models fed with medical imaging (e.g. MRI, X-rays, etc) can assist in the visualization of cells, tissues and organs to enable a more accurate diagnosis, helping to identify any issues or abnormalities.  There’s huge [versatility within imaging sources](https://bmcmedimaging.biomedcentral.com/articles/10.1186/s12880-022-00793-7), since computed tomography (CT) scans and magnetic resonance imaging (MRI) are capable of generating 3D image data, while digital microscopy can generate terabytes of whole slide image (WSI) of tissue specimens.  One example is the UC San Diego Health, which applies CV models to quickly [detect pneumonia](https://health.ucsd.edu/news/releases/Pages/2020-04-07-artificial-intelligence-enables-rapid-covid-19-lung-imaging-analysis.aspx) through X-rays imagery, which are cheaper and faster than other methods. But adding [Transfer Learning (TL)](/learn/transfer-learning/) to these types of problems can increase the performance and [accuracy](https://www.mdpi.com/2076-3417/11/3/1242) of a diagnostic approach that medical professionals can easily use as an auxiliary tool. In CV, [embeddings](https://www.pinecone.io/learn/vector-embeddings/) are often used as [a way to translate knowledge between different contexts](https://www.featureform.com/post/the-definitive-guide-to-embeddings), allowing us to exploit pre-trained models like VGG, AlexNet or ResNet.  ![Pretrained models](/images/ml-life-sciences-2.jpg) <small>In this example, three-types of pre-trained models (ResNet152, DenseNet121 and ResNet18) act as feature extractors. Redefining a classifier for a new task and applying an attention mechanism as a feature selector can improve accuracy over other models. Source: [Attention-Based Transfer Learning for Efficient Pneumonia Detection in Chest X-ray Images](https://www.mdpi.com/2076-3417/11/3/1242)</small>  Due to the need for large datasets to train and tune Deep Learning architectures for CV which are not available for medical images, TL coupled with embeddings can be used to achieve tasks that go from [ocular disease recognition](https://opg.optica.org/boe/fulltext.cfm?uri=boe-8-2-579&id=357053) to [cancer detection](https://www.aimspress.com/article/doi/10.3934/mbe.2021256).  ![Convolutional Neural Network](/images/ml-life-sciences-3.png) <small>A Convolutional Neural Network (CNN) component acts as a feature extractor that takes a grid of patches as input, and encodes each patch as a fixed-length vector representation (i.e. embedding). Source: [Cancer Metastasis Detection With Neural Conditional Random Field](https://openreview.net/pdf?id=S1aY66iiM)</small>  To solve CV challenges, the [classic approach](https://link.springer.com/article/10.1007/s10278-022-00666-z) has been to use TL with pre-trained convolutional neural networks (CNNs) on natural images (e.g., ResNet), tuned on medical images. Today, due to their powerful TL abilities, pre-trained [Transformers](/learn/transformers/) (which are self-attention-based models) are becoming standard models to improve results on CV tasks.  ### Drug discovery  Drug discovery is the process of finding new or existing molecules with specific chemical properties for the treatment of diseases. Since this has traditionally been an extremely long and expensive process, modern predictive models based on ML have gained popularity for their potential to drastically reduce costs and research times.  ML can be used the in the [drug discovery processes to](https://www.neuraldesigner.com/solutions/drug-design):  - **Discover structure patterns**: to study the surface properties, molecular volumes or molecular interactions. - **Identify behavior influences**: to relate the orientation of the molecule to its characteristics. - **Anticipate characteristics**: to develop models capable of predicting the behavior of a molecule in accordance with its design. - **Improve drug designs**: to get better results and design better medicines while reducing costs.  This is what companies like Sanofi are doing in order to [reduce the sheer number of compounds they need to synthesize](https://www.fiercebiotech.com/biotech/next-gen-sanofis-platforms-chief-sets-out-ambition-cut-drug-discovery-years-ai) in the real world by doing much of the analysis on a computer.  How do you represent molecules in the data space? From the [several representation methods available](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00460-5), **embedding** methods like [Mol2Vec](https://chemrxiv.org/engage/chemrxiv/article-details/60c73d3bbdbb89110aa37c15) have emerged as novel approaches to learn high-dimensional representations of molecular substructures. Inspired by word embedding techniques known as Word2Vec, Mol2Vec encodes molecules into a set of vectors that represent similar substructures in proximity to one another in the vector space. In a [Natural Language Processing](https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1) (NLP) analogous fashion, molecules are considered as sentences and substructures as words.  ![Mol2vec vectors](/images/ml-life-sciences-4.jpg) <small>Mol2vec vectors of amino acids (bold arrows). These vectors were obtained by summing the vectors of the Morgan substructures (small arrows) present in the respective molecules (amino acids in the present example). The directions of the vectors provide a visual representation of similarities. Magnitudes reflect importance, i.e. more meaningful words. Source: [Oxford Protein Informatics Group](https://pubs.acs.org/doi/10.1021/acs.jcim.7b00616)</small>  But molecules can also be represented as graphs. **Graphs** are a ubiquitous data structure, employed extensively within computer science and related fields. Social networks, molecular graph structures, biological protein-protein networks, recommender systems — all of these domains and many more can be readily modeled as graphs, which capture interactions (i.e., edges) between individual units (i.e., nodes).  ![Nodes and edges](/images/ml-life-sciences-5.png) <small>The **nodes** can be described as the vertices that correspond to objects. The **edges** can be referred to as the connections between objects. Source: [Java T Point](https://www.javatpoint.com/directed-and-undirected-graph-in-discrete-mathematics)</small>  Intuitively, one could imagine treating the atoms in a molecule as _nodes_ and the bonds as _edges_. Nodes, edges, subgraphs or entire graphs can be embedded into [low-dimensional vectors that summarize the graph position and the structure of their local graph neighborhood](https://www-cs.stanford.edu/people/jure/pubs/graphrepresentation-ieee17.pdf). These low-dimensional embeddings can be viewed as encoding or projecting graph information into a latent space, where geometric relations in this latent space correspond to interactions in the original graph.  ![Molecular graphs](/images/ml-life-sciences-6.png) <small>Embedding molecular graphs into low dimensional space to determine if they are benign or toxic. Source: [A Large-Scale Database for Graph Representation Learning](https://scottfreitas.medium.com/a-large-scale-database-for-graph-representation-learning-c096ab19aa4d)</small>  The idea behind using graph embeddings is to create insights that are not directly evident by looking at the explicit relationships between nodes.  ## The future  The data explosion and initiatives in Life Sciences have the potential to reshape the future of the industry and of patient care, as we witness how ML methods can do amazing things if you give them enough data. Just look at what DeepMind announced only some weeks ago, [releasing the predicted structures for almost every protein](https://www.chemistryworld.com/news/alphafold-has-predicted-the-structures-of-almost-every-known-protein/4016033.article) known to science (over 200 million structures in total), using its AI AlphaFold 2.  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/ml-life-sciences.mp4\" type=\"video/mp4\"> </video>  <small class=\"video\">You can see one prediction Alpha Fold’s model created. In comparison to the time it takes in the lab, this model is able to make a prediction in a mere half an hour with 90% accuracy according to their [statement](https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology). Source: [Communicating Science](https://blogs.ubc.ca/communicatingscience20t2w212/2021/02/26/protein-folding-solved-2/)</small>  But it’s both the volume and diversity of data that force us to rethink how to solve problems in Life Sciences with ML. Life Sciences is demanding us to integrate all sorts of different data types to reach better results, while giving us a glimpse of what’s coming next for all industries: **a multimodal future**.  Consider Electronic Health Records (EHRs), which offer an [efficient way to maintain patient information](https://arxiv.org/ftp/arxiv/papers/2111/2111.04898.pdf) and are becoming more and more widely used by healthcare providers around the world. EHRs can include data that go from images, clinical notes, medication lists, vital signs, to demographic information, which can provide deep insights of a patient’s condition if integrated in an effective manner.  Efforts to integrate these data types are already ongoing, and multimodal ML models trained on numerous types of data could help health professionals to screen patients at risk of developing diseases like cancer more accurately. This is what [Harvard University](https://www.theregister.com/2022/08/09/ai_cancer_multimodal/) is researching, by training ML models with microscopic views of cell tissues from whole-slide images (WSIs) and text-based genomics data. Just imagine what other challenges can be faced when integrating image, sequential, text, 3D, graph and other types of data into the same information space.  ![Combining data](/images/ml-life-sciences-7.jpg) <small>Combining data collected from both home (left) and clinical settings (right), or combining predictive models built at home and in the clinic, has the potential to lead to comprehensive and integrated models that support personalized health management. Comprehensive models are more likely to perform well as they incorporate more information about an individual, and these models have the potential to be applied in the home, clinic, or wherever an individual may be. Source: [NCBI](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7141410/)</small>  ML will transform Life Sciences, and allow us to dream big in solving some of the most transcendental challenges for humanity, like eliminating aging or hyper-personalized healthcare. This is such a powerful Artificial Intelligence (AI) application, that the UK government has established it as a [Strategic Grand Challenge](https://www.gov.uk/government/publications/industrial-strategy-the-grand-challenges/industrial-strategy-the-grand-challenges), and people like [Vitalik Buterin](https://www.lifespan.io/news/vitalik-buterin-the-best-thing-to-donate-money-to-is-the-fight-against-aging/) (the creator of the cryptocurrency Ethereum) and [Jeff Bezos](https://www.pharmaceutical-technology.com/analysis/billionaires-anti-ageing-research/) (founder of Amazon) invested part of their fortune in this idea. From [personalized medicine](https://www.nesfircroft.com/blog/2022/01/how-is-ai-pushing-the-life-science-industry-forward?source=google.com) to [democratizing healthcare](https://www.enago.com/academy/big-10-ways-artificial-intelligence-transforming-life-sciences/) in developing regions, applied ML in Life Sciences can deeply change our lives. ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2f0"
  },
  "title": "Text-to-Image and Image-to-Image Search Using CLIP",
  "headline": "Text-to-Image and Image-to-Image Search Using CLIP",
  "weight": "4",
  "name": "Zoumana Keita",
  "position": "Data Scientist",
  "src": "/images/zoumana-keita.jpg",
  "href": "\"https://www.linkedin.com/in/zoumana-keita/\"",
  "description": "A complete overview of using the OpenAI's CLIP for multi-modal search",
  "images": "[\"/images/clip-image-search-6.png\"]",
  "content": "## Introduction  Industries today deal with ever increasing amounts of data. Especially in retail, fashion, and other industries where the image representation of products plays an important role.  In such a situation, we can often describe one product in many ways, making it challenging to perform accurate and least time-consuming searches.  _Could I take advantage of state-of-the-art artificial intelligence solutions to tackle such a challenge?_  This is where [OpenAI’s CLIP](/learn/clip) comes in handy. A deep learning algorithm that makes it easy to connect text and images.  After completing this conceptual blog, you will understand: (1) what CLIP is, (2) how it works and why you should adopt it, and finally, (3) how to implement it for your own use case using both local and cloud-based vector indexes.  ## What is CLIP?   Contrastive Language-Image Pre-training (CLIP for short) is a state-of-the-art model introduced by OpenAI in February 2021 [1].  CLIP is a neural network trained on about 400 million (text and image) pairs. Training uses a contrastive learning approach that aims to unify text and images, allowing tasks like image classification to be done with text-image similarity.  This means that CLIP can find whether a given image and textual description match without being trained for a specific domain. Making CLIP powerful for out-of-the-box text and image search, which is the main focus of this article.   Besides text and image search, we can apply CLIP to image classification, image generation, image similarity search, image ranking, object tracking, robotics control, image captioning, and more.  ## Why should you adopt the CLIP models?  Below are some reasons that increased the adoption of the CLIP models by the AI community  ### Efficiency  The use of the contrastive objective increased the efficiency of the CLIP model by 4-to-10x more at zero-shot ImageNet classification.  Also, the adoption of the Vision Transformer created an additional 3x gain in compute efficiency compared to the standard ResNet.  ![The efficiency of CLIP at zero-shot transfer](/images/clip-image-search-1.png) <small>The efficiency of CLIP at zero-shot transfer ([source](https://arxiv.org/pdf/2103.00020v1.pdf))</small>  ### More general & flexible  CLIP outperforms existing ImageNet models in new domains because of its ability to learn a wide range of visual representations directly from natural language.  The following graphic highlights CLIP zero-shot performance compared to ResNet models few-shot linear probe performance on fine-grained object detection, geo-localization, action recognition, and optical character recognition tasks.   ![Average linear probe score across 27 datasets](/images/clip-image-search-2.png) <small>Average linear probe score across 27 datasets ([source](https://openai.com/blog/clip/))</small>  ## CLIP Architecture  CLIP architecture consists of two main components: (1) a text encoder, and (2) an Image encoder. These two encoders are jointly trained to predict the correct pairings of a batch of training (image, text) examples.  - The *text encoder’s* backbone is a [transformer](https://arxiv.org/abs/1706.03762) model [2], and the base size uses 63 millions-parameters, 12 layers, and a 512-wide model containing 8 attention heads.  - The *image encoder*, on the other hand, uses both a Vision Transformer (ViT) and a ResNet50 as its backbone, responsible for generating the feature representation of the image.   ### How does the CLIP algorithm work?  We can answer this question by understanding these three approaches: (1) contrastive pre-training, (2) dataset classifier creation from labeled text, and finally, (3) application of the zero-shot technique for classification.  Let’s explain each of these three concepts.   ![Contrastive pre-training](/images/clip-image-search-3.png) <small>Contrastive pre-training ([source](https://openai.com/blog/clip/))</small>  #### 1. Contrastive pre-training  During this phase, a batch of 32,768 pairs of image and text is passed through the text and image encoders simultaneously to generate the vector representations of the text and the associated image, respectively.  The training is done by searching for each image, the closest text representation across the entire batch, which corresponds to maximizing cosine similarity between the actual N pairs that are maximally close.   Also, it makes the actual images far away from all the other texts by minimizing their cosine similarity.  Finally, a symmetric cross-entropy loss is optimized over the previously computed similarity scores.  ![Classification dataset creation and zero-shot prediction](/images/clip-image-search-4.png) <small>Classification dataset creation and zero-shot prediction ([source](https://openai.com/blog/clip/))</small>  #### 2. Create dataset classifier from label text  This second step section encodes all the labels/objects in the following context format: “**_a photo of a {object}_**. The vector representation of each context is generated from the text encoder.   If we have *dog, car*, and *plane* as the classes of the dataset, we will output the following context representations:   - a photo of a dog - a photo of a car - a photo of a plane  ![Image illustration of the context representations](/images/clip-image-search-5.png) <small>Image illustration of the context representations</small>  #### 3. Use of zero-shot prediction  We use the output of section 2 to predict which image vector corresponds to which context vector. The benefit of applying the zero-shot prediction approach is to make CLIP models generalize better on unseen data.   ## Implementation of CLIP With Python  Now that we know the architecture of CLIP and how it works, this section will walk you through all the steps to successfully implement two real-world scenarios. First, you will understand how to perform an image search in natural language. Also, you will be able to perform an image-to-image search using.   At the end of the process, you will understand the benefits of using a vector database for such a use case.   ### General workflow of the use case  *(Follow along with [the Colab notebook](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/projects/clip-search/CLIP_Text_to_Image_Search.ipynb)!)*  The end-to-end process is explained through the workflow below. We start by collecting data from the Hugging Face dataset, which is then processed to further generate vector index vectors through the Image and Text Encoders. Finally, the Pinecone client is used to insert them to a vector index.   The user will then be able to search images based on either text or another image.   ![General workflow for image search](/images/clip-image-search-6.png) <small>General workflow for image search</small>  ### Prerequisites  The following libraries are required to create the implementation.   #### Install the libraries  ```python %%bash # Uncomment this if using it for the first time. -qqq for ZERO-OUT pip3 -qqq install transformers torch datasets   # The following two libraries avoid the UnidentifiedImageError pip3 -qqq install gdcm pip3 -qqq install pydicom pip -qqq install faiss-gpu pip -qqq install pinecone-client ```  #### Import the libraries  ```python import os import faiss import torch import skimage import requests import pinecone import numpy as np import pandas as pd from PIL import Image from io import BytesIO import IPython.display import matplotlib.pyplot as plt from datasets import load_dataset from collections import OrderedDict from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer ```  ### Data acquisition and exploration  The conceptual captions dataset consists of around 3.3M images with two main columns: the image URL and its caption. You can find more details from the corresponding [huggingface link](https://huggingface.co/datasets/conceptual_captions).  ```python # Get the dataset image_data = load_dataset(\"conceptual_captions\", split=\"train\") ```  #### Data preprocessing  Not all URLs in the dataset are valid. We fix that by testing and removing all erroneous URL entries.   ```python def check_valid_URLs(image_URL):    try:      response = requests.get(image_URL)      Image.open(BytesIO(response.content))      return True    except:      return False def get_image(image_URL):    response = requests.get(image_URL)    image = Image.open(BytesIO(response.content)).convert(\"RGB\")    return image ```  The following expression creates a new dataframe with a new column “is_valid” which is True when the URL is valid or False otherwise.   ```python # Transform dataframe image_data_df[\"is_valid\"] = image_data_df[\"image_url\"].apply(check_valid_URLs) # Get valid URLs image_data_df = image_data_df[image_data_df[\"is_valid\"]==True] # Get image from URL image_data_df[\"image\"] = image_data_df[\"image_url\"].apply(get_image) ```  The second step is to download the images from the URLs. This helps us avoid constant web requests.  #### Image and text embeddings implementation  The prerequisites to successfully implement the encoders are the model, the processor, and the tokenizer.   The following function fulfills those requirements from the model ID and the device used for the computation, either CPU or GPU.   ```python def get_model_info(model_ID, device): # Save the model to device \tmodel = CLIPModel.from_pretrained(model_ID).to(device)  \t# Get the processor \tprocessor = CLIPProcessor.from_pretrained(model_ID) # Get the tokenizer \ttokenizer = CLIPTokenizer.from_pretrained(model_ID)        # Return model, processor & tokenizer \treturn model, processor, tokenizer # Set the device device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Define the model ID model_ID = \"openai/clip-vit-base-patch32\" # Get model, processor & tokenizer model, processor, tokenizer = get_model_info(model_ID, device) ```  ##### Text embeddings  We start by generating the embedding of a single text before applying the same function across the entire dataset.   ```python def get_single_text_embedding(text):  inputs = tokenizer(text, return_tensors = \"pt\") \ttext_embeddings = model.get_text_features(**inputs)  \t# convert the embeddings to numpy array \tembedding_as_np = text_embeddings.cpu().detach().numpy() return embedding_as_np def get_all_text_embeddings(df, text_col): df[\"text_embeddings\"] = df[str(text_col)].apply(get_single_text_embedding) return df # Apply the functions to the dataset image_data_df = get_all_text_embeddings(image_data_df, \"caption\") ```  The first five rows look like this:  ![Format of the vector index containing the captions/text embeddings](/images/clip-image-search-7.png) <small>Format of the vector index containing the captions/text embeddings</small>  ##### Image embeddings  The same process is used for image embeddings but with different functions.   ```python def get_single_image_embedding(my_image): image = processor( \t\ttext = None, \t\timages = my_image, \t\treturn_tensors=\"pt\" \t\t)[\"pixel_values\"].to(device) embedding = model.get_image_features(image) # convert the embeddings to numpy array \tembedding_as_np = embedding.cpu().detach().numpy() \treturn embedding_as_np  def get_all_images_embedding(df, img_column): \tdf[\"img_embeddings\"] = df[str(img_column)].apply(get_single_image_embedding) \treturn df image_data_df = get_all_images_embedding(image_data_df, \"image\") ```  The final format of the text and image vector index looks like this:  ![Vector index with image and captions embeddings](/images/clip-image-search-8.png) <small>Vector index with image and captions embeddings (Image by Author)</small>  ### Vector storage approach — Local vector index Vs. A cloud-based vector index  In this section, we will explore two different approaches to storing the embeddings and metadata for performing the searches: The first is using the previous dataframe, and the second is using Pinecone. Both approaches use the cosine similarity metric.  #### Using local dataframe as vector index  The helper function ***get_top_N_images*** generates similar images for the two scenarios illustrated in the workflow above: text-to-image search or image-to-image search.  ```python from sklearn.metrics.pairwise import cosine_similarity def get_top_N_images(query, data, top_K=4, search_criterion=\"text\"):    # Text to image Search    if(search_criterion.lower() == \"text\"):      query_vect = get_single_text_embedding(query)    # Image to image Search    else:      query_vect = get_single_image_embedding(query)    # Relevant columns    revevant_cols = [\"caption\", \"image\", \"cos_sim\"]    # Run similarity Search    data[\"cos_sim\"] = data[\"img_embeddings\"].apply(lambda x: cosine_similarity(query_vect, x))# line 17    data[\"cos_sim\"] = data[\"cos_sim\"].apply(lambda x: x[0][0])    \"\"\"    Retrieve top_K (4 is default value) articles similar to the query    \"\"\"   most_similar_articles = data.sort_values(by='cos_sim',  ascending=False)[1:top_K+1] # line 24    return most_similar_articles[revevant_cols].reset_index() ```  Let’s understand how we perform the recommendation.  → The user provides either a text or an image as a search criterion, but the model performs a text-to-image search by default.  → In line 17, a cosine similarity is performed between each image vector and the user’s input vector.  → Finally, in line 24, sort the result based on the similarity score in descending order, and we return the most similar images by excluding the first one corresponding to the query itself.   #### Example of searches   This helper function makes it easy to have a side-by-side visualization of the recommended images. Each image will have the corresponding caption and similarity score.  ```python def plot_images_by_side(top_images):  index_values = list(top_images.index.values)  list_images = [top_images.iloc[idx].image for idx in index_values]  list_captions = [top_images.iloc[idx].caption for idx in index_values]  similarity_score = [top_images.iloc[idx].cos_sim for idx in index_values]  n_row = n_col = 2  _, axs = plt.subplots(n_row, n_col, figsize=(12, 12))  axs = axs.flatten()  for img, ax, caption, sim_score in zip(list_images, axs, list_captions, similarity_score):      ax.imshow(img)      sim_score = 100*float(\"{:.2f}\".format(sim_score))      ax.title.set_text(f\"Caption: {caption}\\nSimilarity: {sim_score}%\")  plt.show() ```  ##### Text-to-image   → First, the user provides the text that is used for the search.   → Second, we run a similarity search.   → Third, we plot the images recommended by the algorithm.    ```python query_caption = image_data_df.iloc[10].caption # Print the original query text print(\"Query: {}\".format(query_caption)) # Run the similarity search top_images = get_top_N_images(query_caption, image_data_df) # Plot the recommended images plot_images_by_side(top_images) ```  Line 3 generates the following text:   _Query: actor arrives for the premiere of the film_  Line 9 produces the plot below.  ![Images corresponding to the text](/images/clip-image-search-9.png) <small>Images corresponding to the text: “actor arrives for the premiere of the film”</small>  ##### Image-to-image  The same process applies. The only difference this time is that the user provides an image instead of a caption.   ```python # Get the query image and show it query_image = image_data_df.iloc[55].image query_image ```  ![Original image of search (image at the index)](/images/clip-image-search-10.png) <small>Original image of search (image at the index)</small>  ```python # Run the similarity search and plot the result top_images = get_top_N_images(query_image, image_data_df, search_criterion=\"image\") # Plot the result plot_images_by_side(top_images) ```  We run the search by specifying the search_criterion which is “image” in line 2.   The final result is shown below.   ![Images corresponding to the image-to-image search](/images/clip-image-search-11.png) <small>Images corresponding to the image-to-image search (Image by Author)</small>  We can observe that some of the images are less similar which introduces noise in the recommendation. We can reduce that noise by specifying a threshold level of similarity. For instance, consider all the images with at least 60% similarity.   ### Leveraging the power of a managed vector index using Pinecone  [Pinecone](/) provides a fully-managed, easily scalable vector database that makes it easy to build high-performance vector search applications.  This section will walk you through the steps from acquiring your API credentials to implementing the search engine.   #### Acquire your Pinecone API   Below are the eight steps to acquire your API credentials, starting from the [Pinecone website](/).   ![Eight main steps to acquire your Pinecone Client API](/images/clip-image-search-12.png) <small>Eight main steps to acquire your Pinecone Client API</small>  #### Configure the vector index  From the API, we can create the index that allows us to perform all the create, update, delete, and insert actions.   ```python pinecone.init(    api_key = \"YOUR_API_KEY\",    environment=\"YOUR_ENV\"  # find next to API key in console ) my_index_name = \"clip-image-search\" vector_dim = image_data_df.img_embeddings[0].shape[1]   if my_index_name not in pinecone.list_indexes():  # Create the vectors dimension  pinecone.create_index(name = my_index_name,                        dimension=vector_dim,                        metric=\"cosine\", shards=1,                        pod_type='s1.x1') # Connect to the index my_index = pinecone.Index(index_name = my_index_name) ```  - ***pinecone.init*** section initializes the pinecone workspace to allow future interactions.  - from lines 8 to 9 we specify the name we want for the vector index, and also the dimension of the vectors, which is 512 in our scenario.  - from lines 11 to 16 we create the index if it does not already exist.   The result of the following instruction shows that we have no data in the index.   ```python my_index.describe_index_stats() ```  The only information we have is the dimension, which is 512.  ```python {'dimension': 512,             'index_fullness': 0.0,              'namespaces': {},  'total_vector_count': 0} ```  #### Populate the database   Now that we have configured the Pinecone database, the next step is to populate it with the following code.   ```python image_data_df[\"vector_id\"] = image_data_df.index image_data_df[\"vector_id\"] = image_data_df[\"vector_id\"].apply(str) # Get all the metadata final_metadata = [] for index in range(len(image_data_df)):  final_metadata.append({      'ID':  index,      'caption': image_data_df.iloc[index].caption,      'image': image_data_df.iloc[index].image_url  }) image_IDs = image_data_df.vector_id.tolist() image_embeddings = [arr.tolist() for arr in image_data_df.img_embeddings.tolist()] # Create the single list of dictionary format to insert data_to_upsert = list(zip(image_IDs, image_embeddings, final_metadata)) # Upload the final data my_index.upsert(vectors = data_to_upsert) # Check index size for each namespace my_index.describe_index_stats() ```  _Let’s understand what is going on here._  The data to upsert requires three components: the unique identifiers (IDs) of each observation, the list of embeddings being stored, and the metadata containing additional information about the data to store.   → From lines 5 to 12, the metadata is created by storing the “ID”, “caption” and “URL” of each observation.   → On lines 14 and 15, we generate a list of IDs, and convert the embeddings into a list of lists.   → Then, we create a list of dictionaries mapping the IDs, embeddings, and metadata.   → The final data is upserted to the index with the _.upsert()_ function.  Similarly to the previous scenario, we can check that all vectors have been upserted via `my_index.describe_index_stats()`.   #### Start the query  All that remains is to query our index using the text-to-image and image-to-image searches. Both will use the following syntax:   ```python my_index.query(my_query_embedding, top_k=N, include_metadata=True) ```  → *my_query_embedding* is the embedding (as a list) of the query (caption or image) provided by the user.   → *N* corresponds to the top number of results to return.   → *include_metadata=True* means that we want the query result to include metadata.   ##### Text to image  ```python # Get the query text text_query = image_data_df.iloc[10].caption   # Get the caption embedding query_embedding = get_single_text_embedding(text_query).tolist()   # Run the query my_index.query(query_embedding, top_k=4, include_metadata=True) ```  Below is the JSON response returned from the query  ![text-to-image query result](/images/clip-image-search-13.png) <small>text-to-image query result (Image by Author)</small>  From the “matches” attribute, we can observe the top four most similar images returned by the query.   ##### Image-to-image  The same approach applies to image-to-image search.   ```python image_query = image_data_df.iloc[43].image ```  This is the image provided by the user as the search criteria.  ![Query image](/images/clip-image-search-14.png) <small>Query image</small>  ```python # Get the text embedding query_embedding = get_single_image_embedding(image_query).tolist()  # Run the query my_index.query(query_embedding, top_k=4, include_metadata=True) ```  ![image-to-image query result](/images/clip-image-search-15.png) <small>image-to-image query result (Image by Author)</small>  Once you’ve finished don't forget to delete your index to free up your resources with:  ```python pinecone.delete_index(my_index) ```  ## What are the advantages to using a Pinecone over a local pandas dataframe?  This approach using Pinecone has several advantages:  → ***Simplicity***: the querying approach is much simpler than the first approach, where the user has the full responsibility of managing the vector index.   → ***Speed***: Pinecone approach is faster, which corresponds to most industry requirements.   → ***Scalability***: vector index hosted on Pinecone is scalable with little-to-no user effort from us. The first approach would become increasingly complex and slow as we scale.  → ***Lower chance of information loss***: the vector index based on Pinecone is hosted in the cloud with backups and high information security. The first approach is too high risk for production use-cases.   → ***Web-service friendly***: the result provided by the query is in JSON format and can be consumed by other applications, making it a better fit for web-based applications.   ## Conclusion   Congratulations, you have just learned how to fully implement an image search application using both image and natural language. I hope the benefits highlighted are valid enough to take your project to the next level using vector databases.   Multiple resources are available at our [Learning Center](/learn/) to further your learning.   The source code for the article is [available on Colab](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/projects/clip-search/CLIP_Text_to_Image_Search.ipynb).   ## References  [Code Notebook](https://github.com/pinecone-io/examples/blob/update-examples/search/multi-modal/clip-search/clip-text-image-search.ipynb)  [1] A. Radford, J. W. Kim, et al., [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020v1.pdf) (2021)  [2] A. Vaswani, et al., [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (2017), NeurIPS ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2f2"
  },
  "title": "\"Evaluation Measures in Information Retrieval\"",
  "headline": "\"Evaluation Measures in Information Retrieval\"",
  "weight": "9",
  "- name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "How to measure retrieval performance with offline metrics like recall@K, MRR, MAP@K, and NDCG@K.",
  "images": "[\"/images/offline-evaluation-0.jpg\"]",
  "thumbnail": "\"https://www.pinecone.io/images/offline-evaluation-0.jpg\"",
  "content": "Evaluation of information retrieval (IR) systems is critical to making well-informed design decisions. From search to recommendations, evaluation measures are paramount to understanding what does and *does not* work in retrieval.  Many big tech companies contribute much of their success to well-built IR systems. One of Amazon's earliest iterations of the technology was reportedly driving more than 35% of their sales<sup>[1]</sup>. Google attributes 70% of YouTube views to their IR recommender systems<sup>[2][3]</sup>.  IR systems power some of the greatest companies in the world, and behind every successful IR system is a set of evaluation measures.  ---  ## Metrics in Information Retrieval  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/BD9TkvEsKwM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  Evaluation measures for IR systems can be split into *two* categories: *online* or *offline* metrics.  **Online metrics** are captured during actual usage of the IR system when it is *online*. These consider user interactions like whether a user clicked on a recommended show from Netflix or if a particular link was clicked from an email advertisement (the click-through rate or CTR). There are many online metrics, but they all relate to some form of user interaction.  **Offline metrics** are measured in an isolated environment before deploying a new IR system. These look at whether a particular set of *relevant* results are returned when retrieving items with the system.  ![metrics_diagram](/images/offline-evaluation-1.png) <small>Evaluation measures can be categorized as either offline or online metrics. Offline metrics can be further divided into *order-unaware* or *order-aware*, which we will explain soon.</small>  Organizations often use *both* offline and online metrics to measure the performance of their IR systems. It begins, however, with offline metrics to predict the system's performance *before deployment*.  We will focus on the most useful and popular offline metrics:  * Recall@K * **M**ean **R**eciprocal **R**ank (MRR) * **M**ean **A**verage **P**recision@K (MAP@K) * **N**ormalized **D**iscounted **C**umulative **G**ain (NDCG@K)  These metrics are deceptively simple yet provide invaluable insight into the performance of IR systems.  We can use one or more of these metrics in different evaluation stages. During the development of Spotify's podcast search; *Recall@K* (using $K=1$) was used during training on \"evaluation batches\", and after training, [both *Recall@K* and *MRR*](https://www.pinecone.io/learn/spotify-podcast-search/#:~:text=Spotify%20details%20their%20full%2Dretrieval%20setting%20metrics%20as%20using%20Recall%4030%20and%20MRR%4030%2C%20performed%20both%20on%20queries%20from%20the%20eval%20set%20and%20on%20their%20curated%20dataset.) (using $K=30$) were used with a much larger evaluation set.  The last paragraph will make sense by the end of this article. For now, understand that Spotify was able to predict system performance *before* deploying anything to customers. This allowed them to deploy successful A/B tests and significantly increase podcast engagement<sup>[4]</sup>.  We have two more subdivisions for these metrics; *order-aware* and *order-unaware*. This refers to whether the order of results impacts the metric score. If so, the metric is *order-aware*. Otherwise, it is *order-unaware*.  ## Cats in Boxes  Throughout the article, we will be using a *very* small dataset of eight images. In reality, this number is likely to be millions or more.  ![example](/images/offline-evaluation-2.png) <small>Example query and ranking of the eight possible results.</small>  If we were to search for *\"cat in a box\"*, we may return something like the above. The numbers represent the relevance *rank* of each image as predicted by the IR system. Other queries would yield a different order of results.  ![example_highlighted](/images/offline-evaluation-3.png) <small>Example query and ranking with *actual relevant* results highlighted.</small>  We can see that results *2*, *4*, *5*, and *7* are *actual relevant* results. The other results are *not* relevant as they show cats *without* boxes, boxes *without* cats, or a dog.  ### Actual vs. Predicted  When evaluating the performance of the IR system, we will be comparing *actual* vs. *predicted* conditions, where:  * **Actual condition** refers to the true label of every item in the dataset. These are *positive* ($p$) if an item is relevant to a query or *negative* ($n$) if an item is *ir*relevant to a query. * **Predicted condition** is the *predicted* label returned by the IR system. If an item is returned, it is predicted as being *positive* ($\\hat{p}$) and, if it is not returned, is predicted as a *negative* ($\\hat{n}$).  From these actual and predicted conditions, we create a set of outputs from which we calculate all of our offline metrics. Those are the true/false positives and true/false negatives.  The *positive* results focus on what the IR system returns. Given our dataset, we ask the IR system to return *two* items using the *\"cat in a box\"* query. If it returns an *actual relevant* result this is a <span onMouseOver=\"this.style.backgroundColor='#8CF1FF'\" onMouseOut=\"this.style.backgroundColor='transparent'\">*true positive* ($p\\hat{p}$)</span>; if it returns an irrelevant result, we have a <span onMouseOver=\"this.style.backgroundColor='#FAFF00'\" onMouseOut=\"this.style.backgroundColor='transparent'\">*false positive* ($n\\hat{p}$)</span>.  ![example_condition](/images/offline-evaluation-4.png) <small>On returning the top two results, we get <span onMouseOver=\"this.style.backgroundColor='#FAFF00'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$n\\hat{p}$</span> and <span onMouseOver=\"this.style.backgroundColor='#8CF1FF'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$p\\hat{p}$</span> results for the *positives*. For the unreturned *negatives* we have a mix of <span onMouseOver=\"this.style.backgroundColor='#738FAB'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$n\\hat{n}$</span> and <span onMouseOver=\"this.style.backgroundColor='#DFECF9'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$p\\hat{n}$</span>.</small>  For *negative* results, we must look at what the IR system *does not* return. Let's query for two results. Anything that *is relevant* but is *not* returned is a <span onMouseOver=\"this.style.backgroundColor='#DFECF9'\" onMouseOut=\"this.style.backgroundColor='transparent'\">*false negative* ($p\\hat{n}$)</span>. Irrelevant items that were *not* returned are <span onMouseOver=\"this.style.backgroundColor='#738FAB'\" onMouseOut=\"this.style.backgroundColor='transparent'\">*true negatives* ($n\\hat{n}$)</span>.  With all of this in mind, we can begin with the first metric.  ## Recall@K  *Recall@K* is one of the most interpretable and popular offline evaluation metrics. It measures how many relevant items were returned (<span onMouseOver=\"this.style.backgroundColor='#8CF1FF'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$p\\hat{p}$</span>) against how many relevant items exist in the entire dataset (<span onMouseOver=\"this.style.backgroundColor='#8CF1FF'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$p\\hat{p}$</span>$+$<span onMouseOver=\"this.style.backgroundColor='#DFECF9'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$p\\hat{n}$</span>). $$ Recall@K = \\frac{true Positives}{true Positives + false Negatives} = \\frac{p\\hat{p}}{p\\hat{p}+p\\hat{n}} $$ The *K* in this and all other offline metrics refers to the number of items returned by the IR system. In our example, we have a total number of *N = 8* items in the entire dataset, so *K* can be any value between $[1, ..., N]$.  ![recall-at-two](/images/offline-evaluation-5.png) <small>With *recall@2* we return the predicted top *K = 2* most relevant results.</small>  When *K = 2*, our *recall@2* score is calculated as the number of *returned* relevant results over the total number of relevant results in the *entire dataset*. That is: $$ Recall@2 = \\frac{p\\hat{p}}{p\\hat{p}+p\\hat{n}} = \\frac{1}{1+3} = 0.25 $$ With recall@K, the score improves as *K* increases and the scope of returned items increases.  ![recall-at-k graph](/images/offline-evaluation-14.png) <small>With *recall@K* we will see the score increase as K increases and more positives (whether true or false) are returned.</small>  We can calculate the same recall@K score easily in Python. For this, we will define a function named `recall` that takes lists of *actual conditions* and *predicted conditions*, a *K* value, and returns a recall@K score.  ```python # recall@k function def recall(actual, predicted, k):     act_set = set(actual)     pred_set = set(predicted[:k])     result = round(len(act_set & pred_set) / float(len(act_set)), 2)     return result ```  Using this, we will replicate our eight-image dataset with *actual relevant* results in rank positions *2*, *4*, *5*, and *7*.  {{< notebook file=\"metrics-recall-calc\" height=\"full\" >}}  ### Pros and Cons  Recall@K is undoubtedly one of the most easily interpretable evaluation metrics. We know that a perfect score indicates that all relevant items are being returned. We also know that a smaller *k* value makes it harder for the IR system to score well with recall@K.  Still, there are disadvantages to using *recall@K*. By increasing *K* to *N* or near *N*, we can return a perfect score every time, so relying solely on recall@K can be deceptive.  Another problem is that it is an *order-unaware metric*. That means if we used recall@4 and returned one relevant result at rank *one*, we would score the same as if we returned the same result at rank *four*. Clearly, it is better to return the actual relevant result at a higher rank, but recall@K *cannot* account for this.  ## Mean Reciprocal Rank (MRR)  The **M**ean **R**eciprocal **R**ank (MRR) is an *order-aware metric*, which means that, unlike recall@K, returning an actual relevant result at rank *one* scores better than at rank *four*.  Another differentiator for MRR is that it is calculated based on multiple queries. It is calculated as: $$ MRR = \\frac{1}{Q} \\sum_{q=1}^{Q} \\frac{1}{rank_q} $$ $Q$ is the number of queries, $q$ a specific query, and $rank_q$ the rank of the first *actual relevant* result for query $q$. We will explain the formula step-by-step.  Using our last example where a user searches for *\"cat in a box\"*. We add two more queries, giving us $Q = 3$.  ![mrr](/images/offline-evaluation-6.png) <small>We perform three queries while calculating the MRR score.</small>  We calculate the rank reciprocal $\\frac{1}{rank_q}$ for each query $q$. For the first query, the first actual relevant image is returned at position *two*, so the rank reciprocal is $\\frac{1}{2}$. Let's calculate the reciprocal rank for all queries:  $$ query \\space 1: \\frac{1}{rank_1} = \\frac{1}{2} = 0.5 $$  $$ query \\space 2: \\frac{1}{rank_2} = \\frac{1}{1} = 1.0 $$  $$ query \\space 3: \\frac{1}{rank_3} = \\frac{1}{5} = 0.2 $$  Next, we sum all of these reciprocal ranks for queries $q=[1, ..., Q]$ (e.g., all three of our queries):  $$ \\sum_{q=1}^{Q} \\frac{1}{rank_q} = 0.5 + 1.0 + 0.2 = 1.7 $$  As we are calculating the **mean** reciprocal rank (**M**RR), we must take the average value by dividing our total reciprocal ranks by the number of queries $Q$:  $$ MRR = \\frac{1}{Q} \\sum_{q=1}^{Q} \\frac{1}{rank_q} = \\frac{1}{3}1.7 ≅ 0.57 $$  Now let's translate this into Python. We will replicate the same scenario where $Q = 3$ using the same *actual relevant* results.  {{< notebook file=\"metrics-mrr\" height=\"full\" >}}  And as expected, we calculate the same MRR score of *0.57*.  ### Pros and Cons  MRR has its own unique set of advantages and disadvantages. It is *order-aware*, a massive advantage for use cases where the rank of the first relevant result is important, like chatbots or [question-answering](/learn/question-answering).  On the other hand, we consider the rank of the *first* relevant item, but no others. That means for use cases where we'd like to return multiple items like recommendation or search engines, MRR is not a good metric. For example, if we'd like to recommend ~10 products to a user, we ask the IR system to retrieve 10 items. We could return just one *actual relevant* item in rank one and no other relevant items. Nine of ten irrelevant items is a terrible result, but MRR would score a perfect *1.0*.  Another *minor* disadvantage is that MRR is less readily interpretable compared to a simpler metric like recall@K. However, it is still more interpretable than many other evaluation metrics.  ## Mean Average Precision (MAP)  **M**ean **A**verage **P**recision@K (*MAP@K*) is another popular *order-aware* metric. At first, it seems to have an odd name, a *mean* of an *average*? It makes sense; we promise.  There are a few steps to calculating *MAP@K*. We start with another metric called *precision@K*: $$ Precision@K = \\frac{true Positives}{true Positives + false Positives} = \\frac{p\\hat{p}}{p\\hat{p}+n\\hat{p}} $$ You may be think this looks very similar to *recall@K*, and it is! The only difference is that we've swapped <span onMouseOver=\"this.style.backgroundColor='#DFECF9'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$p\\hat{n}$</span> in recall for <span onMouseOver=\"this.style.backgroundColor='#FAFF00'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$n\\hat{p}$</span> here. That means we now consider both actual relevant and non-relevant results *only* from the returned items. We *do not* consider non-returned items with *precision@K*.  ![precision-and-recall](/images/offline-evaluation-18.png) <small>The difference between Recall@K and Precision@K calculation where *K = 2*.</small>  Let's return to the *\"cat in a box\"* example. We will return $K = 2$ items and calculate *precision@2*.  ![precision-at-two](/images/offline-evaluation-7.png) <small>Precision@K calculation where *K = 2*.</small>  $$ Precision@2 = \\frac{p\\hat{p}}{p\\hat{p}+n\\hat{p}} = \\frac{p\\hat{p}}{K} = \\frac{1}{2} = 0.5 $$  Note that the denominator in *precision@K* always equals $K$. Now that we have the *precision@K* value, we move on to the next step of calculating the **A**verage **P**recision@K (*AP@K*):  $$ AP@K = \\frac{\\sum_{k = 1}^{K} (Precision@k * rel_k)}{ number \\space of \\space relevant \\space results} $$  Note that for *AP@K* we are taking the average *precision@k* score for all values of $k = [1, ..., K]$. Meaning that for AP@5 we calculate *precision@k* where $k=[1, 2, 3, 4, 5]$.  We have not seen the $rel_k$ parameter before. It is a *relevance* parameter that (for *AP@K*) is equal to *1* when the $k^{th}$ item is relevant or *0* when it is not.  <center> <img loading=\"lazy\" src=\"/images/offline-evaluation-8.png\" width=\"80%\" /> </center> <small>We calculate $precision@k$ and $rel_k$ iteratively using $k = [1, ..., K]$.</small>  As we multiply *precision@k* and $rel_k$, we only need to consider *precision@k* where the $k^{th}$ item is relevant.  <center> <img loading=\"lazy\" src=\"/images/offline-evaluation-9.png\" width=\"50%\" /> </center> <small>$Precision@k$ and $rel_k$ values for all relevant items across three queries $q = [1, ..., Q]$.</small>  Given these values, for each query $q$, we can calculate the $AP@K_q$ score where $K=8$ as:  $$ AP@8_1 = \\frac{0.5\\*1 + 0.5\\*1 + 0.6\\*1 + 0.57\\*1}{4} = 0.54 $$  $$ AP@8_2 = \\frac{1\\*1 + 0.5\\*1 + 0.4\\*1 + 0.43\\*1}{4} = 0.67 $$  $$ AP@8_3 = \\frac{0.2\\*1 + 0.25\\*1}{2} = 0.22 $$  Each of these individual $AP@K_q$ calculations produces a single **A**verage **P**recision@K score for each query $q$. To get the **M**ean **A**verage **P**recision@K (*MAP@K*) score for all queries, we simply divide by the number of queries $Q$:  $$ MAP@K = \\frac{1}{Q}\\sum_{q=1}^{Q}AP@K_q = \\frac{1}{3}*(0.54+0.67+0.22) = 0.48 $$  That leaves us with a final *MAP@K* score of *0.48*. To calculate all of this with Python, we write:  {{< notebook file=\"metrics-map\" height=\"full\" >}}  Returning the same *MAP@K* score of $0.48$.  ### Pros and Cons  MAP@K is a simple offline metric that allows us to consider the *order* of returned items. Making this ideal for use cases where we expect to return multiple relevant items.  The primary disadvantage of MAP@K is the $rel_K$ relevance parameter is binary. We must either view items as *relevant* or *irrelevant*. It does not allow for items to be slightly more/less relevant than others.  ## Normalized Discounted Cumulative Gain (NDCG@K)  ***N**ormalized **D**iscounted **C**umulative **G**ain **@K*** ($NDCG@K$) is another *order-aware metric* that we can derive from a few simpler metrics. Starting with **C**umulative **G**ain ($CG@K$) calculated like so: $$ CG@K = \\sum_{k=1}^{K}rel_k $$ The $rel_k$ variable is different this time. It is a range of relevance ranks where *0* is the least relevant, and some higher value is the most relevant. The number of ranks does not matter; in our example, we use a range of $0 \\rightarrow 4$.  <center> <img loading=\"lazy\" src=\"/images/offline-evaluation-10.png\" width=\"70%\" /> </center> <small>Using $rel_k$ we rank every item based on its relevance to a particular query $q$.</small>  Let's try applying this to another example. We will use a similar eight-image dataset as before. The circled numbers represent the IR system's *predicted* ranking, and the diamond shapes represent the $rel_k$ *actual ranking*.  ![ndcg_relevance](/images/offline-evaluation-11.png) <small>A small dataset with predicted ranks (circles) and actual ranks (diamonds).</small>  To calculate the cumulative gain at position *K* (*CG@K*), we sum the relevance scores up to the *predicted* rank *K*. When $K = 2$:  $$ CG@2 = \\sum_{k=1}^{2}rel_k = rel_1 + rel_2 = 0 + 4 = 4 $$  It's important to note that *CG@K* is *not* order-aware. If we swap images *1* and *2*, we will return the same score when $K \\geq 2$ despite having the more relevant item placed first.  ![ndcg_relevance_two](/images/offline-evaluation-12.png) <small>Images *1* and *2* have been swapped.</small>  $$ CG@2 = \\sum_{k=1}^{2}rel_k = rel_1 + rel_2 = 4 + 0 = 4 $$  To handle this lack of order awareness, we modify the metric to create *DCG@K*, adding a penalty in the form of $log_{2}(1+k)$ to the formula:  $$ DCG@2 = \\sum_{k=1}^{K}\\frac{rel_k}{log_2(1+k)} $$  Now when we calculate *DCG@2* and swap the position of the first two images, we return different scores:  $$ original: \\space DCG@2 = \\frac{0}{\\log_2(1+1)}+\\frac{4}{\\log_2(1+2)} = 0 + 2.52 = 2.52 $$  $$ swapped: \\space DCG@2 = \\frac{4}{\\log_2(1+1)}+\\frac{0}{\\log_2(1+2)} = 4 + 0 = 4 $$  ```python from math import log2  # initialize variables relevance = [0, 7, 2, 4, 6, 1, 4, 3] K = 8  dcg = 0 # loop through each item and calculate DCG for k in range(1, K+1):     rel_k = relevance[k-1]     # calculate DCG     dcg += rel_k / log2(1 + k) ```  ![DGC@K_graph](/images/offline-evaluation-13.png) <small>$DCG@K$ score as $K$ increases using the *query #1* order of results.</small>  Using the *order-aware* $DCG@K$ metric means the preferred swapped results returns a better score.  Unfortunately, *DCG@K* scores are very hard to interpret as their range depends on the variable $rel_k$ range we chose for our data. We use the **N**ormalized **DCG@K** (*NDCG@K*) metric to fix this.  ---  *$NDCG@K$ is a special modification of standard NDCG that cuts off any results whose rank is greater than $K$. This modification is prevalent in use-cases measuring search performance<sup>[5]</sup>.*  ---  *NDCG@K* normalizes *DCG@K* using the **I**deal **DCG@K** (*IDCG@K*) rankings. For *IDCG@K*, we assume that the most relevant items are ranked highest and in order of relevance.  Calculating *IDCG@K* takes nothing more than reordering the assigned ranks and performing the same *DCG@K* calculation:  $$ IDCG@2 = \\frac{4}{\\log_2(1+1)}+\\frac{4}{\\log_2(1+2)} = 4 + 2.52 = 6.52 $$  ```python # sort items in 'relevance' from most relevant to less relevant ideal_relevance = sorted(relevance, reverse=True)  print(ideal_relevance)  idcg = 0 # as before, loop through each item and calculate *Ideal* DCG for k in range(1, K+1):     rel_k = ideal_relevance[k-1]     # calculate DCG     idcg += rel_k / log2(1 + k) ```  ![DCG_IGC@K_graph](/images/offline-evaluation-15.png) <small>*IDCG@K* score as $K$ increases compared against the *DCG@K* score calculated with using the *query #1* order of results.</small>  Now all we need to calculate *NDCG@K* is to normalize our *DCG@K* score using the *IDCG@K* score:  $$ NDCG@K = \\frac{DCG@K}{IDCG@K} = \\frac{2.52}{6.52} = 0.39 $$  ```python dcg = 0 idcg = 0  for k in range(1, K+1):     # calculate rel_k values     rel_k = relevance[k-1]     ideal_rel_k = ideal_relevance[k-1]     # calculate dcg and idcg     dcg += rel_k / log2(1 + k)     idcg += ideal_rel_k / log2(1 + k)     # calcualte ndcg     ndcg = dcg / idcg ```  ![NDGC@K_graph](/images/offline-evaluation-16.png) <small>*NDCG@K* score as $K$ increases calculated by normalizing *DCG@K* using *IDCG@K*.</small>  Using *NDCG@K*, we get a more interpretable result of *0.41*, where we know that *1.0* is the *best* score we can get with all items ranked perfectly (e.g., the *IDCG@K*).  ### Pros and Cons  *NDCG@K* is one of the most popular offline metrics for evaluating IR systems, in particular web search engines. That is because *NDCG@K* optimizes for highly relevant documents, is *order-aware*, and is easily interpretable.  However, there is a significant disadvantage to *NDCG@K*. Not only do we need to know which items are relevant for a particular query, but we need to know whether each item is more/less relevant than other items; the data requirements are more complex.  ![example of data for other metrics vs for NDCG](/images/offline-evaluation-17.png) <small>Example of data for the other metrics (left) and the more complex data required for NDCG@K (right).</small>  ---  These are some of the most popular offline metrics for evaluating information retrieval systems. A single metric can be a good indicator of system performance. For even more confidence in retrieval performance you can use several metrics, just as Spotify did with recall@1, recall@30, and MRR@30.  These metrics are still best supported with online metrics during A/B testing, which act as *the next step* before deploying your retrieval system to the world. However, these offline metrics are the foundation behind any retrieval project.  Whether you’re prototyping your very first product, or evaluating the latest iteration of Google search, evaluating your retrieval system with these metrics will help you deploy the best retrieval system possible.  {{< newsletter text=\"Subscribe to new content in search and recommendation!\" inputText=\"Email address...\" buttonText=\"Submit\">}}  ## References [Code Walkthrough](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/algos-and-libraries/offline-evaluation/offline-evaluation.ipynb)  [1] G. Linden et al., [Item-to-Item Collaborative Filtering](https://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf) (2003), IEEE  [2] J. Solsman, [YouTube's AI is the puppet master over most of what you watch](https://www.cnet.com/tech/services-and-software/youtube-ces-2018-neal-mohan/) (2018)  [3] P. Covington et al., [Deep Neural Networks for YouTube Recommendations](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45530.pdf) (2016), RecSys  [4] A. Tamborrino, [Introducing Natural Language Search for Podcast Episodes](https://engineering.atspotify.com/2022/03/introducing-natural-language-search-for-podcast-episodes/) (2022), Engineering at Spotify Blog  [5] Y. Wang et al., [A Theoretical Analysis of NDCG Ranking Measures](http://proceedings.mlr.press/v30/Wang13.pdf) (2013), JMLR  ## Nomenclature  $K: number \\space of \\space retrieved \\space results$  $k: position \\space k \\space of \\space retrieved \\space items$  $Q: number \\space of \\space queries$  $q: query$  $rank_q: rank \\space of \\space first \\space relevant \\space result \\space for \\space query \\space q$  $rel_k: relevance \\space of \\space item \\space at \\space position \\space k$  $p: actual \\space relevant \\space result$  $\\hat{p}: predicted \\space relevant \\space result$  $n: actual \\space irrelevant \\space result $  $\\hat{n}: predicted \\space irrelevant \\space result$  <span onMouseOver=\"this.style.backgroundColor='#8CF1FF'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$p\\hat{p}: true \\space positive$</span>  <span onMouseOver=\"this.style.backgroundColor='#FAFF00'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$n\\hat{p}: false \\space positive$</span>  <span onMouseOver=\"this.style.backgroundColor='#738FAB'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$n\\hat{n}: true \\space negative $</span>  <span onMouseOver=\"this.style.backgroundColor='#DFECF9'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$p\\hat{n}: false \\space negative$</span> ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2f4"
  },
  "title": "\"Training Sentence Transformers with Softmax Loss\"",
  "headline": "\"Training Sentence Transformers the OG Way (with Softmax Loss)\"",
  "weight": "3",
  "name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "The original way of training sentence transformers like SBERT for semantic search.",
  "images": "['/images/train-sentence-transformer-1.jpg']",
  "content": "Our article introducing [sentence embeddings and transformers](/learn/sentence-embeddings/) explained that these models can be used across a range of applications, such as semantic textual similarity (STS), semantic clustering, or information retrieval (IR) using concepts rather than words.  This article dives deeper into the training process of the first sentence transformer, *sentence-BERT*, or more commonly known as *SBERT*. We will explore the **N**atural **L**anguage **I**nference (NLI) training approach of *softmax loss* to fine-tune models for producing sentence embeddings.  Be aware that softmax loss is no longer the preferred approach to training sentence transformers and has been superseded by other methods such as MSE margin and multiple negatives ranking loss. But we’re covering this training method as an important milestone in the development of ever improving sentence embeddings.  This article also covers *two approaches* to fine-tuning. The first shows how NLI training with softmax loss works. The second uses the excellent training utilities provided by the `sentence-transformers` library — it’s more abstracted, making building good sentence transformer models *much easier*.  <p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/aSx0jg9ZILo\" title=\"Sentence Embeddings\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>  ## NLI Training  There are several ways of training sentence transformers. One of the most popular (and the approach we will cover) is using Natural Language Inference (NLI) datasets.  NLI focus on identifying sentence pairs that *infer* or *do not infer* one another. We will use two of these datasets; the Stanford Natural Language Inference (SNLI) and Multi-Genre NLI (MNLI) corpora.  Merging these two corpora gives us 943K sentence pairs (550K from SNLI, 393K from MNLI). All pairs include a `premise` and a `hypothesis`, and each pair is assigned a `label`:  * **0** — *entailment*, e.g. the `premise` suggests the `hypothesis`. * **1** — *neutral*, the `premise` and `hypothesis` could both be true, but they are not necessarily related. * **2** — *contradiction*, the `premise` and `hypothesis` contradict each other.  When training the model, we will be feeding sentence A (the `premise`) into BERT, followed by sentence B (the `hypothesis`) on the next step.  From there, the models are optimized using *softmax loss* using the `label` field. We will explain this in more depth soon.  For now, let's download and merge the two datasets. We will use the `datasets` library from Hugging Face, which can be downloaded using `!pip install datasets`. To download and merge, we write:  {{< notebook file=\"prep-snli-mnli\" height=\"full\" >}}  Both datasets contain `-1` values in the `label` feature where no confident class could be assigned. We remove them using the `filter` method.  {{< notebook file=\"remove-empties\" height=\"full\" >}}  We must convert our human-readable sentences into transformer-readable tokens, so we go ahead and tokenize our sentences. Both `premise` and `hypothesis` features must be split into their own `input_ids` and `attention_mask` tensors.  {{< notebook file=\"nli-tokenize\" height=\"full\" >}}  Now, all we need to do is prepare the data to be read into the model. To do this, we first convert the `dataset` features into PyTorch tensors and then initialize a data loader which will feed data into our model during training.  ```python # covert dataset features to PyTorch tensors dataset.set_format(type='torch', columns=all_cols)  # initialize the dataloader batch_size = 16 loader = torch.utils.data.DataLoader(     dataset, batch_size=batch_size, shuffle=True ) ```  And we're done with data preparation. Let's move on to the training approach.    ## Softmax Loss  Optimizing with *softmax loss* was the primary method used by Reimers and Gurevych in the original SBERT paper [1].  Although this was used to train the first sentence transformer model, it is no longer the go-to training approach. Instead, [the MNR loss approach](/learn/fine-tune-sentence-transformers-mnr/) is most common today. We will cover this method in another article.  However, we hope that explaining softmax loss will help demystify the different approaches applied to training sentence transformers. We included a comparison to MNR loss at the end of the article.  #### Model Preparation  When we train an SBERT model, we don't need to start from scratch. We begin with an already pretrained BERT model (and tokenizer).  ```python from transformers import BertModel  # start from a pretrained bert-base-uncased model model = BertModel.from_pretrained('bert-base-uncased') ```  We will be using what is called a *'siamese'*-BERT architecture during training. All this means is that given a sentence pair, we feed *sentence A* into BERT first, then feed *sentence B* once BERT has finished processing the first.  This has the effect of creating a *siamese*-like network where we can imagine two identical BERTs are being trained in parallel on sentence pairs. In reality, there is just a single model processing two sentences one after the other.  ![Start SBERT](/images/train-sentence-transformer-2.jpg) <small>Siamese-BERT processing a sentence pair and then pooling the large token embeddings tensor into a single dense vector.</small>  BERT will output 512 768-dimensional embeddings. We will convert these into an *average* embedding using *mean-pooling*. This *pooled output* is our sentence embedding. We will have two per step — one for sentence A that we call `u`, and one for sentence B, called `v`.  To perform this mean pooling operation, we will define a function called `mean_pool`.  ```python # define mean pooling function def mean_pool(token_embeds, attention_mask):     # reshape attention_mask to cover 768-dimension embeddings     in_mask = attention_mask.unsqueeze(-1).expand(         token_embeds.size()     ).float()     # perform mean-pooling but exclude padding tokens (specified by in_mask)     pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(         in_mask.sum(1), min=1e-9     )     return pool ```  Here we take BERT's token embeddings output (we'll see this all in full soon) and the sentence's `attention_mask` tensor. We then resize the `attention_mask` to align to the higher `768`-dimensionality of the token embeddings.   We apply this resized mask `in_mask` to those token embeddings to exclude padding tokens from the mean pooling operation. Our mean pooling takes the average activation of values across each dimension to produce a single value. This brings our tensor sizes from `(512*768)` to `(1*768)`.  The next step is to concatenate these embeddings. Several different approaches to this were presented in the paper:  | Concatenation | NLI Performance | | -------------------- | --------------- | | `(u, v)` | 66.04 | | `(|u-v|)` | 69.78 | | `(u*v)` | 70.54 | | `(|u-v|, u*v)` | 78.37 | | `(u, v, u*v)` | 77.44 | | **`(u, v, |u-v|)`** | **80.78** | | `(u, v, |u-v|, u*v)` | 80.44 |  <small>Concatenation methods for sentence embeddings `u` and `v` and their performance on STS benchmarks.</small>  Of these, the best performing is built by concatenating vectors `u`, `v`, and `|u-v|`. Concatenation of them all produces a vector three times the length of each original vector. We label this concatenated vector `(u, v, |u-v|)`. Where `|u-v|` is the element-wise difference between vectors `u` and `v`.  ![UV Vectors](/images/train-sentence-transformer-3.jpg) <small>We concatenate `(u, v, |u-v|)` to merge the sentence embeddings from sentence A and B.</small>  We will perform this concatenation operation using PyTorch. Once we have our mean-pooled sentence vectors `u` and `v` we concatenate with:  ```python uv_abs = torch.abs(torch.sub(u, v))  # produces |u-v| tensor # then we concatenate x = torch.cat([u, v, uv_abs], dim=-1) ```  Vector `(u, v, |u-v|)` is fed into a feed-forward neural network (FFNN). The FFNN processes the vector and outputs three activation values. One for each of our `label` classes; *entailment*, *neutral*, and *contradiction*.  ```python # we would initialize the feed-forward NN first ffnn = torch.nn.Linear(768*3, 3) \t... # then later in the code process our concatenated vector with it x = ffnn(x) ```  As these activations and `label` classes are aligned, we now calculate the softmax loss between them.  ![SBERT Training](/images/train-sentence-transformer-4.jpg) <small>The final steps of training. The concatenated `(u, v, |u-v|)` vector is fed through a feed-forward NN to produce three output activations. Then we calculate the softmax loss between these predictions and the true labels.</small>  Softmax loss is calculated by applying a softmax function across the three activation values (or nodes), producing a predicted label. We then use [cross-entropy loss](/learn/cross-entropy-loss/) to calculate the difference between our predicted label and true `label`.  ```python # as before, we would initialize the loss function first loss_func = torch.nn.CrossEntropyLoss() \t... # then later in the code add them to the process x = loss_func(x, label)  # label is our *true* 0, 1, 2 class ```  The model is then optimized using this loss. We use an Adam optimizer with a learning rate of `2e-5` and a linear warmup period of *10%* of the total training data for the optimization function. To set that up, we use the standard PyTorch `Adam` optimizer alongside a learning rate scheduler provided by HF transformers:  ```python from transformers.optimization import get_linear_schedule_with_warmup  # we would initialize everything first optim = torch.optim.Adam(model.parameters(), lr=2e-5) # and setup a warmup for the first ~10% steps total_steps = int(len(dataset) / batch_size) warmup_steps = int(0.1 * total_steps) scheduler = get_linear_schedule_with_warmup( \t\toptim, num_warmup_steps=warmup_steps,   \tnum_training_steps=total_steps - warmup_steps ) \t... # then during the training loop we update the scheduler per step scheduler.step() ```  Now let's put all of that together in a PyTorch training loop.  {{< notebook file=\"softmax-loss-training\" height=\"full\" >}}  We only train for a single epoch here. Realistically this should be enough (and mirrors what was described in the original SBERT paper). The last thing we need to do is save the model.  {{< notebook file=\"save-softmax-loss\" height=\"full\" >}}  Now let's compare everything we've done so far with `sentence-transformers` training utilities. We will compare this and other sentence transformer models at the end of the article.  ## Fine-Tuning With Sentence Transformers  As we already mentioned, the `sentence-transformers` library has excellent support for those of us just wanting to train a model without worrying about the underlying training mechanisms.  We don't need to do much beyond a little data preprocessing (but less than what we did above). So let's go ahead and put together the same fine-tuning process, but using `sentence-transformers`.  ### Training Data  Again we're using the same SNLI and MNLI corpora, but this time we will be transforming them into the format required by `sentence-transformers` using their `InputExample` class. Before that, we need to download and merge the two datasets just like before.  {{< notebook file=\"quick-prep-nli\" height=\"full\" >}}  Now we're ready to format our data for `sentence-transformers`. All we do is convert the current `premise`, `hypothesis`, and `label` format into an *almost* matching format with the `InputExample` class.  {{< notebook file=\"quick-data-for-st\" height=\"full\" >}}  We've also initialized a `DataLoader` just as we did before. From here, we want to begin setting up the model. In `sentence-transformers` we build models using different *modules*.  All we need is the transformer model module, followed by a mean pooling module. The transformer models are loaded from HF, so we define `bert-base-uncased` as before.  {{< notebook file=\"init-model-st\" height=\"full\" >}}  We have our data, the model, and now we define how to optimize our model. Softmax loss is *very* easy to initialize.  {{< notebook file=\"softmax-loss\" height=\"full\" >}}  Now we're ready to train the model. We train for a single epoch and warm up for 10% of training as before.  {{< notebook file=\"train-st\" height=\"full\" >}}  With that, we're done, the new model is saved to `./sbert_test_b`. We can load the model from that location using either the `SentenceTransformer` or HF's `from_pretrained` methods! Let's move on to comparing this to other SBERT models.  ## Compare SBERT Models   We're going to test the models on a set of random sentences. We will build our mean-pooled embeddings for each sentence using *four* models; *softmax-loss* SBERT, *multiple-negatives-ranking-loss* SBERT, the original SBERT `sentence-transformers/bert-base-nli-mean-tokens`, and BERT `bert-base-uncased`.  ```python sentences = [     \"the fifty mannequin heads floating in the pool kind of freaked them out\",     \"she swore she just saw her sushi move\",     \"he embraced his new life as an eggplant\",     \"my dentist tells me that chewing bricks is very bad for your teeth\",     \"the dental specialist recommended an immediate stop to flossing with construction materials\",     \"i used to practice weaving with spaghetti three hours a day\",     \"the white water rafting trip was suddenly halted by the unexpected brick wall\",     \"the person would knit using noodles for a few hours daily\",     \"it was always dangerous to drive with him since he insisted the safety cones were a slalom course\",     \"the woman thinks she saw her raw fish and rice change position\" ] ```  After producing sentence embeddings, we will calculate the cosine similarity between all possible sentence pairs, producing a simple but insightful semantic textual similarity (STS) test.  We define two new functions; `sts_process` to build the sentence embeddings and compare them with cosine similarity and `sim_matrix` to construct a similarity matrix from all possible pairs.  ```python import numpy as np  # build embeddings and calculate cosine similarity def sts_process(sentence_a, sentence_b, model):     vecs = []  # init list of sentence vecs     for sentence in [sentence_a, sentence_b]:         # build input_ids and attention_mask tensors with tokenizer         input_ids = tokenizer(             sentence, max_length=512, padding='max_length',             truncation=True, return_tensors='pt'         )         # process tokens through model and extract token embeddings         token_embeds = model(**input_ids).last_hidden_state         # mean-pool token embeddings to create sentence embeddings         sentence_embeds = mean_pool(token_embeds, input_ids['attention_mask'])         vecs.append(sentence_embeds)     # calculate cosine similarity between pairs and return numpy array     return cos_sim(vecs[0], vecs[1]).detach().numpy()  # controller function to build similarity matrix def sim_matrix(model):     # initialize empty zeros array to store similarity scores     sim = np.zeros((len(sentences), len(sentences)))     for i in range(len(sentences)):         # add similarity scores to the similarity matrix         sim[i:,i] = sts_process(sentences[i], sentences[i:], model)     return sim ```  Then we just run each model through the `sim_matrix` function.  ```python import matplotlib.pyplot as plt import seaborn as sns  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') model = BertModel.from_pretrained('./sbert_test_a')  sim = sim_matrix(model)  # build similarity scores matrix sns.heatmap(sim, annot=True)  # visualize heatmap ```  After processing all pairs, we visualize the results in heatmap visualizations.  ![SBERT Heatmaps](/images/train-sentence-transformer-5.jpg) <small>Similarity score heatmaps for four BERT/SBERT models.</small>  In these heatmaps, we ideally want all dissimilar pairs to have very low scores (near white) and similar pairs to produce distinctly higher scores.  Let's talk through these results. The bottom-left and top-right models produce the correct top three pairs, whereas BERT and *softmax loss* SBERT return 2/3 of the correct pairs.  If we focus on the standard BERT model, we see minimal variation in square color. This is because almost every pair produces a similarity score of between *0.6* to *0.7*. This lack of variation makes it challenging to distinguish between more-or-less similar pairs. Although this is to be expected as BERT has *not* been fine-tuned for semantic similarity.  Our PyTorch *softmax loss* SBERT (top-left) misses the *9-1* sentence pair. Nonetheless, the pairs it produces are much more distinct from dissimilar pairs than the vanilla BERT model, so it's an improvement. The `sentence-transformers` version is better still and did *not* miss the *9-1* pair.  Next up, we have the SBERT model trained by Reimers and Gurevych in the 2019 paper (bottom-left) [1]. It produces better performance than our SBERT models but still has little variation between similar and dissimilar pairs.  And finally, we have an SBERT model trained using *MNR loss*. This model is easily the highest performing. Most dissimilar pairs produce a score *very* close to *zero*. The highest non-pair returns *0.28* — roughly half of the true-pair scores.  From these results, the SBERT MNR model seems to be the highest performing. Producing much higher activations (with respect to the average) for true pairs than any other model, making similarity much easier to identify. SBERT with softmax loss is clearly an improvement over BERT, but unlikely to offer any benefit over the SBERT with MNR loss model.  ---  That's it for this article on fine-tuning BERT for building sentence embeddings! We delved into the details of preprocessing SNLI and MNLI datasets for NLI training and how to fine-tune BERT using the *softmax loss* approach.  Finally, we compared this *softmax-loss* SBERT against vanilla BERT, the original SBERT, and [an *MNR loss* SBERT](/learn/fine-tune-sentence-transformers-mnr/) using a simple STS task. We found that although fine-tuning with *softmax loss* does produce valuable sentence embeddings — it still lacks quality compared to more recent training approaches.  We hope this has been an insightful and exciting exploration of how transformers can be fine-tuned for building sentence embeddings.  ## References  [1] N. Reimers, I. Gurevych, [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084) (2019), ACL",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2f6"
  },
  "title": "\"Language Embedding Models in Financial Services\"",
  "headline": "\"How Language Embedding Models Will Change Financial Services\"",
  "weight": "5",
  "name": "Diego Lopez Yse",
  "position": "Data Scientist",
  "src": "/images/diego-lopez-yse.jpeg",
  "href": "\"https://lopezyse.medium.com/\"",
  "description": "How financial players are using vectorized data to gain insights.",
  "images": "['/images/nlp-financial-services-8.png']",
  "content": "**How financial players are using vectorized data to gain insights.**  In finance, big profits are usually reserved for the ones who get the insights first. Each second counts, and high frequency trading algorithms are clear examples of this. But it’s not only a matter of speed: Data aggregation and knowledge discovery have become essential abilities. This is the reason why so many financial players are turning into data science companies.  Traditional financial data sources, like company filings, are widely available and accessible to everyone. Moreover, so many companies mask their accounts that financial statements are becoming less and less valuable to get to the insight first. So what are financial companies doing? Turning to alternative data.  Alternative data refers to non-traditional data that [can provide an indication of future performance](https://www.refinitiv.com/en/financial-data/alternative-data) of a company outside of traditional sources. You can think of [examples](http://forbes.com/sites/forbesinsights/2019/12/12/alternative-data-what-is-it-who-uses-it-and-why-is-it-interesting/) like records from credit card transactions, web-scraped data, geolocation data from cell phones, satellite images and weather forecasts. These sources are less structured and usually less accessible than the traditional ones, making them ideal for uncovering insights.  ![Sources of alternative data](/images/nlp-financial-services-1.png) <small>Experts divide alternative data roughly into three categories: data generated by individuals, data generated through business processes, and data generated by sensors. Source: [Caserta](https://caserta.com/alternative-data/)</small>  From this growing universe of alternative data, there’s one in particular gaining super-fast traction. The accelerated growth of **text data** has been identified as a valuable source of opportunities to gain insights. Think about it: Text data includes text from social media, consumer reviews, news, documents, and even media formats like video and audio files. We’re surrounded by this type of data, and they carry precious amounts of information.  But how can we decode the information integrated in text data? **Text data is highly unstructured**, which means it doesn’t have a predefined format. Although this has always been a problem for analytical and predictive models, modern AI solutions are capable of processing this type of data effectively. There’s only one caveat. For models to be effective, they have to deal with another important characteristic of text data: its **high dimensionality**.  ## The problem of high dimensionality  Data dimensionality refers to how many attributes a dataset has, and high dimensional data is characterized by having multiple dimensions. How many? There can be hundreds, thousands, or millions of dimensions in a single dataset. A sample of thirty-word Twitter messages that use only the one thousand most common words in the English language, for example, has roughly [as many dimensions as there are atoms in the universe](https://web.stanford.edu/~gentzkow/research/text-as-data.pdf)!  The **Curse of Dimensionality** is the name given to the problem of the exponential increase in volume associated with adding extra dimensions to the data space. High dimensional data brings all sorts of challenges, but there’s one in particular that deserves our attention: the problem of data sparsity. Sparsity of data occurs when moving to higher data dimensions, as the volume of the space represented grows so quickly that the data cannot keep up and thus becomes sparse.  ![Curse of dimensionality](/images/nlp-financial-services-2.png) <br><small>As the data moves from one dimension (at left) to two dimensions (in the middle) and finally to three dimensions (at right), it fills less and less of the data space. In order to maintain an accurate representation of the space, the data for analysis needs to grow exponentially. Source: [Deep AI](https://deepai.org/machine-learning-glossary-and-terms/curse-of-dimensionality)</small>  So how can we manage this complexity and discover insights in our current massive data volumes? The answer is: using **[vector embeddings](/learn/vector-embeddings/)**.  ## What are vector embeddings?  Machine Learning models expect inputs as numbers, not words, images, or videos. In order for these models to extract patterns and make predictions, data has to be transformed into a vector first. **Vector embeddings represent inputs like text as numbers**, so that we can train and deploy our Machine Learning models for tasks like classification or sentiment analysis.  Vector embeddings are low-dimensional representations of high-dimensional data. Typically, a standard vector won’t capture all information contained in the original data, but a good vector embedding will capture enough to solve the problem at hand.  When we use vector embeddings as inputs, the main benefit is their ability to encode information in a format that our models can process and then output something useful to our end goal. In the case of text data, we can use vectors to represent the features we want our model to learn. This vectorization produces meaningful numerical representations for the machines, enabling them to perform different tasks.  This way, a text representation shifts from a sequence of words to **points that occupy a vector embedding space**. Points in space can be close together or far apart, tightly clustered or evenly distributed.  <video autoplay loop muted playsinline class=\"responsive\"><source src=\"/images/nlp-financial-services-3.mp4\" type=\"video/mp4\"></video>  <small class=\"video\">Visualization of vectorized words in 3D through the [TensorFlow Embedding Projector](https://projector.tensorflow.org/)</small>  This vector embedding space is therefore mapped in such a way that words and sentences with similar meanings are closer together, and those that are different are farther apart. By encoding **[similarity](/learn/semantic-search/) as distance**, we can begin to derive the primary components of texts and draw decision boundaries in our semantic space. Once words or sentences get represented as numerical vectors, you can perform mathematical operations with them like addition and subtraction. This is an amazing property of vector embeddings because it means that they carry [important relational information](https://github.com/sid321axn/bank_fin_embedding) that can be used in many different Natural Language Processing (NLP) tasks.  ![Addition and subtraction with financial vectors](/images/nlp-financial-services-4.png) <small>Since vectors can be added and subtracted, in this example we can perform operations like adding the words “Sales” and “Grow”, which will result in the word “Inflation”. Source: [BankFin Embeddings](https://github.com/sid321axn/bank_fin_embedding)</small>  Different models including neural-net language models (NNLM), global vectors for word representation (GloVe), deep contextualized word representations (ELMo), and Word2vec are [designed to learn word embeddings](http://www.scholarpedia.org/article/Neural_net_language_models), which are real-valued feature vectors for each word.  Word embeddings built from a general corpus of sources like Wikipedia or Google News are widely available to use and provide acceptable results to solve general tasks. These are general purpose models, however, which need to be [fine-tuned](/learn/fine-tune-sentence-transformers-mnr/) to the specific vocabulary if you want to increase your model performance. For example, it’s possible to redefine word embeddings by adding text from 10-K filings to improve results on tasks such as document classification, document similarity, sentiment analysis, or readability index.  ## Gain insights with vectorization  Extracting information from unstructured data can be of enormous value. Text from news, social media, and company filings and communications is used to predict asset price movements and study the causal impact of new information. Through vector embeddings, you can perform exploratory analysis and visualizations to derive insights and discover patterns.  ### News  [More than 2 million articles are published every day on the web.](https://www.semanticscholar.org/paper/Text-Similarity-Measures-in-News-Articles-by-Vector-Singh-Singh/2f3bfd8f11cc55aea33b61e23457572236664df8) That’s more than 1,300 per minute. How can you keep pace with them? One strategy is to automatically extract insights from these massive volumes using Machine Learning. Through vectorization, it’s possible to monitor news media in an automated fashion: from digital newspapers to video and audio channels. By embedding all data into the same vectorized space, we can perform searches and look for similarities between data sources that previously seemed dissimilar. News that contains useful information on companies and markets can be exploited for profits, no matter their original format.  ### Consumer sentiment  Through vectorization, it’s also possible to derive people’s sentiment. The idea behind it is to analyze pieces of content (e.g., documents, videos) to determine the emotional tone they carry, usually classifying them by positive, negative, or neutral. The goal? To understand attitudes towards a topic. This is a highly used resource by companies that want to get insights on their customers, through the analysis of reviews, conversations, and posts on social media. Stock market investors also use customer sentiment to anticipate potential market trends.  ![Consumer sentiment over time](/images/nlp-financial-services-5.png) <small>Tesla, Inc. share price over time (left axis) and cumulative news sentiment scores (right axis). Source: [Dow Jones](https://visit.dowjones.com/factiva/content/unlocking-hidden-potential/#lp-pom-text-1115)</small>  Financial data companies like Moody’s have integrated these concepts to develop a [Credit Sentiment Score](https://www.moodysanalytics.com/product-list/credit-sentiment-score) that compiles adverse credit signals from news articles, backed by extensive research and state of the art NLP, text analytics, and Machine Learning techniques. This score helps firms assess credit in the loan origination and portfolio risk monitoring process and track unfavorable media. The higher the score, the stronger the credit distress signal.  ![Credit sentiment over time](/images/nlp-financial-services-6.png) <small>The figure shows the monthly average credit sentiment score of the companies in the run up to a credit event (blue line). For comparison, we also show the long-term average score for the control group (green dashed line) and three times this average (green dotted line). We see that the average credit sentiment score moves away from the long-term average as it moves toward a credit event. At around nine months before a credit event, the score is already around about three times this monthly average. Source: [Moody’s Analytics](https://www.moodysanalytics.com/-/media/products/moodys-analytics-credit-sentiment-score-product-overview.pdf)</small>  Standard & Poors (S&P) is another company exploiting the benefits of vector embeddings. They launched [Textual Data Analytics (TDA)](https://apnews.com/press-release/pr-prnewswire/62e90ac6f5ea7bb0fa5c501e805de6f4), a sophisticated data offering which applies NLP to generate sentiment scores and behavioural metrics based on company transcripts. This solution allows to incorporate more qualitative measures of company performance into investment strategies by quantifying sentiment and behaviour during company calls.  ### Better data interaction  Besides allowing us to create target solutions, vector embeddings also allow you to interact with the data in a much easier way.  *What if you could ask questions to your data like you would ask a human being?*  Solutions like Spawner AI enable you to ask questions about financial instruments and get back [natural language answers](https://towardsdatascience.com/financial-nlp-the-internets-financial-membrane-4da9482781f9). You can ask all sorts of questions about income statements, balance sheets, and cash flows. Instead of using query languages, why not ask “What is the revenue of General Electric”?  <video autoplay loop muted playsinline class=\"responsive\"><source src=\"/images/nlp-financial-services-7.mp4\" type=\"video/mp4\"> </video>  <small class=\"video\">Answers from the [Spawner API](https://spawner.ai/) using Python library. Source: [Towards Data Science](https://towardsdatascience.com/financial-nlp-the-internets-financial-membrane-4da9482781f9)</small>  ### What the future looks like  Sectors beyond finance are paying strong attention to the value of alternative data. Decision makers across all industries are demanding more actionable insights before making inferences about what actions to take. To fill this need, lots of companies have been fast enough to gather, clean, analyse, and interpret useful data from non-traditional sources.  ![Future of alternative data](/images/nlp-financial-services-8.png) <small>Alternative data is quickly becoming the key driver of investment strategies. Source: [Nasdaq](https://data.nasdaq.com/monetize-alternative-data/guide-to-selling-data-to-wall-street.pdf)</small>  Today more than ever, alternative data can give insights that traditional data cannot. Additionally, mobility and smartphones have brought wide possibilities: all  cell phone apps are rich sources of data that can be used while we circulate and interact with the physical world.  But how can we make sense of all these data sources to get insights? It’s estimated that nearly [90% of today’s data is unstructured](https://venturebeat.com/2021/07/22/why-unstructured-data-is-the-future-of-data-management/), and this massive volume will only keep growing. But since unstructured data can’t be easily processed or stored in traditional databases, we need to think about different strategies.  Whether you’re dealing with numeric data, text, images, sounds or rich media, everything can be vectorized into the same space, and [the semantic similarity of these objects and concepts can be quantified by how close they are to each other as points in this vector space](https://www.pinecone.io/learn/vector-embeddings/). Vectorizing unstructured data can allow us to store that information in a much more efficient way and use it to feed Machine Learning models for clustering, recommendation, or classification tasks. Machines only process numbers, and vectorization is an effective way to translate our highly diverse data sources into a language that machines can understand. ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2f8"
  },
  "title": "Pinecone is now available on the Google Cloud Marketplace",
  "headline": "Pinecone is now available on the Google Cloud Marketplace",
  "name": "Gibbs Cullen",
  "position": "Senior Product Marketing Manager",
  "src": "/images/gibbs-cullen.jpg",
  "href": "https://www.linkedin.com/in/gibbscullen/",
  "date": "\"2022-12-22\"",
  "description": "Start building faster with Pinecone through the Google Cloud Marketplace",
  "images": "[\"/images/pinecone-gcp-marketplace-ogimage.png\"]",
  "thumbnail": "\"/images/pinecone-gcp-thumbnail.png\"",
  "content": "We are excited to announce that Pinecone is now available on the Google Cloud Platform (GCP) Marketplace (and as the first vector database, no less). With Pinecone, you can build AI-powered search into your applications without needing to manage your own or modify legacy infrastructures. Companies ranging from small startups to enterprises trust Pinecone’s vector database to securely and efficiently manage their data, whether it’s hundreds of thousands or a billion items. And now, Google Cloud users can get started faster through our [Marketplace offering](https://console.cloud.google.com/marketplace/product/pinecone-public/pinecone).  The GCP Marketplace provides software solutions for users to easily discover, procure, and deploy solutions available on Google Cloud. Joining the GCP Marketplace has many benefits for existing Google Cloud users including:  - Consolidated Google Cloud billing: You can now meet committed spend levels faster by applying Pinecone spend to your account. View your spend across GCP services in a single place. - Faster, simpler procurement: Skip the approvals needed to integrate a new solution, and get started in production faster when purchasing Pinecone through the Marketplace.  Want to learn more? Here are some FAQs for existing Pinecone users. Make sure to also check out our [integration guide](https://docs.pinecone.io/docs/setting-up-gcp-marketplace-billing) and visit our [Marketplace listing](https://console.cloud.google.com/marketplace/product/pinecone-public/pinecone).  ## FAQs:  **Q**: Which plans and pricing are available through GCP Marketplace?  **A**: We currently support Standard and Enterprise plans, with support for annual pre-commitment contracts coming soon. The Starter (free) plan is not available.  **Q**: Can I change plans while being billed through GCP Marketplace?  **A**: Yes, you can still upgrade and downgrade plans through the billing page in the Pinecone console.  **Q**: Do I need a GCP marketplace account for this?  **A**: Yes, you need a GCP Marketplace account. To purchase Pinecone, you must be logged in to the GCP Marketplace and your project must be enabled for purchase by your billing administrator.  **Q**: Where will I see how much I’m being charged for Pinecone?  **A**: All billing through marketplaces will be charged through your cloud provider. Check the billing console from your cloud provider.  **Q**: If I’m already a Pinecone customer, how do I switch to billing on the GCP marketplace?  **A**: Subscribing to GCP marketplace will automatically create a new organization for you. If you want to use GCP billing for an existing organization or projects, please reach out to support@pinecone.io and we’ll help migrate them for you. ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2fa"
  },
  "title": "\"Question Answering\"",
  "headline": "\"An Introduction to Open Domain Question-Answering\"",
  "weight": "7",
  "name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "Illustrated guide to open domain question-answering.",
  "images": "['/images/qa-overview-1.jpg']",
  "content": "Search is a crucial functionality in many applications and companies globally. Whether in manufacturing, finance, healthcare, or *almost* any other industry, organizations have vast internal information and document repositories.  Unfortunately, the scale of many companies’ data means that the organization and accessibility of information can become incredibly inefficient. The problem is exacerbated for language-based information. Language is a tool for people to communicate often abstract ideas and concepts. Naturally, ideas and concepts are harder for a computer to comprehend and store in a meaningful way.  <p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/-td57YvJdHc\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>  Most organizations rely on a cluster of keyword-based search interfaces hosted on various *'internal portals'* to deal with language data. If done well, this can satisfy business requirements for *some* of that data.  If a person knows what they're looking for and they know the keywords and terminology of the information they need, a keyword-based search is ideal. When the keywords and terminology of the answer are unknown, keyword search is inadequate. People searching for unknown answers in large repositories of documents is a drain on productivity.   How do we minimize this problem? The answer lies with *semantic search*, specifically with the question-answering (QA) flavor of semantic search.  Semantic search allows us to [search based on concepts and ideas](/learn/sentence-embeddings/) rather than keywords. Given a phrase, a semantic search tool returns the most *semantically similar* phrases from a repository.  Question-answering takes this idea further by searching using a natural language question and returning relevant documents and specific answers. QA aims to mimic natural language as much as possible. If we asked a shop assistant *\"where are those tasty, freshly baked things that are not cookies but look like cookies?\"*, we would expect directions that take us to those things. This natural form of conversation is what QA aims to reproduce.  This article will introduce the different forms of QA, the components of these *'QA stacks'*, and where we might use them.  ## Question Answering at a Glance  Before we dive into the details, let us paint a high-level picture of QA. First, our focus is on *open-domain* QA (ODQA). ODQA systems deal with questions across broad topics and cannot rely on a specific set of rules in your code. The alternative to *open-domain* is *closed-domain*, which focuses on a limited domain/scope and *can* often rely on explicit logic. We will **not** cover *closed-domain* QA.  For the remainder of the article, I will use **OD**QA and QA interchangeably. ODQA models can be split into a few subcategories.  ![qa_types](/images/qa-overview-2.jpg) <small>There are a few approaches to question answering (QA).</small>  The most common form of QA is **open-book extractive QA** (top-left above). Here we combine an information retrieval (IR) step and a reading comprehension (RC) step.  Any *open-book* QA requires an IR step to *retrieve* relevant information from the 'open-book'. Just as with open-book exams where students can refer to their books for information during the exam, the model can refer to an external source of information. That source of information may be internal company documents, Wikipedia, Reddit, or any other information source that *is not* the model itself.  The IR step retrieves relevant documents and passes them to the RC (reader) step. RC consists of *extracting* a succinct answer from a sentence or paragraph, typically referred to as the *document* or *context*.  ![question_context_answer](/images/qa-overview-3.jpg) <small>Example of a *question*, relevant *context*, and an *answer*.</small>  The other two types of QA rely on *generating* answers rather than *extracting* them. OpenAI's GPT models are well-known generative transformer models.  In *open-book* abstractive QA, the first IR step is the same as extractive QA; relevant contexts are *retrieved* from an external source. These contexts are passed to the text generation model (such as GPT) and used to *generate* (not extract) an answer.  Alternatively, we can use *closed-book* abstractive QA. Here there is only a text generation model and *no* IR step. The generator model will generate an answer based on its own internal learned representation of the world. It *cannot* refer to any external source of information hence the name *closed-book*.  Let's dive into each of these approaches and learn where we might apply each.  ### Extractive QA  Extractive QA is arguably the most widely applicable form of question-answering. It allows us to ask a question and then *extract* an answer from a short text. For example, we have the text (or *context*):  ``` Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. ```  To which we could ask the question, `\"which team represented the AFC at Super Bowl 50?\"`  and we should expect to return `\"Denver Broncos\"`.  The example where we present a single *context* and extract an answer is reading comprehension (RC). Alone, this is not particularly useful, but we can couple it with an external data source and search through *many contexts*, not just one. We call this 'open-book extractive QA'. More commonly referred to as just *extractive QA*. It is not a single model but actually consists of *three* components:  * Indexed data (document store/vector database) * Retriever model * Reader model  Before beginning to ask questions, open-book QA requires indexing data that our retriever model can later access. Typically this will be chunks of sentence-to-paragraph-sized text.  ![retriever_reader_stack](/images/qa-overview-4.jpg) <small>The open-book extractive QA stack includes the 'open-book' database, a retriever model, and a reader model.</small>  Let’s work through an example. First, we need data. A popular QA dataset is the Stanford Question and Answering Dataset (SQuAD). We can download this dataset using Hugging Face's `datasets` library like so:  {{< notebook file=\"get-squad\" height=\"full\" >}}  Here we have the *context* feature. It is these contexts that should be indexed in our database.  Options for the type of database vary based on the retriever model. A traditional retriever uses *sparse vector* retrieval with TF-IDF or BM25. These models return *contexts* based on the frequency of matching words between a *context* and the *question*. More word matches equate to higher relevance. Elasticsearch is the most popular database solution for this thanks to their scalable and strong keyword search capabilities.  The other option is to use *dense vector* retrieval with sentence vectors built by [transformer models](/learn/transformers/) like [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)). Dense vectors have the advantage of enabling search via *semantics*. Searching with the meaning of a question as described in the *'tasty, freshly baked things'* example. For this, a vector database like [Pinecone](/) or a standalone vector index like [Faiss](/learn/faiss/) is needed.  We will try the *dense vector* approach. First, we encode our *contexts* with a QA model like `multi-qa-MiniLM-L6-cos-v1` from sentence-transformers. We initialize the model with:  {{< notebook file=\"init-qa-model\" height=\"full\" >}}  Using the model, we encode the contexts inside our dataset object `qa` to create the sentence vector representations to be indexed in our vector database.  {{< notebook file=\"build-encodings\" height=\"full\" >}}  Now we can go ahead and store these inside a vector database. We will use Pinecone in this example (which does require a [free API key](https://app.pinecone.io)). First, we initialize a connection to Pinecone, create a new index, and connect to it.  {{< notebook file=\"init-pinecone-1\" height=\"full\" >}}  From there, all we need to do is `upsert` (*up*load and in*sert*) our vectors to the Pinecone index. We do this in batches where each sample is a tuple of `(id, vector)`.  {{< notebook file=\"pinecone-upsert\" height=\"full\" >}}  Once the contexts have been indexed inside the database, we can move on to the QA process.  Given a question/query, the *retriever* creates a sparse/dense vector representation called a *query vector*. This query vector is compared against all of the already indexed *context vectors* in the database. The *n* most similar are returned.  {{< notebook file=\"query\" height=\"full\" >}}  These most similar contexts are passed (one at a time) to the *reader* model alongside the original question. Given a question and context, the reader predicts the start and end positions of an answer.  ![start_end_reader](/images/qa-overview-5.jpg) <small>The reader model predicts the start and end positions of an answer given a question and a context containing the answer.</small>  We will use the `deepest/electra-base-squad2` model from HuggingFace’s `transformers` as our reader model. All we do is set up a `'question-answering'` pipeline and pass our *query* and *contexts* to it one by one.  {{< notebook file=\"reader\" height=\"full\" >}}  The reader prediction is repeated for each context. From here — if preferred — we can order the 'answers' using the scores output by the retriever and/or reader models.  As we can see, the model returns the correct answer of `'Denver Broncos'` with a score of 0.99. Most other answers return only minuscule scores, showing that our reader model easily distinguishes between good and bad answers.  ### Abstractive QA  As we saw before, abstractive QA can be split into two types: open-book and *closed*-book. We will start with *open-book* as the natural continuation of the previous extractive QA pipeline.  #### Open Book  Being **open-book** *abstractive* QA, we can use the same database and retriever components used for *extractive QA*. These components work in the same way and deliver a set of *contexts* to our *generator* model, which replaces the *reader* from extractive QA.  ![retriever_reader_stack](/images/qa-overview-6.jpg) <small>Open-book abstractive QA pipeline, note that the *reader* model has been replaced with a *generator* model (highlighted) when compared to the extractive QA stack.</small>  Rather than *extracting* answers, contexts are used as input (alongside the question) to a generative sequence-to-sequence (seq2seq) model. The model uses the question and context to *generate* an answer.  Large transformer models store 'representations' of knowledge in their parameters. By passing relevant contexts and questions into the model, we *hope* that the model will use the context alongside its 'stored knowledge' to answer more *abstract* questions.  The seq2seq model used is commonly BART or T5-based. We will go ahead and initialize a seq2seq pipeline using a BART model fine-tuned for abstractive QA — `yjernite/bart_eli5`.  {{< notebook file=\"bart-eli5-init\" height=\"full\" >}}  The question we asked before is specific. We're looking for a short and concise answer of `Denver Broncos`. Abstractive QA is not ideal for these types of questions:  {{< notebook file=\"bert-eli5-out-0\" height=\"full\" >}}  Instead, the benefit of abstractive QA comes with more 'abstract' questions like `\"Do NFL teams only care about playing at the Super Bowl?\"` Here, we're almost asking for an opinion. There is unlikely to be an *exact* answer. Let's see what the abstractive QA method thinks about this.  {{< notebook file=\"abstract-question\" height=\"full\" >}}  These answers look *much* better than our 'specific' question. The returned contexts don't include direct information about whether the teams care about being in the Super Bowl. Instead, they contain snippets of concrete NFL/Super Bowl details.  The seq2seq model combines those details and its own internal 'knowledge' to produce some insightful thoughts on the question:  * *\"No, because it is the pinnacle of professional football\"* — points out that teams in the Super Bowl (whether they win or not) already know they're at the top; in a way, they've *'already won'*. * *\"They don't care if they lose, they just care if they get a nice, big crowd to cheer\"* — players are happy that they get to entertain their fans; that is, the Super Bowl is less important. * *\"They are paid a lot of money to be in the Superbowl\"* — points out the more obvious 'who *wouldn't* want bucket loads of money?'.  There is plenty of contradiction and opinion, but that is often the case with more abstract questioning, particularly with the question we asked.  Although these results are interesting, they're not perfect. We can tweak parameters such as `temperature` to increase/decrease randomness in the answers, but abstractive QA can be limited in its coherence.  #### Closed Book  The final architecture we will look at is *closed-book* abstractive QA. In reality, this is nothing more than a generative model that takes a question and relies on *nothing more* than its own internal knowledge. There is **no** retrieval step.  ![retriever_reader_stack](/images/qa-overview-7.jpg) <small>The closed-book architecture is much simpler, there is nothing more than a *generator* model.</small>  Although we're dropping the retriever model, that doesn't mean we stick with the same reader model. As we saw before, the `yjernite/bart_eli5` model requires input like:  ``` question: <our question> context: <a (hopefully) relevant context> ```  Without the context input, the previous model does not perform as well. This is to be expected. The seq2seq model is optimized to produce coherent answers when given both question *and* context. If our input is in a new, unexpected format, performance suffers:  {{< notebook file=\"bart-eli5-no-context\" height=\"full\" >}}  The model doesn't know the answer and flips the direction of questioning. Unfortunately, this isn't really what we want. However, there are many alternative models we can try. The GPT models from OpenAI are well-known examples of generative transformers and can produce good results.  GPT-3, the most recent GPT from OpenAI, is locked behind an API, but there are open-source alternatives like GPT-Neo from Eleuther AI. Let's try one of the smaller GPT-Neo models.  {{< notebook file=\"gpt-neo-0\" height=\"full\" >}}  Here we're using the `'text-generation'` pipeline. All we do here is generate text following a question. We do get an interesting answer which is true but doesn't necessarily answer the question. We can try a few more questions.  {{< notebook file=\"gpt-neo-answers\" height=\"full\" >}}  We can tweak parameters to reduce the likelihood of repetition.  {{< notebook file=\"with-do-sample\" height=\"full\" >}}  We do get some interesting results, although it is clear that *closed-book* abstractive QA is a challenging task. Larger models store more internal knowledge; thus, closed-book performance is very much tied to model size. With bigger models, we can get better results, but for consistent answers, the open-book alternatives tend to outperform the closed-book approach.    That's it for our article on open-domain question answering (ODQA). We've worked through the idea behind semantic similarity and how it is applied to QA models.  We explored the various components that produce these 'QA stacks', like [vector databases](/learn/vector-database/), retrievers, readers, and generators. Alongside that, we've learned how to implement these different stacks using different tools and models. All of this should provide a strong foundation for exploring the world and opportunities of ODQA further.    ## Further Reading  * L. Weng, [How to Build an Open-Domain Question Answering System?](https://lilianweng.github.io/lil-log/2020/10/29/open-domain-question-answering.html), GitHub Blog  * D. Khashabi, et al., [UnifiedQA: Crossing Format Boundaries with a Single QA System](https://arxiv.org/pdf/2005.00700.pdf) (2020), EMNLP  * [Extractive Question Answering](https://huggingface.co/transformers/usage.html#extractive-question-answering), Hugging Face Docs ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2fc"
  },
  "title": "\"Optimize Classifier Training with Vector Search\"",
  "headline": "\"Optimize Classifier Training with Vector Search\"",
  "weight": "1",
  "- name": "Edo Liberty",
  "position": "Founder and CEO",
  "src": "/images/company-edo.png",
  "href": "https://edoliberty.github.io/",
  "description": "How to supercharge fine-tuning for classification with vector search",
  "images": "['https://www.pinecone.io/images/classifier-train-vector-search-0.png']",
  "content": "Pretrained models dominate the world of machine learning. Very few ML projects begin by training a new model from scratch. Instead, people often start by taking an off-the-shelf model like Resnet or BERT and fine-tuning it for another domain, or using an existing in-house model for the same purpose.  The ecosystem of pretrained models, both external and in-house, has allowed us to push the limits of what is possible. This doesn't mean, however, that there are no challenges.  Fortunately, we can tackle some of these problems across many different pretrained models, because they often share similar points of failure. One of those is the excessive compute and data needed to fine-tune a pretrained model for classification.  A common scenario will have some model containing a linear layer that outputs a classification. Preceding this linear layer, we can have anything from a small neural network to a billion-parameter language model. In either case, it’s the classification layer producing the final prediction.  That means  we can almost ignore the preceeding model layers, and focus on the classification layer alone. This classification layer can become a single point of failure (or success) for accurate predictions.  The classification layer alone can be fine-tuned, and it often is. A common approach for fine-tuning this layer may look like this:  1. Collect a dataset that focuses on enabling the model to adapt to a new domain or handle data drift, 2. Slog through this dataset, labeling records as per their classification, and 3. Once the records have all been labeled, fine-tune the classifier.  This approach works, but it isn't efficient. There is a better way...  We need to focus fine-tuning efforts on *essential samples* that would have the greatest impact on the performance of the classifier. Otherwise, we waste time and compute by annotating and fine-tuning on samples that make little-to-no difference to model performance.  The question becomes: How do you determine which samples are essential? That’s where vector search comes in. You can use vector search to identify and focus on the essential records that *really* make a difference in model performance. This will save valuable time and compute by skipping all non-essential records when fine-tuning the model.  ---  *All code covering the content of this article can be [found here](https://github.com/pinecone-io/examples/tree/master/analytics-and-ml/model-training/classifier-train-vector-search).*  ---  ## Training with Vector Search  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/pfwBut7E60Q\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  Vector search will play a key role in optimizing our training steps. First, let's understand where vector search fits into all of this.  Many state-of-the-art (SOTA) models are available for use as *pretrained models*. That includes models like Google's BERT and T5, and OpenAI's CLIP. These models use millions, even billions, of parameters and perform many complex operations. Yet, when applied to classification, these models rely on simple linear or feedforward network layers to make the final prediction.  <video autoplay loop muted playsinline class=\"responsive\">  <source src=\"./images/classifier-train-vector-search-1.mp4\" type=\"video/mp4\"> </video>   The reason for this is that these models are *not* trained to make class predictions; they're trained to make [vector embeddings](https://www.pinecone.io/learn/vector-embeddings/).  Vectors created by these models are full of helpful information that belong to a learned structure in a high-dimensional vector space. That helpful information is abstracted beyond human comprehension, but the effect is that similar items are located close to one another in vector space, whereas dissimilar items are not.  The result is that each of these models creates a \"map\" of information. Using this map, they can consume data, like images and text, and output a meaningful vector *representation* of said data.  ![vector-maps](./images/classifier-train-vector-search-2.png)  In these maps, we will find that sentences, images, or whatever form of data you're working with belongs to a specific region based on the data's characteristics.  Pretrained models are very good at producing accurate maps of information. Because of that, all we need to translate these into accurate class predictions is a simple layer that learns to *identify the different regions* in this map.  ### Linear Classifiers  A typical architecture for classification consists of a pretrained model followed by a linear layer. A binary linear classifier (that predicts one of two labels) works by taking the dot product between an input vector $X$ and its own internal weights $W$. Based on a threshold, the output of this operation will be categorized as one of two classes.  ![w-dot-x](./images/classifier-train-vector-search-8.png)   The dot product of two vectors returns a *positive* score if they share a similar direction, $0$ if they are orthogonal, and a *negative* score if they have opposite directions.  There is one key problem with dot product similarity, it considers *both* direction and *magnitude*. Magnitude is troublesome because vectors with greater magnitudes often overpower more similar, lower-magnitude vectors. To avoid this, we normalize the vectors being output by our pretrained models.  The result is that a linear classifier must learn to align its internal weights $W$ with the vectors $X$ labeled as $+1$ and push its internal weights away from vectors labeled as $-1$.  ![vector-maps-learn-binary](./images/classifier-train-vector-search-9.png)  Fine-tuning the classifier like this works, but there are some unecessary limitations. First, imagine we return *only* irrelevant samples for a training batch. They will all be marked as $-1$. The classifier knows to move away from these values but it cannot know which direction to move towards. In high-dimensional spaces, this is problematic and will cause the classifier to move at random.  Second, many training samples may be more or less relevant. \"A dog\" is more relevant than \"a truck\" to the query \"dogs in the snow\", yet, \"a dog in the snow\" is *not* equally relevant as \"a dog\".  ![vector-maps-learn-contrastive](./images/classifier-train-vector-search-10.png)  What we need is a *gradient* of relevance, a continuous range from -1 to +1. The first problem is solved as the *range* of scores gives the classifier information on the best direction of movement. And the second problem is solved as we can now be more precise with our relevance scores.  All of this allows a linear classifier to learn where to place itself within the vector space produced by the model layers preceding it.  That describes the fine-tuning process, but we cannot do this across our entire dataset. It would take too much time annotating everything. To do this efficiently, we must capitalize on the idea of identifying relevant vs. irrelevant vectors within proximity of the model’s learned weights.  ![with-without-efficient-samples](./images/classifier-train-vector-search-11.png)  By identifying the vectors with the highest proximity to the classifier's learned boundaries, we are able to skip irrelevant samples that make little-to-no impact on the classifier performance. Instead, we hone-in on the critical area of vectors near the target vector space.  ### Training Efficiently with Vector Search  During training, we need to feed vectors generated by the preceding layers into our linear classifier. Those vectors also need to be labeled. But, if our classifier is already tuned to understand the vector space generated by the previous layers, most training data is unlikely to be helpful.  We need to focus our fine-tuning efforts on records that are similar enough to our target class to confuse our model. For an already trained classifier, these are the false positives and false negatives predicted by the classifier.  However, we don’t usually have a list of false positives and false negatives. But we do know that the solvable errors will be present near the classifiers decision boundary; the line that separates the positive predictions from negative predicitons.  Due to the proximity of these samples, it is harder for the classifier to find the exact boundary that best identifies true positives vs. true negatives.  Vector search allows us to retrieve the high proximity samples most similar to the model weights $W$. We can then label the returned samples and use them for training our model. The model optimizes its internal weights; we extract them again, search, and repeat.  ![training-process](./images/classifier-train-vector-search-12.png)  We focus annotation and training on essential samples by retrieving the most similar vectors. Doing this avoids wasting time and compute on samples that make little to no difference to our model performance.  ---  ## Putting it All Together  Now let's combine all this to fine-tune a linear classifier with vector search.  There are two parts to our training process:  1. **Indexing** our data: Here we must embed everything as vectors using the \"preceding\" model layers (BERT, ResNet, CLIP, etc.). 2. **Fine-tuning** the classifier: We will query using model weights $W$, return the most similar (or high scoring) records, annotate, and fine-tune the model.  *If you already have an indexed dataset, you can skip ahead to the **Fine-tuning** section. If not, we'll work through the indexing steps next.*  ### Indexing  Given a dataset of images (or other formats), we first need to process everything through the preceding model layers to generate a list of vectors to be indexed. These vectors will later be used as the training data for the model.  ---  *The terms vectors, embeddings, and vector embeddings will be used interchangeably. When specifying embeddings produced by a specific medium (such as images or text), we will refer to them as \"image embeddings\" or \"text embeddings\".*  ---  For our example, we will use a model capable of comparing *both* text and images called CLIP. OpenAI's CLIP has been trained to match similar natural language prompts to images. It does this by encoding pairs as closely as possible in a vector space.   #### Initialization of Dataset and CLIP  We need an image dataset and CLIP (swap these for your dataset and model where relevant). We will use the `frgfm/imagenette` dataset found on Hugging Face datasets.  {{< notebook file=\"query-train-dataset\" height=\"full\" >}}  In the *\"image\"* feature of the dataset, we have ~9.4K images of various sizes stored as PIL objects. Inside a Jupyter notebook, we can view them like so:  {{< notebook file=\"query-train-show-image\" height=\"full\" >}}  We embed these images using CLIP, which we initialize through the HuggingFace *Transformers* library.  ```python # !pip install transformers torch from transformers import CLIPProcessor, CLIPModel import torch  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  model_id = \"openai/clip-vit-base-patch32\" model = CLIPModel.from_pretrained(model_id).to(device) processor = CLIPProcessor.from_pretrained(model_id) ```  We can embed an image and transform it into a *flat* Python list (ready for indexing) like so:  {{< notebook file=\"query-train-embed-image\" height=\"full\" >}}  #### Normalization is Important  The later linear classifier uses dot product to calculate predictions. That means we must also use dot product to measure the similarity between image embeddings during the vector search. Given two similar images of dogs and an image of a radio, we would expect the two dog images to return a higher score.  ![no-norm-image-embeds](./images/classifier-train-vector-search-3.png)  <small>We would expect two nearby embeddings like **a** and **b** to return a higher similarity score than with **c**. Yet, when we calculate the dot product between these embeddings, the magnitude of **c** outputs a higher output.</small>  {{< notebook file=\"query-train-dot-product\" height=\"full\" >}}  Dot product is heavily influenced by vector magnitude. This means two very similar vectors with low magnitude can score lower than if they were compared to a dissimilar vector with greater magnitude.  We solve this problem by normalizing all of our vectors beforehand. By doing this, we \"flatten\" the magnitude across vectors, leaving just the angular difference between them.  ![with-norm-image-embeds](./images/classifier-train-vector-search-4.png)  <small>Normalization \"flattens\" the magnitude of our vectors.</small>  {{< notebook file=\"query-train-dot-product-norm\" height=\"full\" >}}  After normalization of our embedding with `emb = emb / np.linalg.norm(emb)`, we can move on to indexing it in our vector database.  #### Vector Database and Indexing  Here we will use the [Pinecone vector database](https://www.pinecone.io/). All we need is a *free* API key and `environment` variable that can be [found here](https://app.pinecone.io/). To install the Pinecone Python client, we use `pip install pinecone-client`. Finally, we import and initialize the connection.  ```python import pinecone  pinecone.init(api_key=\"YOUR_API_KEY\", environment=\"YOUR_ENV\") # (default env is 'us-east1-gcp') ```  After connecting to Pinecone, we create a new index where we will store our vectors.  ```python index_name = \"imagenet-query-trainer-clip\"  pinecone.create_index(     index_name,     dimension=emb.shape[0],     metric=\"dotproduct\",     metadata_config={\"indexed\": [\"seen\"]} ) # connect to the index index = pinecone.Index(index_name) ```  We specify four parameters for our index:  * `index_name`: The name of our vector index, it can be anything. * `dimensions`: The dimensionality of our vector embeddings. This must match the vector dimensionality output by CLIP. All future vectors must have the same dimensionality. Our vectors have `768` dimensions. * `metric`: This is the similarity metric we will use. Pinecone accepts `\"euclidean\"`, `\"cosine\"`, and `\"dotproduct\"`. As discussed, we will be using `\"dotproduct\"`. * `metadata_config`: Pinecone has both *indexed* and non-indexed metadata. Indexed metadata can be used in [metadata filtering](https://www.pinecone.io/learn/vector-search-filtering/), and we need this for *\" exploring \"* the image dataset. So, we index a single field called `\"seen\"`.  With this, we have indexed a single vector (`emb`) in our Pinecone index. We can check this by running `index.describe_index_stats()` which will return:  ```json {'dimension': 512, 'index_fullness': 0.0, 'namespaces': {'': {'vector_count': 1}}, 'totalVectorCount': 1.0} ```  Those are all the steps we need to embed and index an image. Let's apply these steps to the remainder of the dataset.  ### Index Everything  There's little we can do with a single vector, so we will repeat the previous steps on the rest of our dataset. We place the previous logic into a loop, iterate once over the dataset, and we're done.  ```python from tqdm.auto import tqdm batch_size = 64  for i in tqdm(range(0, len(imagenet), batch_size)):     # select the batch start and end     i_end = min(i + batch_size, len(imagenet))     # some images are grayscale (mode=='L') we only keep 'RGB' images     images = [img for img in imagenet[i:i_end]['image'] if img.mode == 'RGB']     # process images and extract pytorch tensor pixel values     image = processor(         text=None,         images=images,         return_tensors='pt',         padding=True     )['pixel_values'].to(device)     # feed tensors to model and extract image features     out = model.get_image_features(pixel_values=image)     out = out.squeeze(0)     # take the mean across each dimension to create a single vector embedding     embeds = out.cpu().detach().numpy()     # normalize and convert to list     embeds = embeds / np.linalg.norm(embeds, axis=0)     embeds = embeds.tolist()     # create ID values     ids = [str(i) for i in range(i, i_end)]     # prep metadata     meta = [{'seen': 0} for image in images]     # zip all data together and upsert     to_upsert = zip(ids, embeds, meta)     index.upsert(to_upsert) ```  There's a lot of code here, but it's nothing more than a compact version of the previous steps. We can check the number of records added using the `describe_index_stats` method.  ```json {'dimension': 512, 'index_fullness': 0.0, 'namespaces': {'': {'vector_count': 9296}}, 'totalVectorCount': 9296.0} ```  We have slightly fewer records here because we drop grayscale images in the upsert loop (line 8).  ## Fine-Tuning  With everything indexed, we're ready to take our classifier model and optimize it on the most relevant samples in our dataset. You can follow along live using [this Colab notebook](https://colab.research.google.com/github/pinecone-io/examples/blob/master/analytics-and-ml/model-training/classifier-train-vector-search/01-fine-tune-vector-search-contrastive.ipynb).  You may or may not have a classifier already trained. If you **do** have a classifier, you can skip ahead a few paragraphs to the **Classifier** section.  If you **do not** have a classifier, we can begin by setting the model weights $W$ equal to the vector produced by a relevant query. This is where the *text-to-image* capabilities of CLIP come into use. Given a natural language prompt like *\"dogs in the snow\"*, we can use CLIP to embed this into the same vector space as our image embeddings.  {{< notebook file=\"query-train-text-prompt\" height=\"full\" >}}  We will set our initial model weights equal to `xq`, but first, let's retrieve the first batch of training samples.  As with the image embeddings, we need to transform the CLIP output into a flat list for querying with Pinecone and retrieving the image `idx` and vector `values`:  ```python xc = index.query(xq, top_k=10, include_values=True)  # get the index values idx = [int(match['id']) for match in xc['matches']] # get the vectors values = [match['values'] for match in xc['matches']] ```  ![dogs-in-snow-results](./images/classifier-train-vector-search-5.png)  <small>The \"dogs in the snow\" query is mostly accurate, with the two exceptions showing dogs on non-snow yet white backgrounds.</small>  These images and their embeddings act as the training data for our classifier. The embeddings themselves will become the inputs `X`. We allow the user to create the labels ` y` by entering a score from `-1` to `+1`. All of this will be performed by a function called `score_images`, the code for this can be [found here](https://colab.research.google.com/github/pinecone-io/examples/blob/master/analytics-and-ml/model-training/classifier-train-vector-search/01-fine-tune-vector-search-contrastive.ipynb).  {{< notebook file=\"query-train-score\" height=\"full\" >}}  Above we can see the images followed by a printout of their ID values and the scores assigned to them, all of these pairs are stored in `scores` as a dictionary. These scores are our training data; all that is left is to train our model with it. So, we initialize the classifier.  ### Classifier  Here, we will use a simple linear binary classifier in PyTorch. The model weights will act as our future query vectors. As the model learns to distinguish between relevant and irrelevant vectors, it will optimize its internal weights to produce a vector more like the vectors we marked with the label `1` (relevant).  ```python import torch  # initialize the model with 512 input size (equal to vector size) and one output model = torch.nn.Linear(512, 1)  # convert initial query `xq` to tensor paramater for initial model weights init_weight = torch.Tensor(xq).reshape(1, -1) model.weight = torch.nn.Parameter(init_weight)  # init loss and optimizer loss = torch.nn.BCEWithLogitsLoss() # we set the lr high for these examples, in real-world use case this # may need to be lower for more stable fine-tuning optimizer = torch.optim.SGD(model.parameters(), lr=0.2) ```  On lines 7-8, we set the model weights to the initial query we made. If you **already have a classifier**, this part is not necessary. By initializing the model weights like this, we start in a more relevant vector space, from which we can begin fine-tuning the model and optimizing our query.  We will write a small PyTorch training loop and place it in a function called `fit`. The number of iterations `iters` can be set to move slower/faster through the vector space for each training batch.  ```python model.train()  # switch model to training mode  def fit(X: list, y: list, iters: int = 5):     for _ in range(iters):         # get predictions         out = model(torch.Tensor(X))         # calculate loss         loss_value = loss(out, torch.Tensor(y).reshape(-1, 1))         # reset gradients         optimizer.zero_grad()         # backpropagate         loss_value.backward()         # update weights         optimizer.step()          # train fit(X, y) ```  After we've run the training loop, we can extract the new model weights to use as our next query vector, `xq`.  ```python xq = model.weight.detach().numpy()[0].tolist() ```  We can return many of the same items during training if querying with this slightly fine-tuned `xq` vector. Increasing `lr` and `iters` to shift the fine-tuned `xq` values more quickly might avoid this, but it will struggle to converge on an optimal query. On the other hand, decreasing `lr` and `iters` will mean we keep seeing the same set of images and will overfit them.  We want the classifier to see a broader range of both positive and negative images without needing excessive values for `lr` and `iters`. Instead, we keep these two parameters low and filter out all previously seen images.  ---  *These examples use excessively high `lr` and `iters` parameters to demonstrate the movement across vector space. We recommended using lower values to provide a more stable training process.*  ---  Filtering is done via Pinecone's metadata filtering. Earlier we initialized the index with `metadata_config={\"indexed\": [\"seen\"]}` and added `{\"seen\": 0}` to the metadata of every record. All of that was for this next step. We set their metadata for the previous 10 retrieved records to `{\"seen\": 1}`.  ```python # we must update one record at a time for i in idx:     index.update(str(i), set_metadata={\"seen\": 1}) ```  When we query again, we can add a filter condition `filter={\"seen\": 0}` to return only *unseen* records.  ```python # retrieve most similar records xc = index.query(     xq,     top_k=10,     include_values=True,     filter={\"seen\": 0} ) # extract index and vector values idx = [int(match['id']) for match in xc['matches']] values = [match['values'] for match in xc['matches']] ```  Starting with dogs in the snow, let's imagine we'd like to adjust our already well-trained \"dogs in snow\" classifier to become a \"dogs at dog shows\" classifier. How do we influence the model to retrieve images from this new domain?  <video autoplay loop muted playsinline class=\"responsive\">  <source src=\"./images/classifier-train-vector-search-6.mp4\" type=\"video/mp4\"> </video> <small>Traversing across clusters of similar images in the [semantic query trainer app](https://huggingface.co/spaces/pinecone/semantic-query-trainer), this example uses irrelevant/relevant labels. The app has since been updated to use contrastive sliders to score images.</small>  The example above starts with our slightly fine-tuned \"dogs in the snow\" embedding in both windows. We then change what is marked as relevant. The left window shows us traversing from dogs in the snow to garbage trucks and back to dogs. In the right window, we traverse to dogs in fields and finally to dog shows.  We can replicate this process by repeating the logic we have already worked through. You can find an example that wraps this code into a few [training/retrieval functions here](https://colab.research.google.com/github/pinecone-io/examples/blob/master/analytics-and-ml/model-training/classifier-train-vector-search/01-fine-tune-vector-search-contrastive.ipynb).  As we keep doing this, the number of retrieved dog images will quickly increase until they dominate the returned images, or we simply exhaust all relevant images. At this point, we can stop and test our newly trained query vector on the unfiltered dataset. We can do this in one of two ways:  1. We drop the `filter` argument in `query`. This is ideal if performing a quick test but will not work if planning to perform a second loop through the dataset or train another query. 2. We reset the filter values, switching all records with `{\"seen\": 1}` back to `{\"seen\": 0}`.  To apply method *2*, we iteratively query the index with a filter of `{\"seen\": 1}`, resetting the metadata, and stop *only* when we return no more records.  ```python while True:     xc = index.query(xq, top_k=100, filter={\"seen\": 1})     idx = [match['id'] for match in xc['matches']]     if len(idx) == 0: break     for i in idx:         index.update(str(i), set_metadata={\"seen\": 0}) ```  When we search again, we will return a completely unfiltered view of the search results.  ```python xc = index.query(     xq,     top_k=10 ) # extract index and vector values idx = [int(match['id']) for match in xc['matches']]  # show the results for i in idx:     print(i)     plt.imshow(imagenet[i]['image'])     plt.show() ```  ![dog-show-results](./images/classifier-train-vector-search-7.png)  <small>Images retrieved after fine-tuning the query for retrieving dog shows.</small>  Our query has clearly been optimized for finding images of dog shows. We can go ahead and save our classifier model.  ```python with open(\"classifier.pth\", \"wb\") as f:     torch.save(model, f) ```  In the next section, we'll look at classifying images using our model fine-tuned with vector search.  ## Classifier Predictions  We know how to optimize our query and hone in on specific concepts and clusters of images. With this, our classifier has hopefully become great at identifying images of dog shows. Its internal weights $W$ should have aligned to the vectors $X$ that best represent the concept of \"dog shows\".  There is just one more step. How do we make and interpret predictions with our new classifier? We start by loading the classifier from file (you can skip the save/load if preferred and use the same instance).  ```python with open(\"classifier.pth\", \"rb\") as f:     clf = torch.load(f) ```  We will test the predictions on the *validation* split of the imagenette dataset. To download this, we run the same `load_dataset` function as before but change the `split` parameter to `validation`.  ```python imagenet = load_dataset(     'frgfm/imagenette',     'full_size',     split='validation',  # here we switch to validation set     ignore_verifications=False  # set to True if seeing splits Error ) ```  Let's start with a dog show image and see what model outputs. As before, we will process and create the image embedding using CLIP.  {{< notebook file=\"query-train-make-pred-1\" height=\"full\" >}}  The prediction is positive, meaning the model predicts an image of a dog show! Let's try another.  {{< notebook file=\"query-train-make-pred-2\" height=\"full\" >}}  A negative value means the model predicts this is *not* a dog show. We can use this same logic to make predictions for the complete validation set and look at what the model predicts as dog shows.  ```python from tqdm.auto import tqdm  batch_size = 64  preds = []  for i in tqdm(range(0, len(imagenet), batch_size)):     i_end = min(i+batch_size, len(imagenet))     image = processor(         text=None,         images=imagenet[i:i_end]['image'],         return_tensors='pt',         padding=True     )['pixel_values'].to(device)     out = clip.get_image_features(pixel_values=image)     logits = clf(out)     preds.extend(logits.detach().cpu().numpy().reshape(1, -1)[0].tolist()) ```  We add these predictions to our dataset, filter out any results where the prediction is negative, and then sort the results.  {{< notebook file=\"query-train-get-preds\" height=\"full\" >}}  These look like great results. There are *23* results in total, and all but two of them are images of dog shows (find the [complete set of results here](https://colab.research.google.com/github/pinecone-io/examples/blob/master/analytics-and-ml/model-training/classifier-train-vector-search/02-classifier-test.ipynb).  ---  That is how we can optimize fine-tuning for linear classification layers with vector search. With this, we can hone in on what is important for our classifier and focus on these critical samples rather than slogging through the entire dataset and fine-tuning the model at random.  Doing this for an image classifier is just one example. We can apply this to various use cases, from anomaly detection to recommendation engines. The pool of use cases involving vector search is growing daily.  ---  ## Resources  [Semantic query trainer demo](https://huggingface.co/spaces/pinecone/semantic-query-trainer)  [GitHub Notebooks](https://github.com/pinecone-io/examples/tree/master/analytics-and-ml/model-training/classifier-train-vector-search)  [Colab 00: CLIP Indexing](https://colab.research.google.com/github/pinecone-io/examples/blob/master/analytics-and-ml/model-training/classifier-train-vector-search/00-indexer-clip.ipynb)  [Colab 01: Fine-tuning](https://colab.research.google.com/github/pinecone-io/examples/blob/master/analytics-and-ml/model-training/classifier-train-vector-search/01-fine-tune-vector-search-contrastive.ipynb)  [Colab 02: Classifier Testing](https://colab.research.google.com/github/pinecone-io/examples/blob/master/analytics-and-ml/model-training/classifier-train-vector-search/02-classifier-test.ipynb)",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e2fe"
  },
  "title": "\"Bag of Visual Words\"",
  "headline": "\"Bag of Visual Words\"",
  "weight": "2",
  "- name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "A look at one of the best pre-DL embedding methods for classification and retrieval",
  "images": "['https://www.pinecone.io/images/bag-of-visual-words-0.png']",
  "content": "In computer vision, bag of visual words (BoVW) is one of the pre-deep learning methods used for building image embeddings. We can use BoVW for content-based image retrieval, object detection, and image classification.  At a high level, comparing images with the bag of visual words approach consists of five steps:  1. Extract visual features, 2. Create *visual words*, 3. Build sparse frequency vectors with these visual words, 4. Adjust frequency vectors for relevant with tf-idf, 5. Compare vectors with similarity or distance metrics.  We will start by walking through the theory of how all of this works. In the second half of this article we will look at how to implement all of this in Python.  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/jjQetJtQDS4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ## How Bag of Visual Words Works  ### Visual Features  The model derives from **bag of words** in natural language processing (NLP), where a chunk of text is split into words or sub-words and those components are collated into an unordered list, the so-called \"bag of words\" (BoW).  <center><div> <img src=\"/images/bag-of-visual-words-1.png\" alt=\"Drawing\" style=\"width:70%;\"/></div> </center>  Similarly, in bag of *visual* words the images are represented by patches, and their unique patterns (or *visual features*) are extracted from the image.  <center><div> <img src=\"/images/bag-of-visual-words-2.png\" alt=\"Drawing\" style=\"width:80%;\"/></div> </center>  However, despite the similarity, these visual features are *not* visual words just yet; we must perform a few more steps. For now, let's focus on understanding what these visual features are.  Visual features consist of two items:  * **Keypoints** are points in an image, which do **not** change if the image is rotated, expanded, or scaled and  * **Descriptors** are vector representations of an image patch found at a given keypoint.  These visual features can be detected and extracted using a feature detector, such as **SIFT (Scale Invariant Feature Transform)**, **ORB (Oriented FAST and Rotated BRIEF)**, or **SURF (Speeded Up Robust Features)**.  The most common is **SIFT** as it is invariant to scale, rotation, translation, illumination, and blur. SIFT converts each image patch into a $128$-dimensional vector (i.e., the **descriptor** of the visual feature).  A single image will be represented by many SIFT vectors. The order of these vectors is not important, only their presence within an image.  ### Codebooks and Visual Words  After extracting visual features we build a **codebook**, also called a dictionary or vocabulary. This codebook acts as a repository of all existing **visual words** (similar to an **actual** dictionary, like the Oxford English Dictionary).  We use this codebook as a way to translate a potentially infinite variation of visual features into a predefined set of visual words.  How? The idea is to group similar visual features into clusters. Each cluster is assigned a central point which represents the visual word translation (or mapping) for that group of visual features. The standard approach for grouping visual features into visual words is [**k-means clustering**](https://www.pinecone.io/learn/k-means-clustering/).  K-means divides the data into $k$ clusters, where $k$ is chosen by us. Once the data is grouped, k-means calculates the mean for each cluster, i.e., a central point between all of the vectors in a group. That central point is a **centroid** (i.e., a **visual word**).  <center>  <div>    <img src=\"/images/bag-of-visual-words-3.png\" alt=\"Drawing\" style=\"width:90%;\"/>  </div> </center>  After finding the centroids, k-means iterates through each data point (visual feature) and checks which centroid (visual word) is nearest. If the nearest centroid has changed, the data point switches grouping, being assigned to the new nearest centroid.  This process is repeated over a given number of iterations or until the centroid positions have stabilized.  With that in mind, how do we choose the number of centroids, $k$?  It is more of an art than a science, but there are a few things to consider. Primarily, how many visual words can cover the various **relevant** visual features in the dataset.  That's not an easy thing to figure out, and it's always going to require some guesswork. However, we can think of it using the language equivalent, **bag of words**.  If our language dataset covered several documents about a specific topic in a single language, we would find fewer unique words than if we had thousands of documents, spanning several languages about a range of topics.  The same is true for images; dogs and/or animals could be a topic, and buildings could be another topic. As for the equivalent of different languages, this is not a perfect metaphor but we could think of different photography styles, drawings, or cartoons. All of these added layers of complexity increase the number of visual words needed to accurately represent the dataset.  Here, we could start with choosing a smaller $k$ value (e.g., $100$ or $150$) and re-run the code multiple times changing $k$ until convergence and/or our model seems to be identifying images well.  If we choose $k=150$, k-means will generate $150$ centroids and, therefore, $150$ visual words.  When we perform the mapping from new visual feature vectors to the nearest centroid (i.e., visual word), we categorize visual features into a more limited set of visual words. This process of reducing the number of possible unique vectors is called **vector quantization**.  <center><div> <img src=\"/images/bag-of-visual-words-4.png\" alt=\"Drawing\" style=\"width:70%;\"/></div> </center>  Using a limited set of visual words allows us to compress our image descriptions into a set of visual word IDs. And, more importantly, it helps us represent similar features across images using a shared set of visual words.  That means that the **visual words** shared by two images of churches may be quite large, meaning they're similar. However, an image of a church and an image of a dog will share far fewer visual words, meaning they're dissimilar.  After those steps, our images will be represented by a varying number of visual words. From here we move on to the next step of transforming these visual words into image-level frequency vectors.  ### Frequency Vectors  We can count the frequency of these visual words and visualize them with histograms.  The **x-axis** of the histogram is the codebook, while the **y-axis** is the frequency of each visual word (in the codebook) for that image.  If we consider $2$ images, we can represent the image histograms as follows:  <center><div> <img src=\"/images/bag-of-visual-words-5.png\" alt=\"Drawing\" style=\"width:80%;\"/></div> </center>  To create these representations, we have converted each image into a sparse vector where each value in the vector represents an item in the codebook (i.e., the x-axis in the histograms). Most of the values in each vector will be **zero** because most images will only contain a small percentage of total number of visual words, which is why we refer to them as **sparse** vectors.  As for the non-zero values in our sparse vector, they are calculated in the same way that we calculated our histogram bar heights. They are equal to the frequency of a particular visual word in an image.  This works, but it's a crude way to create these sparse vector representations. Because many visual words are actually not that important, we add one more step.  ### Tf-idf  In language there are some words that are more important than others in that they give us more information. If we used the sentence \"the history of Rome\" to search through a set of articles, the words \"the\" and \"of\" should not be given the same importance as \"history\" or \"Rome\".  These less important words are often very common. If we only consider the frequency of words shared with our \"the history of Rome\" query, the article with the most \"the\"s could be scored highest.  This problem is also found in images. A visual word extracted from a patch of sky in an image is unlikely to tell us whether this image is of a church or a dog. Some visual words are more relevant than others.  <center>  <div>    <img src=\"/images/bag-of-visual-words-9.png\" alt=\"Drawing\" style=\"width:70%;\"/>  </div> </center>  In the example above, we would expect a visual word representing the sky *1* to be less relevant than a visual word representing the cross on top of the bell tower *2*.  That is why it is important to adjust the values of our sparse vector to give more weight to more relevant visual words and less weight to less relevant visual words.  To do that, we can use the *tf-idf* (*term-frequency inverse document frequency*) formula, which is calculated as follows:  $$ tf\\textrm{--}idf_{t,d} = tf_{t,d} * idf_t = tf_{t,d} * log\\frac{N}{df_t} $$  Where:  * $tf_{t,d}$ is the term frequency of the visual word $t$ in the image $d$ (the number of times $t$ occurs in $d$), * $N$ is the total number of images, * $df_t$ number of images containing visual word $t$, * $log\\frac{N}{df_t}$ measures how common the visual word $t$ is across all images in the database. This is low if the visual word $d$ occurs many times in the image, high otherwise.  After **tf-idf**, we can visualize the vectors via our histogram again, which will better reflect the image's features.  <center>  <div>    <img src=\"/images/bag-of-visual-words-6.png\" alt=\"Drawing\" style=\"width:80%;\"/>  </div> </center>  Before we were giving the same importance to image's patches in an image; now they're adjusted based on relevance and then normalized.  We've now trained our codebook and learned how to process each vector for better relevance and normalization. When wanting to embed new images with this pipeline, we repeat the process but avoid retraining the codebook. Meaning we:  1. Extract the visual features, 2. Transform them into visual words using the existing codebook, 3. Use these visual words to create a sparse frequency vector, 4. Adjust the frequency vector based on relevance with tf-idf, giving us our final sparse vector representations.  After that, we're ready to compare these sparse vectors to find similar or dissimilar images.  ### Measuring Similarity  There are several metrics we can use to calculate similarity or distance between two vectors. The most common are:  1. **Cosine similarity**,  2. **Euclidean distance**, and  3. **Dot product similarity**.  We will use **cosine similarity** which measures the angle between vectors. Vectors pointing in a similar direction have a lower angular separation and therefore *higher* cosine similarity.  Cosine similarity is calculated as:  $$ cossim(A,B)= cos(\\theta)=\\frac{A \\cdot B}{||A|| \\space ||B||} $$  Cosine similarity generally gives a value ranging $[-1,1]$. However, if we think about the **frequency** of visual words, we cannot consider them as negative. Therefore, the angle between two term frequency vectors cannot be greater than $90°$, and cosine similarity ranges between $[0,1]$.  It equals $1$ if the vectors are pointing in the same direction (the angle equals $0$) and $0$ if vectors are perpendicular.  <center><div> <img src=\"/images/bag-of-visual-words-7.png\" alt=\"Drawing\" style=\"width:100%;\"/></div> </center>  If we consider three different images and we build a matrix based on cosine similarity:  <center><div> <img src=\"/images/bag-of-visual-words-8.png\" alt=\"Drawing\" style=\"width:80%;\"/></div> </center>  We can see that cosine similarity is $1$ when the image is exactly the same (i.e., in the main diagonal). The cosine similarity approaches $0$ as the images have less in common.  Let's now move on to implementing bag of visual words with Python.  ## Implementing Bag of Visual Words  The next section will work through the implementation of everything we've just learned in Python. If you'd like to follow along, use [this Colab notebook](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/image-retrieval/bag-of-visual-words/bag-of-visual-words.ipynb).  ### Imagenette Dataset Preprocessing  First, we want to import a dataset of images to train the model.  Feel free to use any images you like. However, if you’d like to follow along with the same dataset, we will use the `frgfm/imagenette` dataset from HuggingFace Datasets.  {{< notebook file=\"bovw-dataset\" height=\"full\" >}}  The dataset contains 9469 images, covering a range of images with dogs, radios, fishing, cities, etc. The `image` feature contains the images themselves stored as PIL object, meaning we can view them in a notebook like so:  {{< notebook file=\"bovw-show-image\" height=\"full\" >}}  To process these images we need to transform them from PIL objects to numpy arrays.  ```python import numpy as np  # initialize list images_training = []  for n in range(0,len(imagenet)):     # generate np arrays from the dataset images     images_training.append(np.array(imagenet[n]['image'])) ```  The dataset mostly consists of color images containing three color channels (red, green, and blue), but some are also grayscale containing just a single channel (brightness). To optimize processing time and keep everything as simple as possible, we will transform color images to grayscale.  ```python import cv2  # pip install opencv-contrib-python opencv-python  # convert images to grayscale bw_images = [] for img in images_training:     # if RGB, transform into grayscale     if len(img.shape) == 3:         bw_images.append(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY))     else:         # if grayscale, do not transform         bw_images.append(img) ```  The arrays in `bw_images` are what we will be using to create our visual features, visual words, frequency vectors, and tf-idf vectors.  ### Visual Features  With our dataset prepared we're ready to move on to extracting visual features (both keypoints and descriptors). As mentioned earlier, we will use the SIFT feature detection algorithm.  ```python # defining feature extractor that we want to use (SIFT) extractor = cv2.xfeatures2d.SIFT_create()  # initialize lists where we will store *all* keypoints and descriptors keypoints = [] descriptors = []  for img in bw_images:     # extract keypoints and descriptors for each image     img_keypoints, img_descriptors = extractor.detectAndCompute(img, None)     keypoints.append(img_keypoints)     descriptors.append(img_descriptors) ```  It's worth noting that if an image doesn't have any noticeable features (e.g., it is a flat image without any edges, gradients, etc.), extraction with SIFT can return `None`. We don't have that problem with this dataset, but it's something to watch out for with others.  Now that we have extracted the visual features, we can visualize them with `matplotlib`.  {{< notebook file=\"bovw-show-visual-features\" height=\"full\" >}}  The centre of each circle is the keypoint location, and the lines from the centre of each circle represent keypoint orientation. The size of each circle is the scale at which the features were detected.  With our visual features ready, we can move onto the next step of creating visual words.  ### Visual Words and the Codebook  Earlier we described the \"codebook\". The codebook acts as a vocabulary where we store *all* of our visual words. To create the codebook we use k-means clustering to quantize our visual features into a smaller set of visual words.  Our full set of visual features is big, and training k-means with the full set will take some time. So, to avoid that and also emulate a real-world scenario where we are unlikely to train on all images that we'll ever process, we will use a smaller sample of 1000 images.  ```python # set numpy seed for reproducability np.random.seed(0) # select 1000 random image index values sample_idx = np.random.randint(0, len(imagenet)+1, 1000).tolist()  # extract the sample from descriptors # (we don't need keypoints) descriptors_sample = []  for n in sample_idx:     descriptors_sample.append(np.array(descriptors[n])) ```  Our `descriptors_sample` contains a single array for each image, and each array can contain a varying number of SIFT feature vectors. When training k-means, we only care about the feature vectors, we don't care about which image they're coming from. So, we need to flatten `descriptors_sample` into a single array containing *all* descriptors.  {{< notebook file=\"bovw-prep-sample\" height=\"full\" >}}  From this, we get `all_descriptors`, a single array containing all feature vectors from our sample. There are ~1.3M of these.  We now want to group similar visual features (descriptors) using **k-means**. After a few tests, we chose $k=200$ for our model.  After k-means, all images will have been reduced to **visual words**, and the full set of these visual words become our codebook.  ```python # perform k-means clustering to build the codebook from scipy.cluster.vq import kmeans  k = 200 iters = 1 codebook, variance = kmeans(all_descriptors, k, iters) ```  Once built, the codebook does not change. No matter how many more visual features we process, no more visual words are added as we will use it solely as a mapping between new visual features and the existing visual words.  ---  *It can be difficult to find the optimal size of our codebook - if too small, visual words could be unrepresentative of all image regions, and if too large, there could be too many visual words with little to no of them being shared between images (making comparisons very hard or impossible).*  ---  With our codebook complete, we can use it to transform the full dataset of visual features into visual words.  {{< notebook file=\"bovw-map-to-visual-words\" height=\"full\" >}}  We can see here that image `0` contains `397` visual words; the first five of those are represented by `[84, 22, 45, 172, 172]`, which are the index values of the visual word vector found in the codebook. This visual word vector shares the same dimensionality as our SIFT feature vectors because it represents a cluster centroid from those feature vectors.  ### Sparse Frequency Vectors  After building our codebook and creating our image representations with visual words, we can move on to building sparse vector representations from these visual words.  We do this to compress the *many* visual word vectors representing our images into a single vector of set dimensionality. By doing this, we are able to directly compare our image representations using metrics like cosine similarity and Euclidean distance.  To create these frequency vectors, we look at how many times each visual word is found in an image. There are only 200 unique visual words (the length of our codebook), so each of these frequency vectors will have dimensionality 200, where each value becomes a count for a specific visual word.  {{< notebook file=\"bovw-create-freq-vec\" height=\"full\" >}}  After creating the frequency vectors, we're left with a single vector representation for each image. We can see an example of the frequency vector for image *0* below:  {{< notebook file=\"bovw-show-freq-vec\" height=\"full\" >}}  ### Tf-idf  Our frequency vector can already be used for comparing images using our similarity and distance metrics. However, it is not ideal as it does not consider the different levels of relevance of each visual word. So, we must use tf-idf to adjust the frequency vector to consider relevance. $$ tf\\textrm{-}idf_{t,d} = tf_{t,d} * idf_t = tf_{t,d} * log\\frac{N}{df_t} $$ We first calculate $N$ and $df_t$, both of which are shared across the entire dataset as the image $d$ is not considered by either parameter. Naturally, $idf_t$ also produces a single vector shared by the full dataset.  {{< notebook file=\"bovw-idf\" height=\"full\" >}}  With $idf_t$ calculated, we just need to multiply it by each $tf_{t,d}$ vector to get our $tf \\textrm{-} idf_{t,d}$ vectors. Fortunately, we already have the $tf_{t,d}$ vectors, as they are our frequency vectors.  {{< notebook file=\"bovw-tfidf\" height=\"full\" >}}  We now have *9469* 200-dimensional sparse vector representations of our images.  ### Search  These sparse vectors have been built in such a way that images that share many similar visual features should share similar sparse vectors. We can use cosine similarity to compare these images and identify similar images.  We will start by searching with image `1200`:  ![image-20220801174017452](./images/bag-of-visual-words-12.png)  {{< notebook file=\"bovw-search\" height=\"full\" >}}  The top image is of course the same image; as they are exact matches we can see the expected cosine similarity score of `1.0`. Following this, we have two highly similar  results. Interestingly, the fourth image seems to have been pulled through due to similarity in background foliage.  Here are a few more sets of results, showing the varied performance of the approach with different items.  ![retrieval-1](/images/bag-of-visual-words-10.png)  Here we get a good first result, followed by irrelevant images and a final good result in fifth position, a 50% success rate.  ![retrieval-2](/images/bag-of-visual-words-11.png)  Bag of visual words seems to work well with golf balls, identifying 3/4 of relevant images.  If you're interested in seeing more results, check out the [Colab notebook here](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/image-retrieval/bag-of-visual-words/bag-of-visual-words.ipynb).  ---  That's it for this article on bag of visual words, one of the most successful methods for image classification and retrieval *without* the use of neural networks or other deep learning methods.  One great aspect of this approach is that it is fairly reliable and interpretable. There is no black box of AI here, so when applied to a lot of data we will rarely get too many surprising results.  We know that images with similar edges, textures, and colors are likely to be identified as similar; the features being identified are set by the SIFT (or other) algorithms.  All of this makes bag of visual words a good option for image retrieval or classification, where we need to focus on the features that we know algorithms like SIFT can deal with, i.e. we're focused on finding similar object edges (that are resistant to scale, noise, and illumination changes). Finally, these well-defined algorithms give us a huge advantage when interpretability is important.   ## Resources  [Code Notebook](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/image-retrieval/bag-of-visual-words/bag-of-visual-words.ipynb), GitHub [Examples Repo](https://github.com/pinecone-io/examples/tree/master/learn/image-retrieval/bag-of-visual-words)",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e300"
  },
  "title": "\"Training Sentence Transformers with MNR Loss\"",
  "headline": "\"Next-Gen Sentence Embeddings with Multiple Negatives Ranking Loss\"",
  "weight": "4",
  "name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "How to create sentence transformers by fine-tuning with MNR loss.",
  "images": "['/images/fine-tuning-sentence-transformers-mnr-loss-1.jpg']",
  "content": "Transformer-produced sentence embeddings have come a long way in a very short time. Starting with the slow but accurate similarity prediction of BERT cross-encoders, the world of [sentence embeddings](/learn/sentence-embeddings/) was ignited with the introduction of SBERT in 2019 [1]. Since then, many more sentence transformers have been introduced. These models quickly made the original SBERT obsolete.  How did these newer sentence transformers manage to outperform SBERT so quickly? The answer is *multiple negatives ranking (MNR) loss*.  This article will cover what MNR loss is, the data it requires, and how to implement it to fine-tune our own high-quality sentence transformers.  Implementation will cover two training approaches. The first is more involved, and outlines the exact steps to fine-tune the model. The second approach makes use of the `sentence-transformers` library's excellent utilities for fine-tuning.  <p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/or5ew7dqA-c\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>  ## NLI Training  As explained in our article on [softmax loss](/learn/train-sentence-transformers-softmax/), we can fine-tune sentence transformers using **N**atural **L**anguage **I**nference (NLI) datasets.  These datasets contain many sentence pairs, some that *imply* each other, and others that *do not imply* each other. As with the softmax loss article, we will use two of these datasets: the Stanford Natural Language Inference (SNLI) and Multi-Genre NLI (MNLI) corpora.  These two corpora total to 943K sentence pairs. Each pair consists of a `premise` and `hypothesis` sentence, which are assigned a `label`:   * **0** — *entailment*, e.g. the `premise` suggests the `hypothesis`.  * **1** — *neutral*, the `premise` and `hypothesis` could both be true, but they are not necessarily related. * **2** — *contradiction*, the `premise` and `hypothesis` contradict each other.  When fine-tuning with MNR loss, we will be dropping all rows with *neutral* or *contradiction* labels — keeping only the positive *entailment* pairs.  We will be feeding sentence A (the `premise`, known as the *anchor*) followed by sentence B (the `hypothesis`, when the label is **0**, this is called the *positive*) into BERT on each step. Unlike softmax loss, we do not use the `label` feature.  These training steps are performed in batches. Meaning several anchor-positive pairs are processed at once.  The model is then optimized to produce similar embeddings between pairs while maintaining different embeddings for non-pairs. We will explain this in more depth soon.  ### Data Preparation  Let's look at the data preparation process. We first need to download and merge the two NLI datasets. We will use the `datasets` library from Hugging Face.  {{< notebook file=\"prep-nli\" height=\"full\" >}}  Because we are using MNR loss, we only want anchor-positive pairs. We can apply a filter to remove all other pairs (including erroneous `-1` labels).  {{< notebook file=\"filter-mnr\" height=\"full\" >}}  The dataset is now prepared differently depending on the training method we are using. We will continue preparation for the more involved PyTorch approach. If you'd rather just train a model and care less about the steps involved, feel free to skip ahead to the next section.  For the PyTorch approach, we must tokenize our own data. To do that, we will be using a `BertTokenizer` from the `transformers` library and applying the `map` method on our `dataset`.  {{< notebook file=\"tokenizer-mnr\" height=\"full\" >}}  After that, we're ready to initialize our `DataLoader`, which will be used for loading batches of data into our model during training.  {{< notebook file=\"dataloader-mnr\" height=\"full\" >}}  And with that, our data is ready. Let's move on to training.    ### PyTorch Fine-Tuning  When training SBERT models, we don't start from scratch. Instead, we begin with an already pretrained BERT — all we need to do is *fine-tune* it for building sentence embeddings.  ```python from transformers import BertModel  # start from a pretrained bert-base-uncased model model = BertModel.from_pretrained('bert-base-uncased') ```  MNR and softmax loss training approaches use a * 'siamese'*-BERT architecture during fine-tuning. Meaning that during each step, we process a *sentence A* (our *anchor*) into BERT, followed by *sentence B* (our *positive*).  ![Start SBERT](/images/fine-tuning-sentence-transformers-mnr-loss-2.jpg) <small>Siamese-BERT network, the *anchor* and *positive* sentence pairs are processed separately. A mean pooling layer converts token embeddings into sentence embeddings.sentence A is our *anchor* and sentence B the *positive*.</small>  Because these two sentences are processed *separately*, it creates a *siamese*-like network with two identical BERTs trained in parallel. In reality, there is only a single BERT being used twice in each step.  We can extend this further with *triplet*-networks. In the case of triplet networks for MNR, we would pass three sentences, an *anchor*, it's *positive*, and it's *negative*. However, we are *not* using triplet-networks, so we have removed the negative rows from our dataset (rows where `label` is `2`).  ![Triplet networks](/images/fine-tuning-sentence-transformers-mnr-loss-3.jpg) <small>Triplet networks use the same logic but with an added sentence. For MNR loss this other sentence is the *negative* pair of the *anchor*.</small>  BERT outputs 512 768-dimensional embeddings. We convert these into *averaged* sentence embeddings using *mean-pooling*. Using the siamese approach, we produce two of these per step — one for the *anchor* that we will call `a`, and another for the *positive* called `p`.  ```python # define mean pooling function def mean_pool(token_embeds, attention_mask):     # reshape attention_mask to cover 768-dimension embeddings     in_mask = attention_mask.unsqueeze(-1).expand(         token_embeds.size()     ).float()     # perform mean-pooling but exclude padding tokens (specified by in_mask)     pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(         in_mask.sum(1), min=1e-9     )     return pool ```  In the `mean_pool` function, we're taking these token-level embeddings (the 512) and the sentence `attention_mask` tensor. We resize the `attention_mask` to match the higher `768`-dimensionality of the token embeddings.  The resized mask `in_mask` is applied to the token embeddings to exclude padding tokens from the mean pooling operation. Mean-pooling takes the average activation of values across each dimension but *excluding* those padding values, which would reduce the average activation. This operation transformers our token-level embeddings (shape `512*768`) to sentence-level embeddings (shape `1*768`).  These steps are performed in *batches*, meaning we do this for many *(anchor, positive)* pairs in parallel. That is important in our next few steps.  {{< notebook file=\"a-p-shapes\" height=\"full\" >}}  First, we calculate the cosine similarity between each anchor embedding (`a`) and *all* of the positive embeddings in the same batch (`p`).  {{< notebook file=\"cos-sim-mnr\" height=\"full\" >}}  From here, we produce a vector of cosine similarity scores (of size `batch_size`) for each anchor embedding `a_i` *(or size `2 * batch_size` for triplets)*. Each anchor should share the highest score with its positive pair, `p_i`.  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/fine-tuning-sentence-transformers-mnr-loss-4.mp4\" type=\"video/mp4\"> </video>  <small class=\"video\">Cosine similarity scores using five pairs/triples in a triplet network (with `(a, p, n)`). A siamese network is the same but excluding the dark blue `n` blocks (`n`).</small>  To optimize for this, we use a set of increasing label values to mark where the highest score should be for each `a_i`, and categorical [cross-entropy loss](/learn/cross-entropy-loss/).  {{< notebook file=\"mnr-loss-func\" height=\"full\" >}}  And that's every component we need for fine-tuning with MNR loss. Let's put that all together and set up a training loop. First, we move our model and layers to a CUDA-enabled GPU *if available*.  {{< notebook file=\"mnr-layer-prep\" height=\"full\" >}}  Then we set up the optimizer and schedule for training. We use an Adam optimizer with a linear warmup for 10% of the total number of steps.  {{< notebook file=\"mnr-warmup-setup\" height=\"full\" >}}  And now we define the training loop, using the same training process that we worked through before.  {{< notebook file=\"mnr-training-loop\" height=\"full\" >}}  With that, we've fine-tuned our BERT model using MNR loss. Now we save it to file.  {{< notebook file=\"mnr-save\" height=\"full\" >}}  And this can now be loaded using either the `SentenceTransformer` or HF `from_pretrained` methods. Before we move on to testing the model performance, let's look at how we can replicate that fine-tuning logic using the *much simpler* `sentence-transformers` library.    ## Fast Fine-Tuning  As we already mentioned, there is an easier way to fine-tune models using MNR loss. The `sentence-transformers` library allows us to use pretrained sentence transformers and comes with some handy training utilities.  We will start by preprocessing our data. This is the same as we did before for the first few steps.  {{< notebook file=\"mnr-preprocess\" height=\"full\" >}}  Before, we tokenized our data and then loaded it into a PyTorch `DataLoader`. This time we follow a *slightly different format*. We * don't* tokenize; we reformat into a list of `sentence-transformers` `InputExample` objects and use a slightly different `DataLoader`.  {{< notebook file=\"mnr-fast-data-prep\" height=\"full\" >}}  Our `InputExample` contains just our `a` and `p` sentence pairs, which we then feed into the `NoDuplicatesDataLoader` object. This data loader ensures that each batch is duplicate-free — a helpful feature when ranking pair similarity across *randomly* sampled pairs with MNR loss.  Now we define the model. The `sentence-transformers` library allows us to build models using *modules*. We need just a transformer model (we will use `bert-base-uncased` again) and a mean pooling module.  {{< notebook file=\"mnr-fast-model\" height=\"full\" >}}  We now have an initialized model. Before training, all that's left is the loss function — MNR loss.  {{< notebook file=\"mnr-fast-loss\" height=\"full\" >}}  And with that, we have our data loader, model, and loss function ready. All that's left is to fine-tune the model! As before, we will train for a single epoch and warmup for the first 10% of our training steps.  {{< notebook file=\"mnr-fast-train\" height=\"full\" >}}  And a couple of hours later, we have a new sentence transformer model trained using MNR loss. It goes without saying that using the `sentence-transformers` training utilities makes life *much easier*. To finish off the article, let's look at the performance of our MNR loss SBERT next to other sentence transformers.    ## Compare Sentence Transformers  We're going to use a semantic textual similarity (STS) dataset to test the performance of *four models*; our *MNR loss* SBERT (using PyTorch and `sentence-transformers`), the *original* SBERT, and an MPNet model trained with MNR loss on a [1B+ sample dataset](https://huggingface.co/spaces/flax-sentence-embeddings/sentence-embeddings).  The first thing we need to do is download the STS dataset. Again we will use `datasets` from Hugging Face.  {{< notebook file=\"sts-load\" height=\"full\" >}}  STSb (or STS benchmark) contains sentence pairs in features `sentence1` and `sentence2` assigned a similiarity score from *0 -> 5*.  | sentence1 | sentence2 | label | idx | | ------------------------------------ | ------------------------------------ | ----- | ---- | | A man with a hard hat is dancing. | A man wearing a hard hat is dancing. | 5.0 | 0 | | A man is riding a bike. | A woman is riding a horse. | 1.4 | 149 | | A man is buttering a piece of bread. | A slow loris hanging on a cord. | 0.0 | 127 |  <small>Three samples from the validation set of STSb.</small>  Because the similarity scores range from 0 -> 5, we need to normalize them to a range of 0 -> 1. We use `map` to do this.  {{< notebook file=\"sts-norm-labels\" height=\"full\" >}}  We're going to be using `sentence-transformers` evaluation utilities. We first need to reformat the STSb data using the `InputExample` class — passing the sentence features as `texts` and similarity scores to the `label` argument.  {{< notebook file=\"sts-input-example\" height=\"full\" >}}  To evaluate the models, we need to initialize the appropriate evaluator object. As we are evaluating continuous similarity scores, we use the `EmbeddingSimilarityEvaluator`.  {{< notebook file=\"sts-evaluator\" height=\"full\" >}}  And with that, we're ready to begin evaluation. We load our model as a `SentenceTransformer` object and pass the model to our `evaluator`.  The evaluator outputs the * Spearman's rank correlation* between the cosine similarity scores calculated from the model's output embeddings and the similarity scores provided in STSb. A high correlation between the two values outputs a value close to *+1*, and no correlation would output *0*.  {{< notebook file=\"sts-eval\" height=\"full\" >}}  For the model fine-tuned with `sentence-transformers`, we output a correlation of *0.84*, meaning our model outputs good similarity scores according to the scores assigned to STSb. Let's compare that with other models.  | Model | Score | | --------------------------------------------------- | ----- | | `all_datasets_v3_mpnet-base` | 0.89 | | Custom SBERT with MNR (`sentence-transformers`) | 0.84 | | Original SBERT `bert-base-nli-mean-tokens` | 0.81 | | Custom SBERT with softmax (`sentence-transformers`) | 0.80 | | Custom SBERT with MNR (PyTorch) | 0.79 | | Custom SBERT with softmax (PyTorch) | 0.67 | | `bert-base-uncased` | 0.61 |  The top two models are trained using MNR loss, followed by the original SBERT.  These results support the advice given by the authors of `sentence-transformers`, that models trained with MNR loss outperform those trained with softmax loss in building high-performing sentence embeddings [2].  Another key takeaway here is that despite our best efforts and the complexity of building these models with PyTorch, *every* model trained using the easy-to-use `sentence-transformers` utilities far outperformed them.  In short; fine-tune your models with MNR loss, and do it with the `sentence-transformers` library.  ---  That's it for this walkthrough and guide to fine-tuning sentence transformer models with multiple negatives ranking loss — the current best approach for building high-performance models.  We covered preprocessing the two most popular NLI datasets — the Stanford NLI and multi-genre NLI corpora — for fine-tuning with MNR loss. Then we delved into the details of this fine-tuning approach using PyTorch before taking advantage of the excellent training utilities provided by the `sentence-transformers` library.  Finally, we learned how to evaluate our sentence transformer models with the semantic textual similarity benchmark (STSb). Identifying the highest performing models.  {{< newsletter text=\"Subscribe for more NLP and semantic search tutorials!\" inputText=\"Email address...\" buttonText=\"Submit\">}}  ## References  [1] N. Reimers, I. Gurevych, [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084) (2019), ACL  [2] N. Reimers, [Sentence Transformers NLI Training Readme](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/nli), GitHub ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e302"
  },
  "title": "\"The Rise of Vector Data\"",
  "headline": "\"The Rise of <span>Vector Data</span>\"",
  "name": "Edo Liberty",
  "position": "Founder and CEO",
  "src": "/images/company-edo.png",
  "href": "https://edoliberty.github.io/",
  "date": "\"2021-05-21\"",
  "# Date": "May 21, 2021",
  "description": "What happens in your brain when you see someone you recognize? First, the rods and cones in your eyes record the light intensity. Those signals then travel..",
  "images": "['/images/rise-of-vector-data-7.png']",
  "thumbnail": "\"/images/rise-vector-data-thumbnail.jpg\"",
  "content": "What happens in your brain when you see someone you recognize?  First, the rods and cones in your eyes record the light intensity. Those signals then travel to the visual cortex in the back of your head, where they activate neural cells through several layers in your visual cortex. In the end, you have millions of neurons activated in varying intensities. Those activations are transmitted to your temporal lobe, where your brain interprets as: “I see Julie.”  ![](/images/rise-of-vector-data-6.png)  The higher functions related to vision happen on information that hardly resembles the initial intensity of the light that hit your eye. Instead, they deal with the much richer representations output by your visual cortex. When you interpret what you see or read, your brain operates on those neural representations and not the original image.  Deep learning applications process the world in a similar way. Information is converted into [vector embeddings](/learn/vector-embeddings/) — or simply “vectors” — which are then used for predictions, interpretation, comparison, and other cognitive functions.   ![](/images/rise-of-vector-data-5.png)  In Machine Learning, transformer models — or more generally “embedding models” — serve the role of converting raw data into vector embeddings. They generate vector data.  There are embedding models for all kinds of data: audio, images, text, logs, video, structured, unstructured, and so on. By converting raw data into vectors, they enable functions such as image search, audio search, deduplication, semantic search, object and facial recognition, question-answering, and more.  Embedding models are [growing in numbers](https://www.blog.google/technology/ai/lamda), capability, and adoption. They’re also getting easier to access and use. Deep-learning frameworks such as MXNet, TensorFlow, PyTorch, and Caffe have pre-trained models included and accessible with as few as two lines of code.  ![](/images/rise-of-vector-data-4.png)  ```python import torchvision.models as models model = models.squeezenet1_0(pretrained=True) ```  The more models are used, the more vector data gets generated. Often, vectors get immediately discarded after they are generated. But what if you save the vector data you generate? That, it turns out, can be quite valuable. So valuable that Google, Microsoft, Amazon, Facebook, Netflix, Spotify, and other AI trailblazers have already put it at the core of their applications.  ## Making Something of Vector Data  What higher cognitive functions could we unlock by aggregating millions or billions of semantically rich vectors?  ![](/images/rise-of-vector-data-7.png)  One of the most helpful and fundamental things unlocked by storing vectors is simple: **search**.  Given some new vector, find other known vectors that are similar. Since this [similarity search](/learn/what-is-similarity-search/) (or “vector search”) acts on rich vector representations, it performs a lot more like our brains do when we look for similar objects: we use pattern recognition, semantic meaning, relevant context, memory, association, and even intuition.  This fundamentally new method of information retrieval can make many things better: search engines, recommendation systems, chatbots, security systems, analysis tools, and any other application involving user-facing or internal search functions.  And not just *a little* better. If you’ve recently marveled at the personalized product recommendations from Amazon, the sublime music recommendations from Spotify, the mystifyingly relevant search results from Google/Bing, or the can’t-look-away activity feeds from Facebook/LinkedIn/Twitter, then you’ve experienced the power of similarity search.  Some of those companies have written about their use of vector embeddings for search. [Google](http://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html), [Spotify](https://github.com/spotify/annoy), and [Facebook](/learn/faiss/) have even open-sourced the core components of their similarity search technology.  ![](/images/rise-of-vector-data-3.png)  Vector data is growing, and there’s a clear benefit to using it for search. However, there’s a reason why only a few companies with some of the largest and most sophisticated engineering teams are doing similarity searches at scale.  ## The Tangle of Vector Search Algorithms  Vectors have a unique format that requires novel indexing and search algorithms.  There are well-established tools for searching through relational databases, key-value stores, text documents, and even graphs. Vector data requires an entirely new index and search methods involving the geometric relationships — proximity and angles — between items represented as vector embeddings.  ![](/images/rise-of-vector-data-1.png)  Vectors don’t contain discrete attributes or terms you could just filter through. Instead, each vector embedding is an array of hundreds or thousands of numbers. Treating those numbers as coordinates lets you treat vectors as points in a multi-dimensional  [Euclidean space](https://en.wikipedia.org/wiki/Euclidean_space). Then, searching for similar items is equivalent to finding the neighboring points in that space.  ![](/images/rise-of-vector-data-8.png)  It’s relatively easy to do this with two-dimensional vectors: Dissect the space in a way that you can say, apriori, the red circle only intersects the gray rectangles, then focus your search for nearest neighbors there. That describes the well-known k-d tree algorithm. It works well in low dimensions but fails in higher dimensions. In higher dimensions (three-dimensional in the figure above for illustration), there is no simple way to dissect the space into “rectangles” to accelerate the search procedure.  We need a much more complex search algorithm for high-dimension spaces. Fortunately, there are [over a dozen open-source libraries](http://ann-benchmarks.com/) dedicated to solving this problem efficiently. Less fortunately, each of those contains multiple algorithms to choose from, each with varying trade-offs between speed and accuracy, each with different parameters to tune.  ![](/images/rise-of-vector-data-2.png) *Source: ann-benchmarks.com*  As a practical matter, choosing a library, algorithm, and parameters for your data is the first hurdle. There is no “right” answer. Each algorithm comes with a complex set of trade-offs, limitations, and behaviors that may not be obvious. For example, the fastest algorithm might be wildly inaccurate; a performant index could be immutable or very slow to update; memory consumption can grow super linearly; and more surprises like that.  ## The Tall Barrier to Scalable Vector Search  Storing and searching through vector data at scale looks a lot like running a database in production, and building the infrastructure takes just as much work.  Depending on the size of your vector data and your throughput, latency, accuracy, and availability requirements, you may need to build a system with sharding, replication, live index updates, namespacing, filtering, persistence, and consistency. Then you need monitoring, alerting, auto-recovery, auto-scaling, etc, to ensure high availability and operational health.  This work becomes a significant undertaking that companies like Google, Microsoft, and Amazon can afford in terms of time and resources. Most other companies can’t, so they can’t use vector search.  Or can they?  ## The Rise of Vector Tooling  In recent years, [the rise of ML models](https://www.wsj.com/articles/models-will-run-the-world-1534716720) spurred an ecosystem of tools that made it easier to develop and deploy models. As we witness the rise of vector data, we need new tools for working with that data.  We hope to lead the way with our [managed vector search solution](/). We specifically designed it for use in production with just a few lines of code without the user needing to worry about algorithm tuning or distributed infrastructure.  The rise of vector data will have limited impact until more companies have the tools to use it and make their products better. Search is the first and fundamental step in this process, so that’s where we begin. ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb2f2e6c738d0f7e304"
  },
  "title": "\"Domain Transfer with BERT\"",
  "content": "  - NLP for Semantic Search toc: >- weight: 11 author:   name: James Briggs   position: Developer Advocate   src: /images/james-briggs.jpeg   href: \"https://www.youtube.com/c/jamesbriggs\" description: How to apply AugSBERT in domain transfer. # Open graph images: ['/images/augsbert-domain-transfer-0.png'] ---  When building language models, we can spend months optimizing training and model parameters, but it’s useless if we don't have the correct data.  The success of our language models relies first and foremost on data. We covered a part way solution to this problem by applying the [Augmented SBERT training strategy to in-domain problems](/learn/data-augmentation/). That is, given a small dataset, we can artificially enlarge it to enhance our training data and improve model performance.  In-domain assumes that our target use case aligns to that small initial dataset. But what if the only data we have *does not* align? Maybe we have Quora question duplicate pairs, but we want to identify similar questions on StackOverflow.  Given this scenario, we must transfer information from the out-of-domain (or *source*) dataset to our target domain. We will learn how to do this here. First, we will learn to assess which source datasets align best with our target domain quickly. Then we will explain and work through the AugSBERT domain-transfer training strategy [2].  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/a8jyue22SJM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ## Will it Work?  Before we even begin training our models, we can get a good approximation of whether the method will work with some simple *n-gram* matching statistics [1].  We count how many n-grams two different domains share. If our *source* domain shares minimal similarity to a *target* domain, as measured by *n-gram* matches, it is less likely to output good results.  This behavior is reasonably straightforward to understand; given our two *source*-*target* domains, overlapping n-grams indicate the linguistic and semantic gap (or *overlap*) between the two domains.  ![linguistic_gap](/images/augsbert-domain-transfer-1.jpg) <small>Small n-gram overlap indicates a more significant gap between domains. More significant gaps require larger bridges (better models). The closer the two domains, the easier it is to bridge the gap.</small>  The greater the gap, the more difficult it is to bridge it using our training strategy. Although models are becoming better at generalization, they’re [still *brittle* when compared to our human-level ability](https://www.nature.com/articles/d41586-019-03013-5) to adapt knowledge across domains.   The *brittleness* of language models means a small change can hamper performance. The more significant that change, the less likely our model will successfully translate its existing knowledge to the new domain.  We are similar. Although people are much more flexible and can apply pre-existing knowledge across domains incredibly well, we’re not perfect.  Given a book, we can tilt the pages at a slight five-degree angle, and most people will hardly notice the difference and continue reading. Turn the book upside-down, and many people will be unable to read. Others will begin to read slower. Our performance degrades with this small change.  If we are then given the same book in another language, most of us will have difficulty comprehending the book. It is still the same book, presented differently.  The knowledge transfer of models across different domains works in the same way: Greater change results in lower performance.  ### Calculating Domain Correlation  We will measure the *n-gram overlap* between **five** domains, primarily from *[Hugging Face Datasets](https://huggingface.co/datasets)*.  | Dataset                                     | Download Script                                              | | ------------------------------------------- | ------------------------------------------------------------ | | STSb                                        | `load_dataset('glue', 'stsb')`                               | | Quora Question Pairs (QQP)                  | `load_dataset('glue', 'qqp')`                                | | Microsoft Research Paraphrase Corpus (MRPC) | `load_dataset('glue', 'mrpc')`                               | | Recognizing Textual Entailment (RTE)        | `load_dataset('glue', 'rte')`                                | | Medical Question Pairs (Med-QP)             | [see here](https://gist.github.com/jamescalam/2dbc9874b599dde95d8ddcdd018dfcf6) |  To calculate the similarity, we perform three operations:  1. Tokenize datasets  {{< notebook file=\"tokenize-1\" height=\"full\" >}}  2. Merge tokens into bi-grams (two-token pairs)  {{< notebook file=\"ngrams\" height=\"full\" >}}  3. Calculate the Jaccard similarity between different n-grams.  {{< notebook file=\"jaccard\" height=\"full\" >}}  *([Full script here](https://gist.github.com/jamescalam/15b48b1d9689e70ab9073e374ba3dc4a))*  After performing each of these steps and calculating the [Jaccard similarity](/learn/semantic-search/) between each dataset, we should get a *rough indication* of how transferable models trained in one domain could be to another.  ![ngram_similarity](/images/augsbert-domain-transfer-2.jpg) <small>Jaccard similarity scores between each of the five datasets.</small>  We can see that the *MedQP* dataset has the lowest similarity to other datasets. The remainder are all reasonably similar.  Other factors contribute to how well we can expect domain transfer to perform, such as the size of the source dataset and subsequent performance of the source cross encoder model within its own domain. We'll take a look at these statistics soon.    ## Implementing Domain Transfer  The AugSBERT training strategy for domain transfer follows a similar pattern to that explained in our [in-domain AugSBERT article](/learn/data-augmentation/). With the one exception that we train our cross-encoder in one domain and the bi-encoder (sentence transformer) in another.  At a high-level it looks like this:  ![domain_transfer_steps](/images/augsbert-domain-transfer-3.jpg) <small>AugSBERT training strategy for cross-domain use.</small>  We start with a labeled dataset from our *source domain* and an unlabeled dataset in our *target domain*. The source domain should be as similar as possible to our target domain.  The next step is to train the source domain cross-encoder. For this, we want to maximize cross encoder performance, as the bi-encoder will essentially learn to replicate the cross-encoder. Better cross-encoder performance translates to better bi-encoder performance.  If the target dataset is very small (1-3K pairs), we may need to augment the dataset. We do this because bi-encoder models require more data to be trained to the same level as a cross-encoder model. A good target dataset should contain 10K or more pairs, although this can vary by use case.  We label the previously *unlabeled* (and possibly *augmented*) target domain dataset with the trained cross-encoder.  The final step is to take the now labeled target domain data and use it to train the bi-encoder model.  That is all there is to it. We will add additional evaluation steps to confirm that the models are performing as expected, but otherwise, we'll stick with the described process.  We already have our five datasets, and we will use each as both source and target data to see the difference in performance between domains.  When using a dataset for the *target domain*, we emulate a real-world use case (where we have no target data labels) by *not* including existing labels and instead relying solely on the cross-encoder-generated labels.  ### Cross Encoder Training  After downloading our labeled source data, we train the cross encoder. To do this, we need to format the source data into `InputExample` objects, then load them into a PyTorch `DataLoader`.  ```python from sentence_transformers import InputExample from torch.utils.data import DataLoader  data = [] # iterate through each row in dataset for row in ds:     # append InputExample object to the list     data.append(InputExample(         texts=[row['sentence1'], row['sentence2']],         label=float(row['label'])     ))  # initialize PyTorch DataLoader using data source = DataLoader(     data, shuffle=True, batch_size=16 ) ```  It can be a good idea to take validation samples for either the source or target domains and create an `evaluator` that can be passed to the cross encoder training function. With this, the script will output Pearson and Spearman correlation scores that we can use to assess model performance.  ```python from sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator  dev_data = [] # iterate through each row again (this time the validation split) for row in dev:     # build up using InputExample objects     dev_data.append(InputExample(         texts=[row['sentence1'], row['sentence2']],         label=float(row['label'])     ))  # the dev data goes into an evaluator evaluator = CECorrelationEvaluator.from_input_examples(     dev_data ) ```  To train the cross encoder model, we initialize a `CrossEncoder` and use the `fit` method. `fit` takes the source data dataloader, evaluator (optional), where we would like to save the trained model `output_path`, and a few training parameters.  ```python # initialize the cross encoder cross_encoder = CrossEncoder('bert-base-uncased', num_labels=1)  # setup the number of warmup steps, 0.2 == 20% warmup num_epochs = 1 warmup = int(len(source) * num_epochs * 0.2)  cross_encoder.fit(     train_dataloader=source,     evaluator=evaluator,     epochs=num_epochs,     warmup_steps=warmup,     optimizer_params={'lr': 5e-5},  # default 2e-5     output_path=f'bert-{SOURCE}-cross-encoder' ) ```  For the training parameters, it is a good idea to test various learning rates and warm-up steps. A single epoch is usually enough to train the cross-encoder, and anything beyond this is likely to cause overfitting. Overfitting is bad when the target data is *in-domain*, and, when it is out-of-domain, it's *even* worse.  For the five models being trained (plus one more trained on a restricted Quora-QP dataset containing 10K rather than 400K training pairs), the following learning rate and percentage of warm-up steps were used.  | Model                       | Learning Rate | Warmup | Evaluation (Spearman, Pearson) | | --------------------------- | ------------- | ------ | ------------------------------ | | `bert-mrpc-cross-encoder`   | 5e-5          | 35%    | (0.704, 0.661)                 | | `bert-stsb-cross-encoder`   | 2e-5          | 30%    | (0.889, 0.887)                 | | `bert-rte-cross-encoder`    | 5e-5          | 30%    | (0.383, 0.387)                 | | `bert-qqp10k-cross-encoder` | 5e-5          | 20%    | (0.688, 0.676)                 | | `bert-qqp-cross-encoder`    | 5e-5          | 20%    | (0.823, 0.772)                 | | `bert-medqp-cross-encoder`  | 5e-5          | 40%    | (0.737, 0.714)                 |  The Spearman and Pearson correlation values measure the correlation between the predicted and true labels for sentence pairs in the validation set. A value of 0.0 signifies no correlation, 0.5 is a moderate correlation, and 0.8+ represents strong correlation.  These results are fairly good, in particular the `bert-stsb-cross-encoder` and *full* `bert-qqp-cross-encoder` models return great performance. However, the RTE model `bert-rte-cross-encoder` performance is far from good.  The poor RTE performance is in part likely due to the small dataset size. However, as it is not significantly smaller than other datasets (Med-QP and MRPC in particular), we can assume the dataset is (1) not as clean or (2) that RTE is a more complex task.  | Dataset  | Size    | | -------- | ------- | | MRPC     | 3,668   | | STSb     | 5,749   | | **RTE**  | 2,490   | | Quora-QP | 363,846 | | Med-QP   | 2,753   |  We will find that this poor RTE performance doesn't necessarily translate to poor performance in other domains. Indeed, *very good* performance in the source domain can actually hinder performance in the target domain because the model must be able to *generalize* well and not *specialize* too much in a particular domain.  We will later be taking a pretrained BERT model, which already has a certain degree of performance in the target domains. Overtraining in the source domain can pull the pretrained model alignment away from the target domain, hindering performance.  A better measure of potential performance is to evaluate against a small (or big if possible) validation set in the target domain.  ![source_ce_to_target_dev](/images/augsbert-domain-transfer-4.jpg) <small>Correlation scores between source cross-encoder models (x-axis) and target domain dev sets (x-axis). The bottom row indicates baseline performance using a pretrained `Bert-base-uncased` model *without fine-tuning*. Lower scores are marked with red, above with cyan, and *roughly equal* with grey.</small>  These correlation values are a good indication of the performance we can expect from our bi-encoder model. Immediately it is clear that the MedQP domain is not easily bridged as expected from the earlier n-gram analysis.  At this point, we can consider dropping the low performing source domains. Although we will keep them to see how these low cross-encoder scores translate to bi-encoder performance.  ### Labeling the Target Data  The next step is to create our labeled target dataset. We use the cross-encoder trained in the source domain to label the *unlabeled* target data.  This is relatively straightforward. We take the unlabeled sentence pairs, transform them into a *list* of sentence pairs, and feed them into the `cross_encoder.predict` method.  ```python # target data is from the training sets from prev snippets # (but we ignore the label feature, otherwise there is nothing to predict) pairs = list(zip(target['sentence1'], target['sentence2']))  scores = cross_encoder.predict(pairs) ```  We return a set of similarity scores, which we can append to the target data and use it to train our bi-encoder.  ```python import pandas as pd  # store everything in a pandas DataFrame target = pd.DataFrame({     'sentence1': target['sentence1'],     'sentence2': target['sentence2'],     'label': scores.tolist()  # cross encoder predicted labels }) # and save to file target.to_csv('target_data.tsv', sep='\\t', index=False) ```  ### Training the Bi-Encoder  The final step in the training process is training the bi-encoder/sentence transformer itself. Everything we've done so far has been to label the target dataset.  Now that we have the dataset, we first need to reformat it using `InputExample` objects and a `DataLoader` as before.  ```python from torch.utils.data import DataLoader from sentence_transformers import InputExample  # create list of InputExamples train = [] for i, row in target.iterrows():   \ttrain.append(InputExample(       \ttexts=[row['sentence1'], row['sentence2']],       \tlabel=float(row['label'])     )) # and place in PyTorch DataLoader loader = DataLoader(   \ttrain, shuffle=True, batch_size=BATCH_SIZE ) ```  Then we initialize the bi-encoder. We will be using a pretrained `bert-base-uncased` model from *[Hugging Face Transformers](https://huggingface.co/docs/transformers/index)* followed by a mean pooling layer to transform word-level embeddings to sentence embeddings.  {{< notebook file=\"init-sentence-transformer\" height=\"full\" >}}  The labels output by our cross encoder are continuous values in the range 0.0 -> 1.0, which means we can use a loss function like `CosineSimilarityLoss`. Then we're ready to train our model as we have done before.  ```python # setup loss function loss = losses.CosineSimilarityLoss(model=model)  # and training epochs = 1 # warmup for first 30% of training steps (test diff values here) warmup_steps = int(len(loader) * epochs * 0.3)  model.fit(   train_objectives=[(loader, loss)],   epochs=epochs,   warmup_steps=warmup_steps,   output_path=f'bert-target' ) ```  ### Evaluation and Augmentation  At this point, we can evaluate the bi-encoder model performance on a validation set of the target data. We use the `EmbeddingSimilarityEvaluator` to measure how closely the predicted, and true labels correlate ([script here](https://gist.github.com/jamescalam/64e38a2a8e84db61e5739f9fe41c12f2)).  ![target_biencoder_dev](/images/augsbert-domain-transfer-5.jpg) <small>Bi-encoder correlation scores. Many reach very close to the equivalent cross-encoder performance (and some even exceed it). Yellow highlights indicate an improved performance after augmentation via random sampling.</small>  The first bi-encoder results are reasonable, with most scoring higher than the Bert benchmark. Highlighted results indicate the original score (in the center) followed by scores *after* augmenting target datasets with random sampling. Where data augmentation showed little-to-no improvement, scores were excluded.  One reason we might see improvement is quite simple. Bi-encoders require relatively large training sets. Our datasets are all tiny, except for QQP (which does produce a 72% correlation score in `bert-Smedqp-Tqqp`). Augmented datasets help us satisfy the data-hungry nature of bi-encoder training.  Fortunately, we already set up most of what we needed to *augment* our target datasets. We have the cross-encoders for labeling, and all that is left is to generate new pairs.  As covered in our [in-domain AugSBERT article](https://www.pinecone.io/learn/data-augmentation/), we can generate new pairs with *random sampling*. All this means is that we create new sentence pairs by mixing-and-matching sentences from features A and B.  After generating these new pairs, we score them using the relevant cross-encoder. And [like magic](https://gist.github.com/jamescalam/062673282c2a8da13e8084bb7a5bbb35), we have thousands of new samples to train our bi-encoders with.  With or without random sampling, we can see results that align with the performance of our cross-encoder models, which is precisely what we would expect. This similarity in results means that the knowledge from our cross-encoders is being distilled successfully into our *faster* bi-encoder models.    That is it for the Augmented SBERT training strategy and its application to domain transfer. Effective domain transfer allows us to broaden the horizon of sentence transformer use across many more domains.  The most common blocker for new language tools that rely on BERT or other transformer models is a lack of data. We do not eliminate the problem entirely using this technique, but we can reduce it.  Given a new domain that is not *too far* from the domain of existing datasets, we can now build better-performing sentence transformers. Sometimes in the range of just a few percentage point improvements, and at other times, we see much more significant gains.  Thanks to AugSBERT, we can now tackle a few of those previously inaccessible domains.    ## References  [1] D. Shah, et al., [Adversarial Domain Adaption for Duplicate Question Detection](https://aclanthology.org/D18-1131/) (2018), EMNLP Proc.  [2] N. Thakur, et al., [Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks](https://arxiv.org/abs/2010.08240) (2021), NAACL  ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e306"
  },
  "content": "categories:   - Algorithms & Libraries toc: >- weight: 1 author:   name: James Briggs   position: Developer Advocate   src: /images/james-briggs.jpeg   href: \"https://www.youtube.com/c/jamesbriggs\" description: Supercharge search with these stellar technologies. # Open Graph images: ['/images/semantic-search-1.png'] ---  Similarity search is one of the fastest-growing domains in AI and machine learning. At its core, it is the process of matching relevant pieces of information together.  There's a strong chance that you found this article through a search engine — most likely Google. Maybe you searched something like \"what is semantic similarity search?\" or \"traditional vs vector similarity search\".  Google processed your query and used many of the same similarity search essentials that we will learn about in this article, to bring you to — this article.  ---  **Note: Want to replace your keyword search with semantic search powered by NLP? [Pinecone](/) makes it easy, scalable, and free — [start now](https://app.pinecone.io/).**  ---  If similarity search is at the heart of the success of a $1.65T company — the world's fifth most valuable company in the world<sup>[1]</sup>, there's *a good chance* it's worth learning more about.  [Similarity search](/learn/what-is-similarity-search/) is a complex topic and there are countless techniques for building effective search engines.  In this article, we'll cover a few of the most interesting — and powerful — of these techniques — focusing specifically on semantic search. We'll learn how they work, what they're good at, and how we can implement them ourselves.  Watch the videos or continue reading:  <p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/AY62z7HrghY\" title=\"Traditional methods for semantic similarity search\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p> <p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/ziiF1eFM3_4\" title=\"Vector-based methods for semantic similarity search\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>  ## Traditional Search  We start our journey down the road of search in the traditional camp, here we find a few key players like:  - **Jaccard Similarity** - **w-shingling** - Pearson Similarity - **Levenshtein distance** - Normalized Google Distance  All are great metrics to use with similarity search — of which we'll cover three of the most popular, Jaccard similarity, w-shingling, and Levenshtein distance.  ### Jaccard Similarity  Jaccard similarity is a simple, but sometimes powerful similarity metric. Given two sequences, **A** and **B** — we find the number of shared elements between both and divide this by the total number of elements from both sequences.  ![Jaccard similarity measures the intersection between two sequences over the union between the two sequences.](/images/semantic-search-1.png)  <small>Jaccard similarity measures the intersection between two sequences over the union between the two sequences.</small>  Given two sequences of integers, we would write:  {{< notebook file=\"jac-sim\" height=\"full\" >}}  Here we identified **two** shared *unique* integers, `3` and `4` — between two sequences with a total of ten integers in both, of which **eight** are unique values — `2/8` gives us our Jaccard similarity score of `0.25`.  We could perform the same operation for text data too, all we do is replace *integers* with *tokens*.  ![Jaccard similarity calculated between two sentences a and b.](/images/semantic-search-2.png)  <small>Jaccard similarity calculated between two sentences **a** and **b**.</small>  {{< notebook file=\"jaccard-1\" height=\"full\" >}}  We find that sentences `b` and `c` score much better, as we would expect. Now, it isn't perfect — two sentences that share nothing but words like *'the'*, *'a'*, *'is'*, etc — could return high Jaccard scores despite being semantically dissimilar.  These shortcomings can be solved partially using preprocessing techniques like stopword removal, stemming/lemmatization, and so on. However, as we'll see soon — some methods avoid these problems altogether.  ### w-Shingling  Another similar technique is **w-shingling**. w-shingling uses the exact same logic of *intersection / union* — but with 'shingles'. A 2-shingle of sentence **a** would look something like:  ```python a = {'his thought', 'thought process', 'process is', ...} ```  We would then use the same calculation of `intersection / union` between our *shingled* sentences like so:  {{< notebook file=\"w-shingling\" height=\"full\" >}}  Using a 2-shingle, we find three matching shingles between sentences **b** and **c**, resulting in a similarity of **0.125**.  ### Levenshtein Distance  Another popular metric for comparing two strings is the Levenshtein distance. It is calculated as the number of operations required to change one string into another — and it's calculated with:  ![Levenshtein distance formula.](/images/semantic-search-3.png)  <small>Levenshtein distance formula.</small>  Now, this is a pretty complicated-looking formula — if you understand it, great! If not, don't worry — we'll break it down.  The variables `a` and `b` represent our two strings, `i` and `j` represent the character position in `a` and `b` respectively. So given the strings:  ![Levenshtein' and a mispelling 'Livinshten](/images/semantic-search-30.png)  <small>'Levenshtein' and a mispelling 'Livinshten'.</small>  We would find:  ![We index the word itself from 1 to the length of the word, the zeroth index does exist as a none character (more on that next)](/images/semantic-search-4.png)  <small>We index the word itself from 1 to the length of the word, the zeroth index does exist as a none character (more on that next).</small>  Easy! Now, a great way to grasp the logic behind this formula is through visualizing the Wagner-Fischer algorithm — which uses a simple matrix to calculate our Levenshtein distance.  We take our two words `a` and `b` and place them on either axis of our matrix — we include our *none* character as an empty space.  ![Our empty Wagner-Fischer matrix — we'll be using this to calculate the Levenshtein distance between 'Levenshtein' and 'Livinshten'.](/images/semantic-search-5.png)  <small>Our empty Wagner-Fischer matrix — we'll be using this to calculate the Levenshtein distance between **'Levenshtein'** and **'Livinshten'**.</small>  {{< notebook file=\"levenshtein-init\" height=\"full\" >}}  <small>Initializing our empty Wagner-Fischer matrix in code.</small>  Then we iterate through every position in our matrix and apply that complicated formula we saw before.  The first step in our formulae is `if min(i, j) = 0` — all we're saying here is, out of our two positions `i` and `j`, are either `0`? If so, we move across to `max(i, j)` which tells us to assign the current position in our matrix the higher of the two positions `i` and `j`:  ![We start on the right, along the edges where i and/or j is 0, the matrix position will be populated with max(i, j).](/images/semantic-search-6.png)  <small>We start on the right, along the edges where i and/or j is 0, the matrix position will be populated with **max(i, j)**.</small>  {{< notebook file=\"levenshtein-max-ij\" height=\"full\" >}}  <small>The min(i,j) == 0 followed by the max(i,j) operation visualized above — translated into code.</small>  Now, we've dealt with the outer edges of our matrix — but we still need to calculate the inner values — which is where our optimal path will be found.  Back to `if min(i, j) = 0` — what if neither are `0`? Then we move onto that complex part of the equation inside the `min {` section. We need to calculate a value for each row, then we take the **min**imum value.  Now, we already know these values — they're in our matrix:  ![For each new position in our matrix, we take the minimum value from the three neighboring positions (circled — top-left).](/images/semantic-search-7.png)  <small>For each new position in our matrix, we take the minimum value from the three neighboring positions (circled — top-left).</small>  `lev(i-1, j)` and the other operations are all indexing operations — where we extract the value in that position. We then take the minimum value of the three.  There is just one remaining operation. The `+1` on the left should only be applied if `a[i] != b[i]` — this is the penalty for mismatched characters.  ![If a[i] != b[j] we add 1 to our minimum value — this is the penalty for mismatched characters.](/images/semantic-search-8.png)  <small>If a[i] != b[j] we add 1 to our minimum value — this is the penalty for mismatched characters.</small>  Placing all of this together into an iterative loop through the full matrix looks like this:  {{< notebook file=\"levenshtein-full\" height=\"full\" >}}  <small>The full Levenshtein distance calculation using a Wagner-Fischer matrix.</small>  We've now calculated each value in the matrix — these represent the number of operations required to convert from string `a` up to position `i` to string `b` up to position `j`.  We're looking for the number of operations to convert `a` to `b` — so we take the bottom-right value of our array at `lev[-1, -1]`.  ![The optimal path through our matrix — in position [-1, -1] at the bottom-right we have the Levenshtein distance between our two strings.](/images/semantic-search-9.png)  <small>The optimal path through our matrix — in position [-1, -1] at the bottom-right we have the Levenshtein distance between our two strings.</small>  {{< notebook file=\"levenshtein-get-val\" height=\"full\" >}}  ---  ## Vector Similarity Search  For vector-based search, we typically find one of several vector building methods:  - **TF-IDF** - **BM25** - word2vec/doc2vec - **BERT** - USE  In tandem with some implementation of *approximate* nearest neighbors (ANN), these vector-based methods are the MVPs in the world of similarity search.  We'll cover TF-IDF, BM25, and BERT-based approaches — as these are easily the most common and cover both sparse and dense [vector representations](/learn/vector-embeddings/).  ### 1. TF-IDF  The respected grandfather of vector similarity search, born back in the 1970s. It consists of two parts, **T**erm **F**requency (TF) and **I**nverse **D**ocument **F**requency (IDF).  The TF component counts the number of times a term appears within a document and divides this by the total number of terms in that same document.  ![](/images/semantic-search-10.png)  <small>The term frequency (TF) component of TF-IDF counts the frequency of our query ('bananas') and divides by the frequency of all tokens.</small>  That is the first half of our calculation, we have the frequency of our **q**uery within the current **D**ocument `f(q,D)` — over the frequency of all **t**erms within the current **D**ocument `f(t,D)`.  The **T**erm **F**requency is a good measure, but doesn't allow us to differentiate between common and uncommon words. If we were to search for the word 'the' — using TF alone we'd assign this sentence the same relevance as had we searched 'bananas'.  That's fine until we begin comparing documents, or searching with longer queries. We don't want words like *'the'*,* 'is'*, or *'it'* to be ranked as highly as *'bananas'* or *'street'*.  Ideally, we want matches between rarer words to score higher. To do this, we can multiply TF by the second term — IDF. The **I**nverse **D**ocument **F**requency measures how common a word is across *all* of our documents.  ![](/images/semantic-search-11.png)  <small>The inverse document frequency (IDF) component of TF-IDF counts the number of documents that contain our query.</small>  In this example, we have three sentences. When we calculate the IDF for our common word *'is'*, we return a much lower number than that for the rarer word *'forest'*.  If we were to then search for both words *'is'* and *'forest'* we would merge TF and IDF like so:  ![We calculate the TF('is', D) and TF('forest', D) scores for docs a, b, and c.](/images/semantic-search-12.png)  <small>We calculate the **TF('is', D)** and **TF('forest', D)** scores for docs **a**, **b**, and **c**. The IDF value is across all docs — so we calculate just **IDF('is')** and **IDF('forest')** once. Then, we get TF-IDF values for both words in each doc by **multiplying** the **TF** and **IDF** components. Sentence **a** scores highest for **'forest'**, and **'is'** always scores **0** as the **IDF('is')** score is **0**.</small>  {{< notebook file=\"tf-idf-calculation\" height=\"full\" >}}  That's great, but where does *vector* similarity search come into this? Well, we take our vocabulary (a big list of all words in our dataset) — and calculate the TF-IDF for each and every word.  ![We calculate the TF-IDF value for every word in our vocabulary to create a TF-IDF vector. This process is repeated for each document.](/images/semantic-search-13.png)  <small>We calculate the TF-IDF value for every word in our vocabulary to create a TF-IDF vector. This process is repeated for each document.</small>  We can put all of this together to create our TF-IDF vectors like so:  {{< notebook file=\"tf-idf-vector\" height=\"full\" >}}  From there we have our TF-IDF vector. It's worth noting that vocab sizes can easily be in the 20K+ range, so the vectors produced using this method are incredibly sparse — which means we cannot encode any semantic meaning.  <a name=\"bm25\"> </a> ### 2. BM25  The successor to TF-IDF, Okapi BM25 is the result of optimizing TF-IDF primarily to normalize results based on document length.  TF-IDF is great but can return questionable results when we begin comparing several mentions  If we took two 500 word articles and found that article A mentions 'Churchill' six times, and article B mentions 'Churchill' twelve times — should we view article A as half as relevant? Likely not.  BM25 solves this by modifying TF-IDF:  ![The BM25 formula.](/images/semantic-search-14.png)  <small>The BM25 formula.</small>  That's a pretty nasty-looking equation — but it's nothing more than our TF-IDF formula with a few new parameters! Let's compare the two TF components:  ![The TF part of BM25 (left) compared to the TF of TF-IDF (right).](/images/semantic-search-15.png)  <small>The TF part of BM25 (left) compared to the TF of TF-IDF (right).</small>  And then we have the IDF part, which doesn't even introduce any new parameters — it just rearranges our old IDF from TF-IDF.  ![The IDF part of BM25 (left) compared to the IDF of TF-IDF (right).](/images/semantic-search-16.png) <small>The IDF part of BM25 (left) compared to the IDF of TF-IDF (right).</small>  Now, what is the result of this modification? If we take a sequence containing 12 tokens, and gradually feed it more and more 'matching' tokens — we produce the following scores:  ![TF-IDF](/images/semantic-search-17.png)  ![BM25](/images/semantic-search-18.png)  <small>Comparison of TF-IDF (top) and BM25 (bottom) algorithms using a sentence of 12 tokens, and an incremental number of relevant tokens (x-axis).</small>  The TF-IDF score increases linearly with the number of relevant tokens. So, if the frequency doubles — so does the TF-IDF score.  Sounds cool! But how do we implement it in Python? Again, we'll keep it nice and simple like the TF-IDF implementation.  {{< notebook file=\"bm25\" height=\"full\" >}}  We've used the default parameters for `k` and `b` — and our outputs look promising. The query `'purple'` only matches sentence `a`, and `'bananas'` scores reasonable for both `b` and `c` — but slightly higher in `c` thanks to the smaller word count.  To build vectors from this, we do the exact same thing we did for TF-IDF.  {{< notebook file=\"bm25-vecs\" height=\"full\" >}}  Again, just as with our TF-IDF vectors, these are *sparse* vectors. We will not be able to encode semantic meaning — but focus on syntax instead. Let's take a look at how we can begin considering semantics.  ### 3. BERT  BERT — or Bidirectional Encoder Representations from Transformers — is a hugely popular transformer model used for *almost* everything in NLP.  Through 12 (or so) encoder layers, BERT encodes a huge amount of information into a set of *dense* vectors. Each dense vector typically contains 768 values — and we usually have 512 of these vectors for each sentence encoded by BERT.  These vectors contain what we can view as numerical representations of language. We can also extract those vectors — from different layers if wanted — but typically from the final layer.  Now, with two correctly encoded [dense vectors](/learn/dense-vector-embeddings-nlp/), we can use a similarity metric like Cosine similarity to calculate their semantic similarity. Vectors that are more aligned are more semantically alike, and vise-versa.  ![A smaller angle between vectors (calculated with cosine similarity) means they are more aligned. For dense vectors, this correlates to greater semantic similarity.](/images/semantic-search-19.png)  <small>A smaller angle between vectors (calculated with cosine similarity) means they are more aligned. For dense vectors, this correlates to greater semantic similarity.</small>  But there's one problem, each sequence is represented by 512 vectors — not one vector.  So, this is where another — brilliant — adaption of BERT comes into play. Sentence-BERT allows us to create a single vector that represents our full sequence, otherwise known as a *sentence vector* <sup>[2]</sup>.  We have two ways of implementing SBERT — the easy way using the `sentence-tranformers` library, or the slightly less easy way using `transformers` *and* PyTorch.  We'll cover both, starting with the `transformers` with PyTorch approach so that we can get an intuition for how these vectors are built.  If you've used the HF transformers library, the first few steps will look very familiar. We initialize our SBERT model and tokenizer, tokenize our text, and process our tokens through the model.  {{< notebook file=\"sbert-process\" height=\"full\" >}}  We've added a new sentence here, sentence **g** carries the same *semantic* meaning as **b** — without the same keywords. Due to the lack of shared words, all of our previous methods would struggle to find similarity between these two sequences — remember this for later.  {{< notebook file=\"sbert-last-hidden-state\" height=\"full\" >}}  We have our vectors of length 768 — but these are **not** *sentence vectors* as we have a vector representation for each token in our sequence (128 here as we are using SBERT — for BERT-base this is 512). We need to perform a **mean pooling** operation to create the sentence vector.  The first thing we do is multiply each value in our `embeddings` tensor by its respective `attention_mask` value. The `attention_mask` contains **ones** where we have 'real tokens' (eg not padding tokens), and **zeros** elsewhere — this operation allows us to ignore non-real tokens.  {{< notebook file=\"sbert-mean-pooling\" height=\"full\" >}}  And those are our sentence vectors, using those we can measure similarity by calculating the cosine similarity between each.  {{< notebook file=\"sbert-cos-sim\" height=\"full\" >}}  If we visualize our array, we can easily identify higher similarity sentences:  ![Heatmap showing cosine similarity between our SBERT sentence vectors — the score between sentences b and g is circled.](/images/semantic-search-20.png)  <small>Heatmap showing cosine similarity between our SBERT sentence vectors — the score between sentences b and g is circled.</small>  Now, think back to the earlier note about sentences b and g having essentially identical meaning whilst not sharing *any* of the same keywords.  We'd hope SBERT and its superior semantic representations of language to identify these two sentences as similar — and lo-and-behold the similarity between both is our second-highest score at 0.66 (circled above).  Now, **the alternative (easy) approach is to use sentence-transformers**. To get the exact same output as we produced above we write:  {{< notebook file=\"sentence-transformers\" height=\"full\" >}}  Which, of course, is much easier.  ---  That's all for this walk through history with Jaccard, Levenshtein, and Bert!  We covered a total of **six** different techniques, starting with the straight-forward Jaccard similarity, w-shingling, and Levenshtein distance. Before moving onto search with sparse vectors — TF-IDF and BM25, and finishing up with state-of-the-art dense vector representations with SBERT.  ## References  [1] [Market Capitalization of Alphabet (GOOG)](https://companiesmarketcap.com/alphabet-google/marketcap/), Companies Market Cap  [2] N. Reimers, I. Gurevych, [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084) (2019), Proceedings of the 2019 Conference on Empirical Methods in 2019  [Notebooks Repo](https://github.com/pinecone-io/examples/tree/master/semantic_search_intro)  Runnable Colab notebooks: [Jaccard](https://colab.research.google.com/github/pinecone-io/examples/blob/master/semantic_search_intro/jaccard.ipynb), [Levenshtein](https://colab.research.google.com/github/pinecone-io/examples/blob/master/semantic_search_intro/levenshtein.ipynb), [TF-IDF](https://colab.research.google.com/github/pinecone-io/examples/blob/master/semantic_search_intro/tfidf.ipynb), [BM25](https://colab.research.google.com/github/pinecone-io/examples/blob/master/semantic_search_intro/bm25.ipynb), [SBERT](https://colab.research.google.com/github/pinecone-io/examples/blob/master/semantic_search_intro/sbert.ipynb)  **All images are by the author except where stated otherwise* ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e308"
  },
  "title": "\"Multi-modal ML with OpenAI's CLIP\"",
  "headline": "\"Multi-modal ML with OpenAI's CLIP\"",
  "weight": "9",
  "name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "An introduction to OpenAI's CLIP and multi-modal ML",
  "images": "['https://www.pinecone.io/images/clip-0.png']",
  "content": "Language models (LMs) can not rely on language alone. That is the idea behind the \"Experience Grounds Language\" paper, that proposes a framework to measure LMs' current and future progress. A key idea is that, beyond a certain threshold LMs need other forms of data, such as visual input [1] [2].  ![world-scopes](./images/clip-1.png)<small>**W**orld **S**copes (WS), as datasets become larger in scope and span multiple modalities, the capabilities of models trained with them increase.</small>  The next step beyond well-known language models; BERT, GPT-3, and T5 is *”World Scope 3”*. In World Scope 3, we move from large text-only datasets to large multi-modal datasets. That is, datasets containing information from multiple forms of media, like *both* images and text.  The world, both digital and real, is multi-modal. We perceive the world as an orchestra of language, imagery, video, smell, touch, and more. This chaotic ensemble produces an inner state, our \"model\" of the outside world.  AI must move in the same direction. Even specialist models that focus on language or vision must, at some point, have input from the other modalities. How can a model fully understand the concept of the word \"person\" without *seeing* a person?  OpenAI **C**ontrastive **L**earning **I**n **P**retraining (CLIP) is a world scope three model. It can comprehend concepts in both text and image and even connect concepts between the two modalities. In this chapter we will learn about multi-modality, how CLIP works, and how to use CLIP for different use cases like encoding, classification, and object detection.  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/fGwH2YoQkDM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ---  ## Multi-modality  The multi-modal nature of CLIP is powered by two encoder models trained to \"speak the same language\". Text inputs are passed to a text encoder, and image inputs to an image encoder [3]. These models then create a *vector representation* of the respective input.  Both models \"speak the same language\" by encoding similar concepts in text and images into similar vectors. That means that the text \"two dogs running across a frosty field\" would output a vector similar to an *image* of two dogs running across a frosty field.  ![multi-modal-similarity](./images/clip-2.png)  <small>Similar text and images will be encoded into a similar vector space. Dissimilar text and images do not share a similar vector space.</small>  We can think of the language these models speak as the vector space in which they encode vectors. These two models can express nuanced information about text and images through this vector space. However, this \"vector language\" is far too abstract for us to directly understand.  Rather than directly reading this \"language\", we can train other simple neural networks to understand it and make predictions that we can understand. Or we use vector search to identify similar concepts and patterns across text and image domains.  Let's take a look at an example of CLIP in action.  ### Text-to-Image Search  <iframe src=\"https://hf.space/streamlit/pinecone/semantic-query-trainer/+\" data-src=\"https://hf.space/streamlit/pinecone/semantic-query-trainer/+\" data-sdk=\"streamlit\" title=\"Streamlit app\" style=\"width:100%;height:1000px;overflow: hidden;\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\" scrolling=\"yes\"></iframe>  Entering a prompt in the search bar above allows us to search through images based on their *content* rather than any attached textual metadata. We call this **C**ontent **B**ased **I**mage **R**etrieval (CBIR).  With CBIR, we can search for specific phrases such as \"two dogs running across a frosty field\". We can even drop the word \"dogs\" and replace it with everyday slang for dogs like \"good boy\" or \"mans best friend\", and we return the same images showing dogs running across fields.  CLIP can accurately understand language. It understands that *in the context* of running across a field, we are likely referring to dogs and do not literally mean good children or someone's \"human\" best friend.  Amusingly, the dataset contains no images of the food hot dogs (other than one). So, suppose we search for \"hot dogs\". In that case, we first get an image containing a hot dog (and a dog), a dog looking toasty in a warm room, another dog looking warm with wooly clothing, and another dog posing for the camera. All of these portray a hot dog in one sense or another.  ---   *After being processed by CLIP's text or image encoder, we are left with vectors. That means we can search across **any** modality with **any** modality; we can search in either direction. We can also stick to a single modality, like text-to-text or image-to-image.*  ---  Now that we've seen what CLIP can do, let's take a look at *how* it can do this.  ## CLIP  CLIP actually consists of two models trained in parallel. A 12-layer text transformer for building text embeddings and a ResNet or vision transformer (ViT) for building image embeddings [3].  ![clip-architecture](./images/clip-3.png) <small>Architecture diagram of CLIP with the text encoder and ViT *or* ResNet as the image encoder.</small>  The text encoder and image encoder (ResNet *or* ViT) output single vector embeddings for each text/image record fed into the encoders. All vectors are 512 dimensional and can be represented in the same vector space, meaning similar images and text produce vectors that appear near each other.  ### Contrastive Pretraining  Across both [**N**atural **L**anguage **P**rocessing (NLP)](https://www.pinecone.io/learn/nlp/) and computer vision (CV), large pretrained models dominate the SotA. The idea is that by giving a big model a lot of data, they can learn general patterns from the dataset.  For language models, that may be the general rules and patterns in the English language. For vision models, that may be the characteristics of different scenes or objects.  The problem with multi-modality is that these models are trained separately and, by default, have no understanding of one another. CLIP solves this thanks to image-text *contrastive pretraining*. With CLIP, text and image encoders are trained while considering the other modality and context. Meaning that the text and image encoders share an \"indirect understanding\" of patterns in both modalities; language and vision.  Contrastive pretraining works by taking a *(text, image)* pair – where the text describes the image – and learning to encode the pairs as closely as possible in vector space.  For this to work well, we also need negative pairs to provide a contrastive comparison. We need positive pairs that should output similar vectors and negative pairs that should output dissimilar vectors.  This is the general idea behind contrastive learning, which can be found in the training functions of many models, particularly those that produce embedding vectors.  The negative pairs can be extracted directly from positive pairs. If we have positive pairs $(T_1, I_1)$ and $(T_2, I_2)$, we simply swap the components, giving us the negative pairs $(T_1, I_2)$ and $(T_2, I_1)$.  With this, we can apply a loss function that maximizes the similarity between $(T_1, I_1)$ and $(T_2, I_2)$, and minimizes the similarity between $(T_1, I_2)$ and $(T_2, I_1)$. Altogether, this looks like this:  ![pretraining](./images/clip-4.png) <small>Contrastive pretraining with CLIP.</small>  In this image, we can see a single pretraining step on a single batch. The loss function assumes pairs in the diagonal should have a maximized dot product score, and all other pairs should have a minimized dot product score. Both text and image encoder models are optimized for this.  A fundamental assumption is that there are no other positive pairs within a single batch. For example, we assume that \"two dogs running across a frosty field\" is only relevant to the image it is paired with. We assume there are no other texts or images with similar meanings.  This assumption is possible because the datasets used for pretraining are diverse and large enough that the likelihood of two similar pairs appearing in a single batch is negligible. Therefore, rare enough to have a little-to-no negative impact on pretraining performance.  ## Using CLIP  We have a good idea of what CLIP can be used for and how it is trained. With that, how can we get started with it?  OpenAI released a few implementations of CLIP via the Hugging Face library; this is the fastest way to get started. First, we need to install the necessary libraries.  ``` pip install transformers torch datasets ```  Before we can do anything with CLIP, we need some text and images. The `jamescalam/image-text-demo` dataset contains a small number of image-text pairs we can use in our examples.  ```python from datasets import load_dataset  data = load_dataset(     \"jamescalam/image-text-demo\",     split=\"train\" ) ```  ![text-image](./images/clip-5.png) <small>Example of text-image pair found in the dataset. Text is stored in the `\"text\"` feature and images in the `\"image\"` feature.</small>  With these sample records ready, we can move on to initializing CLIP and an image/text preprocessor like so:  ```python from transformers import CLIPProcessor, CLIPModel import torch  model_id = \"openai/clip-vit-base-patch32\"  processor = CLIPProcessor.from_pretrained(model_id) model = CLIPModel.from_pretrained(model_id)  # move model to device if possible device = 'cuda' if torch.cuda.is_available() else 'cpu'  model.to(device) ```  The `model` is CLIP itself. Note that we use the ViT image encoder (the model is `clip-vit`). Text and image data cannot be fed directly into CLIP. The text must be preprocessed to create \"tokens IDs\", and images must be resized and normalized. The `processor` handles both of these functions.  ### Encoding Text  We will start with encoding text using the CLIP text transformer. Before feeding text into CLIP, it must be preprocessed and converted into token IDs. Let's take a batch of sentences from the `unsplash` data and encode them.  {{< notebook file=\"clip-tokenize\" height=\"full\" >}}  This returns the typical text transformer inputs of `input_ids` and `attention_mask`.  The `input_ids` are token ID values where each token ID is an integer value ID that maps to a specific word or sub-word. For example the phrase *\"multi-modality\"* may be split into tokens *\\[\"multi\", \"-\", \"modal\", \"ity\"\\]*, which are then mapped to IDs *\\[1021, 110, 2427, 425\\]*.  A text transformer maps these token IDs to semantic vector embeddings that the model learned during pretraining.  The `attention_mask` is a tensor of 1s and 0s used by the model's internal mechanisms to \"pay attention\" to real token IDs and ignore padding tokens.  ---  *Padding tokens are a special type of token used by text transformers to create input sequences of a fixed length from sentences of varying length. They are appended to the end of shorter sentences, so \"hello world\" may become \"hello world \\[PAD\\] \\[PAD\\] \\[PAD\\]\".*  ---  We then use CLIP to encode all of these text descriptions with `get_text_features` like so:  ```python text_emb = model.get_text_features(     **tokens ) ```  One important thing to note here is that these embeddings are *not* normalized. If we plan on using a similarity metric like the dot product, we must normalize the embeddings:  {{< notebook file=\"clip-get-text-features\" height=\"full\" >}}  Alternatively, we can use cosine similarity as our metric as this only considers angular similarity and not vector magnitude (like dot product). For our examples, we will normalize and use dot product similarity.  We now have our text embeddings; let's see how to do the same for images.  ### Encoding Images  Images will be encoded using the ViT portion of CLIP. Similar to text encoding, we need to preprocess these images using the `preprocessor` like so:  {{< notebook file=\"clip-image-preprocess\" height=\"full\" >}}  Preprocessing images does *not* produce token IDs like those we saw from preprocessing our text. Instead, preprocessing images consists of resizing the image to a 244x244 array with three color channels (red, green, and blue) and normalizing pixel values into a $[0, 1]$ range.  After preprocessing our images, we get the image features with `get_image_features` and normalize them as before:  {{< notebook file=\"clip-get-image-features\" height=\"full\" >}}  With this, we have created CLIP embeddings for both text and images. We can move on to comparing items across the two modalities.  ### Calculating Similarity  CLIP embedding similarities are represented by their angular similarity. Meaning we can identify similar pairs using cosine similarity:  $$ cossim(A, B) = \\frac{A \\cdot B}{||A|| * ||B||} = \\frac{\\sum_i^nA_iB_i}{\\sqrt{\\sum_i^nA_i^2} \\sqrt{\\sum_i^nB_i^2}} $$  Or, if we have normalized the embeddings, we can use dot product similarity:  $$ dotproduct(A, B) = A \\cdot B = \\sum_{i=0}^{n-1}A_iB_i $$ Let's try both. First, for cosine similarity, we do:  {{< notebook file=\"clip-cosine-sim\" height=\"full\" >}}  And if we perform the same operation for dot product similarity, we should return the same results:  {{< notebook file=\"clip-dot-prod-sim\" height=\"full\" >}}  Both of these similarity score arrays look the same, and if we check for the difference between the two arrays, we will see that the scores are the same. We see some slight differences due to floating point errors.  {{< notebook file=\"clip-sim-metrics\" height=\"full\" >}}  Using the embedding functions of CLIP in this way, we can perform a semantic search across the modalities of text and image in any direction. We can search for images with text, text with images, text with text, and images with images.  These use cases are great, but we can make slight modifications to this for many other tasks.  ### Classification  One of the most impressive demonstrations of CLIP is its unparalleled zero-shot performance on various tasks. For example, given the `fragment/imagenette` dataset from Hugging Face *Datasets*, we can write a list of brief sentences that align with the ten class labels.  ![label-to-sentence](./images/clip-6.png)<small>We take the original *imagenette* labels and preappend `\"a photo of a ...\"` to each to create a set of CLIP-friendly sentence representations.</small>  From this, we can calculate the cosine similarity between the text embeddings of these ten labels against an image we'd like to classify. The text that returns the highest similarity is our predicted class.  ### Object Detection  Another compelling use case of zero-shot CLIP is object detection. We can do this by splitting our images into smaller patches and running each patch through the image encoder of CLIP. We then compare these patch embeddings to a text encoding describing what we are looking for. After calculating the similarity scores for all patches, we can collate them into a map of relevance.  For example, given an image of a butterfly and a cat, we could break it into many small patches. Given the prompt `\"a fluffy cat\"`, we will return an outline of the cat, whereas the prompt `\"a butterfly\"` will produce an outline of the butterfly.  ![patches](./images/clip-7.png) <small>Zero-shot object detection with CLIP allows us to find specific objects with natural language prompts.</small>  These are only a few of the use cases of CLIP and only scratch the surface of what is possible with this model and others in the scope of multi-modal ML.  ---  That's it for this introduction to multi-modal ML with OpenAI's CLIP. The past years since the CLIP release have seen ever more fascinating applications of the model.  DALL-E 2 is a well-known example of CLIP. The incredible images generated by DALL-E 2 start by embedding the user’s text prompt with CLIP [4]. That text embedding is then passed to the diffusion model, which generates some mind-blowing images.  The fields of NLP and CV have mainly progressed independently of each other for the past decade. However, with the introduction of world scope three models, they're becoming more entwined into a majestic multi-modal field of Machine Learning.  ## Resources  [1] Y. Bisk et al., [Experience Grounds Language](https://arxiv.org/abs/2004.10151) (2020), EMNLP  [2] J. Alammar, [Experience Grounds Language: Improving language models beyond the world of text](https://www.youtube.com/watch?v=WQm7-X4gts4) (2022), YouTube  [3] A. Radford et al., [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) (2021), arXiv  [4] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, M. Chen, [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/abs/2204.06125) (2022), arXiv ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e30a"
  },
  "title": "High-throughput vector indexes now generally available and free",
  "headline": "High-throughput vector indexes now generally available and free",
  "name": "Jeff Zhu",
  "position": "Staff Product Manager",
  "src": "/images/jeff-zhu.jpg",
  "href": "https://www.linkedin.com/in/jeffrey-s-zhu/",
  "date": "\"2022-12-08\"",
  "description": "P2 pods are now generally available to all users, dot product compatible, and 50% lower in cost.",
  "images": "[\"/images/pods-for-performance-banner.png\"]",
  "thumbnail": "\"/images/pods-for-performance-thumbnail-2.png\"",
  "content": "Back in August, we [announced](/learn/faster-easier-scalable/) a new performance pod type - p2 - in public preview. Different from the existing p1 pods, p2 pods use a new graph-based index to provide blazing fast search speeds (up to 10x faster than p1) and higher throughput (up to 200 QPS) per replica.  Today, we are excited to announce that p2 pods are now generally available for all users. Since the preview announcement, we have made the pods even more efficient and accessible for high-throughput use cases. Along with being generally available, p2 pods are now:  - **Available in the Starter (free) plan**: Users can now easily experiment with high-throughput use cases with a single p2 pod. Try a p2 pod today without any commitment! - **50% lower in cost**: We’ve continued to invest in optimizing p2 memory utilization throughout public preview. These optimizations allow us to reduce the p2 price by 50%, making p2 a more cost-competitive option than ever before. - **Dot product compatible**: Along with support of cosine and euclidean distance metrics, p2 pods now support dot product. Choose the metric that works best for you.  Updated pricing for p2 pods has been in effect since December 1, 2022, starting at $0.144/hour and up depending on plan, pod size, and cloud environment. See the [pricing page](/pricing/) for more details.  ## Deciding between pod types? When to consider p2  In general, p2 pods are designed for applications that require minimal latency (<10ms) and/or high throughput (>100 QPS). Examples of performance-focused use cases are movie recommendations on video streaming applications or personalization in social media feeds.  Here are some sample query latencies using a single p2.x1 pod:  <div class=\"responsive-table centered-table\">  | Vector Dimension | # of vectors | TopK | P50 Query Latency | P95 Query Latency | | ---------------- | ------------ | ---- | ----------------- | ----------------- | | 128              | 1M           | 10   | 7.7 ms            | 9.6 ms            | | 768              | 1M           | 10   | 9.2 ms            | 11.1 ms           | | 2048             | 100k         | 100  | 8.3 ms            | 10.3 ms           |  </div>  Vertically scaling p2 pods also improves each pod’s throughput. For example, with a single p2.x8 pod, you can support over 1000 QPS searching across 10 million 256 dimension vectors.  As always, your performance and accuracy may vary and we encourage you to test with your own data and follow our [tips for performance tuning](https://docs.pinecone.io/docs/performance-tuning). Performance is dependent on vector dimensionality, topK, filter conditions, cloud provider, and other factors.    If you have high storage and low QPS requirements, consider using our [s1 pod type](https://docs.pinecone.io/docs/indexes#s1-pods).      ## Get started  Check out the [documentation](https://docs.pinecone.io/docs/indexes#p2-pods) to learn more and how to start using p2 pods. We will share benchmarks against p1 in the near future, so stay tuned! ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e30c"
  },
  "title": "\"Long Form Question Answering in Haystack\"",
  "headline": "\"Long Form Question Answering in Haystack\"",
  "weight": "1",
  "name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "Learn how to build generative question-answering pipelines with Haystack.",
  "images": "[\"/images/haystack-lfqa-0.png\"]",
  "thumbnail": "\"https://www.pinecone.io/images/haystack-lfqa-0.png\"",
  "content": "**Q**uestion-**A**nswering (QA) has exploded as a subdomain of **N**atural **L**anguage **P**rocessing (NLP) in the last few years. QA is a widely applicable use case in NLP yet was out of reach until the introduction of [transformer models](/learn/transformers/) in 2017.  Without transformer models, the level of language comprehension required to make something as complex as QA work simply was not possible.  Although QA is a complex topic, it comes from a simple idea. The automatic retrieval of information via a more human-like interaction. The task of information retrieval (IR) is performed by almost every organization in the world. Without other options, organizations rely on person-to-person IR and rigid keyword search tools. This haphazard approach to IR generates a lot of friction, particularly for larger organizations.  Consider that many large organizations contain thousands of employees, each producing pages upon pages of unstructured text data. That data quickly gets lost in the void of unused directories and email archives.  QA offers a solution to this problem. Rather than these documents being lost in an abyss, they can be stored within a space where an intelligent QA agent can access them. Unlike humans, our QA agent can scan millions of documents in seconds and return answers from these documents almost instantly.  To interact with a QA agent, we don't need to know any fancy search logic or code. Instead, we just ask a question as we would ask another human being. Suppose we want to understand why process X exists. In that case, we can ask, \"why do we follow process X?\" and the relevant information will be returned within milliseconds.  QA capability is not a \"nice to have\". It is a key that can unlock ~90% of your organization's data. Without it, unstructured data is lost almost as soon as it is made, akin to searching in the dark.  With QA tools, employees can stop wasting time searching for snippets of information and focus on their *real*, value-adding tasks.  A small investment in QA is, for most organizations, a no-brainer.  ---  ## Long-Form Question-Answering  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/O9lrWt15wH8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  There are [two overarching approaches to QA](/learn/question-answering/), *abstractive* (/*generative*) and *extractive*. The difference between the two lies in how the answer is constructed.  For *abstractive* QA, an answer is **generated** by an NLP *generator* model, usually based on external documents. Extractive QA differs from this approach. Rather than *generating* answers, it uses a *reader* model to **extract** them directly from external documents, similar to cutting out snippets from a newspaper.  We are using external documents to inform our generator/reader models in both cases. We call this *open-book* QA as it emulates open-book exams where students (the models) can refer to a book (external documents) during the exam. In our case, the void of unstructured data becomes our *open-book*.  An open-book abstractive QA pipeline looks like this:  ![lfqa-pipeline](/images/haystack-lfqa-1.png)  <small>Open-book abstractive QA pipeline. We start by indexing documents into a document store, and then perform searches with queries encoded by a retriever model. The retrieved contexts and the initial query are passed to a generator to produce an answer.</small>  One form of open-book abstractive QA is **L**ong-**F**orm **Q**uestion-**A**nswering (LFQA). LFQA focuses on the generation of multi-sentence answers to open-ended questions.  Let's work through an implementation of LFQA using the [Haystack library](https://github.com/deepset-ai/haystack).  ## LFQA in Haystack  Haystack is a popular Python library for building QA pipelines, allowing us to create an LFQA pipeline with just a few lines of code. You can find the [full LFQA code here](https://github.com/pinecone-io/examples/tree/master/integrations/haystack).  We start by installing the necessary libraries.  ```bash !pip install -U 'farm-haystack[pinecone]'>=1.3.0 pinecone-client datasets ```  ### Data Preparation  Our first task is to find a dataset to emulate our *\"void\"* of unstructured data. For that, we will use the Wikipedia Snippets dataset. The full dataset contains over 17M passages from Wikipedia, but for this demo, we will restrict the dataset to ~50K passages. Feel free to use the whole dataset, but it will take some time to process.  ```python from datasets import load_dataset  wiki_data = load_dataset(     'vblagoje/wikipedia_snippets_streamed',     split='train',     streaming=True ) ```  We can find the dataset in Hugging Face's *Datasets* library. The `streaming=True` parameter allows us to *stream* the dataset rather than download it. The full dataset is over 9GB, and we don't need it all; `streaming` allows us to iteratively download records one at a time.  The dataset contains *eight* features, of which we are most interested in the `passage_text` and `section_title`.  {{< notebook file=\"lfqa-show-data\" height=\"full\" >}}  As we are limiting our dataset to ~50K passages, We will tighten the scope of topics and only extract records where the `section_title` feature is `History`.  ```python # filter only documents with History as section_title history = wiki_data.filter(     lambda d: d['section_title'].startswith('History') ) ```  Our dataset is now prepared. We can move on to initializing the various components in our LFQA pipeline.  ### Document Store  The document store is (not surprisingly) where we *store* our *documents*. Haystack allows us to use various document stores, each with its pros and cons. A key consideration is whether we want to support a sparse keyword search or enable a full [*semantic search*](/learn/sentence-embeddings/). Naturally, a human-like QA system requires full semantic search capability.  With that in mind, we must use a document store that supports [dense vectors](/learn/dense-vector-embeddings-nlp/). If we'd like to scale to larger datasets, we must also use a document store that supports [**A**pproximate **N**earest **N**eighbors (ANN) search](/learn/vector-indexes/).  To satisfy these requirements, we use the `PineconeDocumentStore`, which also supports:  * Single-stage metadata filtering, if using different `section_title` documents, we could use metadata filtering to tighten the search scope. * Instant index updates, meaning we can add millions of new documents and immediately see these new documents reflected in new queries. * Scalability to billions of documents. * Free hosting for up to 1M documents.  We first need to sign up for a [free API key](https://app.pinecone.io/). After signing up, API keys can be found by clicking on a project and navigating to *API Keys*. Next, we initialize the document store using:  {{< notebook file=\"lfqa-doc-store-init\" height=\"full\" >}}  Here we specify the name of the `index` where we will store our documents, the `similarity` metric, and the embedding dimension `embedding_dim`. The similarity metric and embedding dimension can change depending on the *retriever* model used. However, most retrievers use `\"cosine\"` and `768`.  We can check the current document and embedding count of our document store like so:  {{< notebook file=\"lfqa-get-counts1\" height=\"full\" >}}  If there is an existing index called `haystack-lfqa`, the above will connect to the existing index rather than initialize a new one. Existing indexes can be found and managed by visiting the active project in the [Pinecone dashboard](https://app.pinecone.io/).  We can start adding documents to our document store. To do this, we first create Haystack `Document` objects, where we will store the text *content* alongside some *metadata* for each document. The indexing process will be done in batches of `10_000`.  ```python from haystack import Document from tqdm.auto import tqdm  # progress bar  total_doc_count = 50000 batch_size = 10000  counter = 0 docs = [] for d in tqdm(history, total=total_doc_count):     # create haystack document object with text content and doc metadata     doc = Document(         content=d[\"passage_text\"],         meta={             \"article_title\": d[\"article_title\"],             'section_title': d['section_title']         }     )     docs.append(doc)     counter += 1     if counter % batch_size == 0:         # writing docs everytime 10k docs are reached         document_store.write_documents(docs)         docs.clear()     if counter == total_doc_count:         break ```  Now, if we check the *document* count, we will see that ~50K documents have been added:  {{< notebook file=\"lfqa-get-counts2\" height=\"full\" >}}  When looking at the *embedding* count, we still see *zero*; this reflects  the embedding count in the Pinecone dashboard. This is because we have not created any *embeddings* of our documents. That is another step that requires the *retriever* component.  ### Retriever  The QA pipeline relies on retrieving relevant information from our document store. In reality, this document store is what is called a *vector database*. Vector databases store *vectors* (surprise!), and each vector represents a single document's text content (the *context*).  ![lfqa-semantic-similarity](/images/haystack-lfqa-2.png)  <small>Queries and contexts with similar meaning are embedding into a similar vector space.</small>  Using the retriever model, we can take text content and encode it into a vector embedding that numerically represents the text's original *\"human\"* meaning.  The vector database is where we store these vectors. If we introduce a new *query* vector to an already populated vector database, we could use similarity metrics to measure its proximity to existing vectors. From there, we return the top *k* most similar vectors (e.g., the most *semantically* similar contexts).  ![lfqa-sim-search](/images/haystack-lfqa-3.png)  <small>We return the top *k* (in this case, `k=5`) most similar context vectors to our query vector. This visual demonstrates the top k vectors using Euclidean (or L2) distance, other common metrics include cosine similarity and dot product.</small>  We will use Haystack's `EmbeddingRetriever` component, which allows us to use any retriever model from the *Sentence Transformers* library hosted via the [Hugging Face Model Hub](https://huggingface.co/models?sort=downloads&search=sentence-transformers).  ![lfqa-flax-model](/images/haystack-lfqa-4.png)  <small>The model we use on [Hugging Face Model Hub](https://huggingface.co/flax-sentence-embeddings/all_datasets_v3_mpnet-base).</small>  First, we check that we are using the GPU, as this will make the retriever embedding process *much faster*.  ```python import torch # confirm GPU is available, outputs True if so torch.cuda.is_available() ```  The embedding step will still run if you do not have access to a CUDA-enabled GPU, but it may be slow. We initialize the `EmbeddingRetriever` component with the `all_datasets_v3_mpnet-base` model shown above.  ```python from haystack.retriever.dense import EmbeddingRetriever  retriever = EmbeddingRetriever(    document_store=document_store,    embedding_model=\"flax-sentence-embeddings/all_datasets_v3_mpnet-base\",    model_format=\"sentence_transformers\" ) ```  We call the `document_store.update_embeddings` method and pass in our new `retriever` to begin the embedding process.  ```python document_store.update_embeddings(    retriever,    batch_size=128 ) ```  The `batch_size` parameter can be increased to reduce the embedding time. However, it is limited by GPU/CPU hardware and cannot be increased beyond those limits.  We can confirm that our document store now contains the embedded documents by calling `document_store.get_embedding_count()` or checking the embedding count in the Pinecone dashboard.  ![lfqa-pinecone-vecs](/images/haystack-lfqa-5.png)  <small>Screenshot from the Pinecone dashboard showing *49,995* vectors have been stored. Calling `document_store.get_embedding_count()` will return the same number.</small>  Before moving on to the next step, we can test our document retrieval:  {{< notebook file=\"lfqa-test-retrieval\" height=\"full\" >}}  It looks like we're returning good results. We now have the first two components of our LFQA pipeline: the document store and retriever. Let's move on to the final component.  ### Generator  The generator is the component that builds our answer. Generators are sequence-to-sequence (*Seq2Seq*) models that take the query and retrieved contexts as input and use them to generate an output, the answer.  ![lfqa-query-contexts](/images/haystack-lfqa-6.png)  <small>The input to our generator model is a concatenation of the question and any retrieved contexts. In this example each context is preceded by *\"<P>\"* to mark that the following is a new chunk of information for the generator to consider.</small>  We initialize the generator using Haystack's `Seq2SeqGenerator` with a model trained specifically for LFQA, for example, `vblagoje/bart_lfqa` or `yjernite/bart_eli5` [1].  ```python from haystack.generator.transformers import Seq2SeqGenerator  generator = Seq2SeqGenerator(model_name_or_path=\"vblagoje/bart_lfqa\") ```  Now we can initialize the entire abstractive QA pipeline using Haystack's `GenerativeQAPipeline` object. This pipeline combines all three components, with the document store included as part of the `retriever`.  ```python from haystack.pipelines import GenerativeQAPipeline  pipe = GenerativeQAPipeline(generator, retriever) ```  With that, our abstractive QA pipeline is ready, and we can move on to making some queries.  ## Querying  When querying, we can specify the number of contexts for our retriever to return and the number of answers for our generator to generate using the `top_k` parameters.  {{< notebook file=\"lfqa-ask\" height=\"full\" >}}  There's a lot here, but we can see that we have `1` final answer, followed by the `3` retrieved contexts. It's hard to understand what is happening here, so we can use Haystack's `print_answer` util to clean up the output.  {{< notebook file=\"lfqa-ask2\" height=\"full\" >}}  Our output is now much more readable. The answer looks good, but there is not much detail. When we find an answer is either not good or lacks detail, there can be two reasons for this:  * The generator model has not been trained on data that includes information about the *\"war on currents\"*, so it has not *memorized* this information within its model weights.  * We have not returned any contexts that contain the answer, so the generator has no reliable external sources of information.  If neither condition is satisfied, the generator cannot produce a factually correct answer. However, in our case, we are returning some good external context. We can try and return more detail by increasing the number of contexts retrieved.  {{< notebook file=\"lfqa-ask3\" height=\"full\" >}}  Now we're seeing much more information. The latter half does descend into nonscensical gibberish, most likely because the higher `top_k` value retrieved several irrelevant contexts. However, given a larger dataset we could likely avoid this. We can also compare these results to an answer generated *without* any context by querying the generator directly.  {{< notebook file=\"lfqa-ask4\" height=\"full\" >}}  Clearly, the retrieved contexts are important. Although this isn't always the case, for example, if we ask about a more well-known fact:  {{< notebook file=\"lfqa-ask5\" height=\"full\" >}}  For general knowledge queries, the generator model can often pull the answer directly from its own *\"memory\"* (the model weights optimized during training), where it may have seen training data containing the answer. Larger models have a larger memory and, in turn, are better at direct answer generation.  When we ask more specific questions, like our question about the war on currents, both smaller and larger generators rarely return good answers without an external data source.  We can ask a few more questions:  {{< notebook file=\"lfqa-ask6\" height=\"full\" >}}  To confirm that this answer is correct, we can check the contexts used to generate the answer.  {{< notebook file=\"lfqa-ask6-contexts\" height=\"full\" >}}  In this case, the answer looks correct. If we ask a question and no relevant contexts are retrieved, the generator will typically return nonsensical or false answers, like with this question about COVID-19:  {{< notebook file=\"lfqa-ask7\" height=\"full\" >}}  The issue with nonsensical or false answers is one drawback of the LFQA approach. However, it can be mitigated somewhat by implementing thresholds to filter our low confidence answers and referring to the sources behind generated answers.  Let's finish with a final few questions.  {{< notebook file=\"lfqa-ask8\" height=\"full\" >}}    That's it for this walkthrough of Long-Form Question-Answering with Haystack. As mentioned, there are many approaches to building a pipeline like this. Many different retriever and generator models can be tested by simply switching the model names for other retriever/generator models.  With the wide variety of off-the-shelf models, many use cases can be built with little more than what we have worked through here. All that is left is to find potential use cases and try implementing LFQA (or [other QA pipelines](/learn/question-answering/)) and reap the benefits of enhanced data visibility and workplace efficiency that come with it.  ## Resources  [1] A. Fan, et al., [ELI5: Long Form Question Answering](https://arxiv.org/abs/1907.09190) (2019)  [Haystack Example Notebooks](https://github.com/pinecone-io/examples/tree/master/integrations/haystack)  [PineconeDocumentStore Integration Docs](/docs/integrations/haystack/)  [Haystack Github Repo](https://github.com/deepset-ai/haystack)",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e30e"
  },
  "content": "categories:   - Algorithms & Libraries toc: >- weight: 1 author:   name: Bala Priya C   position: Technical Writer   src: /images/bala-priya.jpg   href: https://www.linkedin.com/in/bala-priya/ description: \"Softmax Activation Function: Everything You Need to Know\" # Open graph images: ['/images/softmax-activation.png'] ---  ![Softmax Activation](/images/softmax-activation.png)  Have you ever trained a neural network to solve the problem of multiclass classification? If yes, you know that the raw outputs of the neural network are often very difficult to interpret. The **softmax activation function** simplifies this for you by making the neural network’s outputs easier to interpret!  The softmax activation function transforms the raw outputs of the neural network into a vector of *probabilities*, essentially a probability distribution over the input classes. Consider a multiclass classification problem with `N` classes. The softmax activation returns an output vector that is `N` entries long, with the entry at index `i` corresponding to the probability of a particular input belonging to the class `i`.  In this tutorial, you’ll learn all about the softmax activation function. You’ll start by reviewing the basics of multiclass classification, then proceed to understand why you cannot use the sigmoid or argmax activations in the output layer for multiclass classification problems.  Finally, you’ll learn the mathematical formulation of the softmax function and implement it in Python.  Let’s get started.  ## Multiclass Classification Revisited  Recall that in *binary* classification, there are *only two* possible classes. For example, a ConvNet trained to classify whether or not a given image is a panda is a binary classifier, whereas, in *multiclass* classification, there are *more than* two possible classes.   Let’s consider the following example: You’re given a dataset containing images of pandas, seals, and ducks. You’d like to train a neural network to predict whether a previously unseen image is that of a seal, a panda, or a duck.   Notice how the input class labels below are one-hot encoded, and the classes are *mutually exclusive*. In this context, mutual exclusivity means that a given image can only be *one* of {seal, panda, duck} at a time.  ![Multiclass Classification](/images/multiclass-classification.png) <small>Multiclass Classification Example (Image by the author)</small>  ## Can You Use Sigmoid or Argmax Activations Instead?  In this section, you’ll learn why the sigmoid and argmax functions are not the optimal choices for the output layer in a multiclass classification problem.  ### Limitations of the Sigmoid Function   Mathematically, the sigmoid activation function is given by the following equation, and it squishes all inputs onto the range [0, 1].  ![Sigmoid Function Equation](/images/sigmoid-func-formula.png) <small>Sigmoid Function Equation (Image by the author)</small>  The sigmoid function takes in any *real* number as the input and maps it to a number between 0 and 1. This is exactly why it’s well-suited for binary classification.   ▶️ You may run the following code cell to plot the values of the sigmoid function over a range of numbers.  ```python import numpy as np import seaborn as sns  def sigmoid(x):   exp_x = np.exp(x)   return np.divide(exp_x,(1 + exp_x))    x = np.linspace(-10,10,num=200) exp_x = np.exp(x) sigmoid_arr = sigmoid(x)  sns.set_theme() sns.lineplot(x = x,y = sigmoid_arr).set(title='Sigmoid Function') ```  ![Sigmoid Function Plot](/images/sigmoid-plot.png) <small>Plot of the Sigmoid Function</small>  Let’s go back to our example of classifying whether an input image is that of a panda or not. In this case, let z be the raw output of the neural network. If σ(z) is the probability that the given image belongs to class 1 (is a panda), then  1 - σ(z) is the probability that the given image does not belong to class 1 and is not a panda. You can think of σ(z) as a *probability score*.  You can now fix a threshold, say T, and predict that class whose probability score is *greater* than the chosen threshold.  However, this won't quite work when you have more than two classes. Softmax to the rescue!    In fact, you can think of the softmax function as a **vector generalization** of the sigmoid activation. We’ll revisit this later to confirm that for *binary* classification—when N = 2—the softmax and sigmoid activations are *equivalent*.  ### Limitations of the Argmax Function   The argmax function returns the **index** of the maximum value in the input array.   Let's suppose the neural network’s raw output vector is given by **z** = [0.25, 1.23, -0.8]. In this case, the maximum value is 1.23 and it occurs at index 1. In our image classification example, index 1 corresponds to the second class—and the image is predicted to be that of a panda.  In vector notation, you’ll have 1 at the index where the maximum occurs (at index 1 for the vector *z*). And you’ll have 0 at all other indices.  ![Argmax Output](/images/argmax-output.png) <small>Argmax Output (Image by the author)</small>  One limitation with using the argmax function is that its *gradients* with respect to the raw outputs of the neural networks are always *zero*. As you know, it’s the [backpropagation of gradients](https://cs231n.github.io/optimization-2/) that facilitates the learning process in neural networks.  As you’ll have to plug in the value 0 for all gradients of the argmax output during backpropagation, you cannot use the argmax function in training. Unless there's backpropagation of gradients, the parameters of the neural network cannot be adjusted, and there's effectively no learning!  From a probabilistic viewpoint, notice how the argmax function puts all the mass on index 1: the predicted class and 0 elsewhere. So it's straightforward to infer the predicted class label from the argmax output. However, we would like to know how likely the image is to be that of a panda, a seal, or a duck, and the softmax scores help us with just that!  ## The Softmax Activation Function, Explained  It's finally time to learn about softmax activation. The softmax activation function takes in a vector of **raw outputs** of the neural network and returns a vector of **probability scores**.  The equation of the softmax function is given as follows:  ![Softmax Function Equation](/images/softmax-formula.png) <small>Softmax Function Equation (Image by the author)</small>  Here, - **z** is the vector of raw outputs from the neural network  - The value of e ≈ 2.718 - The i-th entry in the softmax output vector softmax(**z**) can be thought of as the predicted probability of the test input belonging to class i.  From the plot of e^x, you can see that, regardless of whether the input x is positive, negative, or zero, e^x is always a positive number.  ![Plot of expx](/images/plot-of-expx.png) <small>Plot of exp(x)</small>  Recall that in our example, N = 3 as we have 3 classes: {seal, panda, duck}, and the valid indices are 0, 1, and 2. Suppose you’re given the vector **z** = [0.25, 1.23, -0.8] of raw outputs from the neural network.  Let's apply the softmax formula on the vector **z**, using the steps below:  1. Calculate the exponent of each entry. 2. Divide the result of step 1 by the sum of the exponents of all entries.  ![Computing softmax scores](/images/computing-softmax-scores.png) <small>Computing softmax scores for the 3 classes (Image by the author)</small>  ▶️ Now that we’ve computed the softmax scores, let’s collect them into a vector for succinct representation, as shown below:  ![Softmax Output](/images/softmax-output.png) <small>Softmax Output (Image by the author)</small>  From the softmax output above, we can make the following observations:  - In the vector **z** of raw outputs, the maximum value is 1.23, which on applying softmax activation maps to 0.664: the largest entry in the softmax output vector. Likewise, 0.25 and -0.8 map to 0.249 and 0.087: the second and the third largest entries in the softmax output respectively. Thus, applying softmax preserves the *relative ordering* of scores. - All entries in the softmax output vector are between 0 and 1. - In a multiclass classification problem, where the classes are mutually exclusive, notice how the entries of the softmax output sum up to **1**: 0.664 + 0.249 + 0.087 = 1.  This is exactly why you can think of softmax output as a probability distribution over the input classes, that makes it *readily interpretable*.  As a next step, let's examine the softmax output for our example.   In the vector softmax(**z**) = [0.664, 0.294, 0.087],  0.664 at index 1 is the largest value. This means there’s a 66.4% chance that the given image belongs to class 1, which from our one-hot encoding is a class *panda*.  And the input image has a 29.4% chance of being a seal and around 8.7% chance of being a duck.   Therefore, applying softmax gives *instant* interpretability, as you know how *likely* the test image is to belong to each of the 3 classes. In this particular example, it’s *highly likely* to be a panda and *least likely* to be a duck.  It now makes sense to call the argmax function on the softmax output to get the predicted class label. As the predicted class label is the one with the highest probability score, you can use `argmax(softmax(z))` to obtain the predicted class label. In our example, the highest probability score of 0.664 occurs at index 1, corresponding to class 1 (panda).  ### How to Implement the Softmax Activation in Python  In the previous section, we did some simple math to compute the softmax scores for the output vector **z**.   Now let's translate the math operations into equivalent operations on NumPy arrays. You may use the following code snippet to get the softmax activation for any vector **z**.  ```python import numpy as np  def softmax(z):   '''Return the softmax output of a vector.'''   exp_z = np.exp(z)   sum = exp_z.sum()   softmax_z = np.round(exp_z/sum,3)   return softmax_z ```  We can parse the definition of the softmax function: - The function takes in one required parameter **z**, a vector, and returns the softmax output vector `softmax_z`. - We use `np.exp(z)` to compute `exp(z)` for each `z` in **z**; call the resultant array `exp_z`. - Next, we call sum on the array exp_z to compute the sum of exponents. - We then divide each entry in exp_z by the sum and round off the result to 3 decimal places, storing the result in a variable, say, `softmax_z`. - Finally, the function returns the array `softmax_z`.  You may now call the function with the output array z as the argument and verify that the scores are identical to what we had computed manually.  ```python z = [0.25, 1.23, -0.8] softmax(z)  # Output array([ 0.249, 0.664, 0.087]) ```  Are you wondering if normalizing each value by the sum of entries will suffice, to get relative scores? Let's see why it’s not an efficient solution.  ### Why Won't Normalization by the Sum Suffice  Why use something math-heavy as the softmax activation? Can we not just divide each of the output values by the sum of all outputs?  Well, let's try to answer this by taking a few examples.  Use the following function to return the array normalized by the sum.  ```python def div_by_sum(z):   sum_z = np.sum(z)   out_z = np.round(z/sum_z,3)   return out_z ``` 1️⃣ Consider **z1** = [0.25, 1.23, -0.8], and call the function `div_by_sum`. In this case, though the entries in the returned array sum up to 1, it has both positive and negative values. We still aren’t able to interpret the entries as probability scores.  ```python z1 = [0.25,1.23,-0.8] div_by_sum(z1)  # Output array([ 0.368,  1.809, -1.176]) ```  2️⃣ Let **z2** = [-0.25, 1, -0.75]. In this case, all elements in the vector sum up to zero, so the denominator will always be 0. When you divide by the sum to normalize, you’ll face runtime warnings, as division by zero is not defined.  ```python z2 = [-0.25,1,-0.75] div_by_sum(z2)  # Output RuntimeWarning: divide by zero encountered in true_divide array([-inf,  inf, -inf]) ```  3️⃣ In this example, **z3** = [0.1, 0.9, 0.2]. Let’s check both the softmax and normalized scores.  ```python z3 = [0.1,0.9,0.2] # ratio: 1:9:2 print(div_by_sum(z3)) print(softmax(z3))  # Output [0.083 0.75  0.167] # ratio: 1:9:2 [0.231 0.514 0.255] ``` As shown in the code cell above, when all the inputs are positive, you may interpret the normalized scores as probability scores, but the scores are in the same ratio as in the array **z3**. In this example, the predicted class is still that of a panda.  However, you can’t guarantee that the neural network’s raw output won’t sum up to 0 or have negative entries.  4️⃣ In this example,  **z4** = [0, 0.9, 0.1]. Let’s check both the softmax and normalized scores.  ```python z4 = [0,0.9,0.1] print(div_by_sum(z4)) print(softmax(z4))  # Output [0.  0.9 0.1] [0.219 0.539 0.242] ``` As you can see, when one of the entries is 0, upon calling the `div_by_sum` function, the entry is still 0 in the normalized array. However, in the softmax output, you can see that 0 has been mapped to a score of 0.219.  In some sense you can think of the softmax activation function as a softer version of the argmax function: It *maximizes* the probability score corresponding to the predicted output label. At the same time, it's *soft* because it does assign some probability mass to the less likely classes as well, unlike the argmax function that puts the entire probability mass of 1 on the maximum, and 0 everywhere else.   In essence, the softmax activation can be perceived as a smooth approximation to the argmax function.  ## Equivalence of the Sigmoid, Softmax Activations for N = 2  Now let's revisit our earlier claim that the sigmoid and softmax activations are equivalent for binary classification when N = 2.  Recall that in binary classification, you apply the sigmoid function to the neural network’s output to get a value in the range [0, 1].  When you’re using the softmax function for multiclass classification, **the number of nodes in the output layer = the number of classes N**.  You can think of binary classification as a special case of multiclass classification. Assume that the output layer has two nodes: one outputting the score z and the other 0.  Effectively, there’s *only one* node as the other is not given any weight at all. The raw output vector now becomes **z** =  [z, 0]. Next, we may go ahead and apply softmax activation on this vector **z** and check how it’s equivalent to the sigmoid function we looked at earlier.  ![Equivalence of Sigmoid & Softmax Activations](/images/sigmoid-softmax-equiv.png) <small>Equivalence of Sigmoid & Softmax Activations (Image by the author)</small>  Observe how the softmax activation scores in this case are the same as the sigmoid activation scores: σ(z) and 1 - σ(z).  And with this, we wrap up our discussion on the softmax activation function. Let’s quickly summarize all that we’ve learned.  ## Summing Up  In this tutorial, you’ve learned the following: - How to use the softmax function as output layer activation in a multiclass classification problem. - The working of the softmax function—how it transforms a vector of raw outputs into a vector of probabilities. And how you can interpret each entry in the softmax output as the probability of the corresponding class. - How to interpret the softmax activation as an extension of the sigmoid function to multiclass classification, and their equivalence for binary classification where the number of classes N = 2.   In the next tutorial, we’ll delve deep into [cross-entropy loss](/learn/cross-entropy-loss/)—a widely-used metric to assess how well your multiclass classification model performs.  Until then, check out other interesting [NLP tutorials on vector search, algorithms, and more](/learn/). Happy learning!  {{< newsletter text=\"Subscribe for more deep learning tutorials!\" inputText=\"Email address...\" buttonText=\"Submit\">}}  ## Further Reading  [1] [Lesson on Backpropagation, Chain Rule, and Vectorization from CS231n](https://cs231n.github.io/optimization-2/)  [2] [Layer Activation Functions: Keras API Reference](https://keras.io/api/layers/activations/)  [3] [Implementation of Softmax Regression from Scratch](https://d2l.ai/chapter_linear-networks/softmax-regression-scratch.html)",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e310"
  },
  "title": "\"Color Histograms in Image Retrieval\"",
  "headline": "\"Color Histograms in Image Retrieval\"",
  "weight": "1",
  "name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "An introduction to color histograms, one of the earliest image retrieval techniques",
  "images": "['https://www.pinecone.io/images/color-histograms-0.jpg']",
  "content": "Browsing, searching, and retrieving images has never been easy. Traditionally, many technologies relied on manually appending metadata to images and searching via this metadata. This approach works for datasets with high-quality annotation, but most datasets are too large for manual annotation.  That means any large image dataset must rely on **C**ontent-**B**ased **I**mage **R**etrieval (CBIR). Search with CBIR focuses on comparing the *content* of an image rather than its metadata. Content can be color, shapes, textures – or with some of the latest advances in ML — the *\"semantic meaning\"* behind an image.  Color histograms represent one of the first CBIR techniques, allowing us to search through images based on their color profiles rather than metadata.  ![color-histograms-1](/images/color-histograms-1.png)  <small>Using the top query image we return the top five most similar images (including the same image) based on their color profiles.</small>  These examples demonstrate the core idea of color histograms. That is, we take an image, translate it into color-based histograms, and use these histograms to retrieve images with similar color profiles.  There are many pros and cons to this technique, as we will outline later. For now, be aware that this is a one of the earliest methods for CBIR, and many newer methods may be more useful (particularly for more advanced use-cases). Let's begin by focusing on understanding color histograms and how we can implement them in Python.  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/I3na13AESjw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ---  *The original code notebooks covering the content of this article can be [found here](https://github.com/pinecone-io/examples/tree/master/learn/image-retrieval/color-histograms).*  ---  ## Color Histograms  To create our histograms we first need images. Feel free to use any images you like, but, if you'd like to follow along with the same images, you can download them using HuggingFace *Datasets*.  ```python from datasets import load_dataset  # !pip install datasets  data = load_dataset('pinecone/image-set', split='train', revision='e7d39fc') ```  Inside the *image_bytes* feature of this dataset we have base64 encoded representations of 21 images. We decode them into OpenCV compatible Numpy arrays like so:  ```python from base64 import b64decode import cv2 import numpy as np  def process_fn(sample):     image_bytes = b64decode(sample['image_bytes'])     image = cv2.imdecode(np.frombuffer(image_bytes, np.uint8), cv2.IMREAD_COLOR)     return image  images = [process_fn(sample) for sample in data] ```  This code leaves us with the images in the list `images`. We can display them with *matplotlib*.  {{< notebook file=\"color-histograms-show-bgr\" height=\"full\" >}}  The three dogs look strangely blue; that's not intentional. OpenCV loads images in a **B**lue **G**reen **R**ed (BGR) format. Matplotlib expected RGB, so we must flip the *color channels* of the array to get the true color image.  ![color-histograms-5](/images/color-histograms-5.png)  <small>OpenCV reads images using BGR format, we flip the arrays to RGB so that we can view the true color image in matplotlib.</small>  {{< notebook file=\"color-histograms-invert-color\" height=\"full\" >}}  Note that while the `shape` of the array has remained the same, the three values have reversed order. Those three values are the BGR-to-RGB channel values for a single pixel in the image. As shown, after flipping the order of these channels, we can display the true color image.  ---  *Just want to create some color histogram embeddings? Skip ahead to the [**OpenCV Color Histograms**](https://www.pinecone.io/learn/color-histograms/#:~:text=a%20better%20way.-,OpenCV%20Histograms,-Building%20histograms%20can) section!*  ---  ### Step-by-Step with Numpy  To help us understand how an image is transformed into a color histogram we will work through a step-by-step example using Numpy. We already have our Numpy arrays. For the first image we can see the three BGR color values at pixel zero with:  <script src=\"https://gist.github.com/jamescalam/08140ff66b6004ef9df9e4cb790c44b4.js\"></script>  Every pixel in each image has three BGR color values like this that range on a scale of `0` (no color) to `255` (max color). Using this, we can manually create *RGB* arrays to display colors with Matplotlib like so:  {{< notebook file=\"color-histogram-color-example\" height=\"full\" >}}  From the first pixel of our image with the three dogs, we have the BGR values:  | Blue | Green | Red  | | ---- | ----- | ---- | | 165  | 174   | 134  |  We can estimate that this pixel will be a relatively neutral green-blue color, as both of these colors *slightly* overpower red. We will see this color by visualizing that pixel with matplotlib:  {{< notebook file=\"color-histogram-top-left-color\" height=\"full\" >}}  The color channel values for all pixels in the image are presently stored in an array of equal dimensions to the original image. When comparing image embeddings the most efficient techniques rely on comparing *vectors* not arrays. To handle this, we first reshape the rows and columns of the image array into a single row.  <script src=\"https://gist.github.com/jamescalam/172491f0da52ebb47d1cf5c282e69e9d.js\"></script>  We can see that the top left three pixels are still the same:  {{< notebook file=\"color-histogram-top-left-color-vec\" height=\"full\" >}}  Even now, we still don't have a \"vector\" because there are three color channels. We must extract those into their own vectors (and later during comparison we will concatenate them to form a single vector).  <script src=\"https://gist.github.com/jamescalam/04e3102b4fe9ead468d555e3573faf86.js\"></script>  Now we visualize each with a histogram.  {{< notebook file=\"color-histogram-channel-histograms\" height=\"full\" >}}  Here we can see the three color channels RGB. On the x-axis we have the pixel color value from *0* to *255* and, on the y-axis, is a count of the number of pixels with that color value.  Typically, we would discretize the histograms into a smaller number of bins. We will add this to a function called `build_histogram` that will take our image array `image` and a number of `bins` and build a histogram for us.  ```python def build_histogram(image, bins=256):     # convert from BGR to RGB     rgb_image = np.flip(image, 2)     # show the image     plt.imshow(rgb_image)     # convert to a vector     image_vector = rgb_image.reshape(1, -1, 3)     # break into given number of bins     div = 256 / bins     bins_vector = (image_vector / div).astype(int)     # get the red, green, and blue channels     red = bins_vector[0, :, 0]     green = bins_vector[0, :, 1]     blue = bins_vector[0, :, 2]     # build the histograms and display     fig, axs = plt.subplots(1, 3, figsize=(15, 4), sharey=True)     axs[0].hist(red, bins=bins, color='r')     axs[1].hist(green, bins=bins, color='g')     axs[2].hist(blue, bins=bins, color='b')     plt.show() ```  We can apply this to a few images to get an idea of how the color profile of an image can change the histograms.  {{< notebook file=\"color-histogram-visuals\" height=\"full\" >}}  That demonstrates color histograms and how we build them. However, there is a better way.  ## OpenCV Histograms  Building histograms can be abstracted to be done more easily using the OpenCV library. OpenCV has a function called `calcHist` specifically for building histograms. We apply it like so:  <script src=\"https://gist.github.com/jamescalam/2aa67abb1b94194d6cd32cbe2da6b011.js\"></script>  The values used here are:  ```python cv2.calcHist(  [images], [channels], [mask], [bins], [hist_range] ) ```  Where:  * `images` is our *cv2* loaded image with a BGR color channel. This argument expects a list of images which is why we have placed a single image inside square brackets `[]`. * `channels` is the color channel (BGR) that we'd like to create a histogram for; we do this for a single channel at a time. * `mask` is another image array consisting of `0` and `1` values that allow us to mask (e.g. hide) part of `images` if wanted. We will not use this so we set it to `None`.  ![color-histograms-4](/images/color-histograms-4.png) <small>Example of how the masking layer works to \"hide\" part of an image.</small>  * `bins` is the number of buckets/histogram bars we place our values in. We can set this to 256 if we'd like to keep all of the original values. * `hist_range` is the range of color values we expect. As we're using RGB/BGR, we expect a min value of *0* and max value of *255*, so we write `[0, 256]` (the upper limit is exclusive).  After calculating these histogram values we can visualize them again using `plot`.  {{< notebook file=\"color-histograms-calchist-plot\" height=\"full\" >}}  The `calcHist` function has effectively performed the same operation but with much less code. We now have our histograms; however, we're not done yet.  ## Vectors and Similarity  We have a function for transforming our images into three vectors representing the three color channels. Before comparing our images we must concatenate these three vectors into a single vector. We will pack all of this into `get_vector`:  {{< notebook file=\"color-histograms-get-vec\" height=\"full\" >}}  Using the default `bins=32` this function will return a vector with *96* dimensions, where values *[0, ... 32]* are red, *[32, ... 64]* are green, and *[64, ... 96]* are blue.  Once we have these vectors we can compare them using typical similarity/distance metrics such as Euclidean distance and cosine similarity. To calculate the cosine similarity we use the formula:   ![color-histograms-6 cosine similarity](/images/color-histograms-6.png)   Which we write in Python with just:  ```python def cosine(a, b):     return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)) ```  Using our `cosine` function we can calculate the similarity which varies from *0* (highly dissimilar) to *1* (identical). We can apply this alongside everything else we have done so far to create another `search` function that will return the `top_k` most similar images to a particular query image specified by its index `idx` in `images`.  {{< notebook file=\"color-histogram-search-func\" height=\"full\" >}}  We can add a few more components to this search function to output the images themselves rather than the image index positions; the code for this can [be found here](https://github.com/pinecone-io/examples/blob/master/learn/image-retrieval/color-histograms/01-search-histogram.ipynb). Here are some of the results:  ![color-histograms-1](/images/color-histograms-1.png) <small>Color histogram retrieval with query image (top) and retrieved similar images (bottom).</small>  Here, it is clear that the teal-orange color profile of the first query is definitely shared by the returned results. If we were looking for images with a similar aesthetic, I'd view this as a good result.  ![color-histograms-2](/images/color-histograms-2.png) <small>Color histogram retrieval with query image (top) and retrieved similar images (bottom).</small>  The dog with orange background query returns a cat with orange background as the most similar result (excluding the same image). Again, in terms of color profiles and image aesthetics, the top result is good, and the histogram is also clearly similar.  ---  These are some great results, and you can test the color histogram retrieval using the [notebook here](https://github.com/pinecone-io/examples/blob/master/learn/image-retrieval/color-histograms/01-search-histogram.ipynb). However, this isn't perfect, and these results can highlight some of the drawbacks of using color histograms.  Their key limitation is that they rely solely on image color profiles. That means the textures, edges, or actual meaning behind the content of images is *not* considered.  Further work on color histograms helped improve performance in some of these areas, such as comparing textures and edges, but these were still limited. Other deep learning methods greatly enhanced the performance of retrieving images based on semantic meaning, which is the focus of most modern technologies.  Despite these drawbacks, for a simple content-based image retrieval system, this approach provides several benefits:  * It is incredibly **easy to implement**; all we need to do is extract pixel color values and transform these into a vector to be compared with a simple metric like Euclidean distance or cosine similarity. * The results are highly relevant for color-focused retrieval. If the meaningful content of an image is not important, this can be useful. * Results are highly interpretable. There is no black box operation happening here; we know that every result is returned because it has a similar color profile.  With this in mind, embedding images using color histograms can produce great results for *simple* and interpretable image retrieval systems where aesthetics or color-profiles are important.  {{< newsletter text=\"Subscribe for more computer vision and retrieval content!\" inputText=\"Email address...\" buttonText=\"Submit\">}}  ## Resources  [Color Histogram Notebooks](https://github.com/pinecone-io/examples/tree/master/learn/image-retrieval/color-histograms)  J. Smith, [Integrated Spatial and Feature Image Systems: Retrieval, Analysis and Compression](https://www.ee.columbia.edu/ln/dvmm/publications/PhD_theses/jrsmith-thesis.pdf) (2007) ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e312"
  },
  "content": "categories:   - Company News toc: >- author:   name: Greg Kogan   position: VP Marketing   src: /images/company-greg.png   href: https://www.linkedin.com/in/gkogan/ date: \"2022-02-16\" # Date: February 16, 2022 # Open Graph description: \"Latest version of the Pinecone gives you greater performance, predictability, and control of your vector search applications.\" # No image in article, default will be used thumbnail: \"/images/pinecone-logo-thumbnail.jpg\" ---  **The latest version of Pinecone gives you greater performance, predictability, and control of your vector search applications.**  Low-latency vector search at scale is one of the biggest reasons engineering teams choose Pinecone. This update significantly lowers search latency for large indexes even further. For example, an index with 100M vectors is now 3.4x faster than before.  Engineers also choose Pinecone because they can start and scale a vector search service during their lunch break, without any infrastructure or algorithm hassles. This release provides more predictability and control while minimizing overhead, with a redesigned user console and additional deployment options across GCP and AWS.  This update is effective on all new indexes starting today. Indexes created before today will be automatically updated one month from now, on March 15. If you’d like your existing indexes updated sooner, we can perform a zero-downtime update for you by request.  Continue reading to learn more, then [try it](https://app.pinecone.io) and [join us for a live demo and Q&A](https://pinecone-io.zoom.us/webinar/register/WN_kd6WSbQ5TSONOSqMF0vRdg) on Tuesday, February 22nd.  ## Performance at scale  You’ve always had fast [vector search](/learn/vector-search-basics/) with Pinecone, and now it *stays* remarkably fast even as you scale. Previously, as you added pods to accommodate a growing index, you experienced increasing search latencies. This release flattens the impact of scaling, so the search latency stays low even with *hundreds of millions* of vectors.  In our benchmark tests, indexes running on our performance-optimized pods (p1) maintain search speeds well below 120ms (p95) as they scale from zero to tens of millions of vectors. At 10M 768-dimensional vectors, Pinecone is now 1.6x faster than before, and at 20M vectors it is a whopping 2.5x faster than before.  *Note: These tests used the minimum number of pods required. This is best case in terms of cost (fewer pods) and the \"worst case\" in terms of performance (each pod is at full capacity). Users can reduce latencies by adding more pods, and/or applying filters to queries. In practice, many customers see sub-100ms latencies from Pinecone. Since Pinecone is a cloud service, these latencies include network overhead.*  ![Query latencies at scale on p1 pods](/images/pinecone-query-latencies-on-p1-pods.svg)  The difference is even starker for indexes running on our storage-optimized pods (s1). These pods were designed as a cost-efficient option for teams with larger catalogs and a tolerance for higher latency. However, their progressively slower search speeds at larger index sizes made them impractical for real-time applications... Until today.  With this release, indexes running on s1 pods maintain search latencies under 500ms (p95) even as you scale to 100M+ vectors. **At 50M vectors, Pinecone is 2x faster than before, and at 100M vectors (20 pods) it's an incredible 3.4x faster than before.**  ![Query latencies at scale on s1 pods](/images/pinecone-query-latencies-on-s1-pods.svg)  It doesn’t stop there. If you need to index billions of vectors while keeping sub-second latencies — like some of our customers — [contact us](/contact/) for help in setting up your index.  As always, your performance may vary and we encourage you to test with your own data. Latencies are dependent on vector dimensionality, metadata size, network connection, cloud provider (more on this below), and other factors.  This improvement came from months of engineering efforts to build the most performant, scalable, and reliable vector database. It included rewriting core parts of the Pinecone engine in Rust, optimizing I/O operations, implementing dynamic caching, re-configuring storage formats, and more. This effort is never-ending, so expect even more performance improvements very soon.  ## Predict performance and usage  You need to know what to expect from your search applications. How fast will it be? How consistent is that speed? How much hardware do I need? What will it cost? How long will it take you to upload your data? This release helps you answer all of these questions and puts your mind at ease.  The first thing you want to predict is how many pods you'll need, what they'll cost, and what's the expected latency. We’ve made this planning easier with our [new usage estimator tool](/pricing/#estimate).  Next, you need to know that search speed will be consistent for your users without erratic spikes from one query to the next. This update drastically lowers the variance between p50 and p95 search latencies: It is now within 20% for p1 pods, and just 10% for s1 pods.  ![Query latency variance on s1 pods](/images/pinecone-query-latency-variance-on-s1-pods.svg)  And finally, when you start loading data into Pinecone you want to know it'll be indexed quickly and completely. We've made [data ingestion](https://www.pinecone.io/docs/insert-data/) faster and more reliable. Before, upserts slowed down as the index approached capacity, and if you exceeded capacity then the index would fail. Now, upserts stay fast all the way, and trying to upload beyond capacity will result in a gentle error message — the index will remain up.  ## Control projects and environments  Whether it’s to minimize latencies or to comply with data regulations, many Pinecone users asked for the ability to choose between cloud providers and regions. Now they have it.  **Users on the [Standard plan](/pricing/) can now choose from GCP US-West, GCP EU-West (new), and AWS US-East (new)**. Even more regions are coming soon.  As before, users on the Dedicated plan get a single-tenant environment on GCP or AWS in any region of their choice.  GCP US-West remains the default environment for new projects, and the only one available for users on the Free plan. The environment is set when creating a project, and different projects can use different environments.   And now, creating and managing projects is even easier with the completely redesigned management console. It includes a new page for managing projects, along with a more powerful page for managing indexes and data.  ![Screenshot of the redesigned console](/images/pinecone-february-console-indexes.png)  And, let’s be honest, it’s also easier on the eyes. [See it!](https://app.pinecone.io/)  ## Get Started  For existing users:  * All *new* indexes starting from today come with this update. * If you use the Python client, install the latest version with `pip install pinecone-client` (see [installation docs](https://www.pinecone.io/docs/quickstart/)). * This is a non-breaking change. The updated API is backward compatible. * Existing indexes will be rolled over by March 15th, with zero downtime. * To update existing indexes before March 15, users on the Free plan should re-create the index, and users on the Standard or Dedicated plans should contact us with a preferred time when you want us to update your indexes.  For new users:  * [Create a free account](https://app.pinecone.io) and start building vector-search applications. * [Register for the live office hour](https://pinecone-io.zoom.us/webinar/register/WN_kd6WSbQ5TSONOSqMF0vRdg) on Tuesday, Feb 22, to learn more, see a demo, and get your questions answered! ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e314"
  },
  "title": "\"What is a Vector Database?\"",
  "headline": "\"What is a <span>Vector Database</span>?\"",
  "description": "The nature of vector embeddings require new methods of storage and retrieval. We need a new kind of database.",
  "weight": "3",
  "name": "Bryan Turriff",
  "position": "Director of Product Marketing",
  "src": "/images/company-bryan.jpeg",
  "href": "https://www.linkedin.com/in/bryanturriff/",
  "images": "['/images/vector-database-1.png']",
  "content": "![Traditional vs Vector Data](/images/vector-database-1.jpg)  **Pinecone is a vector database that makes it easy for developers to add vector-search features to their applications, using just an API. [Try it now](https://app.pinecone.io) to see for yourself, or continue reading to learn about vector databases.**  ## Introduction  Complex data is growing at break-neck speed. These are unstructured forms of data that include documents, images, videos, and plain text on the web. Many organizations would benefit from storing and analyzing complex data, but complex data can be difficult for traditional databases built with structured data in mind. Classifying complex data with keywords and metadata alone may be insufficient to fully represent all of its various characteristics.  Fortunately, Machine Learning (ML) techniques can offer a far more helpful representation of complex data by transforming it into vector embeddings. Vector embeddings describe complex data objects as numeric values in hundreds or thousands of different dimensions.   Many technologies exist for building vectors, ranging from vector representations of words or sentences, to cross-media text, images, audio, and video. There are [several existing public models](https://www.sbert.net/) that are high-performance and easy to use as-is. These models can be [fine-tuned for specific applications](https://www.pinecone.io/learn/fine-tune-sentence-transformers-mnr/) and you can also train a new model from scratch, although that is less common.  Vector databases are purpose-built to handle the unique structure of vector embeddings. They index vectors for easy search and retrieval by comparing values and finding those that are most similar to one another. They are, however, difficult to implement.  Until now, vector databases have been reserved for only a handful of tech giants that have the resources to develop and manage them. Unless properly calibrated, they may not provide the performance users require without costing a fortune.  Using a well-constructed vector database gives your applications superior search capability while also meeting performance and cost goals. There are several solutions available to make it easier to implement. These solutions range from plugins and open-source projects to fully-managed services that handle security, availability, and performance. This document will describe common uses of vector databases, core components, and how to get started.  ## What is a Vector Database?  **A vector database indexes and stores vector embeddings for fast retrieval and similarity search, with capabilities like CRUD operations, metadata filtering, and horizontal scaling.**  <div class=\"post-group\">  **vector** noun  ˈvek-tər  in machine learning, an array of numerical measurements that describe and represent the various characteristics of an object </div>  <div class=\"post-group\">  **database** noun  ˈdā-tə-ˌbās  a large collection of data organized especially for rapid search and retrieval (as by a computer) </div>  When we say that vector databases index [vector embeddings](https://www.pinecone.io/learn/vector-embeddings/), we mean that they organize them in a way that we can compare any vector to one another or to the vector of a search query. We will cover algorithms used to index vectors further down. Vector databases are also responsible for executing CRUD operations (create, read, update, and delete) and [metadata filtering](https://www.pinecone.io/learn/vector-search-filtering/). The combination of traditional database functionality with the ability to search and compare vectors in an index makes vector databases the powerful tools that they are.  Vector databases excel at [similarity search](https://www.pinecone.io/learn/what-is-similarity-search/), or “vector search.” Vector search enables users to describe what they want to find without having to know which keywords or metadata classifications are ascribed to the stored objects. Vector search can also return results that are similar or near-neighbor matches, providing a more comprehensive list of results that otherwise may have remained hidden.   ## Why Use a Vector Database?  Vector search in production is the most common reason to use a vector database. Vector search compares the similarity of multiple objects to a search query or subject item. In order to find similar matches, you convert the subject item or query into a vector using the same ML embedding model used to create your vector embeddings. The vector database compares the similarity of these objects to find the closest matches, providing accurate results while eliminating irrelevant results that traditional search technology might have returned.  Let’s look at some common use cases for vector search:  ### 1. [Semantic search](/semantic-search)  Searching text and documents can generally be done in two ways. Lexical search looks for patterns and exact word or string matches, while semantic search uses the meaning of your search query or question and puts it into context. Vector databases store and index vector embeddings from Natural Language Processing models to understand the meaning and context of strings of text, sentences, and whole documents for more accurate and relevant search results.  Using natural language queries to find relevant results is a better experience and allows users to find what they need more quickly without having to know specifics about how the data is classified.  See example code: [Semantic Search](https://www.pinecone.io/docs/examples/semantic-text-search/), [Hybrid Search with Filtering](https://www.pinecone.io/docs/examples/basic-hybrid-search/)  ### 2. Similarity search for images, audio, video, JSON, and other forms of unstructured data  Images, audio, video, and other unstructured datasets can be very challenging to classify and store in a traditional database. This often requires keywords, descriptions, and metadata to be manually applied to each object. The way one individual classifies one of the complex data objects may not be obvious to another. As a result, searching for complex data can be very hit and miss. This approach requires the searcher to understand something about how the data is structured and construct queries that match the original data model.  See example code: [Image Similarity Search](https://www.pinecone.io/docs/examples/image-similarity-search/)  ### 3. Ranking and recommendation engines  Vector databases are a great solution for powering ranking and recommendation engines. For online retailers, they can be used to suggest items similar to past purchases or a current item the customer is researching. Streaming media services can apply a user’s song ratings to create perfectly matched recommendations tailored to the individual rather than relying on collaborative filtering or popularity lists.  The ability to find similar items based on nearest matches makes vector databases ideal for offering relevant suggestions, and can easily rank items based on similarity scores.  See example code: [Movie Recommender](https://www.pinecone.io/docs/examples/movie-recommender/)  ### 4. Deduplication and record matching  Another use case for vector similarity search is record matching and deduplication. Using the similarity service to find near-duplicate records can be used in a wide range of applications.  Consider an application that removes duplicate items from a catalog to make it far more usable and relevant.  See example code: [Document Deduplication](https://www.pinecone.io/docs/examples/document-deduplication/)  ### 5. Anomaly detection  As good as vector databases are in finding similar objects, they can also find objects that are distant or dissimilar from an expected result. These anomalies are valuable in applications used for threat assessment, fraud detection, and IT Operations. It’s possible to identify the most relevant anomalies for further analysis without overwhelming resources with a high rate of false alarms.   See example code: [IT Threat Detection](https://www.pinecone.io/docs/examples/it-threat-detection/)  ## Required Capabilities of a Vector Database  ### 1. Vector Indexes for Search and Retrieval  Vector databases use [algorithms specifically designed to index and retrieve vectors efficiently](https://www.pinecone.io/learn/vector-indexes/). Different use cases require the prioritization of accuracy, latency, or memory usage which can be fine-tuned using different algorithms. Choosing and optimizing these algorithms is a science in itself, and finding the optimum algorithm for different datasets that satisfies use-case requirements can be challenging.  ![Vector Indexes](/images/vector-database-2.jpg)  Alongside indexes, there are also similarity and distance metrics. These metrics are what measure the relevance/similarity between vector embeddings. Some metrics have better recall and precision performance than others. Common metrics in vector indexes include Euclidean distance, cosine similarity, and dot products.   Vector databases use “nearest neighbor” indexes to assess how closely similar objects are to one another or to a search query. Traditional nearest neighbor search is problematic for large indexes as they require a comparison between the search query and every indexed vector. Comparing every vector takes time.  Approximate Nearest Neighbor (ANN) search circumvents this problem by approximating and retrieving a best guess of most similar vectors. While ANN does not guarantee to return the exact closest match, it balances very good precision with very fast performance.  Techniques such as [HNSW](https://www.pinecone.io/learn/hnsw/), IVF, or [PQ](https://www.pinecone.io/learn/product-quantization/) are some of the most popular components used in building effective ANN indexes. Where each technique focuses on improving a particular performance property, such as memory reduction with PQ or fast but accurate search times with HNSW and IVF. It is common practice to mix several components to produce a ‘composite’ index to achieve optimal performance for a given use case.  Without a vector database, designing and building an effective index is not easy. If using a stand-alone framework such as [Faiss](/learn/faiss-tutorial/), the design and deployment of an index requires a team of experienced engineers with a good grasp of indexing and retrieval algorithms. At a minimum, these vectors must be mapped back to the original data using another storage and retrieval pipeline (as stand-alone indexes do not support this). Indexes require periodic retraining and mechanisms for tracking deleted, replaced, or new data. A team must account for these added requirements and any ongoing operations.  ### 2. Single-Stage Filtering  Filtering allows you to limit search results based on vector metadata. This can improve the relevance of search results by returning a subset of available matches based on limiting criteria.    Post-filtering applies approximate nearest neighbor search first and then restricts the results to metadata filter restrictions. ANN typically returns a requested set of nearest matches but does not know how many (if any) of them will match the metadata criteria. This is usually fast but may return too few vectors that match the filter if any at all.  ![Filtering](/images/vector-database-3.png)  Pre-filtering vectors with metadata shrinks the dataset and may return highly relevant results. However, because pre-filtering applies the matching criteria on each vector in the index first, it can also severely slow the performance of vector databases.  Single-stage filtering is a must for effective vector databases. It combines the accuracy and relevance of pre-filtering with speeds that are as fast or faster than post-filtering. By merging vector and metadata indexes into a single index, single-stage filtering offers the best of both approaches.  ### 3. Data Sharding  What is a vector database without scaling? ANN algorithms search vectors with remarkable efficiency. But whatever their efficiency, hardware limits what’s possible on a single machine. You can scale vertically — increase the capacity of a single machine and parallelize aspects of the ANN routine. But you’ll hit a limit to how far you can take this, be it cost or availability of behemoth machines. Enter horizontal scaling. We can divide the vectors into shards and replicas to scale across many commodity-level machines to achieve scalable and cost-effective performance.  Imagine a friend filled a bucket with 100 little slips of paper. And suppose on each slip of paper she wrote someone’s name along with their birthday, month and day, and the actual time of birth. Then she requests: “find the person whose birth date and time is closest to yours”. So you sift through the bucket to find the closest match. In this way, the slips of paper are like vectors, you are like a CPU, and the bucket is like RAM.  Now suppose your friend gave you a bucket with 1000 names and birthdays — you’re going to be searching for a while! Instead, you split the 1000 names into 10 buckets and invite 10 friends to help. Each of you searches only 100 names for the best match in the bucket and then compares the results each of you found to find the very best match. As a result, you find the best match among 1000 names in almost the same amount of time it took you to find the best match among 100 names. You’ve horizontally scaled yourself!   A vector database divides the vectors equally into shards, searches each shard, and combines the results from all the shards at the end to determine the best match. Often, it will use Kubernetes and grant each shard its own Kubernetes pod with at least one CPU and some RAM. The pods work in parallel to search the vectors.  As a result, you get the answer in just a little over the time it takes one pod to search one shard. Have 20M vectors? Use 20 pods and get results in the time it takes one pod to search 1M vectors or use 40 pods (500K vectors per shard) to get results even faster. There is more to it, but put simply, fewer vectors per pod lower query latency and allow you to search as many as billions of vectors in a reasonable amount of time.  ![Shard Router](/images/vector-database-4.jpg)  ### 4. Replication  Vector databases need to handle many requests gracefully. Shards allow it to employ many pods in parallel to perform a vector search faster. But what if you need to perform many different vector searches at the same time or in rapid succession? Even speedy vector searches will get backed up if new requests are coming in fast enough. Enter replicas.  As their name implies, replicas replicate the whole set of pods to handle more requests in parallel. If we think back to our names-in-buckets analogy, this is like creating a copy of the ten buckets and asking another ten friends to handle any new matching request. Suppose ten pods can search 10M vectors in 100 ms. If you issue one request a second, you’re good. If you issue 20 different requests every second, you need backup. Add a replica (ten more pods in this case) to keep up with the demand.  Replicas also improve availability. Machines fail — it’s a fact of life. A vector database needs to bring pods back up as quickly as possible after a failure. But “as quickly as possible” isn’t always quick enough. Ideally, it needs to handle failures immediately without missing a beat. Cloud providers offer so-called availability zones that are highly unlikely to fail simultaneously.   The vector database can spread replicas to different availability zones to ensure high availability. But you, the user, have a part to play here, too — you need to have multiple replicas and replica capacity, such that fewer replicas can handle the query load with acceptable latency in the case of a failure.   ### 5. Hybrid Storage  Vector searches typically run completely in-memory (RAM). For companies with over a billion items in their catalog, the memory costs alone could make vector search too expensive to consider. Some vector search libraries have the option to store everything on disk, but this could come at the expense of search latencies becoming unacceptably high.  In a hybrid storage configuration, a compressed vector index is stored in memory, and the original, full-resolution vector index is stored on disk. The in-memory index is for locating a small set of candidates to search within the complete index on disk. This method provides fast and accurate search results yet cuts infrastructure costs by up to 10x.  Hybrid storage allows you to store more vectors across the same data footprint, lowering the cost of operating your vector database by improving overall storage capacity without negatively impacting database performance.  ### 6. API  Vector databases should take the burden of building and maintaining vector search capability away from developers so they can focus on making their applications the best they can be. An API makes it easy for developers to use or manage the vector database from any other application.  The application makes API calls to the vector database to perform an action such as upserting vectors into the database, retrieving query results, or deleting vectors.     REST APIs add flexibility by initiating the functionality of the vector database from any environment that can make HTTPS calls. Developers may also access it directly through clients using languages like Python, Java, and Go.  ## Getting Started with Vector Databases  Combined with machine learning transformer models, vector databases offer a more intuitive way to find similar objects, answer complex questions, and understand the hidden context of complex data.  So how should you get started?  ### Learn More about Vector Databases  Visit the [Pinecone learning center](/learn/) and read more about key concepts, including vector embeddings, vector indexes, and NLP for semantic search. Here are some of the most popular topics:  - [Sentence Transformers: Meanings in Disguise](/learn/sentence-embeddings/) - This guide discusses core techniques for converting text and documents into vector embeddings and details some of the most popular NLP embedding models.  - [The Missing WHERE Clause in Vector Search](/learn/vector-search-filtering/) - This article explains two common methods for adding metadata filters to vector search, and explores their limitations. Then, we cover how Single-Stage Filtering bridges some of these gaps.  - [Nearest Neighbor Indexes for Similarity Search](/learn/vector-indexes/) - This article explores the pros and cons of some of the most important indexes including Flat, LSH, HNSW, and IVF. It also gives tips for deciding which to use and the impact of parameters in each index.  ### Launch Your First Vector Database  Once you have your vector embeddings, you’ll need a vector database to index, store, and retrieve them.  [Create an account](https://app.pinecone.io) and launch your first vector database.  With Pinecone, you can do this in just a few minutes. Pinecone is a fully managed vector database that makes it easy to add vector search to production applications. It combines vector search libraries, capabilities such as filtering, and distributed infrastructure to provide high performance and reliability at any scale.  {{< newsletter text=\"Subscribe for more on vector databases!\" inputText=\"Email address...\" buttonText=\"Submit\">}} ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e316"
  },
  "title": "\"Pinecone 2.0 is Available and Free\"",
  "headline": "\"Pinecone 2.0 is Available and Free\"",
  "name": "Edo Liberty",
  "position": "Founder and CEO",
  "src": "/images/company-edo.png",
  "href": "https://edoliberty.github.io/",
  "date": "\"2021-10-04\"",
  "# Date": "October 4, 2021",
  "description": "Every use case comes with its own requirements for performance, number of vectors, and throughput. Our new pricing gives you the choice to optimize Pinecone to meet your needs and pay (less) as you go.",
  "thumbnail": "\"/images/pinecone-logo-thumbnail.jpg\"",
  "content": "**Pinecone 2.0 is generally available as of today, with many [new features](/learn/pinecone-v2/) and [new pricing](/pricing/) which is up to 10x cheaper for most customers and, for some, completely free!**   On September 19, 2021, we announced Pinecone 2.0, which introduced many new features that get vector similarity search applications to production faster. Today, we are pleased to announce Pinecone 2.0 is generally available. The all-new pricing lets you choose the plan that works best for you.  Every use case comes with its own requirements for performance, number of vectors, and throughput. Our new pricing gives you the choice to optimize Pinecone to meet your needs and pay (less) as you go.  The new pricing is based on hardware usage (number and types of pods) and better aligns with your actual consumption patterns. We believe that pricing by hardware is simpler, more transparent, and gives you the flexibility to optimize cost and performance for your dataset and use case.  We are introducing three new pricing plans: Free, Standard, and Dedicated.  ## Free  We are committed to helping companies create better products using vector search. We want to remove the obstacles to getting these projects into production. That includes the obstacle — for some — of cost.  Our new Free plan lets you use the fully managed Pinecone service for small workloads without any cost. It comes with a single P1 pod, which is enough to search through roughly a million vectors in around 100ms, or through 100K vectors in around 20ms. For many users this is all they’ll need.  With the Free plan you get the same production-ready, secure, and fully managed vector database as the other plans.  ## Standard  When you are ready to scale up, the new Standard plan lets you choose the type of pod, the number of pods, and the number of replicas to optimize your Pinecone service.  In addition to the P1 pods, you have the option of S1 pods. The S1 pods are storage-optimized and suitable for a large number of vectors when you are more tolerant of slightly higher latency. S1 pods are compatible with Pinecone’s hybrid index. Each S1 pod includes 1 vCPU and 20GB SSD storage.   With the Standard plan, you can achieve low latencies across any number of vectors. Pricing in this plan is based on the number of pods used per hour, with P1 pods priced at $0.070/hour and S1 pods at $0.075/hour. Connect with us for help with choosing the best configuration to meet your requirements.  The Standard plan offers priority support within a 24-hour timeframe and increased reliability with multiple availability zones.  ## Dedicated  If your workload is very large or if you have unique compliance or deployment requirements, our Dedicated plan is for you. Our team will work with you to configure a custom deployment that meets your requirements and is optimized for your workload, in a dedicated GCP or AWS environment in any region.  As with our Standard plan, our Dedicated plan ensures high availability with multi-AZ, with the added peace of mind of a fully dedicated cluster and high-priority support within a 4-hour time window.  ## Get Started  Pinecone 2.0 brings vector similarity search from R&D labs to production applications, for companies of all sizes. Our new hardware-based pricing plans provide the flexibility to get started quickly and scale effortlessly.  What will you build with the Pinecone vector database? Get started now:  * [Visit our pricing page](/pricing/) for more detailed information * [Sign up](https://app.pinecone.io/) and start using Pinecone for free * [Contact us](/contact/) to discuss your project * [Learn more](/learn/pinecone-v2/) about Pinecone 2.0",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e318"
  },
  "title": "\"How to Explain ConvNet Predictions Using Class Activation Maps\"",
  "headline": "\"How to Explain ConvNet Predictions Using Class Activation Maps\"",
  "weight": "1",
  "name": "Bala Priya C",
  "position": "Technical Writer",
  "src": "/images/bala-priya.jpg",
  "href": "https://www.linkedin.com/in/bala-priya/",
  "description": "\"Learn how class activation maps (CAMs) can be used to explain a convolutional neural net (ConvNet), and how to generate them in PyTorch.\"",
  "images": "[\"/images/class-activation-maps.png\"]",
  "content": "![Class activation maps](/images/class-activation-maps.png)  Have you ever used deep learning to solve computer vision tasks? If so, you probably trained a convolutional neural network (ConvNet or CNN) for tasks such as image classification and visual question answering.   In practice, ConvNets are often viewed as black boxes that take in a dataset and give a task-specific output: predictions in image classification, captions in image captioning, and more. For example, in image classification, you’ll optimize the model for prediction accuracy.  **But how do you know *which* parts of the image the network was looking at when it made a prediction? And how do you go from black box to interpretable models?**  Adding a layer of explainability to ConvNets can be helpful in applications such as medical imaging for disease prognosis. For example, consider a classification model trained on medical images, namely, brain scans and X-rays, to predict the presence or absence of a medical condition. Ensuring that the model is using the relevant parts of the images for its predictions makes it more trustworthy than a black box model with a high prediction accuracy.  **Class activation maps** can help explain the predictions of a ConvNet. Class activation maps, commonly called CAMs, are **class-discriminative** saliency maps. While saliency maps give information on the most important parts of an image for a particular class, class-discriminative saliency maps help distinguish between classes.   In this tutorial, you’ll learn how class activation maps (CAM) and their generalizations, Grad-CAM and Grad-CAM++, can be used to explain a ConvNet. You’ll then learn how to generate class activation maps in PyTorch.   Let's begin!  ## Class Activation Maps Explained  In general, a ConvNet consists of a series of convolutional layers, each consisting of a set of filters, followed by fully connected layers.   Activation maps indicate the salient regions of an image for a particular prediction. Class activation map (CAM) uses a global average pooling (GAP) layer after the last convolutional layer. Let’s understand how this works.  ![CAM intro](/images/CAM-intro.png) <small>GAP Layer After the Last CONV Layer (Image by the author)</small>  If there are `n` filters in the last convolutional layer, then there are `n` feature maps. The activation map for a particular output class is the *weighted combination* of all the `n` feature maps.   *So how do we learn these weights?*  **Step 1**: Apply global average pooling to each of the feature maps.   The average value of all pixels in a feature map is its global average. Here’s an example of how global average pooling works. The qualifier global means that the average is computed over *all pixel locations* in the feature map.  ![How GAP works](/images/how-GAP-works.png) <small>How GAP Works - An Example (Image by the author)</small>  After computing the global average for each of the feature maps, we’ll have `n` scalars, $k_1, k_2, …, k_n$. Let’s call them GAP outputs.  ![Feature maps to GAP](/images/feature-maps-to-GAP.png) <small>From Feature Maps to Scalars through GAP (Image by the author)</small>  Step 2: The next step is to learn a linear model from these GAP outputs onto the class labels. For each of the `N` output classes, we should learn a model with weights $w_1, w_2,...,w_n$.  Therefore, we’ll have to learn `N` linear models in all.  ![Linear Models from GAP output](/images/GAP-to-output-classes.png) <small>Linear Models from GAP Output onto the Class Labels (Image by the author)</small>  **Step 3**: Once we’ve obtained the `n` weights for each of the `N` classes, we can weight the feature maps to generate the class activation maps. Therefore, *different* weighted combinations of the *same* set of feature maps give the class activation maps for the different classes.   ![Feature maps](/images/feature-maps-weighted.png) <small>Class Activation Maps as Weighted Combinations of Feature Maps (Image by the author)</small>  Mathematically, the class score for an output class `c` in the CAM model is given by:  \\begin{align} y^c = \\sum_{k} {w_{k}}^c \\frac{1}{Z}\\sum_{i}\\sum_{j} {A_{ij}}^k \\text{ }\\text{ }(1)\\\\\\\\ A_{ij}^k: \\text{ }pixel\\text{ } at\\text{ } location\\text{ } (i,j)\\text{ } in\\text{ } the\\text{ } k-th\\text{ } feature\\text{ } map\\\\\\\\ Z: total\\text{ }number\\text{ }of\\text{ }pixels\\text{ }in\\text{ }the\\text{ }feature\\text{ }map\\\\\\\\ {w_k}^c: weight\\text{ }of\\text{ }the\\text{ }k-th\\text{ }feature\\text{ }map\\text{ }for\\text{ }class \\text{ }c \\end{align}  ### Advantages of CAM   Even though we need to train `N` linear models to learn the weights, CAM does not require a backward pass through the network again. A backward pass through the layers of the network is more expensive than learning a linear mapping.  CAM uses the inherent localization capability of the convolutional layers, so the activation maps can be generated [without any positional supervision on the location of the target](https://arxiv.org/abs/1412.6856) in the image.  ### Limitations of CAM  Using class activation maps involves the overhead of learning `N` linear models to learn the weights $w_1, w_2,..., w_n$ for each of the `N` classes. Training a ConvNet is a computationally intensive task in itself. This overhead can be a limiting factor when both `n`, the number of filters in the last convolutional layer, and `N`, the number of output classes, are especially large.   The introduction of the global average pooling (GAP) layer after the last convolutional layer imposes a *restriction* on the ConvNet architecture. Though CAM is helpful in explaining the predictions in an image classification task, it cannot be used for computer vision tasks such as visual question answering (VQA). As explained, the GAP layer outputs are scalars that are global averages of the preceding convolutional layer’s feature maps. There is no known performance degradation for image classification. However, this requirement for the GAP layer after the convolutional layers may be too restrictive for tasks like VQA.  ## How Gradient-Weighted Class Activation Maps Work   As mentioned, the key limitation of CAM is the overhead of learning the weights for linear mapping. Gradient-weighted class activation map (Grad-CAM) is a generalization to CAM that overcomes this limitation.  Let’s start by making a simple substitution in the equation for output class score $y^c$ in CAM.  \\begin{align} Let\\text{ }F^k = \\frac{1}{Z}\\sum_{i}\\sum_{j} {A_{ij}}^k \\\\\\\\ Substituting\\text{ }F^k\\text{ }in\\text{ }eqn(1), y^c = \\sum_{k} {w_{k}}^cF^k\\\\\\\\ \\end{align}  Next, let’s compute the derivative of the output class score with respect to the pixels $A_{i,j}$ in the feature map.  \\begin{align} \\frac{\\partial{y^c}}{\\partial{F^k}} = {w_{k}}^c\\\\text{ }(2)\\\\\\\\ \\frac{\\partial{y^c}}{\\partial{F^k}} = \\frac{\\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}}}{\\frac{\\partial{F^k}}{\\partial{{A_{ij}}^k}}}\\\\\\\\ \\frac{\\partial{F^k}}{\\partial{{A_{ij}}^k}} = \\frac{1}{Z}\\\\\\\\ \\frac{\\partial{y^c}}{\\partial{F^k}} = \\frac{\\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}}}{\\frac{1}{Z}}\\\\\\\\ \\frac{\\partial{y^c}}{\\partial{F^k}} = \\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}}.{Z}\\\\text{ }(3)\\\\\\\\ From \\text{ }(2) \\text{ }and\\text{ } (3),\\text{ } we \\text{ }have,\\\\\\\\ \\frac{\\partial{y^c}}{\\partial{F^k}} = \\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}}.{Z} = {w_{k}}^c \\end{align}  Summing the above quantities over all the pixels in the feature map, we have the following:  \\begin{align} \\sum_{i}\\sum_{j}{w_{k}}^c = \\sum_{i}\\sum_{j}\\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}}.{Z}\\\\\\\\ {Z}.{w_{k}}^c = {Z}.\\sum_{i}\\sum_{j}\\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}}\\\\\\\\ {w_{k}}^c = \\sum_{i}\\sum_{j}\\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}}-->gradients! \\end{align}  As seen in the above equation, the weights $w_k$ evaluate to the **gradient** of the output score with respect to the kth feature map. This means there’s no need to retrain `N` linear models to learn the weights!  We’ve summed over all pixel locations (i,j). Adding the normalization factor 1/Z  back in, we get:  \\begin{align} {w_{k}}^c = \\frac{1}{Z}\\sum_{i}\\sum_{j}\\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}} \\end{align}  In essence, Grad-CAM uses the global average of the gradients flowing into the feature maps of the last convolutional layer.  ![How Grad-CAM Works](/images/gradcam-working.png) <small>How Grad-CAM Works  (Image by the author)</small>  To retain only the positive correlations in the final activation map, we apply the ReLU function on the weighted combination of feature maps.   \\begin{align} {L^c_{Grad-CAM}} = ReLU\\left(\\sum_{k}{w_k}^cA^k\\right) \\end{align}  - - -  *ReLU function: f(x) = ReLU(x) = x if x >= 0 and 0 otherwise. The ReLU function filters all the negative inputs and passes the positive inputs as they are.* - - -   ### Grad-CAM: Counterfactual Explanations  Given that the gradients of the output with respect to the feature maps identify salient patches in the image, what do negative gradients signify?  \\begin{align} {w_{k}}^c = \\frac{1}{Z}\\sum_{i}\\sum_{j}-\\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}} \\end{align}  Using negative gradients in the weights will give those patches in the image that *adversarially affect* a particular prediction. For example, in an image containing a cat and a dog, if the target class is cat, then the pixel patch corresponding to the dog class affects prediction.  ![Grad-CAM Counterfactual Explanations](/images/gradcam-counterfactual.png) <small>Grad-CAM Counterfactual Explanations  (Image Source: [arxiv](https://arxiv.org/pdf/1610.02391.pdf))</small>  Therefore, by identifying and removing these patches from the images, we can suppress the adversarial effect on prediction. As a result, the confidence of prediction increases.  ### Guided Grad-CAM: Grad-CAM + Guided Backprop  Even though Grad-CAM provides activation maps with good target localization, it fails to capture certain minute details. [Pixel-space gradient visualization](https://arxiv.org/abs/1412.6806) techniques, which were used in earlier approaches to explainability, can provide more granular information on which pixels have the most influence.   To obtain a detailed activation map, especially to understand misclassifications among similar classes, we can use guided backpropagation in conjunction with Grad-CAM. This approach is called **guided Grad-CAM**.  - - - - *The concept of **guided backpropagation** was introduced in [2]. Given a feedforward neural network, the influence of an input x_j on a hidden layer unit h_i is given by the **gradient** of h_i with respect to x_j. This gradient can be interpreted as follows:*  - *a zero-valued gradient indicates no influence,* - *a positive gradient indicates a significant positive influence, and* - *a negative gradient indicates negative influence.*  *So to understand the fine-grained details, we only backpropagate along the path with positive gradients. Since this approach uses information from higher layers during the backprop, it’s called guided backpropagation.* - - - -   ### Advantages of Grad-CAM  - Given that we have the gradients of the output score with respect to the feature maps, Grad-CAM uses these gradients as the weights of the feature maps. This eliminates the need to retrain `N` models to explain the ConvNet’s prediction.  - As we have the gradients of the task-specific output with respect to the feature maps, Grad-CAM can be used for all computer vision tasks such as visual question answering and image captioning.  ### Limitations of Grad-CAM When there are multiple occurrences of the target class within a single image, the spatial footprint of each of the occurrences is substantially lower. Grad-CAM fails to provide convincing explanations under such *“low spatial footprint”* conditions.   ## Understanding Grad-CAM++  Grad-CAM++ provides better localization when the targets have a low spatial footprint in the images.    Let's start by reviewing the equation for the Grad-CAM weights.  \\begin{align} {w_{k}}^c = \\frac{1}{Z}\\sum_{i}\\sum_{j}\\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}} \\end{align}  From the above equation, we see that Grad-CAM scales all pixel gradients $\\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}}$ by the same factor 1/Z. This means that each pixel gradient has the same significance in generating the final activation map. However, in images where the target has a low spatial footprint, the pixel gradients that actually help the prediction should have greater significance.  To achieve this, Grad-CAM++ proposes the following:  - The pixel gradients that are important for a particular class should be scaled by a larger factor, and - The pixel gradients that do not contribute to a particular class prediction should be scaled by a smaller factor.  Mathematically, this can be expressed as:  \\begin{align} {w_{k}}^c = \\sum_{i}\\sum_{j}\\alpha_{ij}^{kc}ReLU\\left(\\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}}\\right) \\end{align}  Let’s parse what $\\alpha_{ij}^{kc}$ means.   - $\\alpha^{kc}$ denotes the values of α for the k-th feature map corresponding to the output class c. - $\\alpha_{ij}^{kc}$ is the value of α at pixel location (i,j) for the k-th feature map corresponding to the output class c.  Applying the ReLU function on the gradients ensures that only the gradients that have a positive contribution to the class prediction are retained.  [Working out the math](https://arxiv.org/abs/1710.11063) like we did for Grad-CAM, the values of $\\alpha_{ij}$ can be given by the following closed-form expression:  \\begin{align} \\alpha_{ij}^{kc} = \\frac{\\frac{\\partial^2{y^c}}{(\\partial{A_{ij}}^k)^2}}{2.\\frac{\\partial^2{y^c}}{(\\partial{A_{ij}}^k)^2} + \\sum_a\\sum_b{A_{ab}}^k \\frac{\\partial^3{y^c}}{(\\partial{A_{ij}}^k)^3}}\\\\\\\\ \\end{align}  Unlike Grad-CAM weights that use first-order gradients, Grad-CAM++ weights use higher order gradients (second and third-order gradients).  The output activation map is given by:  \\begin{align} {L^c_{Grad-CAM++}} = ReLU\\left(\\sum_{k}{w_k}^cA^k\\right)\\\\\\\\ where, \\text{ }{w_{k}}^c = \\sum_{i}\\sum_{j}\\alpha_{ij}^{kc}ReLU\\left(\\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}}\\right) \\end{align}  Now that you’ve learned how class activation maps and the variants, Grad-CAM and Grad-CAM++, work, let's proceed to generate class activation maps for images.  ## How to Generate Class Activation Maps in PyTorch   The [PyTorch Library for CAM Methods](https://jacobgil.github.io/pytorch-gradcam-book/) by  [Jacob Gildenblat](https://github.com/jacobgil) and contributors on GitHub has ready-to-use PyTorch implementations of Grad-CAM, Grad-CAM++, EigenCAM, and much more. This library `grad-cam` is available as a PyPI package that you can install using `pip`.  ```bash pip install grad-cam ```  📥 [Download the Colab notebook and follow along](https://github.com/balapriyac/CAM-Tutorial).  You can customize [this generic CAM example](https://github.com/jacobgil/pytorch-grad-cam#using-from-code-as-a-library) depending on the computer vision task to which you'd like to add explainability. Let’s start by importing the necessary modules.  ```python from torchvision import models import numpy as np import cv2 import PIL ```  Next, we import the necessary classes from the grad_cam library.   ```python from pytorch_grad_cam import GradCAM,GradCAMPlusPlus from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget from pytorch_grad_cam.utils.image import show_cam_on_image,preprocess_image ```  In this example, we’ll use the [pre-trained ResNet50](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html) model from the [PyTorch Torchvision library](https://pytorch.org/vision/stable/index.html) that contains datasets and pre-trained models. We then define the target class, the layer after which we’d like to generate the activation map. In this example, we’ve used the following [ImageNet classes](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a): `Goldfish`, `Siberian Husky`, and `Mushroom`.  ```python # use the pretrained ResNet50 model model = models.resnet50(pretrained=True) model.eval()  # fix target class label (of the Imagenet class of interest!) # 1: goldfish, 250: Siberian Husky, 947: mushroom  targets = [ClassifierOutputTarget(<target-class-number>)]   # fix the target layer (after which we'd like to generate the CAM) target_layers = [model.layer4] ```  We can instantiate the model, preprocess the image, generate and display the class activation map.  ```python # instantiate the model cam = GradCAM(model=model, target_layers=target_layers) # use GradCamPlusPlus class  # Preprocess input image, get the input image tensor img = np.array(PIL.Image.open('<image-file-path>')) img = cv2.resize(img, (300,300)) img = np.float32(img) / 255 input_tensor = preprocess_image(img)  # generate CAM grayscale_cams = cam(input_tensor=input_tensor, targets=targets) cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)  cam = np.uint8(255*grayscale_cams[0, :]) cam = cv2.merge([cam, cam, cam])  # display the original image & the associated CAM images = np.hstack((np.uint8(255*img), cam_image)) PIL.Image.fromarray(images) ```  We can interpret the class activation map as a heatmap in which the regions in red are the most salient for a particular prediction, and the regions in blue are the least salient.  ![Goldfish Grad-CAM](/images/goldfish-gradcam.jpg) <small>Activation Map for Class Goldfish (ImageNet Class #1)</small>  ![Husky Grad-CAM](/images/husky-gradcam.jpg) <small>Activation Map for Class Siberian Husky (ImageNet Class #250)</small>  So far, the targets were present only once in the entire image. Now, consider the following image with many small mushrooms, each having a very small spatial footprint.  ![Mushroom spacial footprint](/images/multiple--instances-mushroom.jpg)  In this case, the activation map generated using GradCAM++ better identifies all instances of mushroom than the one from GradCAM.  ![Mushroom Grad-CAM](/images/mushrooms-gradcam.png) <small>Grad-CAM Output for Multiple Occurrences of Class Mushroom (ImageNet Class #947)</small>  ![Mushroom Grad-CAM++](/images/mushrooms-gradcam++.png) <small>Grad-CAM++ Output for Multiple Occurrences of Class Mushroom (ImageNet Class #947)</small>  As a next step, you can try generating activation maps for any class or other vision task of your choice.   ## Summing Up  I hope you enjoyed this tutorial on explaining ConvNets with activation maps. Here’s a summary of what you’ve learned.  - Class activation map (CAM) uses the notion of global average pooling (GAP) and learns weights from the output of the GAP layer onto the output classes. The class activation map of any target class is a weighted combination of feature maps. - Grad-CAM uses the gradients available in the network and does not require learning additional models to explain the ConvNet’s predictions. The gradients of the output with respect to the feature maps from the last convolutional layer are used as the weights. - Grad-CAM++ provides better performance under *low spatial footprint*. Instead of scaling all pixels in a feature map by a constant factor, Grad-CAM++ uses *larger* scaling factors for pixel locations that are *salient* for a particular class. These scaling factors are obtained from higher-order gradients in the ConvNet.  If you’d like to delve deeper, consider checking out the resources below. Happy learning!  ## References  [1] Bolei Zhou et al., [Learning Deep Features for Discriminative Localization](https://arxiv.org/abs/1512.04150), 2015  [2] Springenberg and Dosovitskiy et al., [Striving for Simplicity: The All Convolutional Net](https://arxiv.org/abs/1412.6806), ICLR 2015  [3] R Selvaraju et al., [Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localizations](https://arxiv.org/abs/1610.02391), ICCV 2017  [4] A Chattopadhyay et al., [Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks](https://arxiv.org/abs/1710.11063), WACV 2018  [5] B Zhou et al., [Object Detectors Emerge in Deep Scene CNNs](https://arxiv.org/abs/1412.6856), ICLR 2015  [6] [Jacob Gildenblat](https://github.com/jacobgil) and contributors, [PyTorch Library for CAM Methods](https://jacobgil.github.io/pytorch-gradcam-book/), GitHub, 2021 ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e31a"
  },
  "title": "\"Building the Self-Organizing Workspace at Mem\"",
  "headline": "\"Building the Self-Organizing Workspace at Mem\"",
  "weight": "2",
  "name": "Isabella Fulford",
  "position": "Software Engineer, Mem Labs",
  "src": "/images/isabella-fulford-2.png",
  "href": "\"https://www.linkedin.com/in/isabella-fulford/\"",
  "description": "How Mem is harnessing large language models and vector search to unlock the collective intelligence of humanity.",
  "images": "[\"/images/building-mem-x.png\"]",
  "thumbnail": "\"/images/building-mem-x.png\"",
  "content": "![Building Mem X](/images/building-mem-x.png)  *Written by [Isabella Fulford](https://www.linkedin.com/in/isabella-fulford/) for [the Mem.ai blog](https://get.mem.ai/blog/building-mem-x). Reposted with permission.*  Over the course of our lives, we spend a vast amount of time creating and capturing information. Yet we lack the ability to usefully draw from this well of knowledge, as it often becomes lost in folders or information silos.  At [Mem](https://mem.ai), we are building a world in which every person has access to the information they need when they need it. We leverage AI technology to create a self-organizing workspace that automatically organizes all of the information in your work life and proactively surfaces relevant knowledge.  Our long-term mission is to unlock the collective intelligence of humanity. To realize our vision for the future, we are harnessing a technological inflection point: the quality of publicly available foundation models.  Recent breakthroughs in large language models (LLMs) like GPT-3 have drastically changed the field of Natural Language Processing (NLP). Unlike previous generations of NLP that required the construction of separate models for each specific language task, these LLMs are not specialized for any particular task. With a small amount of fine-tuning, we have been able to optimize these pre-trained LLMs for our own use cases.  Most recently, creators of LLMs have also started to open up their black boxes, releasing access to internal layers from within those models. [OpenAI’s embeddings models](https://beta.openai.com/docs/guides/embeddings/what-are-embeddings), for example, allow users to access the embedding layers that encode a text’s meaning, giving more insight into the fundamental building blocks of their NLP AI than the direct GPT-3 output alone can provide. [Embeddings](/learn/vector-embeddings/) are high-dimensional vectors that encode different features of text documents, including meaning, structure, content, theme and topic. Texts with similar meanings will have similar vector representations, and by comparing embeddings of different pieces of text, we can measure the similarity between them. Embeddings make natural language tasks such as [semantic search](/learn/nlp/) and clustering of similar documents easy to perform.  The ability to carry out these similarity calculations at query time is critical when building products that rely on embeddings. [Pinecone](/) is a leader in the vector search space, and their vector database allows users to store embeddings and quickly query for similar embeddings based on various similarity measures and filters.  We leverage both OpenAI embeddings models and Pinecone [vector search](/learn/vector-search-basics/) as fundamental pillars of Mem X. These technologies power features such as *similar mems* and *smart results*, among others. *Similar mems* surfaces documents that are semantically similar to the document a user is viewing, allowing users to discover knowledge from across their team, re-discover knowledge they forgot they had, and make new connections between pieces of information they might not have otherwise seen. *Smart results* allows users to ask Mem questions as though it were a person – e.g., “How many people did we add to the Mem X waitlist in March?”.  With *smart results*, Mem understands the semantic meaning of a user's search query and then finds the most relevant results.  OpenAI offers different embeddings models specialized for different functionalities. We use the text similarity and text search models. The similarity embeddings are good at capturing semantic similarity between multiple pieces of text, and the text search embeddings are trained to measure whether long documents are relevant to a short search query.  ![Mem X overview](/images/mem-x-overview.png)  We transform each document into a format that can be embedded, and use OpenAI's embeddings API to create two embeddings for the document, one with a similarity model and the other with a search model. The embeddings are stored in a Pinecone index, along with metadata about the document. We leverage Pinecone's namespaces to create divisions between vectors that are produced by different models. As a user edits a document, we continuously re-compute the embeddings for this document and upsert the new embeddings to the Pinecone index to ensure that our embeddings are always up to date.  ![Smart results Pinecone index](/images/mem-x-pinecone-index.png)  In the case of *smart results*, when a user makes a search, we parse and transform the search query before creating an embedding with one of OpenAI's search query models, and then query the Pinecone index to find the most similar search documents (i.e. documents with the highest cosine similarity score). Pinecone's metadata filtering functionality allows us to query for only those embeddings that represent documents to which the currently signed-in user has access. We then reconcile the search results returned from Pinecone with our non-semantic search service to improve keyword results, and display the documents corresponding to these embeddings.  ![Similar mems Pinecone index](/images/mem-x-similar-mems.png)  In the *similar mems* feature, when a user views a document, we fetch the embedding for the document from the Pinecone index, then query the index for the most similar embeddings according to metadata filters. We re-rank and re-weight these similar embeddings based on our own clustering and length normalization algorithms, and surface the documents that the embeddings most closely correspond to.  Over time, we will be able to automatically organize **all** of the information that exists within an organization, from employee information to customer data, internal documents, research, emails, Slack messages, and more.   **Learn more about Mem:**  - [Mem X Demo Video](https://www.loom.com/share/55d176c8bd70400dac7b4a017dc907d8) - [Introducing Mem X and Teams](https://get.mem.ai/blog/introducing-mem-x-and-teams) ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e31c"
  },
  "content": "categories:   - \"Faiss: The Missing Manual\" toc: >- weight: 3 author:   name: James Briggs   position: Developer Advocate   src: /images/james-briggs.jpeg   href: \"https://www.youtube.com/c/jamesbriggs\" description: The magic, theory, and practice of Locality Sensitive Hashing. #Open Graph images: ['/images/locality-sensitive-hashing-1.jpeg'] ---  **Locality sensitive hashing (LSH)** is a widely popular technique used in *approximate* nearest neighbor (ANN) search. The solution to efficient similarity search is a profitable one — it is at the core of several billion (and even trillion) dollar companies.  Big names like Google, Netflix, Amazon, Spotify, Uber, and countless more rely on similarity search for many of their core functions.  Amazon uses similarity search to compare customers, finding new product recommendations based on the purchasing history of their highest-similarity customers.  Every time you use Google, you perform a similarity search between your query/search term — and Google’s indexed internet.  If Spotify manages to recommend good music, it’s because their similarity search algorithms are successfully matching you to other customers with a similarly good (or not so good) taste in music.  LSH is one of the original techniques for producing high quality search, while maintaining lightning fast search speeds. In this article we will work through the theory behind the algorithm, alongside an easy-to-understand implementation in Python!  You can find a video walkthrough of this article here:  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/e_SBq3s20M8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ## Search Complexity  Imagine a dataset containing millions or even *billions* of samples — how can we efficiently compare all of those samples?  Even on the best hardware, comparing all pairs is out of the question. This produces an at best complexity of O(n²). Even if comparing a single query against the billions of samples, we still return an at best complexity of O(n).  We also need to consider the complexity behind a single similarity calculation — every sample is stored as a vector, often very highly-dimensional vectors — this increases our complexity even further.  How can we avoid this? Is it even possible to perform a search with sub-linear complexity? *Yes, it is!*  The solution is *approximate* search. Rather than comparing every vector (*exhaustive* search) — we can approximate and limit our search scope to only the most relevant vectors.  LSH is one algorithm that provides us with those sub-linear search times. In this article, we will introduce LSH and work through the logic behind the magic.  ---  ## Locality Sensitive Hashing  When we consider the complexity of finding similar pairs of [vectors](/learn/dense-vector-embeddings-nlp), we find that  the number of calculations required to compare everything is unmanageably enormous even with reasonably small datasets.  Let’s consider a vector index. If we were to introduce just one new vector and attempt to find the closest match — we must compare that vector to every other vector in our database. This gives us a *linear time complexity* — which cannot scale to fast search in larger datasets.  The problem is even worse if we wanted to compare all of those vectors against each other — the optimal approach sorting method to achieve this is at best *log-linear time complexity*.  So we need a way to reduce the number of comparisons. Ideally, we want only to compare vectors that we believe to be potential matches — or *candidate pairs*.  Locality sensitive hashing (LSH) allows us to do this.  LSH consists of a variety of different methods. In this article, we’ll be covering the traditional approach — which consists of multiple steps — shingling, MinHashing, and the final banded LSH function.  At its core, the final LSH function allows us to segment and hash the same sample several times. And when we find that a pair of vectors has been hashed to the same value *at least once* , we tag them as *candidate pairs* — that is, *potential* matches.  It is a very similar process to that used in Python dictionaries. We have a key-value pair which we feed into the dictionary. The key is processed through the dictionary hash function and mapped to a specific bucket. We then connect the respective value to this bucket.  ![A typical hash function aims to place different values (no matter how similar) into separate buckets.](/images/locality-sensitive-hashing-2.jpeg)<small>A typical hash function aims to place different values (no matter how similar) into separate buckets.</small>  However, there is a key difference between this type of hash function and that used in LSH. With dictionaries, our goal is to minimize the chances of multiple key-values being mapped to the same bucket — we *minimize collisions*.  LSH is almost the opposite. In LSH, we want to *maximize collisions* — although ideally only for *similar* inputs.  ![An LSH function aims to place similar values into the same buckets.](/images/locality-sensitive-hashing-3.jpeg)<small>An LSH function aims to place similar values into the same buckets.</small>  There is no *single* approach to hashing in LSH. Indeed, they all share the same *‘bucket similar samples through a hash function’* logic , but they can vary a lot beyond this.  The method we have briefly described and will be covering throughout the remainder of this article could be described as the *traditional* approach, using *shingling*, *MinHashing*, and *banding*.  There are several other techniques, such as [Random Projection](/learn/locality-sensitive-hashing-random-projection/) which we cover in another article.  ---  ## Shingling, MinHashing, and LSH  The LSH approach we’re exploring consists of a three-step process. First, we convert text to sparse vectors using *k-shingling (and one-hot encoding)*, then use *minhashing* to create ‘signatures’ — which are passed onto our LSH process to weed out *candidate pairs*.  ![A high-level view of the LSH process we will be working through in this article.](/images/locality-sensitive-hashing-4.jpeg)<small>A high-level view of the LSH process we will be working through in this article.</small>  We will discuss some of the other LSH methods in future articles. But for now, let’s work through the traditional process in more depth.  ### k-Shingling  k-Shingling, or simply shingling — is the process of converting a string of text into a set of ‘shingles’. The process is similar to moving a window of length k down our string of text and taking a picture at each step. We collate all of those pictures to create our *set* of shingles.  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/locality-sensitive-hashing-5.mp4\" type=\"video/mp4\"> </video>  <small class=\"video\">k-Shingling consists of moving through a string and adding **k** characters at a time to a ‘shingle set’.</small>  Shingling also removes duplicate items (hence the word ‘set’). We can create a simple k-shingling function in Python like so:  {{< notebook file=\"shingles\" height=\"full\" >}}  And with this, we have our shingles. Next, we create our sparse vectors. To do this, we first need to union all of our sets to create one big set containing *all* of the shingles across all of our sets — we call this the vocabulary (or vocab).  ![All of our shingled sets are merged to create our **vocab**.](/images/locality-sensitive-hashing-6.jpeg)*All of our shingled sets are merged to create our **vocab**.*  We use this vocab to create our sparse vector representations of each set. All we do is create an empty vector full of zeros and the same length as our vocab — then, we look at which shingles appear in our set.  ![To create our one-hot encoding our single shingle set is matched up to our **vocab** which indicates where in our zero vector we should place **ones** (we use a shingle-to-index dictionary in our code).](/images/locality-sensitive-hashing-7.jpeg)*To create our one-hot encoding our single shingle set is matched up to our **vocab** which indicates where in our zero vector we should place **ones** (we use a shingle-to-index dictionary in our code).*  For every shingle that appears, we identify the position of that shingle in our vocab and set the respective position in our new zero-vector to 1. Some of you may recognize this as *one-hot encoding*.   ### Minhashing  Minhashing is the next step in our process, allowing us to convert our sparse vectors into dense vectors. Now, as a pre-warning — this part of the process can seem confusing initially — but it’s very simple once you get it.  We have our sparse vectors, and what we do is randomly generate one minhash function for every position in our signature (e.g., the dense vector).  So, if we wanted to create a dense vector/signature of 20 numbers — we would use 20 minhash functions.  Now, those MinHash functions are simply a randomized order of numbers — and we count from *1* to the final number (which is len(vocab)). Because the order of these numbers has been randomized, we may find that number *1 *is in position *57* (for example) of our randomized MinHash function.  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/locality-sensitive-hashing-8.mp4\" type=\"video/mp4\"> </video>  <small class=\"video\">Our signature values are created by first taking a randomly permuted count vector (from **1** to **len(vocab)+1**) and finds the minimum number that aligns with a **1** in our sparse vector.</small>  Above, we’re using a smaller vocab containing *six* values so we can easily visualize the process.  We look at our sparse vector and say, “did this shingle at vocab[1] exist in our set?”. If it did — the sparse vector value will be 1 — in this case, it did *not* exist (hence the 0 value). So, we move to number *2*, identify its position (0) and ask the same question. This time, the answer is *yes,* and so our minhash output is **2**.  That’s how we produce one value in our minhash signature. But we need to produce 20 (or more) of these values. So, we assign a different minhash function to each signature position — and repeat the process.  ![Here we using four minhash functions/vectors to create a four-digit signature vector. If you count (from **one**) in each minhash function, and identify the first value that aligns with a **one** in the sparse vector — you will get **2412**.](/images/locality-sensitive-hashing-9.jpeg)*Here we using four minhash functions/vectors to create a four-digit signature vector. If you count (from **one**) in each minhash function, and identify the first value that aligns with a **one** in the sparse vector — you will get **2412**.*  At the end of this, we produce our minhash signature — or dense vector.  Let’s go ahead and write that in code. We have three steps:  **1.** Generate a randomized MinHash vector.**  {{< notebook file=\"hash-examples\" height=\"full\" >}}  **2.** Loop through this randomized MinHash vector (starting at 1), and match the index of each value to the equivalent values in the sparse vector a_1hot. If we find a 1 — that index is our signature value.  {{< notebook file=\"create-signature-val\" height=\"full\" >}}  **3.** Build a signature from multiple iterations of **1** and **2** (we’ll formalize the code from above into a few easier to use functions):  {{< notebook file=\"build-full-signatures\" height=\"full\" >}}  And that is MinHashing — it’s really nothing more complex than that. We’ve taken a sparse vector and compressed it into a more densely packed, 20-number signature.  ### Information Transfer from Sparse to Signature  Is the information truly maintained between our much larger sparse vector and much smaller dense vector? It’s not easy for us to visually identify a pattern in these new dense vectors — but we can calculate the similarity between vectors.  If the information truly has been retained during our downsizing — surely the similarity between vectors will be similar too?  Well, we can test that. We use Jaccard similarity to calculate the similarity between our sentences in *shingle* format — then repeat for the same vectors in signature format:  {{< notebook file=\"check-jaccard-a\" height=\"full\" >}}  We see pretty close similarity scores for both — so it seems that the information is retained. Let’s try again for b and c:  {{< notebook file=\"check-jaccard-bc\" height=\"full\" >}}  Here we find much higher similarity, as we would expect — it looks like the similarity information is maintained between our sparse vectors and signatures! So, we’re now fully prepared to move onto the LSH process.  ---  ## Band and Hash  The final step in identifying similar sentences is the LSH function itself.  We will be taking the banding approach to LSH — which we could describe as the traditional method. It will be taking our signatures, hashing segments of each signature, and looking for hash collisions — as we described earlier in the article.  ![A high-level view of the signature-building process. We take our text, build a shingle set, one-hot encode it using our **vocab**, and process it through our minhashing process.](/images/locality-sensitive-hashing-10.jpeg)<small>A high-level view of the signature-building process. We take our text, build a shingle set, one-hot encode it using our **vocab**, and process it through our minhashing process.</small>  Through this method, we produce these vectors of equal length that contain positive integer values in the range of 1 → len(vocab) — these are the signatures that we typically input into *this* LSH algorithm.  Now, if we were to hash each of these vectors as a whole, we may struggle to build a hashing function that accurately identifies similarity between them — we don’t require that the full vector is equal, only that parts of it are similar.  In most cases, even though parts of two vectors may match perfectly — if the remainder of the vectors are not equal, the function will likely hash them into *separate* buckets.  We don’t want this. We want signatures that share even some similarity to be hashed into the same bucket , thus being identified as candidate pairs.  ### How it Works  The banding method solves this problem by splitting our vectors into sub-parts called *bands* b. Then, rather than processing the full vector through our hash function, we pass each band of our vector through a hash function.  Imagine we split a 100-dimensionality vector into 20 bands. That gives us 20 opportunities to identify matching sub-vectors between our vectors.  ![We split our signature into **b** sub-vectors, each is processed through a hash function (we can use a single hash function, or **b** hash functions) and mapped to a hash bucket.](/images/locality-sensitive-hashing-11.jpeg)<small>We split our signature into **b** sub-vectors, each is processed through a hash function (we can use a single hash function, or **b** hash functions) and mapped to a hash bucket.</small>  We can now add a more flexible condition — given a collision between any two sub-vectors, we consider the respective full vectors as candidate pairs.  ![We split the signatures into subvectors. Each equivalent subvector across all signatures must be processed through the same hash function. However, it is not necessary to use different hash functions for each subvector (we can use just one hash function for them all).](/images/locality-sensitive-hashing-12.jpeg)<small>We split the signatures into subvectors. Each equivalent subvector across all signatures must be processed through the same hash function. However, it is not necessary to use different hash functions for each subvector (we can use just one hash function for them all).</small>  Now, only part of the two vectors must match for us to consider them. But of course, this also increases the number of false positives (samples that we mark as candidate matches where they are not similar). However, we do try to minimize these as far as possible.  We can implement a simple version of this. First, we start by splitting our signature vectors **a**, **b**, and **c**:  {{< notebook file=\"split-vectors\" height=\"full\" >}}  Then we loop through the lists to identify any matches between sub-vectors. If we find *any* matches — we take those vectors as candidate pairs.  {{< notebook file=\"candidates\" height=\"full\" >}}  We find that our two more similar sentences, **b**, and **c **— are identified as candidate pairs. The less similar of the trio, **a** — is not identified as a candidate. This is a good result, but if we want to really test LSH, we will need to work with more data.  ---  ## Testing LSH  What we have built thus far is a very inefficient implementation — if you want to implement LSH, this is certainly not the way to do it. Rather, use a library built for similarity search — like [Faiss](https://www.pinecone.io/learn/faiss/), or a managed solution like Pinecone.  But working through the code like this should — if nothing else — make it clear how LSH works. However, we will now be replicating this for much more data, so we will rewrite what we have so far using Numpy.  The code will function in the same way — and you can find each of the functions (alongside explanations) in [this notebook](https://github.com/pinecone-io/examples/blob/master/locality_sensitive_hashing_traditional/testing_lsh.ipynb).  ### Getting Data  First, we need to get data. There is a great repository [here](https://github.com/brmson/dataset-sts) that contains several datasets built for similarity search testing. We will be extracting a set of sentences from [here](https://github.com/brmson/dataset-sts/blob/master/data/sts/sick2014/SICK_train.txt).  {{< notebook file=\"download\" height=\"full\" >}}  ### Shingles  Once we have our data, we can create our one-hot encodings — this time stored as a NumPy array ([find full code and functions here](https://github.com/pinecone-io/examples/blob/master/locality_sensitive_hashing_traditional/testing_lsh.ipynb)).  {{< notebook file=\"shingle\" height=\"full\" >}}  Now we have our one-hot encodings. The shingles_1hot array contains *4500 *sparse vectors, where each vector is of length *36466*.  ### MinHashing  As before, we will compress our sparse vectors into dense vector *‘signatures’* with minhashing. Again, we will be using our NumPy implementation, which you can find the full code here.  {{< notebook file=\"minhash\" height=\"full\" >}}  We’ve compressed our sparse vectors from a length of *36466* to signatures of length *100*. A big difference, but as we demonstrated earlier, this compression technique retains similarity information very well.  ### LSH  Finally, onto the LSH portion. We will use a Python dictionary here to hash and store our candidate pairs — again. [The full code is here.](https://github.com/pinecone-io/examples/blob/master/locality_sensitive_hashing_traditional/testing_lsh.ipynb)  {{< notebook file=\"lsh\" height=\"full\" >}}  It’s important to note that our lsh.buckets variable actually contains a separate dictionary for each band — we do *not* mix buckets between different bands.  We see in our buckets the vector IDs (row numbers) , so all we need to do to extract our candidate pairs is loop through all buckets and extract pairs.  {{< notebook file=\"check-candidates\" height=\"full\" >}}  After identifying our candidate pairs, we would restrict our similarity calculations to those pairs only — we will find that some will be within our similarity threshold, and others will not.  The objective here is to restrict our scope and reduce search complexity while still maintaining high accuracy in identifying pairs.  We can visualize our performance here by measuring the candidate pair classification (1 or 0) against actual cosine (or Jaccard) [similarity](/learn/semantic-search/).  ![Chart showing the distribution of candidate-pairs (1s) and non-candidates (0s) against the cosine similarity of pair signatures.](/images/locality-sensitive-hashing-13.jpeg)<small>Chart showing the distribution of candidate-pairs (1s) and non-candidates (0s) against the cosine similarity of pair signatures.</small>  Now, this may seem like a strange way to visualize our performance — and you are correct, it is — but we do have a reason.  ### Optimizing the Bands  It is possible to optimize our band value b to shift the similarity threshold of our LSH function. The similarity threshold is the point at which we would like our LSH function to switch from a non-candidate to a candidate pair.  We formalize this probability-similarity relationship as so:  ![Probability (P) of a pair being identified as candidate pairs given a similarity score (s), number of bands (b), and number of rows in each band (r).](/images/locality-sensitive-hashing-14.png)*Probability (P) of a pair being identified as candidate pairs given a similarity score (s), number of bands (b), and number of rows in each band (r).*  Now, if we were to visualize this probability-similarity relationship for our current b and r values we should notice a pattern:  ![Candidate classification (left y-axis) and calculated probability **P** (right y-axis) against similarity (calculated or normalized cosine similarity). This shows that our calculated probability **P **and similarity **s** values indicate the general distribution of candidate/non-candidate pairs. The **b** and **r** values are **20** and **5** respectively.](/images/locality-sensitive-hashing-15.jpeg)<small>Candidate classification (left y-axis) and calculated probability **P** (right y-axis) against similarity (calculated or normalized cosine similarity). This shows that our calculated probability **P **and similarity **s** values indicate the general distribution of candidate/non-candidate pairs. The **b** and **r** values are **20** and **5** respectively.</small>  Although the alignment isn’t perfect, we can see a correlation between the theoretical, calculated probability — and the genuine candidate pair results. Now, we can push the probability of returning candidate-pairs at different similarity scores left or right by modifying b:  ![Calculated probability **P** against similarity **s** for different **b** values. Note that **r** will be **len(signature) / b** (in this case **len(signature) == 100**).](/images/locality-sensitive-hashing-16.jpeg)<small>Calculated probability **P** against similarity **s** for different **b** values. Note that **r** will be **len(signature) / b** (in this case **len(signature) == 100**).</small>  These are our calculated probability values. If we decided that our previous results where b == 20 required too high a similarity to count pairs as candidate pairs — we would attempt to shift the similarity threshold to the left.  Looking at this graph, a b value of 25 looks like it could shift our genuine results just enough. So, let’s visualize our results when using b == 25:  ![Results for real and simulated results when **b == 25** are displayed with blue and magenta. Our previous LSH results (teal) are displayed for comparison. Note that this has created more candidate pairs.](/images/locality-sensitive-hashing-17.jpeg)<small>Results for real and simulated results when **b == 25** are displayed with blue and magenta. Our previous LSH results (teal) are displayed for comparison. Note that this has created more candidate pairs.</small>  Because we are now returning more candidate pairs, this will naturally result in more false positives — where we return ‘candidate pair’ for dissimilar vectors. This is an unavoidable consequence of modifying b, which we can visualize as:  ![Increasing **b** (shifting left) increases  FPs while decreasing FNs.](/images/locality-sensitive-hashing-18.png)<small>Increasing **b** (shifting left) increases  FPs while decreasing FNs.</small>  Great! We’ve built our LSH process from scratch — and even managed to tune our similarity threshold.  That’s everything for this article on the principles of LSH. Not only have we covered LSH, but also shingling and the MinHash function!  In practice, we would most likely want to implement LSH using libraries built specifically for similarity search. We will be covering LSH — specifically the random projection method — in much more detail, alongside its implementation in Faiss.  However, if you’d prefer a quick rundown of some of the key indexes (and their implementations) in similarity search, we cover them all in our [overview of vector indexes](https://www.pinecone.io/learn/vector-indexes/).  {{< newsletter text=\"Subscribe for the latest in similarity search!\" inputText=\"Email address...\" buttonText=\"Submit\">}}  ---  ## Further Resources  * [Jupyter Notebooks](https://github.com/pinecone-io/examples/tree/master/locality_sensitive_hashing_traditional) * J. Ullman et al., [Mining of Massive Datasets](http://mmds.org/#ver30) ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e31e"
  },
  "title": "\"Transformers Are All You Need\"",
  "headline": "\"Transformers Are All You Need\"",
  "weight": "2",
  "name": "Diego Lopez Yse",
  "position": "Data Scientist",
  "src": "/images/diego-lopez-yse.jpeg",
  "href": "\"https://lopezyse.medium.com/\"",
  "description": "A quick tour through the most popular Neural Net architecture.",
  "images": "['/images/transformers-1.png']",
  "content": "**A quick tour through the most popular Neural Net architecture**  Have you ever thought about what happens when you read a book? Unless you have a unique ability, you don’t process and memorize every single word and special character, right? What happens is that we represent events and characters to build our understanding of the story. We do this with a selective memorization that allows us to keep the most relevant pieces of information without needing to accumulate each minor detail.  This is exactly what [Hochreiter and Schmidhuber](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf) were looking for when they created the **Long Short Term Memory** (LSTM) model in 1997. LSTMs are types of [Recurrent Neural Networks](https://www.ibm.com/cloud/learn/recurrent-neural-networks) that emulate a selective memory approach, allowing them to store relevant information about the past in order to optimize a task. This impressive architecture ruled the sequential data landscape for over two decades and drove huge progress towards the way we understand different disciplines today. Unlike human memory, however, LSTMs can struggle when dealing with long-range contexts, as is the case with human language.  This limitation was particularly evident in language research that tried to move from keyword-based to more linguistic approaches in tasks like [information searching](https://www.searchenginewatch.com/2016/04/12/everything-you-need-to-know-about-natural-language-search/), [document classification](https://towardsdatascience.com/going-beyond-keywords-with-nlp-42395f7e7c67), or [question-answering](https://wlv.openrepository.com/handle/2436/254613). The nuances of human language were just too many for the Natural Language Processing discipline to keep up.  Natural Language Processing (NLP) is a field of Artificial Intelligence that gives the machines the ability to read, understand, and derive meaning from human languages. It represents the automatic handling of natural human language like speech or text, and, because, as humans, we excel at understanding our language, we tend to underestimate how hard it is for machines to do it.  Despite this,things have significantly changed in past years, and, although far from new, NLP is living a new age thanks to the invention of **Transformers**.  Transformers represent new architectures of Artificial Neural Networks (ANN) that generalize to many NLP tasks with incredible results.  Transformers have improved the performance of language models in a substantial way, significantly extending the model’s contextual processing. Interested in seeing how they perform? You can test Transformers [here](https://transformer.huggingface.co/) and [here](https://app.inferkit.com/demo).  ## The road to Transformers  Before getting to Transformers, let’s start by understanding what an Artificial Neural Network (ANN) is.  **ANNs are computing systems composed of neurons, where each neuron individually performs only a simple computation.**  The power of an ANN comes from the complexity of the connections these neurons can form. ANNs accept input variables as information, weigh variables as knowledge, and output a prediction. Every ANN works this way.  The concept is far from new, since the first ANN (the Perceptron) was created in 1958. At the beginning, ANNs were built and used to solve basic tasks, but they rapidly evolved, becoming complex mechanisms able to solve challenges in areas like Computer Vision and Natural Language Processing.  ![The Perceptron](/images/transformers-1.png)  <small>The Perceptron is the oldest Artificial Neural Network, created by Frank Rosenblatt in 1958. It has a single neuron and is the simplest form of a neural network. Source: [IBM](https://www.ibm.com/cloud/learn/neural-networks)</small>  ![Deep Neural Network](/images/transformers-2.png)  <small>Artificial Neural Networks are composed of node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network. Source: [IBM](https://www.ibm.com/cloud/learn/neural-networks)</small>  ANNs architectures were expanded and improved in order to fit the complexity of the data we started gathering; Convolutional Neural Networks (CNNs) were designed to process spatial data like images, while Recurrent Neural Networks (RNNs) and Long Short Term Memories (LSTMs) were built to process sequential data like text.  But it wasn’t until just recently that something changed the way we conceived most of our challenges. [Introduced in 2017](https://arxiv.org/abs/1706.03762), Transformers rapidly showed effective results at modelling data with long-range dependencies. Originally thought to solve NLP tasks, the application of Transformers has expanded, reaching incredible accomplishments in many disciplines.  **Healthcare**  Many of the world’s greatest challenges, such as developing treatments for diseases or finding enzymes that break down industrial waste, are fundamentally tied to proteins and the role they play. A protein’s shape is closely linked to its function, and the [ability to predict this structure](https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology) unlocks a greater understanding of what it does and how it works. In a major scientific advance, DeepMind’s [AlphaFold](https://deepmind.com/research/case-studies/alphafold) system has been recognised as a solution to this grand challenge.  <video autoplay loop muted playsinline class=\"responsive\"><source src=\"/images/transformers-1.mp4\" type=\"video/mp4\"></video>  <small class=\"video\">Two examples of protein targets in the free modelling category. AlphaFold predicts highly accurate structures measured against experimental results. Source: [DeepMind](https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology)</small>  **Self-driving cars**  Tesla’s strategy is built around its Artificial Neural Networks. Unlike many self-driving car companies, Tesla does not use lidar, a more expensive sensor that can see the world in 3D. It relies instead on interpreting scenes by using [neural network algorithms to parse input from its cameras and radar](https://www.wired.com/story/why-tesla-designing-chips-train-self-driving-tech/).  <video autoplay loop muted playsinline class=\"responsive\"><source src=\"/images/transformers-2.mp4\" type=\"video/mp4\"></video>  <small class=\"video\">Tesla’s neural networks can consistently detect objects in various visibility conditions. Source: [VentureBeat](https://venturebeat.com/2021/07/03/tesla-ai-chief-explains-why-self-driving-cars-dont-need-lidar/)</small>  **Art generation**  DALL·E parses text prompts and then responds not with words, but in pictures. It has been specifically trained to generate images from text descriptions, using a dataset of text-image pairs. The amazing thing is that DALL·E can do more than just paint a pretty picture from a caption: It can also, in a sense, [answer questions visually](https://thenextweb.com/news/heres-how-openais-magical-dall-e-generates-images-from-text-syndication). DALL·E is often able to solve matrices that involve continuing simple patterns or basic geometric reasoning.  ![DALL-E Output Example](/images/transformers-3.png)  <small>Example of an output from DALL·E when prompted to generate an “armchair in the shape of an avocado”. Source: [OpenAI](https://openai.com/blog/dall-e/)</small>  These examples are impressive, and, although they seem unrelated, they have one common factor: They all use Transformers architectures.   Now let’s see how Transformers work.  ## The anatomy of Transformers  It’s usually helpful to visualize things when trying to understand them. Let’s see what Transformers look like:  ![Transformer Architecture](/images/transformers-4.png)  <small>The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. Source: [HarvardNLP](http://nlp.seas.harvard.edu/2018/04/03/attention.html)</small>  Too much information, right? Let’s start with the basics. In very simple terms, a Transformer’s architecture consists of **encoder and decoder** components. The encoder receives an input (e.g. a sentence to be translated), processes it into a hidden representation, and passes it to the decoder, which returns an output (e.g. the translated sentence).  ![Encoding & Decoding Components](/images/transformers-5.png)  <small>An encoding component, a decoding component, and connections between them. Source: [Jay Alammar](https://jalammar.github.io/)</small>  Going back to the model, we can find the encoder and decoder components as follows:   ![Encoder & Decoder Blocks](/images/transformers-6.png)  <small>The encoder and decoder blocks are actually multiple identical encoders and decoders stacked on top of each other. Both the encoder stack and the decoder stack have the same number of units. Source: [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/)</small>  Take a look at the image above. The encoder block has one layer of a Multi-Head Attention followed by another layer of Feed Forward Neural Network. The decoder, on the other hand, has an extra Masked Multi-Head Attention. What are those “attention” layers about?  Before Transformers, ANN architectures, like RNNs, had severe memory problems. In the case of RNNs, there’s a limited scope they can remember about long-range dependencies (the words they saw a long time ago that are somehow related to the next word). That is, [RNNs put too much emphasis on words being close to one another](https://wiki.pathmind.com/attention-mechanism-memory-network) and too much emphasis on upstream context over downstream context. Reading one word at a time, RNNs need to perform multiple steps to make decisions that depend on words far away from each other, which is incredibly slow.  Self-attention fixes this problem.  Using **self-attention** mechanisms, Transformers can capture the context of a word from distant parts of a sentence, both before and after the appearance of that word, in order to encode valuable information. Sentences are processed as a whole, rather than word by word. This way, Transformer models avoid suffering from long dependency issues and forgetting past information.  ![Encoder & Decoder Blocks](/images/transformers-7.png)  <small>What does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm. As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word. Source: [Jay Alammar](https://jalammar.github.io/)</small>  [Self-attention is computed not once but multiple times in the Transformer’s architecture, in parallel and independently](https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/) (aptly referred to as **Multi-head Attention**).  And what about performance? The sequential nature of [RNNs makes it more difficult to fully take advantage of fast modern computing devices](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) such as TPUs and GPUs, which excel at parallel and non-sequential processing. Since the Transformer architecture lends itself to **parallelization**, we can really boost the speed with which these models can be trained.  <video autoplay loop muted playsinline class=\"responsive\"><source src=\"/images/transformers-3.mp4\" type=\"video/mp4\"></video>  <small class=\"video\">The Transformer starts by generating initial representations, or embeddings, for each word. These are represented by the unfilled circles. Then, using self-attention, it aggregates information from all of the other words, generating a new representation per word informed by the entire context, represented by the filled balls. This step is then repeated multiple times in parallel for all words, successively generating new representations. Source: [Google](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)</small>  Transformers' successful results led to their escalation into massive models trained with absurd amounts of data, capable of performing the most diverse tasks.  ## Big kids on the block  What do you get when you mix Transformers with huge volumes of data? Huge advances happened in the past years after training Transformers with massive data volumes.  **BERT**  BERT, or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers, is one giant model designed by Google. While being conceptually simple, BERT obtains [new state-of-the-art results on eleven NLP tasks](https://www.topbots.com/leading-nlp-language-models-2020/), including question answering, named entity recognition and other tasks related to general language understanding.  Trained on 2.5 billion words, its design allows the model to consider the context from both the left and the right sides of each word. For example, it can understand the semantic meanings of bank in the [following sentences](https://www.ibm.com/blogs/watson/2020/12/how-bert-and-gpt-models-change-the-game-for-nlp/): “Raise your oars when you get to the river bank” and “The bank is sending a new debit card.” To understand this, it uses left-to-right river and right-to-left debit card clues.  **GPT**  Developed by [OpenAI](https://openai.com/blog/gpt-3-apps/), **G**enerative **P**re-trained **T**ransformer (GPT) models require a small amount of input text to generate large volumes of relevant and sophisticated outputs. Unlike BERT, GPT models are unidirectional, and their main advantage is the magnitude of data they were pretrained on: GPT-3, the third-generation GPT model, [was trained on 175 billion parameters](https://openai.com/blog/gpt-3-apps/), about 10 times the size of previous models. This gigantic pretrained model provides users with the ability to fine-tune NLP tasks with very little data to accomplish novel tasks, like creating articles, poetry, stories, news reports and dialogue.  ![GPT Model Dataset](/images/transformers-8.png)  <small>In general, the more parameters a model has, the more data is required to train the model. As per the creators, the OpenAI GPT-3 model has been trained on about 45 TB text data from multiple sources which include Wikipedia and books. Source: [Springboard Blog](https://www.springboard.com/blog/ai-machine-learning/machine-learning-gpt-3-open-ai/)</small>  ## MEGATRON-TURING  You thought GPT-3 was big? A couple of months ago, Microsoft and Nvidia released the Megatron-Turing Natural Language Generation model (MT-NLG), which is more than triple the size of GPT-3 at 530 billion parameters.  As you can imagine, getting to 530 billion parameters required quite a lot of input data and just as much computing power. The algorithm was [trained using an Nvidia supercomputer](https://singularityhub-com.cdn.ampproject.org/c/s/singularityhub.com/2021/10/13/microsofts-massive-new-language-ai-is-triple-the-size-of-openais-gpt-3/amp/) made up of 4,480 GPUs and an [estimated cost](https://www.nextplatform.com/2021/02/11/the-billion-dollar-ai-problem-that-just-keeps-scaling/) of over $85 million.  This massive model is skilled at tasks like completion prediction, reading comprehension, common-sense reasoning, natural language inferences, and word sense disambiguation.  ![NLP Models Size Trend](/images/transformers-9.jpg)  <small>Trend of sizes of state-of-the-art NLP models over time. Source: [Microsoft](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/)</small>  ## What’s next?  Since Google developed Transformers, most contributions in NLP have been more related to implementation volumes rather than to architectural improvements. And there’s a reason for this: Transformers just work.  Transformers are a fascinating architecture to represent a wide variety of tasks, surprisingly versatile and robust enough to ingest incredible amounts of data. These architectures are so adaptable that we’re witnessing an explosion beyond NLP. Just this year, [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929) emerged as an alternative to Convolutional Neural Networks (CNNs), which are currently state-of-the-art models in computer vision. ViT models are [outperforming CNNs](https://viso.ai/deep-learning/vision-transformer-vit/) in terms of computational efficiency and accuracy, achieving highly competitive performance in tasks like image classification, object detection, and semantic image segmentation.  We’re probably only in the middle of the Transformers era, as models keep getting bigger and applied to different disciplines. It’s hard to say for how long, but given this pace, Transformers will keep on building great advancements in Machine Learning for years to come.",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e320"
  },
  "title": "\"Former Snowflake CEO Bob Muglia and Couchbase CEO Bob Wiederhold Bet on Pinecone\"",
  "headline": "\"Former Snowflake CEO Bob Muglia and Couchbase CEO Bob Wiederhold Bet on Pinecone\"",
  "name": "Edo Liberty",
  "position": "Founder and CEO",
  "src": "/images/company-edo.png",
  "href": "https://edoliberty.github.io/",
  "date": "\"2021-12-20\"",
  "# Date": "December 20, 2021",
  "description": "We are ushering in a new era of search.",
  "thumbnail": "\"/images/company-bob.jpeg\"",
  "content": "I’m excited to announce that two former CEOs are joining Pinecone as investors and advisors. Bob Muglia, former CEO of Snowflake, and Bob Wiederhold, former CEO of Couchbase, are working closely with me to help Pinecone become the leader in search infrastructure.  Complex data, including unstructured text documents, call and video transcripts, customer histories, images, and audio, is growing. Companies need to search through this data for their search, personalization, and security applications. Yet they struggle to do so because they are stuck with tooling and infrastructure that was designed for more simple, structured text data.   Our customers and companies like them are increasingly using ML models to transform complex data into vector embeddings. Vector embeddings are representations of complex data that enable searching through that data by similarity and semantic meaning. Existing databases can’t support vector search at scale. While a handful of hyperscalers have already implemented this technology, most companies don't have the time or resources to build and maintain the necessary infrastructure. Pinecone has built a vector database that makes it easy for everyone to leverage this state-of-the-art method of searching through complex data.  ## Bob Muglia  As the former CEO of data warehousing giant, Snowflake, Bob Muglia led the company to five years of unprecedented growth and raised hundreds of millions in funding. Prior to Snowflake, he served as one of four Division Presidents at Microsoft, managing more than 20% of the company's revenue including server products and cloud services.  From Bob Muglia:  \"Complex data such as images and video are a potential goldmine of business insight. People can intuitively understand the contents of an image but until the advent of advanced analytics and machine learning, these contents have been opaque to business systems. That has now changed but this requires a new kind of infrastructure that intersects databases and AI. Pinecone is an innovator and early mover in this space. I believe what they are building is cutting edge today but will become a well-understood standard in this decade.\"  ## Bob Wiederhold  Bob Wiederhold has more than 25 years of high tech experience, having previously served as CEO of Couchbase, a leader in NoSQL databases, and then its Executive Chairman. From 2002 to 2009, Bob was Chairman and CEO of Transitive, the worldwide leader in cross-platform virtualization with over 20M users, where during his tenure the company was acquired by IBM. Until 2001, Bob served as CEO of Tality Corporation, the worldwide leader in electronic design services whose revenues grew to over $200M.  From Bob Wiederhold:  \"Hyperscalers such as Google, Microsoft, Facebook, and Amazon invested years in incorporating vector databases and vector search into their applications. They are already seeing improved business outcomes. The majority of the tech world has either not realized it yet or is struggling to catch up. I'm excited to work with Pinecone on bringing this technology to companies who may lack the time, capital, or bandwidth to build such infrastructures in house.\"  ## Join Us at Pinecone  Bob Wiederhold and Bob Muglia are among the best people in the world to help us scale Pinecone. Their experiences in building great companies map almost exactly to the journey ahead of us. They have already made a huge impact on Pinecone and will undoubtedly continue to do so.   If you want to help us grow Pinecone into the next great innovator of database technology, check out [our open roles](/careers/). We are hiring in several areas including Engineering, Customer Success, Sales, and Product Management. Experience firsthand why the best minds in tech think Pinecone will be the next big thing in data. Come be a part of it.  ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e322"
  },
  "title": "\"Introduction to Facebook AI Similarity Search (Faiss)\"",
  "headline": "\"Introduction to Facebook AI Similarity Search (Faiss)\"",
  "- \"Faiss": "The Missing Manual\"",
  "weight": "1",
  "name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "Learn how Facebook AI Similarity Search changes — search.",
  "images": "['/images/faiss1.png']",
  "content": "<!-- ![Getting Started With FAISS](/images/faiss1.png) -->  Facebook AI Similarity Search (Faiss) is one of the most popular implementations of efficient similarity search, but what is it — and how can we use it?  What is it that makes [Faiss](https://github.com/facebookresearch/faiss) special? How do we make the best use of this incredible tool?  ---  **Note: [Pinecone](/) lets you implement vector search into your applications with just a few API calls, without knowing anything about Faiss. However, you like seeing how things work, so enjoy the guide!**  ---  Fortunately, it’s a brilliantly simple process to get started with. And in this article, we’ll explore some of the options FAISS provides, how they work, and — most importantly — how Faiss can make our search faster.  Check out the video walkthrough here:  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/sKyvsdEv6rk\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ## What is Faiss?  Before we get started with any code, many of you will be asking — what is Faiss?  Faiss is a library — developed by Facebook AI — that enables efficient similarity search.  So, given a set of [vectors](/learn/vector-embeddings/), we can index them using Faiss — then using another vector (the query vector), we search for the most similar vectors within the index.  Now, Faiss not only allows us to build an index and search — but it also speeds up search times to ludicrous performance levels — something we will explore throughout this article.  ## Building Some Vectors  The first thing we need is data, we’ll be concatenating several datasets from this semantic test similarity hub repo. We will download each dataset, and extract the relevant text columns into a single list.  {{< notebook file=\"get-sentence-data\" height=\"full\" >}}  Next, we remove any duplicates, leaving us with 14.5K unique sentences. Finally, we build our dense vector representations of each sentence using the [sentence-BERT](/learn/semantic-search/) library.  {{< notebook file=\"create-embeddings\" height=\"full\" >}}  Now, building these sentence embeddings can take some time — so feel free to download them directly from here (you can use [this script](https://github.com/jamescalam/data/blob/main/sentence_embeddings_15K/download.py) to load them into Python).  ## Plain and Simple  We’ll start simple. First, we need to set up Faiss. Now, if you’re on Linux — you’re in luck — Faiss comes with built-in GPU optimization for any CUDA-enabled Linux machine.  MacOS or Windows? Well, we’re less lucky.  _(Don’t worry, it’s still ludicrously fast)_  So, CUDA-enabled Linux users, type `conda install -c pytorch faiss-gpu`. Everyone else, `conda install -c pytorch faiss-cpu`. If you don’t want to use `conda` there are alternative installation instructions [here](https://github.com/facebookresearch/faiss/blob/master/INSTALL.md).  Once we have Faiss installed we can open Python and build our first, plain and simple index with `IndexFlatL2`.  ## IndexFlatL2  `IndexFlatL2` measures the L2 (or Euclidean) distance between _all_ given points between our query vector, and the vectors loaded into the index. It’s simple, _very_ accurate, but not too fast.  ![L2 distance calculation between a query vector xq and our indexed vectors (shown as y)](/images/faiss2.png)  <small>L2 distance calculation between a query vector <b>xq</b> and our indexed vectors (shown as <b>y</b>)</small>  In Python, we would initialize our `IndexFlatL2` index with our vector dimensionality (`768` — the output size of our sentence embeddings) like so:  {{< notebook file=\"IndexFlatL2-init\" height=\"full\" >}}  Often, we’ll be using indexes that require us to train them before loading in our data. We can check whether an index needs to be trained using the `is_trained` method. `IndexFlatL2` is not an index that requires training, so we should return `False`.  Once ready, we load our embeddings and query like so:  {{< notebook file=\"IndexFlatL2-add\" height=\"full\" >}}  Which returns the top `k` vectors closest to our query vector `xq` as `7460`, `10940`, `3781`, and `5747`. Clearly, these are all great matches — all including either people running with a football or in the _context_ of a football match.  Now, if we’d rather extract the numerical vectors from Faiss, we can do that too.  {{< notebook file=\"reconstruct\" height=\"full\" >}}  ### Speed  Using the `IndexFlatL2` index alone is computationally expensive, it doesn’t scale well.  When using this index, we are performing an _exhaustive_ search — meaning we compare our query vector `xq` to every other vector in our index, in our case that is 14.5K L2-distance calculations for every search.  Imagine the speed of our search for datasets containing 1M, 1B, or even more vectors — and when we include several query vectors?  ![Milliseconds taken to return a result (y-axis) / number of vectors in the index (x-axis) — relying solely on IndexFlatL2 quickly becomes slow](/images/faiss3.png)  <small>Milliseconds taken to return a result (y-axis) / number of vectors in the index (x-axis) — relying solely on IndexFlatL2 quickly becomes slow</small>  Our index quickly becomes too slow to be useful, so we need to do something different.  ## Partitioning The Index  Faiss allows us to add multiple steps that can optimize our search using many different methods. A popular approach is to partition the index into Voronoi cells.  ![We can imagine our vectors as each being contained within a Voronoi cell — when we introduce a new query vector, we first measure its distance between centroids, then restrict our search scope to that centroid’s cell.](/images/faiss4.png)  <small>We can imagine our vectors as each being contained within a Voronoi cell — when we introduce a new query vector, we first measure its distance between centroids, then restrict our search scope to that centroid’s cell.</small>  Using this method, we would take a query vector `xq`, identify the cell it belongs to, and then use our `IndexFlatL2` (or another metric) to search between the query vector and all other vectors belonging to _that specific_ cell.  So, we are reducing the scope of our search, producing an _approximate_ answer, rather than exact (as produced through exhaustive search).  To implement this, we first initialize our index using `IndexFlatL2` — but this time, we are using the L2 index as a quantizer step — which we feed into the partitioning `IndexIVFFlat` index.  {{< notebook file=\"IndexIVFFlat-init\" height=\"full\" >}}  Here we’ve added a new parameter `nlist`. We use `nlist` to specify how many partitions (Voronoi cells) we’d like our index to have.  Now, when we built the previous `IndexFlatL2`-only index, we didn’t need to train the index as no grouping/transformations were required to build the index. Because we added clustering with `IndexIVFFlat`, this is no longer the case.  So, what we do now is train our index on our data — which we must do _before_ adding any data to the index.  {{< notebook file=\"IndexIVFFlat-train\" height=\"full\" >}}  Now that our index is trained, we add our data just as we did before.  Let’s search again using the same indexed sentence embeddings and the same query vector `xq`.  {{< notebook file=\"IndexIVFFlat-search\" height=\"full\" >}}  The search time has clearly decreased, in this case, we don’t find any difference between results returned by our exhaustive search, and this approximate search. But, often this can be the case.  If approximate search with `IndexIVFFlat` returns suboptimal results, we can improve accuracy by increasing the search scope. We do this by increasing the `nprobe` attribute value — which defines how many nearby cells to search.  ![Searching the single closest cell when nprobe == 1 (left), and searching the eight closest cells when nprobe == 8 (right)](/images/faiss5.png)  <small>Searching the single closest cell when <b>nprobe == 1</b> (left), and searching the eight closest cells when <b>nprobe == 8</b> (right)</small>  We can implement this change easily.  {{< notebook file=\"IndexIVFFlat-nprobe\" height=\"full\" >}}  Now, because we’re searching a larger scope by increasing the `nprobe` value, we will see the search speed increase too.  ![Query time / number of vectors for the IVFFlat index with different nprobe values — 1, 5, 10, and 20](/images/faiss6.png)  <small>Query time / number of vectors for the IVFFlat index with different <b>nprobe</b> values — 1, 5, 10, and 20</small>  Although, even with the larger `nprobe` value we still see much faster responses than we returned with our `IndexFlatL2`-only index.  ### Vector Reconstruction  If we go ahead and attempt to use `index.reconstruct(<vector_idx>)` again, we will return a `RuntimeError` as there is no direct mapping between the original vectors and their index position, due to the addition of the IVF step.  So, if we’d like to reconstruct the vectors, we must first create these direct mappings using `index.make_direct_map()`.  {{< notebook file=\"make-direct-map\" height=\"full\" >}}  And from there we are able to reconstruct our vectors just as we did before.  ## Quantization  We have one more key optimization to cover. All of our indexes so far have stored our vectors as full (eg `Flat`) vectors. Now, in very large datasets this can quickly become a problem.  Fortunately, Faiss comes with the ability to compress our vectors using _Product Quantization (PQ)_.  But, what is PQ? Well, we can view it as an additional approximation step with a similar outcome to our use of **IVF**. Where IVF allowed us to approximate by _reducing the scope_ of our search, PQ approximates the _distance/similarity calculation_ instead.  PQ achieves this approximated similarity operation by compressing the vectors themselves, which consists of three steps.  ![Three steps of product quantization](/images/faiss7.png)  <small>Three steps of product quantization</small>  1. We split the original vector into several subvectors. 2. For each set of subvectors, we perform a clustering operation — creating multiple centroids for each sub-vector set. 3. In our vector of sub-vectors, we replace each sub-vector with the ID of it’s nearest set-specific centroid.  To implement all of this, we use the IndexIVF**PQ** index — we’ll also need to `train` the index before adding our embeddings.  {{< notebook file=\"IndexIVFPQ-init\" height=\"full\" >}}  And now we’re ready to begin searching using our new index.  {{< notebook file=\"IndexIVFPQ-search\" height=\"full\" >}}  ### Speed or Accuracy?  Through adding PQ we’ve reduced our IVF search time from ~7.5ms to ~5ms, a small difference on a dataset of this size — but when scaled up this becomes significant quickly.  However, we should also take note of the slightly different results being returned. Beforehand, with our exhaustive L2 search, we were returning `7460`, `10940`, `3781`, and `5747`. Now, we see a slightly different order of results — and two different IDs, `5013` and `5370`.  Both of our speed optimization operations, **IVF** and **PQ**, come at the cost of accuracy. Now, if we print out these results we will still find that each item is relevant:  {{< notebook file=\"IndexIVFPQ-results\" height=\"full\" >}}  So, although we might not get the _perfect_ result, we still get close — and thanks to the approximations, we get a much faster response.  ![Query time / number of vectors for our three indexes](/images/faiss8.png)  <small>Query time / number of vectors for our three indexes</small>  And, as shown in the graph above, the difference in query times become increasingly relevant as our index size increases.  That’s it for this article! We’ve covered the essentials to getting started with building high-performance indexes for search in Faiss.  Clearly, a lot can be done using `IndexFlatL2`, `IndexIVFFlat`, and `IndexIVFPQ` — and each has many parameters that can be fine-tuned to our specific accuracy/speed requirements. And as shown, we can produce some truly impressive results, at lightning-fast speeds very easily thanks to Faiss.  ---  **Want to run Faiss in production? [Pinecone](/) provides vector similarity search that's production-ready, scalable, and fully managed.**",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e324"
  },
  "title": "Hybrid Search and Learning-to-Rank with Metarank",
  "headline": "Hybrid Search and Learning-to-Rank with Metarank",
  "name": "Vsevolod Goloviznin",
  "position": "Co-founder & CEO at Metarank",
  "src": "/images/vsevolod-goloviznin.jpeg",
  "href": "\"https://www.linkedin.com/in/vgoloviznin/\"",
  "description": "How to do hybrid search, recommendation, and learn-to-rank with Elasticsearch, Pinecone, and Metarank.",
  "images": "[\"/images/metarank-1.jpg\"]",
  "content": "As more advanced Large Language Models (LLMs) are released, the dream of an accurate semantic search comes closer to reality. But a classical term search is still hard to beat, even with the largest LLMs. So what if you don't need to choose between two approaches and combine them within a single hybrid multi-retriever system?  In this article, we're going to discuss a case when Elasticsearch, Opensearch, Solr, and [Pinecone](/) are used together to get the best from both words, with the final combined ranking produced with Metarank.  ## A Multiple Retrievers Problem  A typical scenario for the multi-retriever setup is to have a traditional Lucene-based term search and a vector search engine installed side-by-side for the same collection of documents and query both  in parallel. A classical term search will return BM25-scored documents, and a vector search engine will try to produce a more semantically correct set of results.   ![Multi-retriever setup](/images/metarank-1.jpg)  However, when you have multiple sources of documents to merge into a single search page, it may not be obvious how to combine them:  - Different retrievers use non-comparable score types, like BM25 and cosine similarity. - Documents may come from single or multiple sources at the same time. There should be a way to deal with duplicates in the final ranking.  In a beautiful world where BM25 and cosine similarity have the same scale and statistical distribution, a final ranking can be done by just adding both scores together.  ### BM25 Score Distribution  Lucene-based classical search engines like Elastic, SOLR, and OpenSearch use BM25 as a scoring function to rank multiple found documents for a single search result listing. The BM25 formula depends on the lengths of query, document, and term frequencies, and its values are usually distributed within the low double-digits range.  A practical example of BM25 values distribution is the [MSRD dataset](https://github.com/metarank/msrd): 28320 unique movie search queries over a database of 10k TMDB movie descriptions. In the diagram below, you can see how BM25 scores are distributed across different positions within search results:  ![BM25 values distribution, MSRD dataset](/images/metarank-2.png)  From another perspective, BM25 scores of items in the first position have a non-even distribution, as you can see from the graph below:  ![BM25 values distribution on position=0](/images/metarank-3.png)  So in a real-world situation, the BM25 scores you may get from an Elasticsearch are unbound and non-evenly distributed, which is perfectly fine until you try to merge it with another non-even distribution.  ### Cosine Similarity Value Distribution  Does text embedding similarity from a vector search also have a non-even distribution? Let's take the same MSRD dataset of queries and movies, and push it through an **all-MiniLM-L6-v2** LLM with [SBERT](https://www.sbert.net/) to compute text embeddings. These embeddings can be later used inside a vector search engine like Pinecone. Still, to simplify things, we're only going to perform an in-memory k-NN search between queries and movie documents and compute the same cosine similarity distribution for top-N documents.  ![Cosine distance distribution per position, MSRD dataset](/images/metarank-4.png)  As you can see from the graphs above, the cosine similarity distribution between query-item embeddings is also not that even and differs quite a lot from the BM25 one:  - Thick-tailed, - Has finite upper and lower bounds, - Much smaller peak on position-0.  So, what should we do if there’s no simple way to combine multiple rankings in a statistically correct way?  ## Learning-to-Rank for Multiple Retrievers  From another perspective, BM25 scores and cosine similarity can be treated as input ranking features for a ranking-focused ML model like [LambdaMART](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf) and implicit customer feedback as labels.   If a visitor clicks on a document, it’s a positive signal; if ignored, it is negative. So we can ask this ML model to figure out the best way to merge both document sets into a single ranking to maximize a standard search relevance metric like NDCG.  ![BM25 score / cosine similarity](/images/metarank-5.jpg)  But how does the underlying ML model handle holes in the data when a document comes only from a single source and, for example, has only a BM25 score but not the cosine similarity? There are multiple approaches to solving this problem:  - Replace all null values with an average or median value across the dataset/ranking. - Use a ranking algorithm that can handle missing values natively, like decision trees - which is a core building block of the [LambdaMART](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf) Learn-to-Rank algorithm.  All major LambdaMART algorithms implementations like [XGBoost](https://xgboost.readthedocs.io/en/stable/faq.html#how-to-deal-with-missing-values), [LightGBM](https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html#missing-value-handle), and [CatBoost](https://catboost.ai/en/docs/concepts/faq#preprocessing-of-missing-values) all handle missing values in a very similar fashion. A null is a separate, distinct feature value, which can also be part of a split while building yet another decision tree. In other words, each feature split has a default go-to route for a case when the value is missing.  This approach results in much more stable results for cases when the feature value is sparse, so the majority of its values are missing.  ### Metarank as a Secondary Re-ranker  Implementing an in-house solution for a LambdaMART-style multi-retriever problem will require you to build a couple of data pipelines:  - Collecting visitor feedback to log all clicks happening in search results. - Query logging: we should also collect both search queries and all the documents from all retrievers. - Feature logging: we should persist BM25 and cosine similarity scores for all query-document combinations to use it later during ML model training.  It takes time to build such a system in-house, and such an adventure is not always possible without a team with proper expertise in ranking optimization. But instead of doing everything from scratch, we suggest using existing open-source tools as it may save us quite some time in the whole journey.  [Metarank](https://metarank.ai) is an open-source project focused on solving common ranking problems. It is flexible enough to be used as a secondary re-ranker for the multi-retriever use case we’re discussing.  ![Re-ranking](/images/metarank-6.jpg)   A recent release of Pinecone also supports [traditional term search natively](/learn/hybrid-search-intro/), so in some cases you don’t need to configure two separate search engines and just use two different retrieval methods from only a single Pinecone instance.  A typical Metarank integration flow consists of three main processes:  - Metarank receives feedback events with visitor behavior, like clicks and search impressions. Historical feedback events are used for ML model training and real-time events for online model inference and re-ranking. - A backend application receives a search request from a visitor and forwards it to Elasticsearch and Pinecone. Both search result sets are concatenated together without specific ordering and then sent to Metarank for a final ranking. - Metarank runs the LambdaMART model over the candidate document set and produces the final ranking, which is later presented to the visitor.  ![Metarank integration flow](/images/metarank-7.jpg)  ### Historical Click-through Events  Metarank has a serious limitation (which is not a limitation per se, but more like a functional requirement) that you need to have a history of behavioral data of your visitors to start working on ranking optimization. But like with a chicken-and-egg problem, you need to implement some basic initial unified ranking over multiple retrievers to collect visitor clicks and impressions.  A common approach to this task is **interleaving**: both result sets are sorted by relevance score and then merged into a single ranking one by one, like a zip line.  ![Interleaving](/images/metarank-8.jpg)  The interleaving process is a great baseline approach: simple enough to implement and hard enough to beat. It can also be extended to support multiple sampling probabilities so that a document may be taken from one of the result sets more frequently than from another.  With a baseline merging strategy, we can start collecting visitor click-through data. Metarank expects to receive three types of events:  1. Metadata events about users and items. As we’re not using any user/item-specific information in the ranking process, we can skip emitting these. 2. Ranking events: what was presented to a visitor.  3. Interaction events: how the visitor reacted to the ranking.  An example ranking event is a JSON object with query information and per-document scores taken from primary search requests. All per-item fields are optional, so in a case when a document was retrieved only by Pinecone and has no BM25 score, skip the field:  ![Ranking event](/images/metarank-9.png)  When a visitor makes a click on a document after seeing the ranking, we have to emit an “interaction” event. It describes what was clicked and within which context:  ![Interaction event](/images/metarank-10.png)  Metarank can pull historical ranking and interaction events from multiple sources (S3 files, Apache Kafka, Apache Pulsar, AWS Kinesis, and GCP Pubsub are supported), so you’re free to choose the best method to collect and store them. There is also an option to use [Snowplow Analytics telemetry collector](https://docs.metarank.ai/reference/integrations-overview/snowplow) instead of setting up your analytical collection pipeline.  ### Re-ranking Setup  With click-through event collection in place, we can create a Metarank configuration. The config file consists of four main parts: state, click-through, feature, and model sub-sections.  **State configuration**. The place where all ranking state about users and items should be stored. Ironically, in our case, all the ranking features are present directly in the re-ranking request. There is no user/item state apart from the ML model itself.  In this guide, we’re going to use an in-memory store, but it’s only useful for demonstrational and development purposes. In a production environment, you should prefer Redis.  **Click-through store configuration**. Metarank persists a complete click-chain when a visitor clicks on a document. It includes a list of items presented to the visitor, performed interactions, and ranking features used to build the original ranking event. Such events are quite large, and instead of saving all of them in Redis, there are other more cost-effective alternatives.  The click-through store section is optional and by default, uses the main state store to persist click-throughs, so for the sake of simplicity, we’ll omit this.  **Feature extractor configuration**. How to map semi-structured incoming metadata/ranking/interaction events to the actual ML ranking features.  In our use case, we have only two ranking features, using BM25 and cosine scores supplied by the backend app directly in the ranking request:  ![Ranking features](/images/metarank-11.png)  **Model configuration**. ML model options and used features. Metarank can serve multiple models at once, but in this guide we’ll define only a single “xgboost” model:  ![xgboost model](/images/metarank-12.png)  ### Import and Model Training  With a configuration file in place, we can finally invoke Metarank to process the historical data and build the ML ranking model. Metarank can be run as a JVM application with the **java -jar** command line or as a docker container. We will use the latter in this guide, as it’s more common (but a bit more complicated due to a volume setup).  Metarank has a set of separate sub-commands for data import, model training, and serving (todo: links), but there is a shortcut to run all of them sequentially together, the **standalone mode**. It’s also not very well suited for production but is extremely useful in simple demos and prototype projects.  `docker run -i -t -p 8080:8080 -v <YOUR DIR WITH CONFIG>:/opt/metarank metarank/metarank:latest standalone --config /opt/metarank/config.yml --data /opt/metarank/events.jsonl.gz`  Metarank in the standalone mode will import the historical data, train the ranking ML model and switch to the inference mode to serve real-time re-ranking requests.  ### Sending Requests  When the visitor-facing backend app receives a search request, the request needs to be passed through to the underlying retrievers. So, for example, we can translate a “terminator” query to the following Elasticsearch request:  ![Elasticseach request](/images/metarank-13.png)  The request must first be converted into a vector representation for a semantic search using your favorite text embedding method. For example, the term “terminator” embedding for a MiniLM model looks like this 384-dimensional vector:  ![Vector representation](/images/metarank-14.png)  With this vector, we can then query a Pinecone vector search engine for semantically close documents:  ![Pinecone vector search query](/images/metarank-15.png)  Elasticsearch and Pinecone search responses share some of the found documents. We concatenate these responses and send them to Metarank for a final ranking:  ![Ranking event](/images/metarank-9.png)  And finally Metarank will respond with the final ranking:  ![Final ranking](/images/metarank-16.png)  Big success!  ## Next steps  In this article, we attempted to highlight a typical re-ranking setup with Metarank, adapted for a hybrid search with multiple diverse retrievers.   But in the multi-retriever example here, we only used two specific ranking features: BM25 and cosine similarity scores supplied by search engines. Metarank can also do much more, as it can also compute complex behavioral features like:  - Rates: CTR, conversion rate. - User: User-Agent, GeoIP and Referer - Profile: interaction history - Item: price and category.  These ranking features can improve the relevance of the final ranking but require a bit more complicated infrastructure setup: the re-ranking process may become stateful (e.g., the next response depends on the previous one).  ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e326"
  },
  "title": "\"Testing p2 Pods, Vertical Scaling, and Collections\"",
  "headline": "\"Testing p2 Pods, Vertical Scaling, and Collections\"",
  "weight": "10",
  "- name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "A look at the newest features coming to Pinecone",
  "images": "['https://www.pinecone.io/images/10x-faster-launch-thumb.png']",
  "content": "The world of machine learning is powered by vectors. Not just any vectors, but *dense* vectors capable of representing human meaning in numeric form. These meaningful vectors are quickly replacing more traditional forms of data as the digital world becomes more ML-powered and more people-centric.  We expect intuitive natural language search, intelligent recommendations, and much more by *default*. To achieve this, we need dense vectors, but we also need a database for these vectors.  That is where vector databases like Pinecone come in. The vector database enables scalable, super fast, and accurate retrieval of dense vectors. Already with Pinecone, we see customers searching through billions of vectors and returning results with sub-second latency.  Today, vector search just became up to 10x faster, easier to set up, and vertically scalable. In this article, we will show you how you can get started with the latest features in Pinecone, covering:  * Vertical scaling for p1 and s1 pods.  * Phase one of collections, enabling static snapshots of indexes.\\*  * The latest graph-based p2 pod with  up to 10x faster query times.  Although we won't talk about it here, there are also three more upgrades to note:  * p1 and s1 pods now have ~50% lower latency and ~50% more throughput per replica.  * s1 pods are now available on the free Standard plan, meaning you get 5x greater capacity.  * Updated [pricing](https://www.pinecone.io/pricing/) as of September 1st for new customers.  Without any further ado, let's explore the latest features.  *\\*Future updates to collections will allow import/export between S3 and GCS blob storage, write streaming, and bulk data upload directly to collections.*  ---  ## Vertical Scaling on p1 and s1  [Pods](https://www.pinecone.io/docs/manage-indexes/#pods-and-pod-types) are the hardware components that all of our vectors are stored in. Naturally, they have limits. A p1 pod is expected to hold ~1M 768-dimensional vectors.  ---  *The free Standard tier comes with access to one p1 pod, and as of today, your free capacity can now be increased 5x using the newly included s1 pod.*  ---  In the past, we had to know ahead of time how many pods we needed for our vector database. Unfortunately, this isn't always realistic. Our data needs can change over time, and we often find ourselves outgrowing our initial pod confines or overprovisioning and wasting resources.  As a result we would need  to create a new index from scratch, which isn't fun - especially when you have millions of vectors.  Fortunately, we now have *vertical scaling*. Every index using p1 or s1 pods can be scaled in multiples of two up to *eight-times* their original size with *zero downtime*. Let's see how.  {{< notebook file=\"aug-22-init\" height=\"full\" >}}  Starting with a very full index (see `'index_fullness'`), we need to increase the index size to add more vectors and maintain reasonable latency. We use the new `pinecone.index_config` method to do this.  {{< notebook file=\"aug-22-vertical-scale\" height=\"full\" >}}  By default when creating an index with `pod_type` as `p1` or `s1`, we are actually creating a `p1.x1` or `s1.x1` pod. From either of those, we can scale up to *eight times*. In this case, we scaled by `x2`, doubling our capacity.  ## Collections  Another major feature of this release is *collections*. In the past, after we created an index, we could only reuse those vectors by keeping a local copy or iteratively retrieving them all. Neither option is ideal. Collections are the solution to this. These are essentially static indexes that we can think of as the *\"source of truth\"* for our vector data.  We can create a collection using an existing index, like the `oscar-minilm` index we just scaled.  {{< notebook file=\"aug-22-collections\" height=\"full\" >}}    The syntax for creating and describing collections mirrors that of the same operations for indexes. We create a new collection with `pinecone.create_collection(\"collection_name\", \"index_name\")`. To view collection information we describe it with `pinecone.describe_collection(\"collection_name\")`.  We will be able to see the existence of the collection immediately. However, the collection will take some time to fully initialize and be ready for use elsewhere. We can see the collection status after describing it via the `status` value.  Once the collection status switches to `\"Ready\"` we can use it to create new indexes. All we need is:  {{< notebook file=\"aug-22-index-from-collection\" height=\"full\" >}}  Here we checked the collection status for `\"Ready\"`. Then, using the same `pinecone.create_index` method we usually use, we initialized a p2 pod index and specified `source_collection` to build it from our `oscar-minilm-collection` collection. The creation time is not instant. In this case, it took 23 minutes, a typical time for a collection of this size with the p2 pod type. p1 and s1 index creation is faster (~5 minutes).  ---  ## p2 Pods  We've already seen some of the p2 pod when we initialized a new index from our collection. p2 pods are a new index type that enables up to 10x faster search speeds by utilizing a graph-based index. There are both pros and cons, **Q**ueries **P**er **S**econd (QPS) is faster, but the vector ingestion rate is much slower.  |                                      | s1            | p1            | p2           | | ------------------------------------ | ------------- | ------------- | ------------ | | Capacity (768-d vectors)             | ~5M           | ~1M           | ~1M          | | Query latency at full capacity (p95) | <200ms        | <50ms         | <10ms        | | QPS at full capacity / replica       | ~5 QPS        | ~20 QPS       | ~200 QPS     | | Ingestion rate / pod                 | 10k vectors/s | 10k vectors/s | 50 vectors/s |  The decision between p1, s1, and p2 relies on your application priorities. p2 is ideal for minimal latency, high-throughput, and indexes with relatively low update rates.  {{< notebook file=\"aug-22-query\" height=\"full\" >}}  If we test our two indexes, the p1 index and the p2 index, with a random query, p2 cuts latency from ~35ms to just ~15ms (including network latency).  ---  That's a quick rundown of the latest features in Pinecone. All of these are currently in public preview and are not yet covered by Pinecone's standard SLAs. Therefore we do not recommend them for production use just yet.  These are the three key features, but there are other changes too. Both p1 and s1 pods now (on average) have 50% lower latency and 50% higher throughput. s1 pods have been added to the free Standard plan, meaning standard users can store and query up to 5M 768-dimensional vectors *for free*.  With all of these new features, there's plenty to be excited about. As we all move towards an increasingly vector-centric future, there's no better time to [get started with vector search](https://app.pinecone.io/) than today.  Learn more about these new features in our [announcement](https://pinecone.io/learn/faster-easier-scalable/).  ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e328"
  },
  "title": "\"Composite Indexes and the Faiss Index Factory\"",
  "headline": "\"Facebook AI and the Index Factory\"",
  "- \"Faiss": "The Missing Manual\"",
  "weight": "7",
  "name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "Complete guide to composite search indexes and Faiss Index Factory.",
  "images": "['/images/composite-indexes-1.jpg']",
  "content": "![Facebook AI and the Index Factory](/images/composite-indexes-1.jpg)  In the world of [vector search](/learn/what-is-similarity-search/), there are many indexing methods and vector processing techniques that allow us to prioritize between recall, latency, and memory usage.  Using specific methods such as IVF, [PQ](/learn/product-quantization/), or [HNSW](/learn/hnsw/), we can often return good results. But for *best performance* we will usually want to use *composite indexes*.  ---  *Note: [Pinecone](/) lets you build scalable, high-performance vector search into your applications without knowing anything about composite indexes. However, we know you like seeing how things work, so enjoy learning about composite indexes and the Faiss Index Factory!*  ---  We can view a composite index as a step-by-step process of vector transformations and one or more indexing methods. Allowing us to place multiple indexes and/or processing steps together to create our 'ideal' index.  For example, we can use an inverted file (IVF) index to reduce the scope of our search (increasing search speed), and then add a compression technique such as [product quantization (PQ)](/learn/product-quantization/) to keep larger indexes within a reasonable size limit.  Where there is the ability to customize indexes, there is the risk of producing indexes with unnecessarily poor recall, latency, or memory usage.  We must know how composite indexes work if we want to build robust and high-performance vector similarity search applications. It is essential to understand where different indexes or vector transformations can be used — and when they are not needed.  In this article, we will learn how to build high-performance composite indexes using [Facebook AI Similarity Search (Faiss)](/learn/faiss/) — a powerful library used by many for building fast and accurate vector similarity search indexes. We will also introduce the Faiss `index_factory` which allows us to build composite indexes with clearer, more elegant code.  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/GEhmmcx1lvM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ## What are Composite Indexes  Composite indexes are akin to *lego blocks*; we place one on top of another. We will find that most blocks fit together — but different combinations can produce anything from an artistic masterpiece to an unrecognizable mess.  The same applies to Faiss. Most components *can* be placed together — but that does not mean they *should* be placed together.  A composite index is built from any combination of:  * **Vector transform** — a pre-processing step applied to vectors before indexing (PCA, OPQ).  * **Coarse quantizer** — *rough* organization of vectors to sub-domains (for restricting search scope, includes IVF, IMI, and HNSW). * **Fine quantizer** — a *finer* compression of vectors into smaller domains (for compressing index size, such as PQ). * **Refinement** — a final step at search-time which re-orders results using distance calculations on the original flat vectors. Alternatively, another index (non-flat) index can be used.  Note that coarse quantization refers to the 'clustering' of vectors (such as inverted indexing with IVF). By using coarse quantization, we enable *non-exhaustive* search by limiting the search scope.  Fine quantization describes the compression of vectors into *codes* (as with PQ) <sup>\\[1\\]\\[2\\]\\[3\\]</sup>. The purpose of this is to reduce the memory usage of the index.  ### Index Components  We can build a composite index using the following components:  | Vector transform | Coarse quantizer | Fine quantizer | Refinement | | ------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | -------------------- | | `PCA`, `OPQ`, `RR`, `L2norm`, `ITQ`, `Pad` | `IVF,Flat`, `IMI`, `IVF,HNSW`, `IVF,PQ`, `IVF,RCQ`, `HNSW,Flat`, `HNSW,SQ`, `HNSW,PQ` | *`Flat*`*, `PQ`, `SQ`, *`Residual*`*, `RQ`, `LSQ`, `ZnLattice`, `LSH` | `RFlat`, *`Refine*`* |  For example, we could build an index where we:  * Transform incoming vectors using `OPQ`. * Perform coarse quantization of vectors by storing them in an inverted file list `IVF`, enabling non-exhaustive search. * Compress vectors, reducing memory usage with `PQ` within each IVF cell *(the vectors are quantized, but their cell assignment does not change)*. * After the search, re-order results based on their original flat vectors `RFlat`.  When building these indexes, it can get messy to use a list of the different Faiss classes — so it is often clearer to build our indexes using the Faiss `index_factory`.  ![Example composite index](/images/composite-indexes-8.jpg) <small>We can merge IVF and PQ indexes to store quantized PQ vectors in an IVF structure.</small>  ## Faiss Index Factory  The Faiss `index_factory` function allows us to build composite indexes using little more than a string. It allows us to switch:  <p>```python quantizer = faiss.IndexFlatL2(128) index = faiss.IndexIVFFlat(quantizer, 128, 256) ```</p>  For this:  <p>```python index_f = faiss.index_factory(128, \"IVF256,Flat\") ```</p>  *We haven't specified the L2 distance in our `index_factory` example because the `index_factory` uses L2 by default. If we'd like to use `IndexFlatIP` we add `faiss.METRIC_INNER_PRODUCT` to our `index_factory` parameters.*  We can confirm that both methods produce the same composite index by comparing their performance. First, do they return the same nearest neighbors?  <p>{{< notebook file=\"index-factory-nn-check\" height=\"full\" >}}</p>   Identical results, and how do they compare for search speed and memory usage?  <p>{{< notebook file=\"speed-and-memory\" height=\"full\" >}}</p>  The `get_memory` function returns an exact match for memory usage. Search speeds are incredibly close, with the `index_factory` version 5µs faster — a negligible difference.  *We calculate recall as the percentage of matches from the top-`k` between a flat L2 index and the tested index.*  *The more commonly used metric in literature is recall@k; this is **not** the recall calculated here. Recall@k is the percentage of queries that returned its nearest neighbor in the top `k` returned records.*  *If we returned the ground-truth nearest neighbor 50% of the time when using a `k` value of `100`, we would say the recall@100 performance is 0.5.*  ### Why Use the Index Factory  Judging from our tests, we can be confident that these two index-building methods are nothing more than separate paths to the same destination.  With that in mind — why should we care to learn how we use `index_factory`? First, it can depend on personal preference. If you prefer the class-based index building approach, stick with it.  However, through using the `index_factory` we can greatly improve the elegance and clarity of our code. We will see that five lines of complicated code can be represented in a single — more readable — line of code when using the `index_factory`.  Let's put together a composite index where we pre-process vectors with OPQ, cluster with IVF, quantize using PQ, then re-order with a flat index.  <p>```python d = xb.shape[1] m = 32 nbits = 8 nlist = 256  # we initialize our OPQ and coarse+fine quantizer steps separately opq = faiss.OPQMatrix(d, m) # d now refers to shape of rotated vectors from OPQ (which are equal) vecs = faiss.IndexFlatL2(d) sub_index = faiss.IndexIVFPQ(vecs, d, nlist, m, nbits) # now we merge the preprocessing, coarse, and fine quantization steps index = faiss.IndexPreTransform(opq, sub_index) # we will add all of the previous steps to our final refinement step index = faiss.IndexRefineFlat(q) # train the index, and index vectors index.train(xb) index.add(xb) ```</p>  This code demonstrates the complexity that adding several components to our index can create. If we rewrite this using the `index_factory`, we get much simpler code:  <p>```python d = xb.shape[1] # in index string, m==32, nlist==256, nbits is 8 by default  index = faiss.index_factory(d, \"OPQ32,IVF256,PQ32,RFlat\") # train and index vectors index.train(xb) index.add(xb) ```</p>  Both approaches produce the exact same index. The performance for each:  | | Recall | Search Time | Memory Usage | | ----------------------- | ------ | ----------- | ------------ | | Without `index_factory` | 31% | 181µs | 552MB | | *With* `index_factory` | 31% | 174µs | 552MB |  Search time does tend to be slightly faster when using the `index_factory` — but otherwise, there are no performance differences between equivalent indexes built with or without the `index_factory`.  ## Popular Composite Indexes  Now that we know how to quickly build composite indexes using the `index_factory`, let's explore a few popular and high-performance combinations.  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/3Wqh4iUupbM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ### IVFADC  We have covered a modified **IVFADC** index above — the `IVF256,PQ32` portion of our previous examples make up the core of IVFADC. Let's dive into it in a little more detail.  The index was introduced alongside product quantization in 2010 <sup>[4]</sup>. Since then, it has remained one of the most popular indexes — thanks to being an easy-to-use index that produces reasonable recall, fast speeds, and *incredible* memory usage.  IVFADC is ideal when our main priority is to minimize memory usage while maintaining fast search speeds. This comes at the cost of *okay* — but not *good* recall performance.  There are two steps to indexing with IVFADC:  1. Vectors are assigned to different lists (*or* Voronoi cells) in the IVF structure. 2. The vectors are compressed using PQ.  ![Indexing process for IVFADC](/images/composite-indexes-21.jpg) <small>Indexing process for IVFADC, adapted from [4].</small>  After indexing vectors, an **A**symmetric **D**istance **C**omputation (ADC) is performed between query vectors `xq` and our indexed, quantized vectors.  The search is referred to as being *asymmetric* because it compares `xq` — which is not compressed, against compressed PQ vectors (that we previously indexed).  ![Symmetric Distance Computation (SDC)](/images/composite-indexes-9.jpg) <small>With **s**ymmetric **d**istance **c**omputation (SDC, left) we quantize `xq` before comparing it to our previously quantized `xb` vectors. ADC (right) skips the quantization of `xq` and compares it directly to the quantized `xb` vectors.</small>  To implement the index using the `index_factory` we can write:  <p>{{< notebook file=\"ivfadc\" height=\"full\" >}}</p>  With this, we create an IVFADC index with `256` IVF cells; each vector is compressed with PQ using `m` and `nbits` values of `32` and `8`, respectively. PQ uses `nbits == 8` by default so we can also write `\"IVF256,PQ32\"`.  *`m`: number of subvectors that original vectors are split into*  *`nbits`: number of bits used by each subquantizer, we can calculate the number of centroids used by each subquantizer as `2**nbits`*  We can decrease `nbits` to reduce index memory usage or increase to improve recall and search speed. However, the current version of Faiss does restrict `nbits` to `>= 8` for `IVF,PQ`.  It is also possible to increase the `index.nprobe` value to search more IVF cells — by default, this value is `1`.  <p>{{< notebook file=\"nprobe\" height=\"full\" >}}</p>  Here we have our index performance for various `nbits` and `nprobe` values:  | Index | nprobe | Recall | Search Time | Memory | | --------------- | ------ | ------ | ----------- | ------ | | `IVF256,PQ32x4` | `1` | 27% | 329µs | 25MB | | `IVF256,PQ32x4` | `6` | 45% | 975µs | 25MB | | `IVF256,PQ32x8` | `1` | 30% | 136µs | 40MB | | `IVF256,PQ32x8` | `8` | 74% | 729µs | 40MB |  #### Optimized Product Quantization  **IVFADC** and other indexes using PQ can benefit from **O**ptimized **P**roduct **Q**uantization (OPQ).  OPQ works by rotating vectors to flatten the distribution of values across the subvectors used in PQ. This is particularly beneficial for unbalanced vectors with uneven data distributions.  In Faiss, we add OPQ as a pre-processing step. For IVFADC, the OPQ index string looks like `\" OPQ32,IVF256,PQ32\"` where the `32` in `OPQ32` *and* `PQ32` refers to the number of bytes `m` in the PQ generated codes.  *The OPQ matrix in Faiss is **not** the whole rotation and PQ process. It is only the rotation. A PQ step must be included downstream for OPQ to be implemented.*  As before, we will need to `train` the index on initialization.  <p>{{< notebook file=\"opq\" height=\"full\" >}}</p>  The data distribution of the Sift1M dataset is already well balanced, so OPQ gives us only a minor increase in recall performance. With an `nprobe == 1` we have increased recall from 30% -> 31%.  We can increase our `nprobe` value to improve recall (at the cost of speed). However, because we added a pre-processing step to our index, we cannot access `nprobe` directly with `index.nprobe` as this `index` no longer refers to the IVF portion of our index.  Instead, we must *extract* the IVF index before modifying the `nprobe` value — we can do this using the `extract_index_ivf` function.  <p>{{< notebook file=\"extract-index-ivf\" height=\"full\" >}}</p>  With a higher `nprobe` value of `14` — we return a recall of 74%. A similar recall result to PQ alone, alongside an increased search time from 729µs -> 1060µs.  | Index | nprobe | Recall | Speed | Memory | | ------------------- | ------ | ------ | ------ | ------ | | `IVF256,PQ32` | `1` | 30% | 136µs | 40.2MB | | `OPQ32,IVF256,PQ32` | `1` | 31% | 143µs | 40.3MB | | `IVF256,PQ32` | `8` | 74% | 729µs | 40.2MB | | `OPQ32,IVF256,PQ32` | `13` | 74% | 1060µs | 40.3MB |  We will see later in the article that OPQ can be used to improve performance, but as we can see here, that is not *always* the case.  ![Performance of composite indexes](/images/composite-indexes-19.jpg) <small>Search time (top) and recall (bottom) for various `nprobe` values. We have included `\"IVF256,Flat\"` for comparison. The *flat* index has much higher memory usage at 520MB.</small>  OPQ can also be used to reduce the dimensionality of our vectors in this pre-processing step. This dimensionality `D` must be a multiple of `M`, preferably `D == 4M`. To reduce dimensionality to `64`, we could use `\"OPQ16_64,IVF256,PQ16\"`.  ### Multi-D-ADC  Multi-D-ADC refers to **multi-d**imensional indexing, alongside a PQ step which produces an **a**symmetric **d**istance **c**omputation at search time (as we discussed previously) <sup>[5]</sup>.  The **multi-D-ADC** index is based on the inverted multi-index (IMI), an extension of IVF.  IMI can outperform IVF in both recall and search speed but does increase memory usage <sup>[7]</sup>. This makes IMI indexes (such as multi-D-ADC) ideal in cases where IVFADC doesn’t quite reach the recall and speed required, and you can spare more memory usage. The IMI index works in a very similar way to IVF, but Voronoi cells are split across vector dimensions. What this produces is akin to a multi-level Voronoi cell structure.  ![IMI Index](/images/composite-indexes-6.jpg) <small>Voronoi cells split across multiple vector subspaces. Given a query vector `xq`, we would compare each `xq` subvector to its respective subspace cells.</small>  When we add a vector compression to IMI using PQ, we produce the **multi-D-ADC** index. Where ADC refers to the asymmetric distance computation that is made when comparing query vectors to PQ vectors.  Putting all of this together, we can create a multi-D-ADC index using the index factory string `\" IMI2x8,PQ32\"`.  <p>{{< notebook file=\"multi-d-adc\" height=\"full\" >}}</p>  To return a similar recall to our IVFADC equivalent, we increased search time to 1.3ms, which is very slow. However, if we add OPQ to our index, we will return much better results.  <p>{{< notebook file=\"o-multi-d-adc\" height=\"full\" >}}</p>  For a recall of 74%, our OPQ multi-D-ADC index is fastest at an average search time of just 461µs.  | Index | Recall | Search Time | Memory | | ------------------- | ------ | ----------- | ------ | | `IVF256,PQ32` | 74% | 729µs | 40.2MB | | `IMI2x8,PQ32` | 72% | 1350µs | 40.8MB | | `OPQ32,IMI2x8,PQ32` | 74% | 461µs | 40.7MB |  As before, we can fine-tune the index to prioritize recall or speed using `nprobe`.  ![Performance of OPQ composte index](/images/composite-indexes-5.jpg) <small>Search time (top) and recall (bottom) for various `nprobe` values. We have included `\"IMI2x8,Flat\"` for comparison. The *flat* index has much higher memory usage at 520MB.</small>  `\"OPQ32,IMI2x8,PQ32\"` is one of our best indexes in terms of recall and speed at low memory. However, we'll see that we can improve these metrics even further with the following index.  ### HNSW Indexes  IVF with **H**ierarchical **N**avigable **S**mall-**W**orld (HNSW) graphs is our final composite index. This index splits our indexed vectors into cells as per usual with IVF, but this time we will optimize the process using HNSW.  Compared to our previous two indexes, IVF with HNSW produces comparable or better speed and significantly higher recall — at the cost of *much* higher memory usage.  At a high level, HNSW is based on the *small-world graph theory* that all vertices (*nodes*) in a network — no matter how large — can be traversed in a small number of steps.  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/composite-indexes-11.mp4\" type=\"video/mp4\"> </video>  <small class=\"video\">Example of a navigable small-world graph, all nodes within the graph are connected by a small number of edge traversals. Small world graph theory assumes the same to be true even for huge networks with billions of vertices.</small>  In this small world graph, we see both short-range and long-range links. When traversing across long-range links, we move more quickly across the graph.  HNSW takes advantage of this by splitting graph links into multiple layers. At the higher entry layers, we find only long-range links. As we move down the layers, shorter-range links are added.  When searching, we start at these higher layers with long-range links. Meaning our first traversals are across long-range links. As we move down the layers, our search becomes finer as we traverse across more short-range links.  ![HNSW graph layers](/images/composite-indexes-3.jpg) <small>HNSW graphs break the typical graph containing both long-range and short-range links into multiple layers (hierarchies). During the search, we begin at the highest layer, which consists of long-range links. As we move down through each layer, the links become more granular.</small>  This approach should minimize the number of traversals (speeding up search) while still performing a very fine search in the lower layers (maintaining high recall).  That is HNSW, but how can we merge HNSW with IVF?  Using vanilla IVF, we introduce our query vector and compare it to every cell centroid, identifying the nearest centroids for restricting our search scope.  To pair this process with HNSW, we produce an HNSW graph of all of these cell centroids, making the exhaustive centroid search *approximate*.  ![HNSW graph with IVF](/images/composite-indexes-4.jpg) <small>HNSW can be used to quickly find the approximate nearest neighbor using IVF cell centroids.</small>  Previously, we have been using IVF indexes with 256 cell centroids. An exhaustive search of 256 is fast, and there is no reason to use an approximate search with so few centroids.  And because we have so few cells, each cell must contain many vectors - which will still be searched using an exhaustive search. In this case, IVF+HNSW on the cell centroids does not help.  With IVF+HNSW indexes, we need to swap *' few centroids and large cells'* for *' many centroids and small cells'*.  For our 1M index, an `nlist` value of `65536` is recommended <sup>[8]</sup>. However, we should provide *at least* `30*nlist == 1.97M` vectors to `index.train`, which we do not have. So a smaller `nlist` of `16384` or less is more suitable. For this dataset, `nlist == 4096` returned the highest recall (at slower speeds).  Using IVF+HNSW, we quickly identify the approximate nearest cell centroids using HNSW, then restrict our *exhaustive* search to those nearest cells.  The standard IVF+HNSW index can be built with ` \"IVF4096_HNSW32,Flat\"`. Using this, we have:  * `4096` IVF cells. * Cell centroids are stored in an HNSW graph. Each centroid is linked to `32` other centroids. * The vectors themselves have not been changed. They are `Flat` vectors.  <p>{{< notebook file=\"ivf-hnsw\" height=\"full\" >}}</p>  With this index, we can produce incredible performance ranging from 25% -> 100% recall at search times of 58.9µs -> 916µs.  ![Performance of HNSW IVF composite index](/images/composite-indexes-20.jpg) <small>Search time (top) and recall (bottom) for various `nprobe` values. At the cost of longer search times, we can increase recall by decreasing `nlist`.</small>  However, the IVF+HNSW index is not without its flaws. Although we have incredible recall and fast search speeds, the memory usage of this index is *huge*. Our 1M 128-dimensional vectors produce an index size of 523MB+.  As we have done before, we can reduce this using PQ and OPQ, but this will reduce recall and increase search times.  | Index | Recall | Search Time | Memory | | ------------------------------- | ------ | ----------- | ------ | | `IVF4096_HNSW,Flat` | 90% | 550µs | 523MB | | `IVF4096_HNSW,PQ32` (PQ) | 69% | 550µs | 43MB | | `OPQ32,IVF4096_HNSW,PQ32` (OPQ) | 74% | 364µs | 43MB |  If a lower recall is acceptable for minimizing search time and memory usage, the IVF+HNSW index with OPQ is ideal. On the other hand, IVF+HNSW with PQ offers no benefit over our previous *IVFADC* and *Multi-D-ADC* indexes.  | Name | Index String | Recall | Search Time | Memory | | ----------- | ------------------- | ------ | ----------- | ------ | | IVFADC | `IVF256,PQ32` | 74% | 729µs | 40MB | | Multi-D-ADC | `OPQ32,IMI2x8,PQ32` | 74% | 461µs | 41MB |  ---  That's it for this article! We introduced composite indexes and how to build them using the Faiss `index_factory`. We explored several of the most popular composite indexes, including:  * IVFADC * Multi-D-ADC * IVF-HNSW  By indexing and searching the Sift1M dataset, we learned how to modify each index's parameters to prioritize recall, speed, and memory usage.  With what we have covered here, you will be able to design and test a variety of composite indexes and better decide on an index structure that best suits your needs.  {{< newsletter text=\"Subscribe for the latest in Faiss and similarity search!\" inputText=\"Email address...\" buttonText=\"Submit\">}}  ## References  [1] Y.Chen, et al., [Approximate Nearest Neighbor Search by Residual Vector Quantization](https://www.researchgate.net/publication/51873001_Approximate_Nearest_Neighbor_Search_by_Residual_Vector_Quantization) (2010), *Sensors*  [2] Y. Matsui, et al., [A Survey of Product Quantization](https://www.jstage.jst.go.jp/article/mta/6/1/6_2/_pdf) (2018), *ITE Trans. on MTA*  [3] T. Ge, et. al., [Optimized Product Quantization](http://kaiminghe.com/publications/pami13opq.pdf) (2014), *TPAMI*  [4] H. Jégou, et al., [Product quantization for nearest neighbor search](https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf) (2010), *TPAMI*  [5] A. Babenko, V. Lempitsky, [The Inverted Multi-Index](http://sites.skoltech.ru/app/data/uploads/sites/25/2014/12/TPAMI14.pdf) (2012), *CVPR*  [6] H. Jégou, et al., [Searching in One Billion Vectors: Re-rank with Source Coding](https://arxiv.org/pdf/1102.3828.pdf) (2011), *ICASSP*  [7] D. Baranchuk, et al., [Revisiting the Inverted Indices for Billion-Scale Approximate Nearest Neighbors](https://arxiv.org/pdf/1802.02422.pdf) (2018), *ECCV*  [8] [Guidelines to choose an index](https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index), Faiss wiki  [9] [The Index Factory](https://github.com/facebookresearch/faiss/wiki/The-index-factory), Faiss wiki ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e32a"
  },
  "title": "Introducing the hybrid index to enable keyword-aware semantic search",
  "headline": "Introducing the hybrid index to enable keyword-aware semantic search",
  "name": "Gibbs Cullen",
  "position": "Senior Product Marketing Manager",
  "src": "/images/gibbs-cullen.jpg",
  "href": "https://www.linkedin.com/in/gibbscullen/",
  "date": "\"2022-10-31\"",
  "description": "New approach to hybrid search leads to more relevant results",
  "thumbnail": "\"/images/hybrid-search-thumbnail.jpg\"",
  "images": "['https://www.pinecone.io/images/hybrid-search.png']",
  "content": "![Keyword-Aware Semantic Search](/images/hybrid-search.png)  ## New approach to hybrid search leads to more relevant results   Growing expectations around search (that our applications should automatically understand our intent) have led to important advancements — like semantic search — that go beyond keyword search capabilities. However, both keyword and semantic search have important tradeoffs to consider:  - **Keyword search can miss important context.** Keyword methods can struggle even with simple synonyms without manual tuning.  - **Semantic search can miss important keywords.** Semantic search may overlook details searching for certain keywords (such as names, industry-specific jargon, or rare words), providing related but not the best results.  In fact, for text-search use cases, a hybrid approach — combining keyword and semantic search — provides more relevant results than either one alone. But running two separate search solutions plus a third system for combining the results is an engineering nightmare.  That’s why we’re excited to announce the **hybrid vector index**, a first-of-its-kind solution that lets engineers easily build keyword-aware semantic search into their applications. Continue reading to learn more, and [request early access today](/hybrid-search-early-access/).  ## Level-up your search with keyword-aware semantic search, powered by the Pinecone hybrid vector index  Companies are turning to hybrid search techniques to help users get more relevant search results. The ability to search based on both what users say and what they mean leads to better results and happier users.  [Our research](https://arxiv.org/abs/2210.11934) shows the impact of hybrid search on relevance compared to standalone keyword and semantic search: Whether searching in-domain or out-of-domain from the original training data, the hybrid results are better across the board.   ![Evaluating lexical, semantic, and hybrid retrieval](/images/hybrid-search-1.png) <small>Figure: Evaluating lexical, semantic, and hybrid retrieval, NDCG@1000</small>  We also know there’s a growing area of research around using hybrid vectors for use cases outside of text (e.g. creating sparse vectors from a learned sparse model (like [SPLADE](https://dl.acm.org/doi/10.1145/3404835.3463098)) instead of BM25). However, existing solutions make doing this no easy feat. Not only do you need to run multiple solutions — keyword and vector search systems alongside a reranking system — but you also have to know which levers to pull in order to transform your vectors to work with these existing solutions.    With the new hybrid vector index, you don’t need to be an ML expert to build hybrid search for these use cases. We’ve designed it to be:   - **Simple**: No need to manage multiple solutions. It’s not a vector index, an inverted index, and a re-ranker duct-taped together. It’s one hybrid index. - **Flexible**: A first-of-its-kind hybrid index to store and search across both dense and sparse representations of any kind of data, not just text. - **Scalable**: Support for billions of vectors with low latency and zero-downtime scaling.  And since text is the predominant use case for hybrid search, we’re adding a hybrid endpoint to the Pinecone API. This endpoint accepts vector embeddings (dense vectors) and term frequencies (sparse vectors) for uploading or querying the hybrid index. This new, hybrid API endpoint provides:  - **Convenience**: Saves you time and pre-processing steps to normalize and combine vectors. Accepts dense vectors from any language model such as SentenceBERT (SBERT), and sparse vectors from any tokenization library such as Hugging Face Tokenizers. - **Consistency**: Gives you the benefits of tried-and-true BM25 scoring for the keyword part of hybrid search. - **Control**: Adjust the weight of keyword and semantic relevance when querying. Gives you control of importance for keyword vs. semantic.  Hybrid search is a powerful capability that we believe should be accessible to all. As [Nils Reimers](https://www.nils-reimers.de/), the creator of Sentence Transformers, put it:  > Semantic search can largely improve search performance, but there are still some shortcomings, especially when it comes to keyword-specific queries. Combining semantic search capabilities with traditional BM25 solves many of these issues, but so far the available solutions are not practical to deploy as you need to use two different systems. This is why I am so excited that Pinecone is adding keyword semantic search functionality to their managed vector database. It will give even better search results for many use-cases.  ### How it works  Before diving into how our hybrid search solution works, let’s define some key terms:  - A **dense vector** is a vector of fixed dimensions, typically between 100-1000, where every entry is almost always non-zero. They represent the learned semantic meaning of texts by ML models like [SBERT](https://www.sbert.net/). - A **sparse vector** is a vector of a very large dimension (e.g. 1,000,000), where only a small fraction of its entries are non-zero. They represent important keywords inside documents.  We designed our hybrid search to be easy to use and scale, which we’ll demonstrate with the following example.  Imagine you need to build a feature to let users browse and analyze employee survey responses. You want to support searches for both general concepts (e.g. company offsite in Greece) and company-specific terms (e.g. Pinecone).  Here’s how to do it with Pinecone’s hybrid index:  1. [Sign in](https://app.pinecone.io) to Pinecone to get an API key and create a hybrid index (`s1h`).     ```python    headers = {\"Api-Key\": APIKEY}    config = {        \"name\": \"my-index\",        \"dimension\": 328,        \"metric\": \"dotproduct\",        \"pods\": 1,        \"pod_type\": \"s1h\",    }    requests.post('https://controller.<env>.pinecone.io/databases', headers=headers, json=config)    ```  2. Generate dense vectors for the survey responses through a dense embedding model such as [SBERT](/learn/sentence-embeddings/). Use a tokenizer or analyzer tool (such as those from [spaCy](https://spacy.io/) or [HuggingFace](https://huggingface.co/docs/tokenizers/index)) to generate sparse vectors (based on term frequency) for the same survey responses.     ```python    from sentence_transformers import SentenceTransformer    from transformers import AutoTokenizer    from collections import Counter    import requests     tokenizer = AutoTokenizer.from_pretrained('transfo-xl-wt103')    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')     doc = \"Visiting the parthenon during the Pinecone offsite was an awesome experience.\"    vector = model.encode([doc]).tolist()  # [0.1, -0.1, 0.2, ...]    tokens = dict(Counter(tokenizer.encode(doc)))  # {5:1, 10500:1, 7:1, ... }    ```  3. Upload both dense and sparse vectors into a Pinecone hybrid index using the hybrid API. Your sparse vectors will be automatically normalized and transformed to provide search results equivalent to [BM25](https://www.pinecone.io/learn/semantic-search/#bm25).     ```python    upsert = {        \"vectors\": [{            \"id\": \"example-id-1\",            \"values\": vector,  # Dense Vector            \"sparse_values\": tokens,  # Sparse Vector            \"metadata\": {'text': doc}        }],    }    requests.post('https://<index-name>-<project-id>.svc.<env>.pinecone.io/hybrid/vectors/upsert', json=payload, headers=headers)    ```  4. Now you can query the index, providing the sparse and dense vectors (which are combined into sparse-dense hybrid vectors using linear-combination fusion) along with a weight for keyword relevance (“alpha”). `Alpha=1` will provide a purely semantic-based search result and `alpha=0` will provide a purely keyword-based result equivalent to BM25. The default value is `0.5`.     ```python    question = \"pinecone athens offsite\"     query = {        \"topK\": 2,        \"vector\": model.encode([question]).tolist(),        \"sparseVector\": dict(Counter(tokenizer.encode(question))),        \"alpha\": 0.5  # Weight    }    resp = requests.post('https://<index-name>-<project-id>.svc.<env>.pinecone.io/hybrid/query', json=query, headers=headers)    ```     Note: The below diagrams show the effects of `alpha` values on sample datasets.. When using a model that is not trained for the corpus (out-of-domain), you should downweight the semantic score with lower values of alpha (e.g. 0.3-0.6). When using a model that is fine-tuned (in-domain), use values closer to 1.     ![Diagram](/images/hybrid-search-2.png)  5. Query results are then retrieved (scored by max dot product), and you’re able to see the top results for survey responses related to “Greece offsite”, specifically those about “Pinecone”.     ```python    # Matches    resp.json()['matches']    [{'id': '3706692',      'score': 0.763926864,      'values': [],      'sparseValues': {},      'metadata': {'text': 'Visiting the parthenon during the Pinecone offsite was an awesome experience.'}},    {'id': '3393693',      'score': 0.582026243,      'values': [],      'sparseValues': {},      'metadata': {'context': “Last time i visited greece was on my own.”}}]    ```  Just like that, you can build keyword-aware semantic search into your applications, and provide great results without tuning models or indexes, or managing multiple systems.  The below diagram displays both the upsert and query paths.  ![Hybrid API](/images/hybrid-search-3.png)  Pinecone is built for high-performance vector search at massive scale, and this new hybrid index is no exception. You can expect the same capacity (around 5 million 768-dimension vectors per pod), throughput, and latency as our storage-optimized [s1 pods](https://www.pinecone.io/docs/indexes/#s1-pods). As always, actual capacity and performance may vary based on use case and datasets, so we encourage you to experiment and [contact us](/contact/) for help if needed. All index types in Pinecone come with metadata filtering, vertical and horizontal scaling, snapshots, expert support, [and more](/pricing/).   ## Try it today  Our hybrid search solution is currently in private preview. [Request early access](/hybrid-search-early-access/) to try it, read the docs, and stay tuned for more updates and technical deep dives (including [how to get started with hybrid search](/learn/hybrid-search-intro/)). ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e32c"
  },
  "title": "\"$28M to Bring Search into the AI Age\"",
  "headline": "\"$28M to Bring Search into the AI Age\"",
  "name": "Edo Liberty",
  "position": "Founder and CEO",
  "src": "/images/company-edo.png",
  "href": "https://edoliberty.github.io/",
  "date": "\"2022-03-29\"",
  "# Date": "March 29, 2022",
  "description": "Pinecone raised $28M in Series A funding to bring search into the AI age.",
  "images": "[\"/images/series-a-rectangle.png\"]",
  "thumbnail": "\"/images/28m-thumbnail.png\"",
  "content": "As demand grows for better search results and recommendations, the path to better search applications is through AI, specifically vector search. Last year, we launched [Pinecone](/) to make it easy for developers to build high-performance vector search applications — at any scale and without infrastructure hassles.  Today I’m excited to announce **we raised $28M in Series A funding**. This investment, along with our rapidly growing number of users and customers, is an undeniable testament to what we believed from day one: The future of search is [vector search](/learn/vector-search-basics/). And the future of vector search is Pinecone.  I’d like to share how we got here and where we’re headed. It all started in the year 1200...  ## Search from 1200 AD to Today  In the 13th century, close to 200 years before the invention of the printing press, the cardinal Hugh of Saint-Cher created the first concordance of the Latin Bible by listing important keywords along with the page or passage numbers where they appear.  It would seem that search technology has changed a lot since then — we have modern indexing and ranking methods, with databases that can store and search through billions of records in milliseconds. And yet the core idea hasn’t changed: Eight centuries after the _Concordantiae Sancti Jacobi_ was penned, search technology still revolves around keywords.  In recent years, advancements in AI/ML have made it possible to capture the meaning of any data in a machine-readable format called [vector embeddings](/learn/vector-embeddings/). That opened the door to vector search, a revolutionary information-retrieval method that searches through data using meaning and not only keywords.  ![Traditional vs. Vector Search](/images/traditional-vs-vector-search.png)  The biggest tech companies have already adopted this technology. When you search on Google, get recommended products on Amazon, and read relevant stories on your Facebook feed, you see vector search in action.  It’s no coincidence this revolution started at the largest, most advanced tech companies: Leveraging vector search inside large-scale and high-performance applications requires a new kind of infrastructure to be built and maintained, along with extensive engineering and data science work. In other words: It’s _hard_.  ## Enter Pinecone  We founded Pinecone to make it easy for engineers to build vector search applications. That meant creating a completely new kind of infrastructure and indexing algorithm, standing it up as a managed service, and exposing it through a simple API. We needed to call it something, so we came up with “[vector database](/learn/vector-database/).”  ![The vector database is part of the new search infrastructure](/images/new-search-infrastructure.png)  Since launching in 2021, we:  - Released new features such as real-time index updates and [single-stage filtering](/docs/metadata-filtering/). - Released a new REST API built using the OpenAPI standard. - Made significant performance and scalability improvements to the core engine. - Launched a [vector-search community](/community/) and the Pinecone Pioneers program. - Invested in educational materials about vector search, including a free online course on [NLP for semantic search](/learn/nlp/). - Tripled the size of our team, including Ram Sriharsha who joined as VP of Engineering after holding the same position at Splunk. - Released a [free plan](https://app.pinecone.io/) for experimentation and small applications, and scalable usage-based plans for everyone else. - Onboarded thousands of users, ranging from fast-growing startups to Fortune 500 companies.  The impact has been astounding. Engineering teams of all sizes and machine-learning skill levels — are already running vector search in production thanks to Pinecone.  To give a few of my favorite examples: Pinecone powers the semantic search inside [Mem](https://mem.ai/) to help people stay organized, the alert management at [Expel](https://expel.io/) to protect cloud infrastructure, the file search at [Searchable](https://www.searchable.ai/) to make teams more productive, and the feed ranking inside a major social app to bring people together.  We have now crossed another milestone by raising $28M in Series A financing, led by Menlo Ventures with participation from new investor Tiger Global and previous investors, including Wing Venture Capital. Tim Tully, Partner at Menlo Ventures and former CTO of Splunk, will join the board of directors.  I first met Tim when we worked at Yahoo, where where he led the data organization and media properties. Since then he has led multiple engineering organizations (including Splunk) and advised some of the fastest-growing cloud infrastructure startups. He is the most technically experienced investor I know, with an exceptional understanding of the AI and data infrastructure space. I couldn’t have asked for a better partner to join us on this journey.  ## The Future of Search  We’re on a mission to build search and database technology for the AI age. In the near term, that means two things: First, we must make it incredibly fast and easy for developers to use vector search applications, regardless of their experience with machine learning. Second, we must push the boundaries of vector search to provide faster and more relevant results at any scale.  To that end, we’re investing even more in product and engineering, developer advocacy and customer success, and core research into machine learning, natural language processing, and information retrieval. That includes growing an incredibly talented and ambitious team in those areas. If this sounds exciting to you, [join us](/careers/)! ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e32e"
  },
  "title": "\"Vector Similarity Explained\"",
  "headline": "\"Vector Similarity <span>Explained</span>\"",
  "weight": "4",
  "name": "Roie Schwaber-Cohen",
  "position": "Developer Advocate",
  "src": "/images/company-roie-s-c.jpeg",
  "href": "https://www.linkedin.com/in/roiecohen/",
  "description": "A basic introduction to vector similarity.",
  "images": "[\"/images/similarity-metrics-og.png\"]",
  "content": "[Vector embeddings](https://www.pinecone.io/learn/vector-embeddings/) have proven to be an effective tool in a variety of fields, including natural language processing and computer vision. Comparing vector embeddings and determining their similarity is an essential part of semantic search, recommendation systems, anomaly detection, and much more.  In fact, this is one of the primary determining factors in how [Pinecone](https://www.pinecone.io/) produces its results. In this article, we will look at three common vector similarity metrics: Euclidean distance, cosine similarity, and dot product similarity. Understanding the benefits and drawbacks of each metric will enable you to make more informed decisions when deciding on the best similarity metric for your use case.  The **basic rule of thumb** in selecting the best similarity metric for your [Pinecone index](https://docs.pinecone.io/docs/manage-indexes) is to **match it to the one used to train your embedding model**. For example, the [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) model was trained using cosine similarity — so using cosine similarity for the index will produce the most accurate result. If you used a Euclidean distance measure to train your model, the same similarity metric should be used in the index, etc. Pinecone will be able to use the best algorithms as long as we follow this rule.  For example, the [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) model was trained using cosine similarity — so using cosine similarity for the index will produce the most accurate result. If you used a Euclidean distance measure to train your model, the same similarity metric should be used in the index, etc.  In this article, we’ll take a deeper look at how these similarity metrics work under the covers to get some intuition as to what it means for two vector embeddings to be similar, in the context of a specific use case. For example, in [natural language processing](https://www.pinecone.io/learn/nlp/), two vectors representing word meanings may be “close” to one another if they are used in similar contexts or related to similar ideas. In the context of recommendation systems, two vectors representing user preferences can be similar if they share interests or have previously made the same choices.  In the table below, you can see the similarity metrics we’ll discuss in this post and the properties of the vectors that influence the metric.  **Table 1: Similarity metrics**  <table>   <tr>     <td><b>Similarity Metric</b></td>     <td><b>Vector properties considered</b></td>   </tr>   <tr>     <td>Euclidean distance</td>     <td>Magnitudes and direction</td>   </tr>   <tr>     <td>Cosine similarity</td>     <td>Only direction</td>    </tr>    <tr>     <td>Dot product similarity     <td>Magnitudes and direction</td>    </tr> </table>  ## Euclidean distance  Euclidean distance is the straight-line distance between two vectors in a multidimensional space.  **Figure 1: Euclidean distance measurement in two dimensions**  <center>   <div>     <img src=\"/images/vector-similarity-euclidean-distance-diagram.png\" style=\"width:50%\" alt=\"Euclidean distance measurement in two dimensions\">   </div> </center>  It is computed as the square root of the sum of the squares of the differences between the vectors' corresponding components. Here’s the equation for calculating the Euclidean distance between two vectors **a** and **b**:  $$d(\\mathbf{a},\\mathbf{b}) = \\sqrt{(\\mathbf{a_1}-\\mathbf{b_1})^2 + (\\mathbf{a_2}-\\mathbf{b_2})^2 + ... + (\\mathbf{a_n}-\\mathbf{b_n})^2}$$  To calculate the Euclidean distance between two vectors **a** and **b**, the equation first calculates the difference between the first components of the two vectors (a<sub>1</sub>-b<sub>1</sub>), then the difference between the second components (a<sub>2</sub>-b<sub>2</sub>), and so on, until it reaches the n<sup>th</sup> component (a<sub>n</sub>-b<sub>n</sub>). The differences are then squared and added together, and the square root of the sum is taken to give the final distance.  This metric is sensitive to scale as well as the vectors' relative location in space. This means that vectors with large values will have a larger Euclidean distance than vectors with small values, even if the vectors are otherwise similar. This can be expressed formally as follows:  $$d(\\alpha \\mathbf{x}, \\alpha \\mathbf{y}) = \\alpha d(\\mathbf{x},\\mathbf{y})$$  Euclidean distance is a very straightforward similarity metric in that it literally reflects the distance between each of the values of the vectors being compared: if the Euclidean distance is very small, then the values of each coordinate in the vectors are very close. This is not true in general for dot product or cosine.  In most cases, you won’t use it with deep learning models but rather with models created with more basic vector encoding methods like [LSH (Locality Sensitive Hashing)](https://www.pinecone.io/learn/locality-sensitive-hashing/). More generally, Euclidean distance is a natural choice when the model wasn’t trained with a specific loss function.  Since Euclidean distance is sensitive to magnitudes, it is helpful where the embeddings contain information that has to do with counts or measures of things. For example, in a recommendation system where the goal is to recommend items that are similar to a user's previous purchases: Euclidean distance can be used to measure the absolute difference between the embeddings of the times an item was purchased.  ## Dot product Similarity  The dot product similarity metric for two vectors is calculated by adding the products of the vectors' corresponding components. The dot product for vectors **a** and **b** is calculated as follows:  $$\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i b_i = a_1b_1 + a_2b_2 + a_3+b_3 + \\ldots + a_nb_n$$  where **_a_** and **_b_** are the vectors being compared and _a<sub>i</sub>_ and _b<sub>i</sub>_ are the components of the vectors. The dot product is calculated by multiplying the vector components and adding the results.  As shown below, the dot product can also be expressed as the product of the magnitudes of the vectors and the cosine of the angle between them:  $$\\mathbf{a} \\cdot \\mathbf{b} = |\\mathbf{a}||\\mathbf{b}|cos\\alpha$$  **Figure 2: Dot product measurement in two dimensions**  <center>   <div>     <img src=\"images/vector-similarity-dot-product-diagram.png\" style=\"width:50%\" alt=\"Dot product measurement in two dimensions\">   </div> </center>  The dot product is a scalar value, which means it is a single number rather than a vector. The dot product is positive if the angle between the vectors is less than 90 degrees, negative if the angle between the vectors is greater than 90 degrees, and zero if the vectors are orthogonal.  The dot product can be affected by the length and direction of the vectors. When two vectors have the same length but different directions, the dot product will be larger if the two vectors are pointing in the same direction and smaller if they are pointing in opposite directions. imagine two vectors represented by arrows, vector **a** and vector **b**. If vectors **a** and **b** are pointing in the same direction as each other, the dot product of **a** and **b** will be larger than if **a** and **b** were pointing in opposite directions.  You’re likely to run into many Large Language Models (LLMs) that use dot product for training and as we stated before, the rule of thumb would be to use dot product as the similarity metric for your Pinecone metric. For example, the [msmarco-bert-base-dot-v5](https://huggingface.co/sentence-transformers/msmarco-bert-base-dot-v5) model on Hugging Face specifies the “suitable scoring functions” to be only dot product.  In a recommender system based on collaborative filtering and matrix factorization, every user and every item (e.g. movie) has an embedding, and the model is learned such that the dot product between a user embedding and an item embedding is a good predictor of the rating that user will assign this item. Now, if two products have embeddings with the same direction but different magnitudes, this can mean that the two products are about the same topic, but the one that has a larger magnitude is just better / more popular than the other.  ## Cosine Similarity  Cosine similarity is a measure of the angle between two vectors. It is computed by taking the dot product of the vectors and dividing it by the product of their magnitudes. This metric is not affected by the size of the vector but only by the **angle** between them. This means that vectors with large or small values will have the same cosine similarity as long as they point in the same direction. Here’s how cosine similarity is calculated for vectors **a** and **b**:  $$sim(\\mathbf{a},\\mathbf{b}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{||\\mathbf{a}|| \\cdot ||\\mathbf{b}||}$$  where **a** and **b** are the vectors being compared, \"•\" stands for the dot product, and ||**a**|| and ||**b**|| stand for the vectors' lengths. The cosine similarity is between -1 and 1, where 1 means an angle of 0 (vectors are as close as they can be), 0 means orthogonal, and -1 means that the vectors are pointing in opposite directions.  **Figure 3: Cosine similarity measurement in two dimensions**  <center>   <div>     <img src=\"/images/vector-similarity-cosine-sim-diagram.png\" style=\"width:50%\" alt=\"Cosine similarity measurement in two dimensions\">   </div> </center>  First, the equation calculates the dot product of the vectors by multiplying their components and adding the results. The dot product is then divided by the product of the magnitudes of the vectors, which is calculated by taking the square root of the sum of the squares of the components of the vectors.  If the model was trained using cosine similarity, you either use cosine similarity or normalize and use dot product. Note that options are mathematically equivalent. In some cases normalizing and using the dot product is better, and in some cases where using cosine similarity is better.  An example use case for cosine similarity is solving semantic search and document classification problems since it allows you to compare the direction of the vectors (_i.e._, the overall content of the documents). Similarly, recommendation systems that aim to recommend items to users based on their past behavior could use this similarity metric.  Cosine similarity is probably not suitable when you have data where the magnitude of the vectors is important and should be taken into account when determining similarity. For example, it is not appropriate for comparing the similarity of image embeddings based on pixel intensities.  ## Wrap-Up  Remember the general principle we mentioned in the introduction of this post when selecting the similarity metric for your index: use the same similarity metric that was used to train your embedding model. Otherwise, you should experiment with various other similarity metrics to see if you can produce even better results (for instance, if you don't know what similarity metric was used in the embedding mode or if the method the vectors were created with does not have such a metric in the generation process ). ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e330"
  },
  "title": "Weight Initialization Techniques in Neural Networks",
  "headline": "Weight Initialization Techniques in Neural Networks",
  "weight": "12",
  "name": "Bala Priya C",
  "position": "Technical Writer",
  "src": "/images/bala-priya.jpg",
  "href": "https://www.linkedin.com/in/bala-priya/",
  "description": "How to optimize weight initialization of neural networks for faster convergence and better overall performance.",
  "images": "[\"/images/weight-initialization-1.jpg\"]",
  "content": "![Weight Initialization](/images/weight-initialization-1.jpg)  You can build better deep learning models that train _much_ faster by using the correct weight initialization techniques. A neural network _learns_ the weights during the training process. But how much do the initial weights of the network benefit or _hinder_ the optimization process?  Though the neural network “learns” the optimal values for the weights, the initial values of the weights play a significant role in how quickly and to _which_ local optimum the weights converge.  Initial weights have this impact because the loss surface of a deep neural network is a complex, high-dimensional, and [non-convex landscape](https://www.cs.cornell.edu/courses/cs6787/2021fa/lectures/Lecture7.pdf) with many local minima. So the point where the weights start on this loss surface determines the local minimum to which they converge; the better the initialization, the better the model.  This tutorial will discuss the early approaches to weight initialization and the limitations of zero, constant, and random initializations. We’ll then learn better weight initialization strategies based on the number of neurons in each layer, choice of activation functions, and more.  Let’s begin!  ## Early Approaches to Weight Initialization  When training deep neural networks, finding the optimal initial value for weights was one of the earliest challenges faced by deep learning researchers. In 2006, Geoffrey Hinton and Salakhutdinov introduced a weight initialization strategy called _Greedy Layerwise Unsupervised Pretraining_ [1]. To parse the algorithm’s definition, let’s understand how it works.  Given the input layer and the first hidden layer, an unsupervised learning model, such as an autoencoder, is used to learn the weights between the input and the first hidden layer.  ![Pretraining the first hidden layer](/images/weight-initialization-2.png) <small>Learning the initial weights for the first hidden layer (Image by the author)</small>  These weights are frozen and are used as inputs to learn the weights that flow into the next hidden layer.  ![Pretraining the second hidden layer](/images/weight-initialization-3.png) <small>Learning the initial weights for the second hidden layer (Image by the author)</small>  The process continues until all the layers in the neural network have been traversed. The weights learned this way are fine-tuned and used as the initial weights to train the neural network.  ![Pretraining the third hidden layer](/images/weight-initialization-4.png) <small>Learning the initial weights for the third hidden layer (Image by the author)</small>  We can parse the terms now that we understand how this algorithm works. This approach is **greedy** because it does not optimize the initial weights across all layers in the network but only focuses on the current layer. The weights are learned **layerwise** in an **unsupervised** setting. The term **pretraining** signifies that this process occurs ahead of the actual training process.  This approach to weight initialization was widely used in the deep learning research community before the advent of newer weight initialization techniques that do not require pretraining.  ## Zero or Constant Initialization  The need for a complex algorithm like the _greedy layerwise unsupervised pretraining_ for weight initialization suggests that trivial initializations don't necessarily work.  This section will explain why initializing all the weights to a zero or constant value is suboptimal. Let’s consider a neural network with two inputs and one hidden layer with two neurons, and initialize the weights and biases to zero, as shown.  ![Nueral network](/images/weight-initialization-5.png) <small>A simple neural network with one hidden layer, with the biases set to zero (Image by the author)</small>  For this neural network, $a_{11}$ and $a_{12}$ are given by the following equations:  \\begin{align} a_{11} = w_{11}x_1 + w_{12}x_2\\\\\\\\ a_{12} = w_{21}x_1 + w_{22}x_2 \\end{align}   \\begin{align} Setting \\text{ }w_{11}, w_{12}, w_{21}, and \\text{ }w_{22} \\text{ }to\\text{ } 0,\\\\\\\\ a_{11} = a_{12} = 0\\\\\\\\ \\implies h_{11} = \\sigma{(a_{11})} = 0\\\\\\\\ and\\text{ } h_{12} = \\sigma{(a_{12})} = 0 \\end{align}  Let $\\sigma$ denote the activation function.  Given a loss function $\\mathcal{L}$, the updates that each of the weights in the neural network receives during backpropagation are computed as follows:  \\begin{align} \\nabla w_{11} = \\frac{\\partial{\\mathcal{L}}}{\\partial{y}}.\\frac{\\partial{y}}{\\partial{h_{11}}}.\\frac{\\partial{h_{11}}}{\\partial{a_{11}}}.x_1\\\\\\\\ \\nabla w_{21} = \\frac{\\partial{\\mathcal{L}}}{\\partial{y}}.\\frac{\\partial{y}}{\\partial{h_{12}}}.\\frac{\\partial{h_{12}}}{\\partial{a_{12}}}.x_1\\\\\\\\ But\\text{ } h_{11} = h_{12} \\text{ }(since \\text{ } a_{11} = a_{12})\\\\\\\\ \\implies \\text{ }\\nabla w_{11} = \\nabla w_{21}  \\end{align}  After the first update, the weights $w_{11}$ and $w_{21}$ move away from zero but are equal.  Similarly, we see that the weights $w_{12}$ and $w_{22}$ are equal after the first update.  \\begin{align} \\nabla w_{12} = \\frac{\\partial{\\mathcal{L}}}{\\partial{y}}.\\frac{\\partial{y}}{\\partial{h_{11}}}.\\frac{\\partial{h_{11}}}{\\partial{a_{11}}}.x_2\\\\\\\\ \\nabla w_{22} = \\frac{\\partial{\\mathcal{L}}}{\\partial{y}}.\\frac{\\partial{y}}{\\partial{h_{12}}}.\\frac{\\partial{h_{12}}}{\\partial{a_{12}}}.x_2\\\\\\\\ But\\text{ }  h_{11} = h_{12} \\text{ }(since \\text{ }a_{11} = a_{12})\\\\\\\\ \\implies \\text{ } \\nabla w_{12} = \\nabla w_{22}  \\end{align}  The weights are initially equal and receive the same update at each step. The neurons, therefore, evolve _symmetrically_ as the training proceeds, and we will _not_ be able to **break this symmetry**. This is true even when the weights are initialized to any constant k. The weights are initially at k, then receive the same update, leading to the symmetry problem yet again!  _But why is this a problem?_  The main advantage of using a neural network over traditional machine learning algorithms is its ability to learn a complex mapping from the input space onto the output. It is for this reason neural networks are called [universal function approximators](http://neuralnetworksanddeeplearning.com/chap4.html). The various parameters of the network (weights) enable the neurons in the different layers to learn other aspects of this mapping. However, so long as the weights flowing into a neuron stay equal, all the neurons in a layer learn the “_same_” thing. Such a model performs poorly in practice.  **Key takeaway**: Under zero or constant weight initialization, the neurons in a layer change symmetrically throughout the training process.  ---  📑 **Use of Regularization in Neural Networks**: When training deep neural networks, you can use regularization techniques such as **dropout** to avoid overfitting.  If you implement dropout, a specific fraction of neurons in each layer are randomly switched off during training. As a result, those neurons may not get updates as the training proceeds, and it **is** possible to break the symmetry.  However, the scope of this tutorial is to explain how the weights in a neural network should be carefully initialized in the absence of other regularization techniques.  ---  ## Random Initialization  Given that we cannot initialize the weights to all zeros or any constant k, the next natural step is to initialize them to random values. But does random initialization work?  ### Initializing the Weights to Small Random Values  Let’s try initializing the weights to small random values. We’ll take an example to understand what happens in this case.   ```python import numpy as np ```  Consider a neural network with five hidden layers, each with 50 neurons. The input to the network is a vector of length 100.   ```python # x: input vector  x = np.random.randn(1,100)  # 5 hidden layers each with 50 neurons hidden_layers = [50]*5 use_activation = ['tanh']*len(hidden_layers) # available activations activation_dict = {'tanh':lambda x:np.tanh(x),'sigmoid':lambda x:1/(1+np.exp(-x))} H_mat = {} ```  Let’s observe what happens during the forward pass through this network. The weights are drawn from a standard normal distribution with zero mean and unit variance, and they’re all scaled by a factor of 0.01.  ```python for i in range(len(hidden_layers)):   if i == 0:     X = x   else:     X = H_mat[i-1]    # define fan_in and fan_out    fan_in = X.shape[1]   fan_out = hidden_layers[i]    # weights are small random values   W = np.random.randn(fan_in,fan_out)*0.01    H = np.dot(X,W)   H = activation_dict[use_activation[i]](H)   H_mat[i] = H ```  For small random values of weights, we observe that the activations grow smaller as we go deeper into the neural network.   ![tanh activations across the hidden layers](/images/weight-initialization-6.png) <small>Vanishingly small activations in the deeper layers of the network</small>  During backpropagation, the gradients that flow into a neuron are proportional to the activation they receive. When the magnitude of activations is small, the gradients are vanishingly small, and the neurons do not learn anything!  ### Initializing the Weights to Large Random Values  Let’s try initializing the weights to larger random values. Replace the weight matrix with the following `W`, where the samples are drawn from a standard normal distribution.  ```python W = np.random.randn(fan_in,fan_out) ```  When the weights have a large magnitude, the sigmoid and tanh activation functions take on values very close to saturation, as shown below. When the activations become saturated, the gradients move close to zero during backpropagation.  ![sigmoid and tanh activations](/images/weight-initialization-7.png) <small>Saturation of sigmoid and tanh activations (Image by the author)</small>  ![tanh activations for large random weights](/images/weight-initialization-8.png) <small>Saturating tanh activations for large random weights</small>  Let’s summarize the observations from the above experiments.  1. In the first case, we sampled the initial weights from a standard normal distribution with zero mean and unit variance and scaled them by a factor of 0.01. This is equivalent to drawing samples from a standard normal distribution with zero mean and variance $(0.01)^2$, which is negligibly small. When the weight distribution has a small variance, both activations during forward pass and gradients during backprop vanish.  2. In the second case, we sampled from a standard normal distribution, without scaling the samples. We faced the problem of saturating activations and vanishing gradients during backpropagation.  However, suppose we pick the optimal scaling factor, or equivalently, find the optimal variance of the weight distribution. In that case, we can get the network to operate in the region between vanishing and saturating activations.  ## A Better Weight Initialization Strategy  Let us assume that the inputs have been normalized to have zero mean and unit variance. The weights are drawn from a distribution with zero mean and a fixed variance. But what should that variance be? Let’s analyze!  To compute the optimal variance, we’ll use $a_{11}$ as the first input to the first neuron in the second hidden layer, instead of $h_{11} = \\sigma(a_{11})$. $h_{11}$ is proportional to $a_{11}$, so we ignore the explicit effect of activations to simplify the derivation.  ![Weights flowing into the first neuron](/images/weight-initialization-9.png) <small>Weights flowing into the first neuron in the first hidden layer (Image by the author)</small>  \\begin{align} a_{11} = \\sum_{i=1}^{n} w_{1i}x_i \\\\\\\\ Var(a_{11}) = Var\\left(\\sum_{i=1}^{n} w_{1i}x_i\\right) \\end{align}  **Assumption**: The weights and inputs are _[uncorrelated](https://www.stat.cmu.edu/~cshalizi/uADA/13/reminders/uncorrelated-vs-independent.pdf)_.  \\begin{align} Var(a_{11}) = \\sum_{i=1}^{n} Var(w_{1i}x_i)\\\\\\\\ \\implies Var(a_{11}) = \\sum_{i=1}^{n} \\{ (\\mathbb{E}[w_{1i}])^2 Var(x_i) + (\\mathbb{E}[x_i])^2 Var(w_{1i}) + Var(x_i)Var(w_{1i})\\} \\end{align}   Substituting $\\mathbb{E}[w_{1i}] = 0$ and $\\mathbb{E}[x_i] = 0$ in the above equation:  \\begin{align} Var(a_{11}) = \\sum_{i=1}^{n} Var(x_i)Var(w_{1i})\\\\\\\\ Let \\text{ } Var(x_i) = Var(x) \\text{ } and \\text{ } Var(w_{1i}) = Var(w_1)\\\\\\\\ \\implies Var(a_{11}) = n.Var(w)Var(x) \\end{align}  Let’s compute the variance of $a_{21}$ in the second hidden layer.  ![Variance in the second hidden layer](/images/weight-initialization-10.png)  \\begin{align} Var(a_{21}) = \\sum_{i=1}^{n} Var(a_{1i})Var(w_{2i}) \\text{ }(1)\\\\\\\\ Substituting\\text{ } Var(a_{11}) = n.Var(w)Var(x) \\text{ }and \\\\\\\\ Var(w_{1i}) = Var(w_2)\\\\\\\\ \\implies Var(a_{21}) = n.Var(w_2)n.Var(w_1)Var(x) \\end{align}  \\begin{align} Var(a_{21}) =  n^2[Var(w)]^2Var(x)\\\\\\\\ \\end{align}  By induction, the variance of input to neuron `i` in the hidden layer `k` is given by:  \\begin{align} Var(a_{ki}) = [n.Var(w)]^kVar(x)\\\\\\\\ \\end{align}  To ensure that the quantity `n.Var(w)` neither vanishes nor grows exponentially (leading to instability in the training process), we need `n.Var(w) =1`.  \\begin{align} n. Var(w) = 1\\\\\\\\ Var(w) = \\frac{1}{n} \\end{align}  \\begin{align} If \\text{ }Var(X) = \\sigma^2, \\text{ }then \\text{ }Var(c.X) = c^2\\sigma^2 \\end{align}  To achieve this, we can sample the weights from a standard normal distribution and scale them by a factor of $\\frac{1}{\\sqrt{n}}$.  ## Xavier or Glorot Initialization  X. Glorot and Y. Bengio proposed an improved weight initialization strategy named the Xavier or Glorot initialization [2] (after the researcher Xavier Glorot).   In a neural network, the number of weights that flow into each neuron in a neural network is called $fan_{in}$, and the number of weights that flow out of the neuron is called $fan_{out}$.  ![Explaining fan_in and fan_out](/images/weight-initialization-11.png) <small>Explaining fan_in and fan_out (Image by the author)</small>  When the  weight distribution's variance is set to $\\frac{1}{fan_{in}}$, the activations neither vanish nor saturate during the forward pass.   \\begin{align} fan_{in}. Var(w) = 1\\\\\\\\ Var(w) = \\frac{1}{fan_{in}} \\end{align}  However, during backpropagation, the gradients flow backward through the full network from the output layer to the input layer. We know that the $fan_{out}$ of a neuron is the number of weights that flow out of the neuron into the next layer. But the $fan_{out}$ of a particular neuron is also the number of paths through which gradients flow *into* it during backpropagation. Therefore, having the variance of the weights equal to $\\frac{1}{fan_{out}}$ helps overcome the problem of vanishing gradients.  To account for both the forward pass and backprop, we do the following: When computing the variance, instead of $fan_{in}$ or $fan_{out}$, we consider the average of $fan_{in}$ and $fan_{out}$.  \\begin{align} \\frac{fan_{in}+fan_{out}}{2}. Var(w) = 1\\\\\\\\ Var(w) = \\frac{2}{fan_{in}+fan_{out}} \\end{align}  A random variable that is uniformly distributed in an interval centered around zero has zero mean. So we can sample the weights from a uniform distribution with variance $\\frac{2}{fan_{in}+fan_{out}}$. But how do we find the endpoints of the interval?  A continuous random variable `A` that is uniformly distributed in the interval [-a,a] has zero mean and variance of $\\frac{a^2}{3}$.   ![](/images/weight-initialization-12.png)  \\begin{align} Var(A) = \\frac{(2a)^2}{12} = \\frac{a^2}{3} \\end{align}  We know that the variance should be equal to $\\frac{2}{fan_{in}+fan_{out}}$; we can work backward to find the endpoints of the interval.  \\begin{align} Var(w) = \\frac{(2a)^2}{12} = \\frac{a^2}{3}\\\\\\\\ We\\text{ }have,\\text{ } Var(w) = \\frac{2}{fan_{in}+fan_{out}}\\\\\\\\ \\implies \\frac{a^2}{3} =  \\frac{2}{fan_{in}+fan_{out}}\\\\\\\\ a^2 = \\frac{6}{fan_{in}+fan_{out}}\\\\ \\implies a = \\sqrt{\\frac{6}{fan_{in}+fan_{out}}}\\\\\\\\ w \\in \\mathcal{U}\\left[-\\sqrt{\\frac{6}{fan_{in}+fan_{out}}},\\sqrt{\\frac{6}{fan_{in}+fan_{out}}}\\right] \\end{align}  --- ### 📑  Glorot Initialization in Keras  To implement Glorot initialization in your deep learning models, you can use either the `GlorotUniform` or `GlorotNormal` class in the Keras `initializers` module. If you do not specify a kernel initializer when defining the model, it defaults to `GlorotUniform`.  - The `GlorotNormal` class initializes the weight tensors with samples from a truncated normal distribution with variance $\\frac{2}{fan_{in}+fan_{out}}$. When samples are drawn from a “truncated” normal distribution, samples that lie farther than two standard deviations away from the mean are discarded.  - The `GlorotUniform` class initializes the weight tensors by sampling from a uniform distribution in the interval $\\left[-\\sqrt{\\frac{6}{fan_{in}+fan_{out}}},\\sqrt{\\frac{6}{fan_{in}+fan_{out}}}\\right]$. ---  ## He Initialization  It was found that Glorot initialization did not work for networks that used ReLU activations as the backflow of gradients was impacted [3].   But why does this happen?  Unlike the sigmoid and tanh activations, the ReLU function, which maps all negative inputs to zero: `ReLU(x) = max(0,x)`, does not have a zero mean.   ![ReLU Activation](/images/weight-initialization-13.png)  The ReLU function, therefore, outputs `0` for one-half of the input spectrum, whereas tanh and sigmoid activations give non-zero outputs for all values in the input space. Kaiming He et al. introduced a new initialization technique that takes this into account by introducing a factor of 2 when computing the variance [4].  \\begin{align} \\frac{fan_{in}}{2}. Var(w) = 1\\\\\\\\ Var(w) = \\frac{2}{fan_{in}} \\end{align}  As with Glorot initialization, we can also draw the weights from a uniform distribution for He initialization.  \\begin{align} Var(w) = \\frac{(2a)^2}{12} = \\frac{a^2}{3}\\\\\\\\ We\\text{ }have,\\text{ } Var(w) = \\frac{2}{fan_{in}}\\\\\\\\ \\implies \\frac{a^2}{3} =  \\frac{2}{fan_{in}}\\\\\\\\ a^2 = \\frac{6}{fan_{in}}\\\\\\\\ \\implies a = \\sqrt{\\frac{6}{fan_{in}}}\\\\\\\\ w \\in \\mathcal{U}\\left[-\\sqrt{\\frac{6}{fan_{in}}},\\sqrt{\\frac{6}{fan_{in}}}\\right] \\end{align}  --- ### 📑 He Initialization in Keras  The Keras `initializers` module provides the `HeNormal` and `HeUniform` for He initialization.  - The `HeNormal` class initializes the weight tensors with samples drawn from a truncated normal distribution with zero mean and variance $\\frac{2}{fan_{in}}$.  - The `HeUniform` class initializes the weight tensors with samples drawn from a uniform distribution in the interval $\\left[-\\sqrt{\\frac{6}{fan_{in}}},\\sqrt{\\frac{6}{fan_{in}}}\\right]$. ---  Considering the $fan_{out}$ when initializing weights, you can draw the weights from normal distribution with the following variance:  \\begin{align} \\frac{\\frac{fan_{in}+fan_{out}}{2}}{2}.Var(w) = 1\\\\\\\\ Var(w) = \\frac{4}{fan_{in}+fan_{out}} \\end{align}  Equivalently, you may sample the initial weights from a uniform distribution with variance $\\frac{4}{fan_{in}+fan_{out}}$.  \\begin{align} Var(w) = \\frac{(2a)^2}{12}= \\frac{a^2}{3}\\\\\\\\ We\\ have,\\ Var(w) = \\frac{4}{fan_{in}+fan_{out}}\\\\\\\\ \\implies \\frac{a^2}{3} =  \\frac{4}{fan_{in}+fan_{out}}\\\\\\\\ a^2 = \\frac{12}{fan_{in}+fan_{out}}\\\\\\\\ \\implies a = \\sqrt{\\frac{12}{fan_{in}+fan_{out}}}\\\\\\\\ w \\in \\mathcal{U}\\left[-\\sqrt{\\frac{12}{fan_{in}+fan_{out}}},\\sqrt{\\frac{12}{fan_{in}+fan_{out}}}\\right] \\end{align}  ## Summing Up  I hope this tutorial helped you understand the importance of weight initialization when training deep learning models.  - Initializing the weights to **zero or a constant value** leads to the symmetry-breaking problem. This problem stems from the weights receiving the same updates at each step and _updating symmetrically_ as the training proceeds.  - Initializing the weights to **small random values** leads to the problem of vanishing gradients. This is because the gradients flowing into a particular neuron are proportional to the activation that it receives. On the other hand, initializing the weights to **large random values** causes the activations to get saturated, resulting in vanishing gradients during backpropagation.  - To prevent the weights from being drawn from a distribution whose variance is neither too large nor too small, the variance of the distribution must be approximately 1.   - **Xavier or Glorot initialization** works well for networks using activations with zero mean, such as the sigmoid and tanh functions.  - When using **ReLU** activation that does not have zero mean, it's recommended to use the **He initialization**.  When using deep learning in practice, you can experiment with weight initialization, regularization, and techniques such as [batch normalization](/learn/batch-layer-normalization/) to improve the neural network’s training process. Happy learning and coding!  ## Resources  [1]  G. E. Hinton, R. R. Salakhutdinov, [Reducing the Dimensionality of Data with Neural Networks](https://www.cs.toronto.edu/~hinton/science.pdf) (2006), Sciences  [2] X. Glorot, Y. Bengio, [Understanding the difficulty of training deep feedforward neural networks](https://proceedings.mlr.press/v9/glorot10a.html) (2010), AISTATS 2010  [3] K. Kumar, [On weight initialization in deep neural networks](https://arxiv.org/abs/1704.08863) (2017)  [4] He et al., [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852) (2015), CVPR 2015  [5] [Introduction to Statistical Signal Processing](https://ee.stanford.edu/~gray/sp.html), Gray and Davisson  [6] [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/), Boyd and Vandenberghe  [7] [Adaptive Methods and Non-convex Optimization](https://www.cs.cornell.edu/courses/cs6787/2021fa/lectures/Lecture7.pdf), Advanced Machine Learning Systems, Cornell University, Fall 2021  [8] [Layer weight initializers](https://keras.io/api/layers/initializers/), keras.io ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e332"
  },
  "title": "Explore the power of Pinecone with public collections",
  "headline": "Explore the power of Pinecone with public collections",
  "name": "Gibbs Cullen",
  "position": "Senior Product Marketing Manager",
  "src": "/images/gibbs-cullen.jpg",
  "href": "https://www.linkedin.com/in/gibbscullen/",
  "date": "\"2022-09-16\"",
  "thumbnail": "\"/images/public-collections-thumbnail.png\"",
  "description": "Start your vector-database journey with a click.",
  "images": "[\"/images/public-collections-cover.png\"]",
  "content": "![Public collections](/images/public-collections-cover.png)  Last month, we [announced](/learn/faster-easier-scalable/) a new feature in public preview: [collections](https://www.pinecone.io/docs/collections/). Collections allow users to save vectors and metadata from an index as a snapshot, and create new indexes from any collection.   Today we are excited to announce the addition of **public collections** to help users quickly run a sample index pre-loaded with data and experience the power of the Pinecone [vector database](/learn/vector-database/).  ## What are public collections?   For users to run a query in Pinecone, they need to upload data to an index. This takes time. Public collections make it easier to explore Pinecone by providing public data from real-world data sources that can be used to create an index in one click.  Pinecone users can now create an index from pre-loaded vector embeddings in one of three example collections. Each collection features data from Pinecone partners:  - Glue SSTB collection from OpenAI  - Text REtrieval Conference (TREC) question classification collection from Cohere  - Stanford Question Answering Dataset (SQuAD) collection from Stanford   These collections contain real-world data, load in less than a minute, and have matching guides to get started:   - [Guide for OpenAI](https://www.pinecone.io/docs/integrations/openai/)  - [Guide for Cohere](https://www.pinecone.io/docs/integrations/cohere/) - [Guide for SQuAD](https://www.pinecone.io/docs/examples/extractive-question-answering/)  ## How do they work?   The collections are available under **Public Collections** within the [Pinecone console](https://app.pinecone.io). You can create an index from the example collections and use the [guides](https://www.pinecone.io/docs/collections#public-collections-contain-real-world-data) to get started including code snippets in Python showing how to use the particular index.   ![Public collections](/images/public-collections.png)  To create an index from a [public collection](https://www.pinecone.io/docs/manage-indexes/#create-an-index-from-a-public-collection), follow these steps:  1. Open the [Pinecone console](https://app.pinecone.com/). 2. Click the name of the project in which you want to create the index. 3. In the left menu, click **Public Collections**. 4. Find the public collection from which you want to create an index. Next to that public collection, click **Create Index**. 5. When index creation is complete, a message appears stating that the index is created and that vectors are successfully upserted. The **Click to View** button will take you to the new index.  ![Create index](/images/public-collections-create-index.png)  ## Get started today   If you don’t have an embedding model or ready-to-use data to start testing Pinecone, then public collections can help. All Pinecone users will have access to three example collections — Glue SSTB, TREC question classification, and SQuaD — starting today. We will add more public collections over time.  To learn more about public collections, check out the [guides](https://www.pinecone.io/docs/collections#public-collections) or [try them for yourself](https://app.pinecone.io) in the console.  ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e334"
  },
  "content": "categories:   - Projects toc: >- weight: 3 author:   name: James Briggs   position: Developer Advocate   src: /images/james-briggs.jpeg   href: \"https://www.youtube.com/c/jamesbriggs\" description: How to use OpenAI's Whisper for better speech-enabled (audio) search. # Open graph images: ['https://www.pinecone.io/images/openai-whisper-0.png'] ---  OpenAI’s *Whisper* is a new state-of-the-art (SotA) model in speech-to-text. It is able to almost flawlessly transcribe speech across dozens of languages and even handle poor audio quality or excessive background noise.  The domain of spoken word has always been somewhat out of reach for ML use-cases. Whisper changes that for speech-centric use cases. We will demonstrate the power of Whisper alongside other technologies like transformers and vector search by building a new and improved YouTube search.  Search on YouTube is good but has its limitations, especially when it comes to answering questions. With trillions of hours of content, there should be an answer to almost every question. Yet, if we have a specific question like *\"what is OpenAI's CLIP?\"*, instead of a concise answer we get lots of very long videos that we must watch through.  What if all we want is a short 20-second explanation? The current YouTube search has no solution for this. Maybe there's a good reason to encourage users to watch as much of a video as possible (more ads, anyone?).  Whisper is the solution to this problem *and many others involving the spoken word*. In this article, we'll explore the idea behind a better speech-enabled search.  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/vpU_6x3jowg\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ---  ## The Idea  We want to get specific timestamps that answer our search queries. YouTube does support time-specific links in videos, so a more precise search with these links should be possible.  <video autoplay loop muted playsinline class=\"responsive\">  <source src=\"./images/openai-whisper-1.mp4\" type=\"video/mp4\"> </video> <small>Timestamp URLs can be copied directly from a video, we can use the same URL format in our search app.</small>  To build something like this, we first need to transcribe the audio in our videos to text. YouTube automatically captions every video, and the captions are okay — *but* OpenAI just open-sourced something called \"Whisper\".  Whisper is best described as the GPT-3 or DALL-E 2 of speech-to-text. It's open source and can transcribe audio in real-time *or faster* with *unparalleled performance*. That seems like the most exciting option.  Once we have our transcribed text and the timestamps for each text snippet, we can move on to the [question-answering (QA)](https://www.pinecone.io/learn/question-answering) part. QA is a form of search where given a natural language query like *\"what is OpenAI's Whisper?\"* we can return accurate natural language answers.  We can think of QA as the most intuitive form of searching for information because it is how we ask other people for information. The only difference being we type the question into a search bar rather than verbally communicate it — for now.  How does all of this look?  ![whisper-architecture](./images/openai-whisper-2.png) <small>Overview of the process used in our demo. Covering OpenAI's Whisper, sentence transformers, the Pinecone vector database, and more.</small>  Now let's color in the details and walk through the steps.  ## Video Data  The first step is to download our YouTube video data and extract the audio attached to each video. Fortunately, there's a Python library for exactly that called `pytube`.  With `pytube`, we provide a video ID (found in the URL bar or downloadable if you have a channel). I directly downloaded a summary of channel content, including IDs, titles, publication dates, etc., via YouTube. This same data is available via Hugging Face *Datasets* in a dataset called `jamescalam/channel-metadata`.  {{< notebook file=\"whisper-yt-search-channel-meta\" height=\"full\" >}}  We're most interested in the `Title` and `Video ID` fields. With the video ID, we can begin downloading the videos and saving the audio files with `pytube`.  ```python from pytube import YouTube  # !pip install pytube from pytube.exceptions import RegexMatchError from tqdm.auto import tqdm  # !pip install tqdm  # where to save save_path = \"./mp3\"  for i, row in tqdm(videos_meta):     # url of video to be downloaded     url = f\"https://youtu.be/{row['Video ID']}\"      # try to create a YouTube vid object     try:         yt = YouTube(url)     except RegexMatchError:         print(f\"RegexMatchError for '{url}'\")         continue      itag = None     # we only want audio files     files = yt.streams.filter(only_audio=True)     for file in files:         # from audio files we grab the first audio for mp4 (eg mp3)         if file.mime_type == 'audio/mp4':             itag = file.itag             break     if itag is None:         # just incase no MP3 audio is found (shouldn't happen)         print(\"NO MP3 AUDIO FOUND\")         continue      # get the correct mp3 'stream'     stream = yt.streams.get_by_itag(itag)     # downloading the audio     stream.download(         output_path=save_path,         filename=f\"{row['Video ID']}.mp3\"     ) ```  After this, we should find ~108 audio MP3 files stored in the `./mp3` directory.  ![mp3 files directory](./images/openai-whisper-3.png) <small>Downloaded MP3 files in the `./mp3` directory.</small>  With these, we can move on to transcription with OpenAI's Whisper.  ## Speech-to-Text with Whisper  OpenAI’s Whisper speech-to-text-model is completely open source and available via [OpenAI's Whisper library](https://github.com/openai/whisper) available for `pip install` via GitHub:  ``` !pip install git+https://github.com/openai/whisper.git ```  Whisper relies on another software called FFMPEG to convert video and audio files. The installation for this varies by OS [1]; the following cover the primary systems:  ``` # on Ubuntu or Debian sudo apt update && sudo apt install ffmpeg  # on Arch Linux sudo pacman -S ffmpeg  # on MacOS using Homebrew (https://brew.sh/) brew install ffmpeg  # on Windows using Chocolatey (https://chocolatey.org/) choco install ffmpeg  # on Windows using Scoop (https://scoop.sh/) scoop install ffmpeg ```  After installation, we download and initialize the *large* model, moving it to GPU if CUDA is available.  ```python import whisper import torch  # install steps: pytorch.org  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  model = whisper.load_model(\"large\").to(device) ```  Other models are available, and given a smaller GPU (or even CPU) should be considered. We transcribe the audio like so:  {{< notebook file=\"whisper-yt-search-transcribe\" height=\"full\" >}}  From this, we have a list of ~27K transcribed audio segments, including text alongside start and end seconds. If you are waiting a long time for this to process, a pre-built version of the dataset is available. Download instructions are in the following section.  The last cell from above is missing the logic required to extract and add the metadata from our `videos_dict` that we initialized earlier. We add that like so:  ```python data = []  for i, path in enumerate(tqdm(paths)):     _id = path.split('/')[-1][:-4]     # transcribe to get speech-to-text data     result = model.transcribe(path)     segments = result['segments']     # get the video metadata...     video_meta = videos_dict[_id]     for segment in segments:         # merge segments data and videos_meta data         meta = {             **video_meta,             **{                 \"id\": f\"{_id}-t{segments[j]['start']}\",                 \"text\": segment[\"text\"].strip(),                 \"start\": segment['start'],                 \"end\": segment['end']             }         }         data.append(meta) ```  After processing all of the segments, they are saved to file as a JSON lines file with:  ```python import json  with open(\"youtube-transcriptions.jsonl\", \"w\", encoding=\"utf-8\") as fp:     for line in tqdm(data):         json.dump(line, fp)         fp.write('\\n') ```  With that ready, let's build the QA embeddings and vector search component.  ## Question-Answering  On Hugging Face *Datasets*, you can find the data I scraped in a dataset called `jamescalam/youtube-transcriptions`:  {{< notebook file=\"whisper-yt-search-get-transcriptions\" height=\"full\" >}}  For now, the dataset only contains videos from my personal channel, but I will add more videos from other ML-focused channels in the future.  The data includes a short chunk of text (the transcribed audio). Each chunk is relatively meaningless:  {{< notebook file=\"whisper-yt-search-short-segments\" height=\"full\" >}}  Ideally, we want chunks of text 4-6x larger than this to capture enough meaning to be helpful. We do this by simply iterating over the dataset and merging every *six* segments.  {{< notebook file=\"whisper-yt-search-longer-segments\" height=\"full\" >}}  A few things are happening here. First, we're merging every six segments, as explained before. However, doing this alone will likely cut a lot of meaning between related segments.  ![window-no-overlap](./images/openai-whisper-4.png)  <small>Even when merging segments we're still left with a point where we must split the text (annotated with red cross-mark above). This can lead to us missing important information.</small>  A common technique to avoid cutting related segments is adding some *overlap* between segments, where `stride` is used. For each step, we move *three* segments forward while merging *six* segments. By doing this, any meaningful segments cut in one step will be included in the next.  ![window-overlap](./images/openai-whisper-5.png)  <small>We can avoid this loss of meaning by adding an overlap when merging segments. It returns more data but means we are much less likely to cut between meaning segments.</small>  With this, we have larger and more meaningful chunks of text. Now we need to encode them with a QA embedding model. Many high-performing, pretrained QA models are available via Hugging Face *Transformers* and the *Sentence Transformers* library. We will use one called [`multi-qa-mpnet-base-dot-v1`](https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1).  {{< notebook file=\"whisper-yt-search-init-encoder\" height=\"full\" >}}  Using this model, we can encode a passage of text to a *meaningful* 768-dimensional vector with `model.encode(\"<some text>\")`. Encoding all of our segments at once or storing them locally would require too much compute or memory — so we first initialize the vector database where they will be stored:  {{< notebook file=\"whisper-yt-search-init-pinecone\" height=\"full\" >}}  We should see that the index (vector database) is currently empty with a `total_vector_count` of `0`. Now we can begin encoding our segments and inserting the embeddings (and metadata) into our index.  {{< notebook file=\"whisper-yt-search-index-vecs\" height=\"full\" >}}  That is everything we needed to prepare our data and add everything to the vector database. All that is left is querying and returning results.  ## Making Queries  Queries are straightforward to make; we:  1. Encode the query using the same embedding model we used to encode the segments. 2. Pass to query to our index.  We do that with the following:  {{< notebook file=\"whisper-yt-search-query\" height=\"full\" >}}  These results are relevant to the question; three, in particular, are from a similar location in the same video. We might want to improve the search interface to be more user-friendly than a Jupyter Notebook.  One of the easiest ways to get a web-based search UI up and running is with Hugging Face *Spaces* and Streamlit (or Gradio if preferred).  We won't go through the code here, but if you're familiar with Streamlit, you can build a search app quite easily within a few hours. Or you can use our example and do it in 5-10 minutes.  <iframe src=\"https://hf.space/streamlit/jamescalam/ask-youtube/+\" data-src=\"https://hf.space/streamlit/jamescalam/ask-youtube/+\" data-sdk=\"streamlit\" title=\"Streamlit app\" style=\"width:100%;height:1000px;overflow: hidden;\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\" scrolling=\"yes\"></iframe>  You can test the app above or on the [homepage here](https://huggingface.co/spaces/jamescalam/ask-youtube). When querying again for `\"what is OpenAI's clip?\"` we can see that multiple results from a single video are merged. With this, we can jump to each segment by clicking on the part of the text that is most interesting to us.  Try a few more queries like:  ``` What is the best unsupervised method to train a sentence transformer?  What is vector search?  How can I train a sentence transformer with little-to-no data? ```  ---  We can build incredible speech-enabled search apps very quickly using Whisper alongside Hugging Face, sentence transformers, and Pinecone’s [vector database](https://www.pinecone.io/learn/vector-database).  Whisper has unlocked a entire modality — the spoken word — and it’s only a matter of time before we see a significant increase in speech-enabled search and other speech-centric use cases.  Both machine learning and vector search have seen exponential growth in the past years. These technologies already seem like sci-fi. Yet, despite the incredible performance of everything we used here, it's only a matter of time before all of this gets *even better*.  {{< newsletter text=\"Subscribe for more appled ML walkthroughs!\" inputText=\"Email address...\" buttonText=\"Submit\">}}  ## Resources  [All Code Notebooks](https://github.com/jamescalam/ask-youtube/tree/main/youtube-search)  * [MP3 Download](https://colab.research.google.com/github/jamescalam/ask-youtube/blob/main/youtube-search/00-download-videos.ipynb) * [Whisper Transcription](https://colab.research.google.com/github/jamescalam/ask-youtube/blob/main/youtube-search/01-openai-whisper.ipynb) * [Encode and Query](https://colab.research.google.com/github/jamescalam/ask-youtube/blob/main/youtube-search/02-build-embeddings.ipynb)  [Demo App](https://huggingface.co/spaces/jamescalam/ask-youtube)  [1] [OpenAI Whisper Repo](https://github.com/openai/whisper) (2022), GitHub ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e336"
  },
  "title": "\"Random Projection for Locality Sensitive Hashing\"",
  "headline": "\"Random Projection for Locality Sensitive Hashing\"",
  "- \"Faiss": "The Missing Manual\"",
  "weight": "4",
  "name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "Practical implementation of Random Projection LSH for approximate nearest neighbors search.",
  "images": "['/images/locality-sensitive-hashing-random-projection-8.jpeg']",
  "content": "![Similarity Search with Locality Sensitive Hashing with Random Projection](/images/locality-sensitive-hashing-random-projection-8.jpeg)  Locality sensitive hashing (LSH) is a widely popular technique used in *approximate* similarity search. The solution to efficient similarity search is a profitable one — it is at the core of several billion (and even trillion) dollar companies.  The problem with similarity search is *scale*. Many companies deal with millions-to-billions of data points every single day. Given a billion data points, is it feasible to compare all of them with every search?  Further, many companies are not performing single searches — Google deals with more than 3.8 million searches every *minute*<sup>[1]</sup>.  Billions of data points combined with high-frequency searches are problematic — and we haven’t considered the dimensionality nor the similarity function itself. Clearly, an exhaustive search across all data points is unrealistic for larger datasets.  The solution to searching impossibly huge datasets? *Approximate search.* Rather than *exhaustively* comparing every pair, we *approximate* — restricting the search scope only to high probability matches.  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/8bOrMqEdfiQ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ---  ## Locality Sensitive Hashing  [Locality Sensitive Hashing (LSH)](/learn/locality-sensitive-hashing/) is one of the most popular approximate nearest neighbors search (ANNS) methods.  At its core, it is a hashing function that allows us to group similar items into the same hash buckets. So, given an impossibly huge dataset — we run all of our items through the hashing function, sorting items into buckets.  Unlike most hashing functions, which aim to *minimize* hashing collisions — LSH algorithms aim to *maximize* hashing collisions.  ![Two hashing functions, the top (blue) **min**imizes hashing collisions. The bottom (magenta) maximizes hashing collisions — LSH aims to **max**imize collisions between similar items.](/images/locality-sensitive-hashing-random-projection-14.jpeg)<small>Two hashing functions, the top (blue) **min**imizes hashing collisions. The bottom (magenta) maximizes hashing collisions — LSH aims to **max**imize collisions between similar items.</small>  This result for LSH is that similar [vectors](/learn/vector-embeddings/) produce the same hash value and are bucketed together. In contrast, dissimilar vectors will *hopefully* not produce the same hash value — being placed in different buckets.  ### Search With LSH  Performing search with LSH consists of three steps:  1. Index all of our vectors into their hashed vectors.  1. Introduce our query vector (search term). It is hashed using the same LSH function.  1. Compare our hashed query vector to all other hash buckets via Hamming distance — identifying the nearest.  At a very high level, that is the process of the LSH methodology we will be covering. However, we will explain all of this in much more detail further in the article.  ### Effects of Approximation  Before diving into the detail of LSH, we should note that grouping vectors into lower resolution *hashed* vectors also means that our search is not exhaustive (e.g., comparing *every* vector), so we expect a lower search quality.  ![We compress potentially huge dense vectors into highly compressed, grouped binary vectors.](/images/locality-sensitive-hashing-random-projection-12.jpeg)*We compress potentially huge dense vectors into highly compressed, grouped binary vectors.*  But, the reason for accepting this lower search quality is the potential for *significantly* faster search speeds.  ### Which Method?  We’ve been purposely lax on the details here because there are several versions of LSH — each utilizing different hash building and distance/similarity metrics. The two most popular approaches are:  * Shingling, MinHashing, and banded LSH (traditional approach)  * Random hyperplanes with dot-product and Hamming distance  This article will focus on the random hyperplanes method , which is more commonly used and implemented in various popular libraries such as [Faiss](/learn/faiss/).  ---  ## Random Hyperplanes  The random hyperplanes (also called random *projection*) approach is deceptively simple — although it can be hard to find details on the method.  Let’s learn through an example — we will be using the Sift1M dataset throughout our examples, which you can download using [this script](https://gist.github.com/jamescalam/a09a16c17b677f2cf9c019114711f3bf).  Now, given a single query vector `xq` we want to identify the top `k` nearest neighbors from the `xb` array.  ![Here we are returning the **three** nearest neighbors to our query vector **xq**.](/images/locality-sensitive-hashing-random-projection-2.png)<small>Here we are returning the **three** nearest neighbors to our query vector **xq**.</small>  Using the random projection method, we will reduce our highly-dimensional vectors into low-dimensionality binary vectors. Once we have these binary vectors, we can measure the distance between them using Hamming distance.  Let’s work through that in a little more detail.  ### Creating Hyperplanes  The hyperplanes in this method are used to split our datapoints and assign a value of *0* for those data points that appear on the negative side of our hyperplane — and a value of *1* for those that appear on the positive side.  ![We assign a value of **1** to vectors on the +ve side of our hyperplane and a value of **0** to vectors on the -ve side of the hyperplane.](/images/locality-sensitive-hashing-random-projection-13.jpeg)<small>We assign a value of **1** to vectors on the +ve side of our hyperplane and a value of **0** to vectors on the -ve side of the hyperplane.</small>  To identify which side of the hyperplane our data point is located, all we need is the normal vector of the plane — e.g., a vector perpendicular to the plane. We feed this normal vector (alongside our datapoint vector) into a dot product function.  If two vectors share the same direction, the resultant dot product is positive. If they do *not* share the same direction, it is negative.  ![Where our hyperplane normal vector produces a +ve dot-product with another vector, we can view that vector as being in front of the hyperplane. The reverse is true for vectors that produce a -ve dot-product.](/images/locality-sensitive-hashing-random-projection-3.jpeg)<small>Where our hyperplane normal vector produces a +ve dot-product with another vector, we can view that vector as being in front of the hyperplane. The reverse is true for vectors that produce a -ve dot-product.</small>  *In the unlikely case of both vectors being perfectly perpendicular (sitting on the hyperplane edge), the dot product is 0 — we will group this in with our negative direction vectors.*  A single binary value doesn’t tell us much about the similarity of our vectors, but when we begin adding *more* hyperplanes — the amount of encoded information rapidly increases.  ![We add more hyperplanes to increase the amount of positional information stored in our binary vectors.](/images/locality-sensitive-hashing-random-projection-11.jpeg)<small>We add more hyperplanes to increase the amount of positional information stored in our binary vectors.</small>  By projecting our vectors into a lower-dimensional space using these hyperplanes, we produce our new *hashed* vectors.  In the image above, we have used two hyperplanes, and realistically we will need many more — a property that we define using the `nbits` parameter. We will discuss `nbits` in more detail later, but for now, we will use **four** hyperplanes by setting `nbits = 4`.  Now, let’s create the normal vectors of our hyperplanes in Python.  {{< notebook file=\"random-hyperplanes\" height=\"full\" >}}  Through `np.random.rand` we create a set of random values in the range *0* → *1*. We then add `-.5` to center our array values around the origin *(0, 0)*. Visualizing these vectors, we see:  ![The normal vectors that define the positions of our hyperplanes, which are all centered around the origin (0, 0).](/images/locality-sensitive-hashing-random-projection-10.jpeg)<small>The normal vectors that define the positions of our hyperplanes, which are all centered around the origin (0, 0).</small>  ### Hashing Vectors  Now let’s add three vectors — **a**, **b**, and **c** — and work through building our hash values using our four *normal vectors and their hyperplanes*.  {{< notebook file=\"creating-hashes\" height=\"full\" >}}  Visualizing this again, we have our three vectors **a**, **b**, and **c** — alongside our four hyperplanes (perpendicular to their respective normal vectors). Taking the +ve and -ve dot-product values for each gives us:  ![A **zero** shows that the vector is behind the plane (-ve dot product), and a **one** shows that the vector is in front of the plane (+ve dot product). We combine these to create our binary vectors.](/images/locality-sensitive-hashing-random-projection-6.jpeg)<small>A **zero** shows that the vector is behind the plane (-ve dot product), and a **one** shows that the vector is in front of the plane (+ve dot product). We combine these to create our binary vectors.</small>  Which produces our hashed vectors. Now, LSH uses these values to create buckets — which will contain some reference back to our vectors (Eg their IDs). Note that we do not store the original vectors in the buckets — which would significantly increase the size of our LSH index.  As we will see with implementations such as [Faiss](/learn/faiss-tutorial/) — the position/order that we added the vector is usually stored. We will use this same approach in our examples.  {{< notebook file=\"buckets\" height=\"full\" >}}  Now that we have bucketed our three vectors, let’s consider the change in the complexity of our search. Let’s say we introduce a query vector that gets hashed as `0111`.  With this vector, we compare it to every bucket in our LSH index — which in this case is only two values — `1000` and `0110`. We then use Hamming distance to find the closest match, and this happens to be `0110`.  ![Hamming distance, there are **four** mismatches between the first two vectors — resulting in a Hamming distance of four. The next two contain just **one** mismatch, giving a Hamming distance of one.](/images/locality-sensitive-hashing-random-projection-9.jpeg)<small>Hamming distance, there are **four** mismatches between the first two vectors — resulting in a Hamming distance of four. The next two contain just **one** mismatch, giving a Hamming distance of one.</small>  We have taken a linear complexity function, which required us to compute the distance between our query vector and all of the previously indexed vectors — to a sub-linear complexity — as we no longer need to compute the distance for *every* vector — because they’re grouped into buckets.  Now, at the same time, vectors **1** and **2** are both equal to `0110`. So we cannot possibly find which of those is closest to our query vector. This means there is a degree of search quality being lost — however, this is simply the cost of performing an approximate search. *We trade quality for speed.*  ---  ## Balancing Quality vs. Speed  As is often the case in [similarity search](/learn/what-is-similarity-search/), a good LSH index requires balancing search quality vs. search speed.  We saw in our mini-example that our vectors were not easy to differentiate, because out of a total of three vectors — random projection had hashed two of them to the same binary vector.  Now imagine we scale this same ratio to a dataset containing one million vectors. We introduce our query vector `xq` — hash it and calculate the hamming distance between that (`0111`) and our *two* buckets (`1000` and `0110`).  Wow, a million sample dataset searched with just two distance calculations? That’s *fast*.  Unfortunately, we return around 700K samples, all with the same binary vector value of `0110`. Fast yes, accurate — *not at all*.  Now, in reality, using an `nbits` value of `4` would produce **16** buckets:  {{< notebook file=\"binary\" height=\"full\" >}}  We stuck with the two buckets of `1000` and `0110` solely for dramatic effect. But even with *16* buckets — 1M vectors split into just 16 buckets still produces massively imprecise buckets.  In reality, we use many more hyperplanes — more hyperplanes mean higher resolution binary vectors — producing much more precise representations of our vectors.  We control this resolution through hyperplanes with the nbits value. A higher `nbits` value improves search quality by increasing the resolution of hashed vectors.  ![Increasing the **nbits** parameter increases the number of hyperplanes used to build the binary vector representations.](/images/locality-sensitive-hashing-random-projection-4.jpeg)<small>Increasing the **nbits** parameter increases the number of hyperplanes used to build the binary vector representations.</small>  Adding more possible combinations of hash values increases the potential number of buckets , increasing the number of comparisons and, therefore, *search time*.  {{< notebook file=\"nbits-and-buckets\" height=\"full\" >}}  It’s worth noting that not *all* buckets will necessarily be used — particularly with higher `nbits` values. We will see through our Faiss implementation that a `nbits` value of `128` or more is completely valid and still faster than using a [flat index](/learn/vector-indexes/).  There is also this [notebook that covers a simple implementation of LSH](https://github.com/pinecone-io/examples/blob/master/locality_sensitive_hashing_random_projection/random_projection.ipynb) in Python.  ---  ## LSH in Faiss  We have [discussed Faiss before](/learn/faiss-tutorial/), but let’s briefly recap. Faiss — or Facebook AI Similarity Search — is an open-source framework built for enabling similarity search.  <p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/ZLfdQq_u7Eo\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>  Faiss has many super-efficient implementations of [different indexes](/learn/vector-indexes/) that we can use in similarity search. That long list of indexes includes `IndexLSH` — an easy-to-use implementation of everything we have covered so far.  We initialize our LSH index and add our Sift1M dataset `wb` like so:  {{< notebook file=\"lsh-init\" height=\"full\" >}}  Once our index is ready, we can begin searching using `index.search(xq, k)` — where `xq` is our one-or-more query vectors, and `k` is the number of nearest matches we’d like to return.  {{< notebook file=\"search\" height=\"full\" >}}  The `search` method returns two arrays. Index positions `I` (e.g., row numbers from `wb`) of our `k` best matches. And the distances `D` between those best matches and our query vector `xq0`.  ### Measuring Performance  Because we have those index positions in `I`, we can retrieve the *original vectors* from our array `wb`.  {{< notebook file=\"cosine-sim\" height=\"full\" >}}  And from those original vectors, we see if our LSH index is returning relevant results. We do this by measuring the *cosine similarity* between our query vector `xq0` and the top `k` matches.  There are vectors in this index that should return similarity scores of around 0.8. We’re returning vectors with similarity scores of just 0.2 — why do we see such poor performance?  ### Diagnosing Performance Issues  We know that the `nbits` value controls the number of *potential* buckets in our index. When initializing this index, we set `nbits == 4` — so all of our vectors must be stored across 4-digit, low-resolution buckets.   If we try to cram 1M vectors into just 16 hash buckets, each of those buckets very likely contain 10–100K+ vectors.  So, when we hash our search query, it matches one of these 16 buckets perfectly — but the index cannot differentiate between the huge number of vectors crammed into that single bucket — they all have the same hashed vector!  We can confirm this by checking our distances `D`:  {{< notebook file=\"get-D\" height=\"full\" >}}  We return a perfect distance score of *zero* for every item — but why? Well, we know that the Hamming distance can only be *zero* for *perfect* matches — meaning all of these hashed vectors must be identical.  If all of these vectors return a perfect match, they must all have the same hash value. Therefore, our index cannot differentiate between them — they all share the same position as far as our LSH index is concerned.  Now, if we were to increase `k` until we return a non-zero distance value, we should be able to infer the number of vectors that have been bucketed with this same hash code. Let’s go ahead and try it.  {{< notebook file=\"D\" height=\"full\" >}}  A single bucket containing `172_039` vectors. That means that we are choosing our top `k` values at random from those 172K vectors. Clearly, we need to reduce our bucket size.  With 1M samples, which `nbits` value gives us enough buckets for a more sparse distribution of vectors? It’s not possible to calculate the exact distribution, but we can take an average:  {{< notebook file=\"nbits-1m-avg\" height=\"full\" >}}  With an `nbits` value of 16 we’re still getting an average of `15.25` vectors within each bucket — which seems better than it is. We must consider that some buckets will be significantly larger than others, as different regions will contain more vectors than others.  Realistically, the `nbits` values of `24` and `32` may be our tipping point towards genuinely effective bucket sizes. Let’s find the `mean` cosine similarity for each of these values.  {{< notebook file=\"nbits-tipping-point\" height=\"full\" >}}  It looks like our estimate is correct — the overall similarity for our top 100 vectors experiences a sudden with each nbits increment before leveling off at the `nbits == 24` point. But what if we run the process with even larger `nbits` values?  ![As we increase vector resolution with **nbits**, our results will become more precise — here, we can see that a larger **nbits** value results in higher cosine similarity in our results.](/images/locality-sensitive-hashing-random-projection-7.jpeg)<small>As we increase vector resolution with **nbits**, our results will become more precise — here, we can see that a larger **nbits** value results in higher cosine similarity in our results.</small>  Here, the results are apparent. A fast increase in similarity as we declutter the LSH buckets — followed by a slower increase in similarity.  The latter, slower increase in similarity is thanks to the increasing resolution of our hashed vectors. Our buckets are already very sparse — we have many more *potential* buckets than we have vectors , so we can find little to no performance increase there.  *But* — we are increasing the resolution and, *therefore, the precision* of those buckets, so we pull the additional performance from here.  ### Extracting The Binary Vectors  While investigating our index and the distribution of vectors across our buckets above, we inferred the problem of our bucket sizes. This is useful as we’re using what we have already learned about LSH and can see the effects of the indexes' properties.  However, we can take a more direct approach. Faiss allows us to *indirectly* view our buckets by extracting the binary vectors representations of `wb`.  Let’s revert to our `nbits` value of **4** and see what is stored in our LSH index.  {{< notebook file=\"extract-binary\" height=\"full\" >}}  From this, we can visualize the distribution of vectors across those 16 buckets — showing the most used buckets and a few that are empty.  ![Distribution of vectors in different buckets when **nbits == 4**.](/images/locality-sensitive-hashing-random-projection-1.jpeg)<small>Distribution of vectors in different buckets when **nbits == 4**.</small>  This and our previous logic are all we need to diagnose the aforementioned bucketing issues.  ---  ## Where to Use LSH  While LSH can be a swift index, it is less accurate than a Flat index. Using increasingly larger portions of our Sift1M dataset, the best recall score was achieved using an `nbits` value of `768` (better recall is possible at excessive search times).  ![Recall against the number of indexed vectors. The recall is measured as the % of matches with exhaustive search results (using **IndexFlatL2**).](/images/locality-sensitive-hashing-random-projection-15.jpeg)<small>Recall against the number of indexed vectors. The recall is measured as the % of matches with exhaustive search results (using **IndexFlatL2**).</small>  Although it’s worth noting that using an `nbits` value of `768` only returns marginally faster results than if using a flat index.  ![Search time as a factor of search time for IndexFlatL2 at different index sizes and using various **nbits** values.](/images/locality-sensitive-hashing-random-projection-5.jpeg)<small>Search time as a factor of search time for IndexFlatL2 at different index sizes and using various **nbits** values.</small>  More realistic recall rates — while maintaining a reasonable speed increase — are closer to 40%.  However, varying dataset sizes and dimensionality can make a huge difference. An increase in dimensionality means a higher `nbits` value must be used to maintain accuracy, but this can still enable faster search speeds. It is simply a case of finding the right balance for each use-case and dataset.  Now, of course, there are many options out there for vector similarity search. Flat indexes and LSH are just two of many options — and choosing the right index is a mix of experimentation and know-how.  As always, similarity search is a balancing act between different indexes and parameters to find the best solutions for our use-cases.  ---  We’ve covered *a lot* on LSH in this article. Hopefully, this has helped clear up any confusion on one of the biggest algorithms in the world of search.  LSH is a complex topic, with [many different approaches](/learn/locality-sensitive-hashing/) — and even more implementations available across several libraries.  Beyond LSH, we have many more algorithms suited for efficient similarity search, such as [HNSW](/learn/hnsw/), IVF, and [PQ](/learn/product-quantization). You can learn more in our [overview of vector search indexes](/learn/vector-indexes/).  {{< newsletter text=\"Subscribe for updates on similarity search!\" inputText=\"Email address...\" buttonText=\"Submit\">}}  ## References  * [Jupyter Notebooks](https://github.com/pinecone-io/examples/tree/master/locality_sensitive_hashing_random_projection) * [1] [Google Searches](https://skai.io/monday-morning-metrics-daily-searches-on-google-and-other-google-facts/), Skai.io Blog ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e338"
  },
  "title": "Inside the Pinecone",
  "headline": "Inside the Pinecone",
  "name": "Edo Liberty",
  "position": "Founder and CEO",
  "src": "/images/company-edo.png",
  "href": "https://edoliberty.github.io/",
  "date": "\"2022-08-22\"",
  "thumbnail": "\"/images/inside-the-pinecone.jpg\"",
  "description": "Edo Liberty, Pinecone CEO, provides a glimpse into the journey behind building a database company, including some insights into the latest innovations around the product and vector search.",
  "images": "[\"/images/inside-the-pinecone.jpg\"]",
  "content": "![Inside the Pinecone](/images/inside-the-pinecone.jpg)  Last week we [announced](/learn/faster-easier-scalable/) a major [update](https://www.pinecone.io/docs/release-notes/#august-16-2022). The incredible work that led to the launch and the reaction from our users — a combination of delight and curiosity — inspired me to write this post. This is a glimpse into the journey of building a database company up to this point, some of the internal debates and decisions we made along the way (bad and good alike), and on-going innovation at the core of Pinecone that I find exciting.  ## In-memory graph-based index  In May 2010, I wrote Professor [Yuval Rabani](https://scholar.google.com/citations?user=vTigGywAAAAJ&hl=en) an email. Yuval is one of the inventors of approximate nearest neighbor (ANN) search as a concept. I suggested we work together on answering a seemingly simple question:   _“Why are graph based data structures so good at vector search?”_  You see, they were much better in practice than the theory predicted they should be. This hinted that we were lacking some fundamental understanding of either [vector search](/learn/vector-search-basics/) or graph based indexes (or both). We decided to analyze the behavior of vector search with graphs using mathematical machinery from a different domain. Specifically, those spearheaded by [Dan Speilman](http://www.cs.yale.edu/homes/spielman/) for bounding the running time of solving systems of linear equations (for the CS theoreticians, think about max-dot product search and its similarity to the simplex method, and it will make more sense). We made some progress with this approach, but never quite got a full answer. Graph based vector search still seemed to be “too good”.  Fast forward twelve years, graph based techniques now take center stage for vector search software. These include [HNSW](https://www.pinecone.io/learn/hnsw/) by [Yuri Malkov](https://www.linkedin.com/in/yury-malkov-b6597382/) (an advisor to Pinecone), implementations in [Faiss](https://www.pinecone.io/learn/faiss/), and a flurry of further research and software progress. It was therefore only natural for Pinecone to consider using graph indexes for its vector database.  Bringing graph based search into a production-grade database, however, was not going to be simple. As a managed database, we have to make sure our systems always work and without the user tweaking and experimenting with different configurations. We could not afford to have best-effort-heuristics and magic-constants floating around. We also knew we needed to efficiently combine metadata filtering and graph traversal, optimize updates and deletes, and other functionality which HNSW or Faiss do not provide.  In the end, we did it. It took many months of work by Pinecone’s technical leaders [Marek Galovic](https://www.linkedin.com/in/marek-galovic/?originalSubdomain=cz), [Roei Mutay](https://www.linkedin.com/in/roei-mutay-bb3743200/?originalSubdomain=il), and [Ram Sriharsha](https://www.linkedin.com/in/harsha340/) along with Yuri Malkov. We now have a high-performance, stable, fully dynamic, and filterable graph index!   However, my curiosity from twelve years ago hasn’t diminished one bit. There is still so much we don’t know. Under what conditions can we guarantee the convergence of the search method? Can we check in advance what datasets are good for graph based indexes? Can we reliably prune graphs and keep their search effectiveness? I truly hope to be able to make some progress on these questions soon.  ## A new storage engine for vectors  Because Pinecone is a managed vector database and not just an index, we need to take care of storage and persistence of the data in every index. We also need to constantly update and fetch thousands of raw vectors in milliseconds. Think about batch updates (upserts), raw vector-metadata fetch operations, and most importantly, raw *exact* comparisons between queries and candidate matches suggested by the ANN vector search index. We need an object storage engine.   We originally chose [RocksDB](http://rocksdb.org/) for that. RocksDB is commonly used as an embedded layer in other database offerings. And, developing a completely new storage engine seemed like an insurmountable effort; the specter of data-loss looms big in our collective psyche.   We were very happy with our decision. We loved RocksDB. It is a great piece of software. It was 100% the right choice, until it wasn’t…  When we tried to vertically scale our pods, we started hitting RocksDB limits. Write throughput became a big issue. Simultaneous fetches of thousands (sometimes tens of thousands) of objects started becoming inefficient, especially when fetching from collections of tens of millions of objects. The fetch latency variance also grew. Higher p99 latencies started leaking into our p50s when operating horizontally distributed indexes. Write amplification started really hurting our pods’ capacities… To make matters worse, we were also spending a ton of time and energy grappling with RocksDB trying to maximize our performance.  Late last year, our VP of Engineering, [Ram Sriharsha](https://www.linkedin.com/in/harsha340/), came up with a brillant [bitcask-like](https://en.wikipedia.org/wiki/Bitcask) design for a completely new vector store that is optimized for random multigets and updates, low fetch variance, minimal storage overhead, and maximal operational simplicity.  [Marek](https://www.linkedin.com/in/marek-galovic/?originalSubdomain=cz) and Ram took on the herculean challenge to build the initial new vector store (internally nicknamed memkey). The resulting object store is up to 10x faster than RocksDB on large datasets. It reduces our overall operating costs 30-50%. We passed these improvements on to our customers when we [updated p1 and s1 pods to use the new vector store](https://www.pinecone.io/docs/release-notes/#improved-p1-and-s1-performance). Customers will seamlessly get more capacity and lower latency without changing a single line of code on their end. How cool is that?  ![Multiget latency](/images/multiget-latency.png) <small>Memkey’s low (and flat!) multiget latency as collections grow compared to RocksDB. On the y-axis: Multiget times in milliseconds for fetching 1000 random objects (vectors) from local SSD. On the x-axis: Collection size being 5m, 10mm, 20m, and 40mm.</small>  ## Rust: A hard decision pays off  Although this change was implemented inside Pinecone a while ago, this is the first time we’re talking about it publically.   In the beginning, Pinecone was built with a mix of C/C++ and Python. The argument for this went something like this: a C/C++ core will be highly efficient, and experienced C/C++ engineers are easy to find. Anything not at the core doesn’t have to be efficient. It can therefore be easily and quickly delivered in Python, which sacrifices running speed with easy development. It was, indeed, relatively easy to develop, and we got some impressive results pretty quickly. So, that seemed to be the winning formula. We went for it full tilt.   Not until a few weeks before a big release last year did the number of issues we had to fix begin to pile up. We kept fixing one issue only to discover (or create) another. Every few hours we would find some major bug or oversight in our codebase; and each cycle to fix and redeploy took hours of work. To make matters worse, we would discover issues only after deploying (or in production!) due to Python’s run time nature. Sometimes these would be minor bugs that any compiler would have easily flagged, and sometimes they were complex runtime issues which were almost impossible to reproduce or isolate.  That’s when internal murmurs about a complete rewrite started brewing…   I personally vehemently resisted the idea. Rewrites are notoriously dangerous. They feel exciting at first, but often turn sour. First, they always end up being much bigger projects than you plan for. This is especially dangerous in a startup where a significant delay in shipping new features could be your undoing. Second, rewrites often backfire in spectacular ways; the new codebase often produces completely new challenges, often much worse than those you originally had.  Nevertheless, we reached a tipping point. We decided to move our entire codebase to Rust (and Go for the k8s control plane). Rust seemed to give us all the capabilities we needed, however, there was still one *minor* problem -  no one on the team knew Rust. It was (and still is) a hard-to-master, young language with a much smaller community than either C++ or Python.  We started with a small team of senior engineers and managers learning Rust and developing the skeleton of the DB and dev environment (for others to build on). Then, slowly, others joined in rewriting and contributing different components until we eventually got rid of the old codebase altogether (I still remember the day my original C modules, from the first days of Pinecone, were taken out). Unbeknownst to most Pinecone customers, the new Rust core was deployed in March this year. And in the process of taking over running workloads, we managed not to drop a single API call!  So, what did we learn? We all expect performance and dev processes to improve. Those indeed happened. What we didn’t expect was the extent to which dev velocity increased and operational incidents decreased. Dev velocity, which was supposed to be the claim to fame of Python, improved dramatically with Rust. Built-in testing, CI/CD, benchmarking, and an overzealous compiler increased engineers’ confidence in pushing changes, and enabled them to work on the same code sections and contribute simultaneously without breaking the code base. Most impressively though, real time operational events dropped almost to zero overnight after the original release. Sure, there are still surprises here and there but, by and large, the core engine has been shockingly stable and predictable.  ---  *Note: Interested in hearing more about our Rust rewrite? Don't miss [our talk at the NYC Rust meetup on August 31](https://www.meetup.com/rust-nyc/events/287821884/), and subscribe for updates at the bottom of this page to get notified when we post the full writeup.*  ---   ## Closing thoughts  While I could keep rambling on about more exciting developments within the space like vertical scaling, data collections, more advanced monitoring, and others, those will have to wait until the next post.  If building the best vector database in the world sounds exciting, give me a ping. [We are hiring](https://www.pinecone.io/careers/) on all fronts. Also, if you are building on Pinecone and missing a feature or experiencing something other than expected, please don’t be shy and write to me directly (edo@pinecone.io). ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e33a"
  },
  "title": "\"Vector search just got up to 10x faster, easier to set up, and vertically scalable\"",
  "headline": "\"Vector search just got up to 10x faster, easier to set up, and vertically scalable\"",
  "name": "Gibbs Cullen",
  "position": "Senior Product Marketing Manager",
  "src": "/images/gibbs-cullen.jpg",
  "href": "https://www.linkedin.com/in/gibbscullen/",
  "date": "\"2022-08-16\"",
  "# Date": "August 8, 2022",
  "description": "Announcing new features and performance improvements to make it easier and more cost-effective than ever for engineers to start and scale a vector database in production.",
  "images": "[\"/images/10x-faster-launch-resized.png\"]",
  "thumbnail": "\"/images/10x-faster-launch-thumb.png\"",
  "content": "**Pinecone is paving the way for developers to easily start and scale with vector search.**  We created the first [vector database](/learn/vector-database/) to make it easy for engineers to build fast and scalable vector search into their cloud applications. In the past year, hundreds of companies like Gong, Clubhouse, and Expel added capabilities like semantic search, AI recommendations, image search, and AI threat detection to their applications using vector search with Pinecone.  Yet for teams who are new to [vector search](/learn/vector-search-basics/), some challenges remained: * It was hard to determine the type and size of index needed for their data and performance needs. * Supporting high throughput required a lot of hardware, which might’ve been cost-prohibitive. * Scaling up indexes meant re-uploading data to a new index and interrupting service for the switch-over.  Not anymore. Today we’re excited to announce new features and performance improvements to Pinecone that make it easier and more cost-effective than ever for engineers to start and scale a vector database in production.  ## What’s new   As of today, these new features are available in Pinecone for all Standard, Enterprise, and Enterprise Dedicated users:   * **Vertical Scaling**: Scale your vector database with zero downtime * **Collections**: Centrally store and reuse your vector embeddings and metadata to experiment with different index types and sizes. * **p2 pods**: Achieve up to 10x better performance for high-traffic applications.  We are also announcing: * Around **50% faster queries** on p1 and s1 pods (varies by use case). * **5x greater capacity** on the Starter (free) plan, with the storage-optimized s1 pod now available on the plan. * **Updated pricing** that will go into effect for new users starting September 1st, but not for existing customers.  Continue reading for more details, then [get started today](https://app.pinecone.io). Also [register for our upcoming demo](https://pinecone-io.zoom.us/webinar/register/WN_mlCKGFF_QoOKWW1UNKZtjA) and [read the hands-on walkthrough](/learn/testing-p2-collections-scaling/) of these new features.  ## Vertical Scaling: Scale index capacity with zero downtime  > *“Vertical scaling means no more migrating to bigger indexes or writing to an index already at storage capacity. This is going to be a huge timesaver for us.”* > *&mdash; Isabella Fulford, Software Engineer at Mem Labs*  The volume of data that a Pinecone index can hold is limited by the number of pods the index is running on. Previously, if your index grew beyond the available capacity you would need to create a new index with more pods, re-upload data to that index, then switch over traffic to the new index, or overprovision the number of pods and pay for unused capacity.  Either way, valuable resources — spend and engineering time — are taken away from more impactful areas of your business.   With vertical scaling, pod capacities can be doubled for a live index with zero downtime. Pods are now available in different sizes —  x1, x2, x4, and x8 — so you can start with the exact capacity you need and easily scale your index. Your hourly cost for pods will change to match the new sizes, meaning you still only pay for what you use. See [documentation](https://www.pinecone.io/docs/manage-indexes/#changing-pod-sizes) for more detail.  ## Collections: Experiment with and store vector data in one place  ![Collections in Pinecone](/images/collections.png)  Users rely on Pinecone indexes to store their vector embeddings and associated metadata; they want it to be their source of truth. Before the addition of collections, actions such as temporarily shutting down or creating a new index would require re-uploading the original vector data from a different source. That meant users had to maintain an up-to-date copy of the data outside of Pinecone. Collections will alleviate this pain by providing users with a single source of truth for their vector data within Pinecone.  Today, we are launching the public preview of collections in all Pinecone environments. Users can save data (i.e. vectors and metadata) from an index as a snapshot, and create new indexes from any collection. Whether using collections for backing up and restoring indexes, testing different index types with the same data, or moving data to a new index, users can now do it all within Pinecone. In the near future, collections will allow users to import and export data to and from S3 or GCS blob storage, and write streaming and bulk data directly to collections.  Storage costs for collections will be $0.025/GB per month for all Standard, Enterprise, and Enterprise Dedicated users. Users on the Starter (free) plan can have one collection at no cost. See [documentation](https://www.pinecone.io/docs/collections/) for more detail.   ## p2 Pods: Purpose built for performance and high-throughput use cases   While p1 pods provide low search latencies with uniquely fast ingestion speeds, high recall (accuracy of results), and fast filtering, users with high-throughput applications such as social media apps or streaming services require much higher throughput above all else.  The new p2 pod type provides blazing fast search speeds under 10ms and throughput up to 200 queries per second (QPS) per replica.* That’s up to 10x lower latencies and higher throughput than the p1 pod type. It achieves this with a new graph-based index that trades off ingestion speed, filter performance, and recall in exchange for higher throughput. It still supports filtering, live index updates, and all other index operations.   Today, we are launching p2 pods as a public preview available in all Pinecone environments. If you currently use p1 pods with multiple replicas to achieve a high throughput, switching to p2 pods may dramatically lower your total costs. See [documentation](https://www.pinecone.io/docs/indexes/#p2-pods) for more detail.  <em>\\* Your performance may vary and we encourage you to test with your own data and follow our [tips for performance tuning](https://www.pinecone.io/docs/performance-tuning/). Latencies are dependent on vector dimensionality, metadata size, metadata cardinality, network connection, cloud provider, and other factors.</em>  ## Other updates  ### Faster indexes on s1 and p1 pods Query speed and performance are only becoming more and more critical for vector search applications, especially for those consumer facing features. That’s why we significantly improved performance for s1 and p1 pods. Users of these pods will now achieve on average 50% lower latency and 50% higher throughput per replica.  The specific performance gain depends on your workload, so you might see a higher or lower difference than this. This change is in effect for all new indexes starting today, and will be rolled out to existing indexes in the coming weeks.   ### Starter plan to include s1 pods Getting started with Pinecone is even easier with the addition of s1 pods to our Starter (free) plan. Previously, only p1 pods were available on the Starter plan.  As of today, users can choose between p1 and s1 pods and store 5x more vectors than before. This enhancement gives users the flexibility to more easily experiment with and fully realize the power of vector search with Pinecone.    ### Pricing update The features and enhancements announced today provide meaningful cost saving opportunities for all existing and new Pinecone customers, notably:   * s1 and p1 pods now support 50% higher throughput for typical workloads, meaning fewer replicas are needed. * The new p2 pods, while being more expensive on a per pod-hour basis, provide up to 10x greater throughput, meaning even fewer replicas are needed. * Vertical scaling eliminates the need to overprovision at the start. * Collections allow users to delete (and not be billed for) indexes when they’re not being used without losing data. * Starter users can now store 5x more vectors for free with s1 pods.  Updated pricing for p1 and s1 pods will also go into effect for all new users as of September 1, 2022, starting at $0.096/hour and up depending on plan, pod size, and cloud environment.  Existing users on a paid plan with a running index as of August 31, 2022 will not be affected, and will retain their current rates for s1 and p1.  This means anyone not yet on a paid plan can lock in current rates by upgrading and launching at least a one-pod index by August 31, 2022. See the [pricing page](/pricing/) for more details or [contact us](/contact/) with questions.   ## Get Started  Today’s releases make it even easier for engineers to start, test, and scale vector databases with greater flexibility, lower cost, and better performance. Get started today by [launching your first vector database](https://app.pinecone.io), [contacting us](/contact/) for more information, or [registering for the upcoming demo](https://pinecone-io.zoom.us/webinar/register/WN_mlCKGFF_QoOKWW1UNKZtjA) of these new features.    ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e33c"
  },
  "title": "\"Plagiarism Detection Using Transformers\"",
  "headline": "\"Plagiarism Detection Using Transformers\"",
  "weight": "5",
  "name": "Zoumana Keita",
  "position": "Data Scientist",
  "src": "/images/zoumana-keita.jpg",
  "href": "\"https://www.linkedin.com/in/zoumana-keita/\"",
  "description": "A complete guide to building a more robust plagiarism detector using transformer models.",
  "images": "[\"/images/plagiarism-detection-0.jpg\"]",
  "content": "Plagiarism is one of the biggest issues in many industries, especially in academia. This phenomenon worsened with the rise of the internet and open information, where anyone can access any information at a click about a specific topic.   Based on this observation, researchers have been trying to tackle the issue using different text analysis approaches. In this article, we will tackle two main limitations of plagiarism detection tools: (1) _content paraphrasing plagiarism_ and (2) _content translation plagiarism_.   **_(1) Rephrased contents can be difficult to capture by traditional tools_** because they do not take into consideration synonyms and antonyms of the overall context.  **_(2) Contents written in a language different from the original one_** are also a big issue faced by even the most advanced machine learning-based tools since the context is being completely shifted to another language.  In this conceptual blog, we will explain how to use transformer-based models to tackle these two challenges in an innovative way. First of all, we will walk you through the analytical approach describing the entire workflow, from data collection to performance analysis. Then, we will dive into the scientific/technical implementation of the solution before showing the final results.  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/_PqHRH55hV0\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ## Problem statement  Imagine you are interested in building a scholarly content management platform. You might want to only accept articles not shared on your platform. In this case, your goal will be to reject all new articles that are similar to existing ones at a certain threshold.   To illustrate this scenario, we will use the [cord-19 dataset](https://www.kaggle.com/datasets/allen-institute-for-ai/CORD-19-research-challenge), which is an open research challenge data made freely available on Kaggle by the [Allen Institute for AI](https://allenai.org/).  ## Analytical Approach  Before going further with the analysis, let’s clarify what we are trying to achieve here from the following question:   _Problem: Can we find within our dataset one or more documents that are similar (at a certain threshold) to a new submitted document?_  The following workflow highlights all the main steps required to better answer this question.  ![Plagiarism detection workflow](/images/plagiarism-detection-1.png)  _Let’s understand what is happening here 💡._  After collecting the source data, we start by preprocessing the content, then create a vector index from BERT.   Then, whenever we have a new incoming document, we check the language and perform plagiarism detection. More details are given later in the article.  ## Scientific Implementation   This section is focused on the technical implementation of each component in the approach.  ### Data preprocessing  We are only interested in the **_abstract_** column of the source data, and also, for simplicity’s sake, we will use only 100 observations to speed up the preprocessing.   ```python import pandas as pd   def preprocess_data(data_path, sample_size):     # Read the data from specific path   data = pd.read_csv(data_path, low_memory=False)    # Drop articles without Abstract   data = data.dropna(subset = ['abstract']).reset_index(drop = True)    # Get \"sample_size\" random articles   data = data.sample(sample_size)[['abstract']]    return data   # Read data & preprocess it data_path = \"./data/cord19_source_data.csv\" ```  Below are the five random observations from the source data set.  ![Five random observations](/images/plagiarism-detection-2.png)  ### Document vectorizer   ![Document vectorizer](/images/plagiarism-detection-3.png)  The challenges observed in the introduction lead to respectively choosing the following two transformer-based models:  **_(1) A BERT model_**: to solve the first limitation because it provides a better contextual representation of textual information. To do so, we will have:  - `create_vector_from_text`: used to generate the vector representation of a single document. - `create_vector_index`: responsible for creating an index containing for each document the corresponding vector.  ```python # Useful libraries import numpy as np import torch from keras.preprocessing.sequence import pad_sequences from transformers import BertTokenizer,  AutoModelForSequenceClassification   # Load bert model model_path = \"bert-base-uncased\" tokenizer = BertTokenizer.from_pretrained(model_path,                                          do_lower_case=True) model = AutoModelForSequenceClassification.from_pretrained(model_path,                                                          output_attentions=False,                                                          output_hidden_states=True)                                                          device = \"cuda\" if torch.cuda.is_available() else \"cpu\" model.to(device)   def create_vector_from_text(tokenizer, model, text, MAX_LEN = 510):        input_ids = tokenizer.encode(                         text,                         add_special_tokens = True,                         max_length = MAX_LEN,                               )        results = pad_sequences([input_ids], maxlen=MAX_LEN, dtype=\"long\",                               truncating=\"post\", padding=\"post\")     # Remove the outer list.     input_ids = results[0]     # Create attention masks        attention_mask = [int(i>0) for i in input_ids]     # Convert to tensors.     input_ids = torch.tensor(input_ids)     attention_mask = torch.tensor(attention_mask)     # Add an extra dimension for the \"batch\" (even though there is only one     # input in this batch.)     input_ids = input_ids.unsqueeze(0)     attention_mask = attention_mask.unsqueeze(0)     # Put the model in \"evaluation\" mode, meaning feed-forward operation.     model.eval()     # Run the text through BERT, and collect all of the hidden states produced     # from all 12 layers.     with torch.no_grad():                logits, encoded_layers = model(                                     input_ids = input_ids,                                     token_type_ids = None,                                     attention_mask = attention_mask,                                     return_dict=False)      layer_i = 12 # The last BERT layer before the classifier.     batch_i = 0 # Only one input in the batch.     token_i = 0 # The first token, corresponding to [CLS]            # Extract the vector.     vector = encoded_layers[layer_i][batch_i][token_i]     # Move to the CPU and convert to numpy ndarray.     vector = vector.detach().cpu().numpy()     return(vector)   def create_vector_index(data):       # The list of all the vectors    vectors = []       # Get overall text data    source_data = data.abstract.values       # Loop over all the comment and get the embeddings    for text in tqdm(source_data):               # Get the embedding        vector = create_vector_from_text(tokenizer, model, text)               #add it to the list        vectors.append(vector)       data[\"vectors\"] = vectors    data[\"vectors\"] = data[\"vectors\"].apply(lambda emb: np.array(emb))    data[\"vectors\"] = data[\"vectors\"].apply(lambda emb: emb.reshape(1, -1))    return data # Create the vector index vector_index = create_vector_index(source_data) vector_index.sample(5) ```  The last line of the code shows the following five random observations from the vector index, with the new vectors column.  ![New column](/images/plagiarism-detection-4.png)  **_2) A Machine Translation_** transformer model is used to translate the language of the incoming document into English because the source documents are in English in our case. The translation is performed only if the document’s language is one of the following five: _German, French, Japanese, Greek, and Russian_. Below is the helper function to implement this logic using the [MarianMT model](https://huggingface.co/docs/transformers/model_doc/marian).  ```python from langdetect import detect, DetectorFactory DetectorFactory.seed = 0 def translate_text(text, text_lang, target_lang='en'):   # Get the name of the model   model_name = f\"Helsinki-NLP/opus-mt-{text_lang}-{target_lang}\"   # Get the tokenizer   tokenizer = MarianTokenizer.from_pretrained(model_name)   # Instantiate the model   model = MarianMTModel.from_pretrained(model_name)     # Translation of the text   formated_text = \">>{}<< {}\".format(text_lang, text)   translation = model.generate(**tokenizer([formated_text],                                 return_tensors=\"pt\", padding=True))   translated_text = [tokenizer.decode(t, skip_special_tokens=True) for t in       translation][0]     return translated_text ```  ## Plagiarism analyzer  There is plagiarism when the incoming document’s vector is similar to one of the index vectors at a certain threshold level.  ![Plagiarism analyzer](/images/plagiarism-detection-5.png)  **_But, when are two vectors similar?_**  **_→ When they have the same magnitude and direction._**  This definition requires our vectors to have the same magnitude, which can be an issue because the dimension of a document vector depends on the length of that document. Luckily, we have multiple similarity measure approaches that can be used to overcome this issue, and one of them is the cosine similarity, which will be used in our case.   (If you are interested in other approaches, you can refer to this [semantic search overview](https://www.pinecone.io/learn/semantic-search/). It explains how each approach works and its benefits, with guides through their implementation.)  The plagiarism analysis is performed using the **_run_plagiarism_analysis_** function. We start by checking the document language using the check_incoming_document function to perform the right translation when required.  The final result is a dictionary with four main values:   - `similarity_score`: the score between the incoming article and the most similar existing article in the index.  - `is_plagiarism`: the value is true whether the similarity score is equal to or beyond the threshold. It is false otherwise. - `most_similar_article`: the textual information of the most similar article.  - `article_submitted`: the article that was submitted for approval.  ```python def process_document(text):   \"\"\"   Create a vector for given text and adjust it for cosine similarity search   \"\"\"   text_vect = create_vector_from_text(tokenizer, model, text)   text_vect = np.array(text_vect)   text_vect = text_vect.reshape(1, -1)   return text_vect  def is_plagiarism(similarity_score, plagiarism_threshold):   is_plagiarism = False   if(similarity_score >= plagiarism_threshold):       is_plagiarism = True   return is_plagiarism  def check_incoming_document(incoming_document):   text_lang = detect(incoming_document)   language_list = ['de', 'fr', 'el', 'ja', 'ru']   final_result = \"\"   if(text_lang == 'en'):     final_result = incoming_document   elif(text_lang not in language_list):     final_result = None   else:     # Translate in English     final_result = translate_text(incoming_document, text_lang)   return final_result   def run_plagiarism_analysis(query_text, data, plagiarism_threshold=0.8):   top_N=3   # Check the language of the query/incoming text and translate if required.   document_translation = check_incoming_document(query_text)   if(document_translation is None):     print(\"Only the following languages are supported: English, French, Russian, German, Greek and Japanese\")     exit(-1)   else:     # Preprocess the document to get the required vector for similarity analysis     query_vect = process_document(document_translation)      # Run similarity Search     data[\"similarity\"] = data[\"vectors\"].apply(lambda x:                                             cosine_similarity(query_vect, x))     data[\"similarity\"] = data[\"similarity\"].apply(lambda x: x[0][0])     similar_articles = data.sort_values(by='similarity',                                         ascending=False)[1:top_N+1]     formated_result = similar_articles[[\"abstract\", \"paper_id\",                                         \"similarity\"]].reset_index(drop = True)     similarity_score = formated_result.iloc[0][\"similarity\"]     most_similar_article = formated_result.iloc[0][\"abstract\"]     is_plagiarism_bool = is_plagiarism(similarity_score, plagiarism_threshold)     plagiarism_decision = {'similarity_score': similarity_score,                           'is_plagiarism': is_plagiarism_bool,                           'most_similar_article': most_similar_article,                           'article_submitted': query_text                           }     return plagiarism_decision ```  ## Experimentation of the system  We have covered and implemented all the components of the workflow. Now, it is time to test our system using three of the languages accepted by our system: _German, French, Japanese, Greek, and Russian_.  ### Candidate articles and their submission evaluation  These are the abstract text of the articles we want to check whether the authors plagiarized or not.   #### English article   This article is actually an example from the source data.  ```python english_article_to_check = \"The need for multidisciplinary research to address today's complex health and environmental challenges has never been greater. The One Health (OH) approach to research ensures that human, animal, and environmental health questions are evaluated in an integrated and holistic manner to provide a more comprehensive understanding of the problem and potential solutions than would be possible with siloed approaches. However, the OH approach is complex, and there is limited guidance available for investigators regarding the practical design and implementation of OH research. In this paper we provide a framework to guide researchers through conceptualizing and planning an OH study. We discuss key steps in designing an OH study, including conceptualization of hypotheses and study aims, identification of collaborators for a multi-disciplinary research team, study design options, data sources and collection methods, and analytical methods. We illustrate these concepts through the presentation of a case study of health impacts associated with land application of biosolids. Finally, we discuss opportunities for applying an OH approach to identify solutions to current global health issues, and the need for cross-disciplinary funding sources to foster an OH approach to research.\"  # Select an existing article from the data new_incoming_text = source_data.iloc[0]['abstract']   # Run the plagiarism detection analysis_result = run_plagiarism_analysis(new_incoming_text,                                           vector_index, plagiarism_threshold=0.8) ```  ![English article similarity score](/images/plagiarism-detection-6.png)  After running the system we get a similarity score of 1, which is a 100% match with an existing article. This is obvious because we took exactly the same article from the vector index.  #### French article  This article is freely available from the [French agriculture website](https://agriculture.gouv.fr/quel-avenir-pour-les-reseaux-dinnovation-et-de-transfert-agricoles-et-les-systemes-recherche).  ```python french_article_to_check = \"\"\"Les Réseaux d'Innovation et de Transfert Agricole (RITA) ont été créés en 2011 pour mieux connecter la recherche et le développement agricole, intra et inter-DOM, avec un objectif d'accompagnement de la diversification des productions locales. Le CGAAER a été chargé d'analyser ce dispositif et de proposer des pistes d'action pour améliorer la chaine Recherche – Formation – Innovation – Développement – Transfert dans les outre-mer dans un contexte d'agriculture durable, au profit de l'accroissement de l'autonomie alimentaire.\"\"\"  analysis_result = run_plagiarism_analysis(french_article_to_check,                                           vector_index, plagiarism_threshold=0.8) analysis_result ```  ![French article similarity score](/images/plagiarism-detection-7.png)  There is no plagiarism in this situation because the similarity score is less than the threshold.   #### German article  Let’s imagine that some really liked the fifth article in the data, and decided to translate it into German. Now let’s see how the system will judge that article.  ```python german_article_to_check = \"\"\"Derzeit ist eine Reihe strukturell und funktionell unterschiedlicher temperaturempfindlicher Elemente wie RNA-Thermometer bekannt, die eine Vielzahl biologischer Prozesse in Bakterien, einschließlich der Virulenz, steuern. Auf der Grundlage einer Computer- und thermodynamischen Analyse der vollständig sequenzierten Genome von 25 Salmonella enterica-Isolaten wurden ein Algorithmus und Kriterien für die Suche nach potenziellen RNA-Thermometern entwickelt. Er wird es ermöglichen, die Suche nach potentiellen Riboschaltern im Genom anderer gesellschaftlich wichtiger Krankheitserreger durchzuführen. Für S. enterica wurden neben dem bekannten 4U-RNA-Thermometer vier Hairpin-Loop-Strukturen identifiziert, die wahrscheinlich als weitere RNA-Thermometer fungieren. Sie erfüllen die notwendigen und hinreichenden Bedingungen für die Bildung von RNA-Thermometern und sind hochkonservative nichtkanonische Strukturen, da diese hochkonservativen Strukturen im Genom aller 25 Isolate von S. enterica gefunden wurden. Die Hairpins, die eine kreuzförmige Struktur in der supergewickelten pUC8-DNA bilden, wurden mit Hilfe der Rasterkraftmikroskopie sichtbar gemacht.\"\"\"  analysis_result = run_plagiarism_analysis(german_article_to_check,                                           vector_index, plagiarism_threshold=0.8) analysis_result  ```  ![German article similarity score](/images/plagiarism-detection-8.png)  97% of similarity — this is what the model captured! The result is quite impressive. This article is definitely a plagiat.  ## Conclusion  Congratulations, now you have all the tools to build a more robust plagiarism detection system, using BERT and Machine Translation models combined with Cosine Similarity.  You can also find more resources related to vector search and vector databases from our [learning center](/learn/).   ## Additional Resources  [MarianMT model from HuggingFace](https://huggingface.co/docs/transformers/model_doc/marian)  [Document classification with BERT](https://www.chrismccormick.ai/products/bert-document-classification-tutorial-with-code/categories/2108744/posts/7038793)  [Source code of the article](https://colab.research.google.com/drive/17FQHRb45-ZlKDqGZ7O3bti-klCVyGEjb?usp=sharing)  [Pinecone learning center](/learn/)  [Allen Institute for AI](https://allenai.org/) ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e33e"
  },
  "content": "categories:   - Algorithms & Libraries toc: >- prettyTables: true weight: 1 draft: true ---  ## Executive Summary  [Pinecone](/) is a managed vector search solution. At its core is the Pinecone Vector Index, a proprietary set of algorithms and data structures that enable fast and accurate similarity search of vectorized objects.  This benchmark report compares the Pinecone Vector Index with two other state-of-the-art nearest-neighbor search libraries, [NMSLIB (HNSW)](https://github.com/nmslib/nmslib) and [Faiss-IVF](https://github.com/facebookresearch/faiss), across a range of quality and performance metrics using their production-grade library implementations.  Our benchmarking is concerned with two aspects: accuracy and performance. We ran the benchmarks using a standard open-source benchmarking library and datasets.  ### Result highlights:  - **Accuracy**: The Pinecone Vector Index achieves top accuracy results across datasets, and its theoretical guarantees assure this will be true with new, unseen data. Faiss is also reasonably accurate, while the accuracy of HNSW is insufficient for many machine learning applications. - **Performance (indexing time)**: The Pinecone Vector Index has the fastest indexing time while still having a low memory footprint. Faiss exhibits reasonably good indexing times. NMSLIB (HNSW) cannot build real-time indexes. - **Performance (throughput)**: NMSLIB (HNSW) has the highest throughput results, although it is less accurate than Faiss and the Pinecone Vector Index. (In practice, the Pinecone Vector Index can meet any throughput requirement thanks to the sharding and auto-scaling features of the managed Pinecone product.)  ## Benchmark Report  This is a comparison of a single-node deployment of the proprietary Pinecone Vector Index with other commonly used nearest-neighbor-search solutions.  The comparison is concerned primarily with accuracy and performance. For measuring accuracy we utilize a set of standard ranking metrics along with a new, more refined, metric. Performance metrics measure the total time for constructing an index, average throughput, and average query latency.  A detailed description of the considered metrics, along with the full detailed results appear in Appendices A1 and A4.  ### Benchmarking Outline  The benchmarking is based on the standard open-source [ANNS-benchmarking library](https://github.com/erikbern/ann-benchmarks) running over a single machine (AWS EC2 machine, c4.5xlarge, with 16 vCPUs and 32 GB RAM), limited to use two CPUs. We focused on the cosine-similarity metric, and used datasets of various sizes ranging from 60k items to 10M items, and dimensions from 25 to 960. These datasets are part of the library benchmark data, and the corresponding results of various off-the-shelf open source solutions are published on the ANNS-benchmarking library website. Using these results, we chose the top performing solutions in terms of accuracy: _Faiss (IVF-Flat)_ and _NMSLIB (HNSW)_.  We compared the Pinecone Vector Index against the alternatives over the different datasets with a focus on accuracy and performance. For accuracy we used a set of standard ranking metrics including recall, precision, F-score, MRR, DCG, and a new metric called approximation-loss which measures how much the ranking metric scores of the approximated solution deviates from the optimal (exact search) scores. Performance is measured by the total time for constructing an index, average throughput, and average latency. The appendix of this report contains detailed descriptions of the metrics, as well as the full results over all datasets and numbers of desired results (i.e., k values defining k-top values).  In designing approximate nearest neighbor algorithms, one can always trade off accuracy and speed. As part of an enterprise solution, the Pinecone Vector Index is held to very high standards with respect to accuracy. This is not the case for other algorithms we tested. HNSW (_NMSLIB_) exhibits the highest throughput results on a single machine, whereas Faiss and the Pinecone Vector Index lag behind. Note, however, that we report the throughput and latency (Figure 1) over just a single machine. In practice, the Pinecone Vector Index can meet any throughput requirement thanks to sharding, replication, namespacing, and auto-scaling across many machines that comes with the Pinecone fully managed service.  ### Accuracy  In terms of accuracy, all algorithms can be configured to achieve near optimal accuracy (which is the exact search accuracy). For example, Faiss-IVF can be configured in such a way that it is essentially performing exact search (see second paragraph in the [Faiss FAQ](https://github.com/facebookresearch/faiss/wiki/FAQ#what-does-it-mean-when-a-search-returns--1-ids)). Alas, this hurts performance, so these algorithms require workload-specific tuning. The Pinecone Vector Index, in contrast, automatically tunes its configuration to achieve high accuracy while keeping good performance.  Figure 1 below depicts a few typical results for the considered accuracy metrics. Here we depict the F1-score that balances between recall and precision. The optimal value for the figures is 1.0. All considered solution configurations achieve high accuracy results. The Pinecone Vector Index, depicted as a red cross marker, is a top performer, while Faiss-IVF is a runner-up, and HNSW lags behind. For some large-scale datasets, HNSW failed to run due to high memory consumption. This is typical for the majority of the datasets.  ![Benchmark figure 1](/images/benchmark-figure1.png)  <small>  _**Figure 1.** Depicting typical accuracy results over various datasets, and metrics tested. Here we present the F1 score metric results. Each marker corresponds to a single run of a specific library configuration. Note that y-axis values are log-scaled. Observe that the Pinecone Vector Index has a single marker because it  auto-tunes its configuration, while alternative libraries manually tune their configurations, thus, have a marker per test configuration. The optimal F-1 score metric value is 1.0. The Pinecone Vector Index is designed to be provably accurate and gains an accuracy advantage across different datasets. In contrast, both HNSW and Faiss get a performance boost by sacrificing accuracy._  </small>  ### Index Build Time  Near real-time retrieval applications require fast index creation and update times. Index creation time is a good indication for time overhead for ‘write’ operations: insert, update, delete. Figure 2 depicts the index creation time in seconds versus the index size. Here for each solution we chose the most accurate configuration. The x-axis is on a logarithmic scale. The Pinecone Vector Index has a clear advantage over alternatives, with Faiss coming in second, and NMSLIB (i.e., HNSW) being far behind.  Indexing smaller than million-item datasets takes the Pinecone Vector Index a fraction of a second, a million items take a few seconds, and 10 million items take roughly one minute. Faiss gives good indexing time and can be treated as a real-time engine. NMSLIB lags behind, and even fails to run on large datasets due to lack of memory (given the AWS machine configuration we used).  ![Benchmark figure 2](/images/benchmark-figure2.png)  <small>  _**Figure 2.** Index creation time v.s. number of items times dimension. Note that the x and y axes scale is logarithmic. Each marker corresponds to a dataset/algorithm-configuration pair. Observe that the Pinecone Vector Index is a clear winner, Faiss is the runner-up, and HNSW (Nmslib) is far behind. (Recall that Nmslib failed to run on some datasets.)_  </small>  ### Pinecone Vector Index Highlights  The Vector Index is our proprietary nearest-neighbor search indexing solution at the core of the Pinecone product. Its advantages seen in this benchmark report are as follows:  - Able to run on all datasets and never produced empty results, unlike NMSLIB (HNSW). - Achieves the best accuracy results. - Automatically tunes its configuration (i.e., hyper parameters). Other solutions require manual tuning to balance accuracy and performance. Each dataset requires different configuration, and non-optimal choices degrade accuracy or speed. - The fastest indexing time. Faiss exhibits reasonably good indexing times. NMSLIB (HNSW) cannot build real-time indexes. - HNSW has the highest throughput results at the expense of accuracy. Among the high-accuracy algorithms, the Pinecone Vector Index and Faiss, the former is faster overall. Note the benchmarking focused on a single-node comparison only. In practice, Pinecone is a fully managed service that combines sharding and auto-scaling to meet any throughput requirement.  ---  ## Appendix: Metrics and Results  ### A1. Accuracy Metrics  Listing the set of accuracy metrics that have been utilized in the benchmarking:  - Recall@k / precision@k: taking the k-rank score (+epsilon) as a threshold defining relevant/non-relevant items. Precision@k is the ratio of top-k items with score above the threshold value. Recall@k is the fraction of top-k items with score above the threshold from all relevant items in the database. We defined the threshold to be the average score differences between consecutive top-k items. - F-score: $F_β= (1+β^2) * \\frac{precision * recall}{(β^2*precision) + recall}$; the parameter $β$ indicates how much the recall is more important than precision. We can take $β$=1, 0.5, 2. We should take the corresponding “@k” measures. - Mean reciprocal rank: $\\frac{1}{|Q|} \\sum_{i=1}^{|Q|}= \\frac{1}{rank_i}$ wherein $rank_i$ is the first rank that is relevant (if none then we should use a high penalty value). - DCG: $\\sum_{i=1}^k \\frac{2^{rel_i}-1}{log_2(i+1)}$ wherein the relevancy score is its divergence from the exact-rank-i’th score. - Average Approximation Loss @ 3 / 10: For every query we retrieve the sorted top-k approximated items and their corresponding scores. We then compute the average difference between each of these scores and their corresponding exact search positional score. The resulting value is the Average-Approximation-Loss@k. - Distribution information of average-approximation-loss: mean / p50 / p75 / p90 / p99 of the average-approximation-loss over a test set of queries.  ### A2. Performance Metrics  - Indexing time (build time): How fast does a system get fully operational. - Throughput: Queries per second (QPS). - Query latency: Distributional information, mean / p50 / p95 / p99.  ### A3. Missing Results  The benchmark results do not contain all metrics for all libraries and parameter configurations. Missing results indicate failure. There are two sources of failures:  - **Out of memory**: NMSLIB HNSW uses graph-based indexes with large memory footprints. In fact, their memory consumption is super linear in the size of the data. As a result, larger datasets could not be indexed into a single machine’s memory. In those cases, the rows corresponding to NMSLIB HNSW are missing. - **Empty result set**: For some reason, libraries sometimes return empty result sets. This is clearly unintended behavior. Accuracy for empty result sets are meaningless. They are left blank in the tables below.  ### A4. Detailed Results  Best results are appear in **bold**. Scroll sideways to see more columns.  #### NYTimes-256-angular k=3  | Algorithm                                                                              | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-Latency | QPS          | |----------------------------------------------------------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------| | Pinecone Vector Index                                                                                 | **2.18E+00** | **1.99E-08** | **3.10E-10** | **7.40E-17** | **0.00E+00** | **1.00E+00** | **2.32E-08** | **7.28E-01** | **8.19E-01** | **6.54E-01** | 6.74E-03     | 1.48E+02     | | FaissIVF (n\\_list=256, n\\_probe=50)                                                    | 6.77E+00     | 9.20E-02     | 1.16E-02     | 9.93E-09     | **0.00E+00** | 9.68E-01     | 6.82E-03     | 6.57E-01     | 7.40E-01     | 5.91E-01     | 5.75E-03     | 1.74E+02     | | FaissIVF (n\\_list=256, n\\_probe=100)                                                   | 6.77E+00     | 4.14E-02     | 1.99E-08     | 3.33E-16     | **0.00E+00** | 9.88E-01     | 2.31E-03     | 6.97E-01     | 7.85E-01     | 6.27E-01     | 1.08E-02     | 9.29E+01     | | FaissIVF (n\\_list=512, n\\_probe=50)                                                    | 1.07E+01     | 1.27E-01     | 1.96E-02     | 9.93E-09     | **0.00E+00** | 9.53E-01     | 1.05E-02     | 6.37E-01     | 7.17E-01     | 5.72E-01     | 2.99E-03     | 3.34E+02     | | FaissIVF (n\\_list=512, n\\_probe=100)                                                   | 1.07E+01     | 7.63E-02     | 7.00E-03     | 1.55E-10     | **0.00E+00** | 9.74E-01     | 5.22E-03     | 6.73E-01     | 7.58E-01     | 6.05E-01     | 5.74E-03     | 1.74E+02     | | FaissIVF (n\\_list=1024, n\\_probe=100)                                                  | 2.14E+01     | 1.08E-01     | 1.27E-02     | 9.93E-09     | **0.00E+00** | 9.64E-01     | 8.24E-03     | 6.56E-01     | 7.39E-01     | 5.90E-01     | 3.09E-03     | 3.24E+02     | | FaissIVF (n\\_list=1024, n\\_probe=50)                                                   | 2.14E+01     | 1.53E-01     | 2.65E-02     | 1.99E-08     | **0.00E+00** | 9.42E-01     | 1.36E-02     | 6.26E-01     | 7.05E-01     | 5.63E-01     | 1.68E-03     | 5.95E+02     | | Nmslib (method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 5.78E+02     | 3.47E-02     | 9.93E-09     | 1.48E-16     | **0.00E+00** | 9.82E-01     | 1.95E-03     | 6.97E-01     | 7.85E-01     | 6.27E-01     | 1.05E-03     | 9.51E+02     | | Nmslib (method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 3.04E+02     | 3.31E-01     | 1.48E-02     | 3.88E-11     | **0.00E+00** | 9.25E-01     | 2.40E-02     | 6.42E-01     | 7.24E-01     | 5.78E-01     | **3.48E-04** | **2.88E+03** | | Nmslib (method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 3.04E+02     | 1.23E-01     | 1.99E-08     | 1.48E-16     | **0.00E+00** | 9.71E-01     | 1.09E-02     | 6.89E-01     | 7.76E-01     | 6.19E-01     | 1.12E-03     | 8.92E+02     | | Nmslib (method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 5.78E+02     | 4.77E-03     | 4.97E-09     | 7.40E-17     | **0.00E+00** | 9.97E-01     | 2.25E-04     | 7.21E-01     | 8.12E-01     | 6.48E-01     | 3.64E-03     | 2.74E+02     |  #### NYTimes-256-angular qk=10  | Algorithm                                                                             | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-Latency | QPS          | |---------------------------------------------------------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------| | Pinecone Vector Index                                                                                | **2.18E+00** | **1.19E-08** | **5.96E-09** | **1.49E-09** | **0.00E+00** | **1.00E+00** | **5.69E-07** | **8.84E-01** | **9.24E-01** | **8.47E-01** | 6.74E-03     | 9.82E+01     | | FaissIVF(n\\_list=256, n\\_probe=50)                                                    | 6.92E+00     | 7.58E-02     | 1.94E-02     | 5.47E-03     | 5.96E-09     | 9.67E-01     | 1.88E-02     | 7.04E-01     | 7.36E-01     | 6.75E-01     | 5.75E-03     | 1.76E+02     | | FaissIVF(n\\_list=256, n\\_probe=100)                                                   | 6.92E+00     | 3.61E-02     | 6.61E-03     | 4.87E-04     | 1.49E-09     | 9.88E-01     | 6.82E-03     | 7.97E-01     | 8.34E-01     | 7.64E-01     | 1.08E-02     | 9.46E+01     | | FaissIVF(n\\_list=512, n\\_probe=50)                                                    | 1.12E+01     | 1.05E-01     | 2.70E-02     | 8.17E-03     | 5.96E-09     | 9.50E-01     | 2.70E-02     | 6.71E-01     | 7.01E-01     | 6.43E-01     | 2.99E-03     | 3.35E+02     | | FaissIVF(n\\_list=512, n\\_probe=100)                                                   | 1.12E+01     | 6.59E-02     | 1.40E-02     | 3.25E-03     | 2.98E-09     | 9.74E-01     | 1.42E-02     | 7.40E-01     | 7.73E-01     | 7.09E-01     | 5.74E-03     | 1.74E+02     | | FaissIVF(n\\_list=1024, n\\_probe=100)                                                  | 2.13E+01     | 8.24E-02     | 2.01E-02     | 4.86E-03     | 2.98E-09     | 9.62E-01     | 2.00E-02     | 7.11E-01     | 7.43E-01     | 6.81E-01     | 3.09E-03     | 3.19E+02     | | FaissIVF(n\\_list=1024, n\\_probe=50)                                                   | 2.13E+01     | 1.13E-01     | 3.18E-02     | 8.86E-03     | 5.96E-09     | 9.39E-01     | 3.14E-02     | 6.60E-01     | 6.90E-01     | 6.33E-01     | 1.68E-03     | 5.88E+02     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 5.80E+02     | 2.38E-02     | 2.87E-03     | 5.96E-09     | 2.22E-17     | 9.81E-01     | 4.21E-03     | 8.13E-01     | 8.50E-01     | 7.79E-01     | 1.05E-03     | 9.03E+02     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 3.03E+02     | 1.25E-01     | 1.53E-02     | 1.92E-04     | 1.33E-16     | 9.25E-01     | 2.60E-02     | 7.47E-01     | 7.81E-01     | 7.16E-01     | **3.48E-04** | **3.07E+03** | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 3.03E+02     | 2.91E-02     | 3.14E-03     | 5.96E-09     | 2.22E-17     | 9.77E-01     | 6.47E-03     | 8.10E-01     | 8.47E-01     | 7.76E-01     | 1.12E-03     | 9.59E+02     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 5.80E+02     | 6.22E-03     | 1.19E-08     | 2.98E-09     | **0.00E+00** | 9.97E-01     | 6.70E-04     | 8.61E-01     | 9.00E-01     | 8.25E-01     | 3.64E-03     | 2.72E+02     |  #### Glove-25-angular k=3  | Algorithm                                                                             | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-latency | QPS          | |---------------------------------------------------------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------| | Pinecone Vector Index                                                                                | **8.94E+00** | **9.93E-09** | **4.97E-09** | **4.97E-09** | **2.48E-09** | **1.00E+00** | **3.55E-09** | **8.39E-01** | **9.29E-01** | **7.65E-01** | 1.37E-02     | 7.30E+01     | | FaissIVF(n\\_list=256, n\\_probe=50)                                                    | 1.02E+01     | 1.24E-08     | 7.45E-09     | **4.97E-09** | **2.48E-09** | **1.00E+00** | 1.81E-05     | 8.38E-01     | 9.28E-01     | 7.64E-01     | 3.88E-03     | 2.58E+02     | | FaissIVF(n\\_list=256, n\\_probe=100)                                                   | 1.02E+01     | 9.93E-09     | 7.45E-09     | **4.97E-09** | **2.48E-09** | **1.00E+00** | 4.51E-07     | **8.39E-01** | 9.29E-01     | **7.65E-01** | 6.85E-03     | 1.46E+02     | | FaissIVF(n\\_list=512, n\\_probe=50)                                                    | 1.42E+01     | 1.51E-03     | 7.45E-09     | **4.97E-09** | **2.48E-09** | 9.99E-01     | 9.69E-05     | 8.35E-01     | 9.24E-01     | 7.61E-01     | 2.03E-03     | 4.94E+02     | | FaissIVF(n\\_list=512, n\\_probe=100)                                                   | 1.42E+01     | 1.24E-08     | 7.45E-09     | **4.97E-09** | **2.48E-09** | **1.00E+00** | 6.38E-06     | **8.39E-01** | 9.28E-01     | **7.65E-01** | 3.92E-03     | 2.55E+02     | | FaissIVF(n\\_list=1024, n\\_probe=100)                                                  | 2.22E+01     | 1.49E-08     | 7.45E-09     | **4.97E-09** | **2.48E-09** | **1.00E+00** | 4.27E-05     | 8.38E-01     | 9.27E-01     | 7.64E-01     | 2.07E-03     | 4.83E+02     | | FaissIVF(n\\_list=1024, n\\_probe=50)                                                   | 2.22E+01     | 7.21E-03     | 7.45E-09     | **4.97E-09** | **2.48E-09** | 9.97E-01     | 3.02E-04     | 8.28E-01     | 9.17E-01     | 7.55E-01     | 1.09E-03     | 9.16E+02     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 6.68E+02     | **9.93E-09** | **4.97E-09** | **4.97E-09** | **2.48E-09** | **1.00E+00** | 7.01E-06     | **8.39E-01** | 9.28E-01     | **7.65E-01** | 3.36E-04     | 2.97E+03     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 4.34E+02     | 6.60E-03     | 7.45E-09     | **4.97E-09** | **2.48E-09** | 9.97E-01     | 2.87E-04     | 8.23E-01     | 9.11E-01     | 7.50E-01     | **1.12E-04** | **8.90E+03** | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 4.34E+02     | 9.93E-09     | **4.97E-09** | **4.97E-09** | **2.48E-09** | **1.00E+00** | 1.43E-05     | 8.38E-01     | 9.28E-01     | **7.65E-01** | 3.67E-04     | 2.73E+03     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 6.68E+02     | **9.93E-09** | **4.97E-09** | **4.97E-09** | **2.48E-09** | **1.00E+00** | **3.55E-09** | **8.39E-01** | **9.29E-01** | **7.65E-01** | 1.12E-03     | 8.95E+02     |  #### Glove-25-angular k=10  | Algorithm                                                                             | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-latency | QPS          | |---------------------------------------------------------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------| | Pinecone Vector Index                                                                                | **8.94E+00** | **7.45E-09** | **4.47E-09** | **3.73E-09** | **2.24E-09** | **1.00E+00** | **8.11E-09** | **9.39E-01** | **9.75E-01** | **9.06E-01** | 1.40E-02     | 7.12E+01     | | FaissIVF(n\\_list=256, n\\_probe=50)                                                    | 1.07E+01     | 1.11E-03     | 5.96E-09     | 4.47E-09     | 2.98E-09     | **1.00E+00** | 8.60E-05     | 9.35E-01     | 9.71E-01     | 9.02E-01     | 3.88E-03     | 2.58E+02     | | FaissIVF(n\\_list=256, n\\_probe=100)                                                   | 1.07E+01     | 8.94E-09     | 5.96E-09     | 4.47E-09     | 2.98E-09     | **1.00E+00** | 3.07E-06     | 9.39E-01     | **9.75E-01** | **9.06E-01** | 6.84E-03     | 1.46E+02     | | FaissIVF(n\\_list=512, n\\_probe=50)                                                    | 1.42E+01     | 3.26E-03     | 7.45E-09     | 4.47E-09     | 2.98E-09     | 9.99E-01     | 3.09E-04     | 9.27E-01     | 9.62E-01     | 8.94E-01     | 2.01E-03     | 4.98E+02     | | FaissIVF(n\\_list=512, n\\_probe=100)                                                   | 1.42E+01     | 1.19E-08     | 5.96E-09     | 4.47E-09     | 2.98E-09     | **1.00E+00** | 3.44E-05     | 9.38E-01     | 9.73E-01     | 9.05E-01     | 3.91E-03     | 2.56E+02     | | FaissIVF(n\\_list=1024, n\\_probe=100)                                                  | 2.32E+01     | 1.93E-03     | 5.96E-09     | 4.47E-09     | 2.98E-09     | **1.00E+00** | 1.54E-04     | 9.33E-01     | 9.68E-01     | 9.00E-01     | 2.30E-03     | 4.35E+02     | | FaissIVF(n\\_list=1024, n\\_probe=50)                                                   | 2.32E+01     | 6.20E-03     | 3.24E-04     | 5.22E-09     | 2.98E-09     | 9.97E-01     | 8.05E-04     | 9.09E-01     | 9.43E-01     | 8.77E-01     | 1.21E-03     | 8.27E+02     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 6.57E+02     | 2.30E-04     | 5.22E-09     | **3.73E-09** | **2.24E-09** | **1.00E+00** | 3.15E-05     | 9.37E-01     | 9.73E-01     | 9.04E-01     | 3.40E-04     | 2.94E+03     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 4.21E+02     | 6.40E-03     | 1.39E-03     | 5.96E-09     | 2.98E-09     | 9.97E-01     | 1.14E-03     | 8.82E-01     | 9.15E-01     | 8.51E-01     | **1.16E-04** | **8.62E+03** | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 4.21E+02     | 5.19E-04     | 5.22E-09     | **3.73E-09** | **2.24E-09** | **1.00E+00** | 4.62E-05     | 9.37E-01     | 9.72E-01     | 9.04E-01     | 3.73E-04     | 2.68E+03     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 6.57E+02     | **7.45E-09** | **4.47E-09** | **3.73E-09** | **2.24E-09** | **1.00E+00** | **8.11E-09** | **9.39E-01** | **9.75E-01** | **9.06E-01** | 1.12E-03     | 8.91E+02     |  ##### Glove-100-angular k=3  | Algorithm                                                                             | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-latency | QPS          | |---------------------------------------------------------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------| | Pinecone Vector Index                                                                                | **8.29E+00** | **1.99E-08** | **9.93E-09** | **0.00E+00** | **0.00E+00** | **1.00E+00** | **3.16E-09** | **8.23E-01** | **9.21E-01** | **7.43E-01** | 1.45E-02     | 6.91E+01     | | FaissIVF(n\\_list=256, n\\_probe=50)                                                    | 1.17E+01     | 2.02E-02     | **9.93E-09** | 9.93E-09     | **0.00E+00** | 9.90E-01     | 1.10E-03     | 7.98E-01     | 8.93E-01     | 7.21E-01     | 9.06E-03     | 1.10E+02     | | FaissIVF(n\\_list=256, n\\_probe=100)                                                   | 1.17E+01     | 6.43E-03     | **9.93E-09** | 4.97E-09     | **0.00E+00** | 9.97E-01     | 3.01E-04     | 8.15E-01     | 9.13E-01     | 7.37E-01     | 1.72E-02     | 5.82E+01     | | FaissIVF(n\\_list=512, n\\_probe=50)                                                    | 1.57E+01     | 2.69E-02     | 1.47E-03     | 9.93E-09     | **0.00E+00** | 9.83E-01     | 1.80E-03     | 7.79E-01     | 8.72E-01     | 7.04E-01     | 5.05E-03     | 1.98E+02     | | FaissIVF(n\\_list=512, n\\_probe=100)                                                   | 1.57E+01     | 1.59E-02     | **9.93E-09** | 9.93E-09     | **0.00E+00** | 9.93E-01     | 7.21E-04     | 8.04E-01     | 9.00E-01     | 7.27E-01     | 9.15E-03     | 1.09E+02     | | FaissIVF(n\\_list=1024, n\\_probe=100)                                                  | 2.65E+01     | 2.27E-02     | 1.99E-08     | 9.93E-09     | **0.00E+00** | 9.86E-01     | 1.44E-03     | 7.88E-01     | 8.82E-01     | 7.12E-01     | 5.00E-03     | 2.00E+02     | | FaissIVF(n\\_list=1024, n\\_probe=50)                                                   | 2.65E+01     | 3.36E-02     | 5.53E-03     | 9.93E-09     | **0.00E+00** | 9.71E-01     | 2.97E-03     | 7.54E-01     | 8.44E-01     | 6.81E-01     | 2.58E-03     | 3.87E+02     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 1.59E+03     | 2.52E-02     | 3.49E-03     | 9.93E-09     | **0.00E+00** | 9.76E-01     | 2.07E-03     | 7.59E-01     | 8.49E-01     | 6.86E-01     | 6.28E-04     | 1.59E+03     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 8.61E+02     | 8.77E-02     | 2.19E-02     | 4.36E-03     | **0.00E+00** | 8.93E-01     | 1.04E-02     | 6.45E-01     | 7.22E-01     | 5.83E-01     | **1.92E-04** | **5.20E+03** | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 8.61E+02     | 3.48E-02     | 5.11E-03     | 9.93E-09     | **0.00E+00** | 9.65E-01     | 3.01E-03     | 7.47E-01     | 8.36E-01     | 6.75E-01     | 6.23E-04     | 1.60E+03     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 1.59E+03     | 5.85E-03     | **9.93E-09** | 4.97E-09     | **0.00E+00** | 9.97E-01     | 2.40E-04     | 8.13E-01     | 9.09E-01     | 7.34E-01     | 2.13E-03     | 4.69E+02     |  #### Glove-100-angular k=10  | Algorithm                                                                             | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-latency | QPS          | |---------------------------------------------------------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------| | Pinecone Vector Index                                                                                | **8.29E+00** | **8.94E-09** | **5.96E-09** | **2.98E-09** | **2.98E-09** | **1.00E+00** | **2.60E-07** | **9.32E-01** | **9.72E-01** | **8.96E-01** | 1.56E-02     | 6.42E+01     | | FaissIVF(n\\_list=256, n\\_probe=50)                                                    | 1.19E+01     | 1.25E-02     | 2.27E-03     | 6.71E-09     | **2.98E-09** | 9.90E-01     | 2.38E-03     | 8.71E-01     | 9.08E-01     | 8.37E-01     | 9.04E-03     | 1.11E+02     | | FaissIVF(n\\_list=256, n\\_probe=100)                                                   | 1.19E+01     | 5.76E-03     | 8.94E-09     | 5.96E-09     | **2.98E-09** | 9.97E-01     | 6.71E-04     | 9.16E-01     | 9.54E-01     | 8.80E-01     | 1.71E-02     | 5.84E+01     | | FaissIVF(n\\_list=512, n\\_probe=50)                                                    | 1.62E+01     | 1.67E-02     | 4.26E-03     | 4.24E-04     | **2.98E-09** | 9.83E-01     | 3.96E-03     | 8.30E-01     | 8.65E-01     | 7.97E-01     | 5.04E-03     | 1.98E+02     | | FaissIVF(n\\_list=512, n\\_probe=100)                                                   | 1.62E+01     | 9.80E-03     | 1.18E-03     | 5.96E-09     | **2.98E-09** | 9.93E-01     | 1.60E-03     | 8.88E-01     | 9.26E-01     | 8.54E-01     | 9.26E-03     | 1.08E+02     | | FaissIVF(n\\_list=1024, n\\_probe=100)                                                  | 2.56E+01     | 1.47E-02     | 3.23E-03     | 1.19E-08     | **2.98E-09** | 9.86E-01     | 3.14E-03     | 8.49E-01     | 8.85E-01     | 8.16E-01     | 5.04E-03     | 1.98E+02     | | FaissIVF(n\\_list=1024, n\\_probe=50)                                                   | 2.56E+01     | 2.15E-02     | 7.06E-03     | 1.76E-03     | 4.47E-09     | 9.70E-01     | 6.43E-03     | 7.80E-01     | 8.13E-01     | 7.50E-01     | 2.59E-03     | 3.87E+02     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 1.58E+03     | 1.90E-02     | 6.18E-03     | 1.50E-03     | **2.98E-09** | 9.75E-01     | 5.34E-03     | 7.80E-01     | 8.13E-01     | 7.49E-01     | 6.32E-04     | 1.58E+03     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 8.61E+02     | 5.74E-02     | 2.20E-02     | 7.91E-03     | 8.32E-04     | 8.89E-01     | 2.16E-02     | 6.15E-01     | 6.41E-01     | 5.91E-01     | **1.99E-04** | **5.02E+03** | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 8.61E+02     | 2.39E-02     | 7.37E-03     | 1.67E-03     | **2.98E-09** | 9.65E-01     | 6.72E-03     | 7.70E-01     | 8.02E-01     | 7.39E-01     | 6.40E-04     | 1.56E+03     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 1.58E+03     | 5.40E-03     | 6.13E-04     | 5.96E-09     | **2.98E-09** | 9.97E-01     | 7.79E-04     | 8.97E-01     | 9.35E-01     | 8.62E-01     | 2.13E-03     | 4.69E+02     |  #### Glove-200-angular k=3  | Algorithm                                                                             | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-latency | QPS          | |---------------------------------------------------------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------| | Pinecone Vector Index(k\\_bits=1024, p\\_factor=1)                                                     | **8.63E+00** | **1.99E-08** | **9.93E-09** | **0.00E+00** | **0.00E+00** | **1.00E+00** | **4.05E-07** | **8.20E-01** | **9.19E-01** | **7.40E-01** | 1.61E-02     | 6.20E+01     | | FaissIVF(n\\_list=256, n\\_probe=50)                                                    | 1.30E+01     | 3.39E-02     | 1.28E-03     | 4.97E-09     | **0.00E+00** | 9.81E-01     | 2.13E-03     | 7.78E-01     | 8.72E-01     | 7.02E-01     | 1.65E-02     | 6.06E+01     | | FaissIVF(n\\_list=256, n\\_probe=100)                                                   | 1.30E+01     | 1.63E-02     | 1.99E-08     | 0.00E+00     | **0.00E+00** | 9.93E-01     | 7.65E-04     | 8.05E-01     | 9.03E-01     | 7.27E-01     | 3.24E-02     | 3.09E+01     | | FaissIVF(n\\_list=512, n\\_probe=50)                                                    | 1.79E+01     | 4.32E-02     | 5.11E-03     | 9.93E-09     | **0.00E+00** | 9.71E-01     | 3.27E-03     | 7.54E-01     | 8.45E-01     | 6.81E-01     | 8.54E-03     | 1.17E+02     | | FaissIVF(n\\_list=512, n\\_probe=100)                                                   | 1.79E+01     | 2.76E-02     | 1.99E-08     | 0.00E+00     | **0.00E+00** | 9.86E-01     | 1.61E-03     | 7.87E-01     | 8.82E-01     | 7.10E-01     | 1.62E-02     | 6.16E+01     | | FaissIVF(n\\_list=1024, n\\_probe=100)                                                  | 2.82E+01     | 3.72E-02     | 3.16E-03     | 9.93E-09     | **0.00E+00** | 9.77E-01     | 2.64E-03     | 7.65E-01     | 8.58E-01     | 6.91E-01     | 8.57E-03     | 1.17E+02     | | FaissIVF(n\\_list=1024, n\\_probe=50)                                                   | 2.82E+01     | 5.33E-02     | 9.37E-03     | 1.99E-08     | **0.00E+00** | 9.57E-01     | 4.78E-03     | 7.26E-01     | 8.13E-01     | 6.55E-01     | 4.67E-03     | 2.14E+02     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 2.42E+03     | 5.11E-02     | 9.35E-03     | 9.93E-09     | **0.00E+00** | 9.46E-01     | 4.84E-03     | 7.14E-01     | 8.00E-01     | 6.44E-01     | 1.02E-03     | 9.76E+02     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 1.25E+03     | 1.45E-01     | 4.02E-02     | 1.03E-02     | **0.00E+00** | 8.34E-01     | 1.97E-02     | 5.87E-01     | 6.57E-01     | 5.29E-01     | **3.13E-04** | **3.20E+03** | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 1.25E+03     | 6.68E-02     | 1.13E-02     | 1.99E-08     | **0.00E+00** | 9.35E-01     | 6.26E-03     | 7.03E-01     | 7.88E-01     | 6.34E-01     | 9.91E-04     | 1.01E+03     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 2.42E+03     | 1.83E-02     | 2.42E-04     | **0.00E+00** | **0.00E+00** | 9.87E-01     | 1.19E-03     | 7.80E-01     | 8.75E-01     | 7.04E-01     | 3.48E-03     | 2.87E+02     |  #### Glove-200-angular k=10  | Algorithm                                                                             | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-latency | QPS          | |---------------------------------------------------------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------| | Pinecone Vector Index(k\\_bits=1024, p\\_factor=1)                                                     | **8.63E+00** | **1.19E-08** | **5.96E-09** | **2.98E-09** | **0.00E+00** | **1.00E+00** | **1.84E-06** | **9.30E-01** | **9.71E-01** | **8.93E-01** | 1.85E-02     | 5.41E+01     | | FaissIVF(n\\_list=256, n\\_probe=50)                                                    | 1.34E+01     | 1.91E-02     | 3.75E-03     | 3.55E-04     | 2.98E-09     | 9.82E-01     | 4.10E-03     | 8.37E-01     | 8.73E-01     | 8.03E-01     | 1.65E-02     | 6.07E+01     | | FaissIVF(n\\_list=256, n\\_probe=100)                                                   | 1.34E+01     | 1.04E-02     | 5.74E-04     | 5.96E-09     | 7.45E-10     | 9.94E-01     | 1.48E-03     | 8.98E-01     | 9.37E-01     | 8.62E-01     | 3.23E-02     | 3.09E+01     | | FaissIVF(n\\_list=512, n\\_probe=50)                                                    | 1.78E+01     | 2.33E-02     | 6.44E-03     | 1.66E-03     | 5.96E-09     | 9.71E-01     | 6.52E-03     | 7.83E-01     | 8.17E-01     | 7.52E-01     | 8.52E-03     | 1.17E+02     | | FaissIVF(n\\_list=512, n\\_probe=100)                                                   | 1.78E+01     | 1.55E-02     | 2.80E-03     | 1.19E-08     | 2.98E-09     | 9.86E-01     | 3.15E-03     | 8.54E-01     | 8.91E-01     | 8.20E-01     | 1.63E-02     | 6.14E+01     | | FaissIVF(n\\_list=1024, n\\_probe=100)                                                  | 3.00E+01     | 2.13E-02     | 5.20E-03     | 9.60E-04     | 2.98E-09     | 9.77E-01     | 5.28E-03     | 8.06E-01     | 8.42E-01     | 7.74E-01     | 8.59E-03     | 1.16E+02     | | FaissIVF(n\\_list=1024, n\\_probe=50)                                                   | 3.00E+01     | 3.04E-02     | 9.64E-03     | 3.18E-03     | 5.96E-09     | 9.56E-01     | 9.56E-03     | 7.35E-01     | 7.67E-01     | 7.06E-01     | 4.70E-03     | 2.13E+02     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 2.41E+03     | 3.21E-02     | 1.12E-02     | 3.32E-03     | 5.96E-09     | 9.40E-01     | 1.05E-02     | 7.13E-01     | 7.44E-01     | 6.84E-01     | 1.01E-03     | 9.91E+02     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 1.25E+03     | 9.29E-02     | 3.28E-02     | 1.26E-02     | 1.78E-03     | 8.23E-01     | 3.60E-02     | 5.58E-01     | 5.82E-01     | 5.35E-01     | **3.14E-04** | **3.18E+03** | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 1.25E+03     | 4.04E-02     | 1.20E-02     | 3.53E-03     | 5.96E-09     | 9.30E-01     | 1.23E-02     | 7.09E-01     | 7.40E-01     | 6.81E-01     | 9.78E-04     | 1.02E+03     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 2.41E+03     | 1.31E-02     | 3.28E-03     | 7.75E-05     | 2.98E-09     | 9.86E-01     | 2.92E-03     | 8.30E-01     | 8.67E-01     | 7.97E-01     | 3.33E-03     | 3.01E+02     |  #### Gist-960-angular k=3  | Algorithm                            | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-latency | QPS          | |--------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------| | Pinecone Vector Index                               | **9.64E+00** | **2.97E-16** | **0.00E+00** | **0.00E+00** | **0.00E+00** | **1.00E+00** | **5.62E-06** | 8.30E-01     | 9.22E-01     | 7.54E-01     | 4.93E-02     | 2.03E+01     | | FaissIVF(n\\_list=256, n\\_probe=50)   | 2.17E+01     | 2.29E-06     | 2.48E-09     | **0.00E+00** | **0.00E+00** | **1.00E+00** | 1.55E-05     | 8.29E-01     | 9.21E-01     | 7.53E-01     | 7.90E-02     | 1.27E+01     | | FaissIVF(n\\_list=256, n\\_probe=100)  | 2.17E+01     | 4.97E-09     | 2.48E-09     | **0.00E+00** | **0.00E+00** | **1.00E+00** | 7.20E-06     | **8.31E-01** | **9.23E-01** | **7.56E-01** | 1.50E-01     | 6.65E+00     | | FaissIVF(n\\_list=512, n\\_probe=50)   | 2.72E+01     | 1.28E-03     | 2.48E-09     | **0.00E+00** | **0.00E+00** | **1.00E+00** | 5.80E-05     | 8.18E-01     | 9.08E-01     | 7.43E-01     | 4.05E-02     | 2.47E+01     | | FaissIVF(n\\_list=512, n\\_probe=100)  | 2.72E+01     | 4.97E-09     | 2.48E-09     | **0.00E+00** | **0.00E+00** | **1.00E+00** | 4.16E-07     | 8.32E-01     | 9.24E-01     | 7.57E-01     | 7.98E-02     | 1.25E+01     | | FaissIVF(n\\_list=1024, n\\_probe=100) | 4.20E+01     | 9.78E-04     | 2.48E-09     | **0.00E+00** | **0.00E+00** | **1.00E+00** | 3.98E-05     | 8.23E-01     | 9.15E-01     | 7.49E-01     | 4.11E-02     | 2.43E+01     | | FaissIVF(n\\_list=1024, n\\_probe=50)  | 4.20E+01     | 2.50E-03     | 1.45E-04     | 2.48E-09     | **0.00E+00** | **1.00E+00** | 1.89E-04     | 7.92E-01     | 8.79E-01     | 7.20E-01     | **2.09E-02** | **4.77E+01** |  #### Gist-960-angular k=10  | Algorithm                            | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-latency | QPS          | |--------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------| | Pinecone Vector Index                               | **9.62E+00** | 3.64E-04     | **0.00E+00** | **0.00E+00** | **0.00E+00** | **1.00E+00** | 3.14E-05     | 9.23E-01     | 9.61E-01     | 8.88E-01     | 5.07E-02     | 1.97E+01     | | FaissIVF(n\\_list=256, n\\_probe=50)   | 2.14E+01     | 5.59E-04     | 1.49E-09     | 7.45E-10     | **0.00E+00** | **1.00E+00** | 4.18E-05     | 9.25E-01     | 9.63E-01     | 8.90E-01     | 7.90E-02     | 1.27E+01     | | FaissIVF(n\\_list=256, n\\_probe=100)  | 2.14E+01     | **2.98E-09** | 1.49E-09     | 7.45E-10     | **0.00E+00** | **1.00E+00** | 1.14E-05     | **9.32E-01** | **9.71E-01** | **8.96E-01** | 1.51E-01     | 6.61E+00     | | FaissIVF(n\\_list=512, n\\_probe=50)   | 2.46E+01     | 9.32E-04     | 1.97E-04     | 1.49E-09     | 7.45E-10     | **1.00E+00** | 1.60E-04     | 8.96E-01     | 9.33E-01     | 8.62E-01     | 4.08E-02     | 2.45E+01     | | FaissIVF(n\\_list=512, n\\_probe=100)  | 2.46E+01     | 2.23E-04     | 1.49E-09     | 7.45E-10     | **0.00E+00** | **1.00E+00** | **1.12E-05** | 9.30E-01     | 9.69E-01     | 8.95E-01     | 8.00E-02     | 1.25E+01     | | FaissIVF(n\\_list=1024, n\\_probe=100) | 4.48E+01     | 7.84E-04     | 2.26E-05     | 7.45E-10     | 7.77E-17     | **1.00E+00** | 1.20E-04     | 9.12E-01     | 9.50E-01     | 8.77E-01     | 4.08E-02     | 2.45E+01     | | FaissIVF(n\\_list=1024, n\\_probe=50)  | 4.48E+01     | 1.70E-03     | 5.55E-04     | 1.20E-04     | 7.45E-10     | **1.00E+00** | 4.94E-04     | 8.37E-01     | 8.72E-01     | 8.05E-01     | **2.08E-02** | **4.80E+01** |  #### Fashion-mnist-784-angular k=3  | Algorithm                                                                             | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-latency | QPS          | |---------------------------------------------------------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------| | Pinecone Vector Index                                                                                | **5.52E-01** | 2.45E-07     | **1.64E-07** | **1.26E-07** | 8.98E-08     | **1.00E+00** | 1.67E-06     | **8.39E-01** | **9.29E-01** | **7.66E-01** | 5.61E-03     | 1.78E+02     | | FaissIVF(n\\_list=256, n\\_probe=50)                                                    | 5.57E+00     | **2.43E-07** | **1.64E-07** | **1.26E-07** | 8.96E-08     | **1.00E+00** | 3.19E-06     | **8.39E-01** | **9.29E-01** | **7.66E-01** | 3.81E-03     | 2.63E+02     | | FaissIVF(n\\_list=256, n\\_probe=100)                                                   | 5.57E+00     | **2.43E-07** | **1.64E-07** | **1.26E-07** | **8.94E-08** | **1.00E+00** | **1.45E-07** | **8.39E-01** | **9.29E-01** | **7.66E-01** | 7.16E-03     | 1.40E+02     | | FaissIVF(n\\_list=512, n\\_probe=50)                                                    | 4.33E+00     | 2.45E-07     | **1.64E-07** | **1.26E-07** | 8.97E-08     | **1.00E+00** | 3.27E-06     | **8.39E-01** | **9.29E-01** | **7.66E-01** | 2.18E-03     | 4.59E+02     | | FaissIVF(n\\_list=512, n\\_probe=100)                                                   | 4.33E+00     | **2.43E-07** | **1.64E-07** | **1.26E-07** | **8.94E-08** | **1.00E+00** | 1.60E-06     | **8.39E-01** | **9.29E-01** | **7.66E-01** | 4.06E-03     | 2.46E+02     | | FaissIVF(n\\_list=1024, n\\_probe=100)                                                  | 4.69E+00     | 2.45E-07     | **1.64E-07** | **1.26E-07** | 8.96E-08     | **1.00E+00** | 4.53E-06     | **8.39E-01** | **9.29E-01** | **7.66E-01** | 2.39E-03     | 4.18E+02     | | FaissIVF(n\\_list=1024, n\\_probe=50)                                                   | 4.69E+00     | 2.48E-07     | **1.64E-07** | 1.27E-07     | 9.00E-08     | **1.00E+00** | 1.17E-05     | **8.39E-01** | **9.29E-01** | 7.65E-01     | 1.40E-03     | 7.14E+02     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 2.94E+01     | 2.56E-07     | **1.64E-07** | 1.27E-07     | 9.00E-08     | **1.00E+00** | 6.86E-05     | 8.38E-01     | 9.28E-01     | 7.65E-01     | 5.98E-04     | 1.67E+03     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 2.81E+01     | 2.77E-07     | 1.65E-07     | 1.27E-07     | 9.06E-08     | 9.99E-01     | 1.13E-04     | 8.37E-01     | 9.26E-01     | 7.63E-01     | **2.91E-04** | **3.44E+03** | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 2.81E+01     | 2.51E-07     | **1.64E-07** | 1.27E-07     | 9.00E-08     | **1.00E+00** | 5.62E-05     | **8.39E-01** | 9.28E-01     | 7.65E-01     | 7.46E-04     | 1.34E+03     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 2.94E+01     | 2.47E-07     | **1.64E-07** | **1.26E-07** | 9.00E-08     | **1.00E+00** | 4.21E-05     | **8.39E-01** | 9.28E-01     | 7.65E-01     | 1.45E-03     | 6.90E+02     |  #### Fashion-mnist-784-angular k=10  | Algorithm                                                                             | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-latency | QPS          | |---------------------------------------------------------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------| | Pinecone Vector Index                                                                                | **5.50E-01** | 1.96E-07     | **1.39E-07** | **1.15E-07** | **9.24E-08** | **1.00E+00** | 3.08E-06     | **9.40E-01** | **9.75E-01** | **9.08E-01** | 7.01E-03     | 1.43E+02     | | FaissIVF(n\\_list=256, n\\_probe=50)                                                    | 5.07E+00     | 1.96E-07     | **1.39E-07** | **1.15E-07** | **9.24E-08** | **1.00E+00** | 6.31E-06     | **9.40E-01** | **9.75E-01** | **9.08E-01** | 3.91E-03     | 2.56E+02     | | FaissIVF(n\\_list=256, n\\_probe=100)                                                   | 5.07E+00     | **1.95E-07** | **1.39E-07** | **1.15E-07** | **9.24E-08** | **1.00E+00** | 1.36E-06     | **9.40E-01** | **9.75E-01** | **9.08E-01** | 7.16E-03     | 1.40E+02     | | FaissIVF(n\\_list=512, n\\_probe=50)                                                    | 5.31E+00     | 2.00E-07     | 1.40E-07     | **1.15E-07** | **9.24E-08** | **1.00E+00** | **1.04E-05** | **9.40E-01** | **9.75E-01** | 9.07E-01     | 2.22E-03     | 4.50E+02     | | FaissIVF(n\\_list=512, n\\_probe=100)                                                   | 5.31E+00     | 1.96E-07     | **1.39E-07** | **1.15E-07** | **9.24E-08** | **1.00E+00** | 4.07E-06     | **9.40E-01** | **9.75E-01** | **9.08E-01** | 4.07E-03     | 2.45E+02     | | FaissIVF(n\\_list=1024, n\\_probe=100)                                                  | 3.35E+00     | 1.99E-07     | 1.40E-07     | **1.15E-07** | **9.24E-08** | **1.00E+00** | 1.16E-05     | **9.40E-01** | **9.75E-01** | 9.07E-01     | 2.42E-03     | 4.13E+02     | | FaissIVF(n\\_list=1024, n\\_probe=50)                                                   | 3.35E+00     | 2.17E-07     | 1.41E-07     | **1.15E-07** | 9.26E-08     | **1.00E+00** | 3.08E-05     | 9.39E-01     | 9.74E-01     | 9.07E-01     | 1.42E-03     | 7.06E+02     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 2.93E+01     | 2.15E-07     | 1.41E-07     | **1.15E-07** | 9.28E-08     | **1.00E+00** | 9.01E-05     | 9.38E-01     | 9.73E-01     | 9.06E-01     | 5.88E-04     | 1.70E+03     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 2.81E+01     | 3.40E-04     | 1.43E-07     | 1.16E-07     | 9.31E-08     | 9.99E-01     | 1.80E-04     | 9.35E-01     | 9.70E-01     | 9.03E-01     | **2.89E-04** | **3.46E+03** | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 2.81E+01     | 2.02E-07     | 1.40E-07     | **1.15E-07** | **9.24E-08** | **1.00E+00** | 7.17E-05     | 9.39E-01     | 9.74E-01     | 9.07E-01     | 7.34E-04     | 1.36E+03     | | Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 2.93E+01     | 1.99E-07     | 1.40E-07     | **1.15E-07** | **9.24E-08** | **1.00E+00** | 5.15E-05     | **9.40E-01** | 9.74E-01     | 9.07E-01     | 1.48E-03     | 6.74E+02     |  #### Deep-image-96 k=3  | Algorithm                            | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-latency | QPS          | |--------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------| | Pinecone Vector Index                               | 7.00E+01     | **1.79E-07** | **1.22E-07** | **8.94E-08** | **5.96E-08** | **1.00E+00** | **9.90E-08** | **8.36E-01** | **9.27E-01** | **7.61E-01** | 6.74E-02     | 1.48E+01     | | FaissIVF(n\\_list=256, n\\_probe=50)   | **6.95E+01** | 1.89E-07     | 1.24E-07     | **8.94E-08** | **5.96E-08** | **1.00E+00** | 1.56E-05     | 8.35E-01     | 9.26E-01     | **7.61E-01** | 6.62E-02     | 1.51E+01     | | FaissIVF(n\\_list=256, n\\_probe=100)  | **6.95E+01** | 1.81E-07     | **1.22E-07** | **8.94E-08** | **5.96E-08** | **1.00E+00** | 4.30E-06     | **8.36E-01** | **9.27E-01** | **7.61E-01** | 1.29E-01     | 7.75E+00     | | FaissIVF(n\\_list=512, n\\_probe=50)   | 7.56E+01     | 1.94E-07     | 1.24E-07     | **8.94E-08** | **5.96E-08** | 9.99E-01     | 3.37E-05     | 8.35E-01     | 9.26E-01     | 7.60E-01     | 3.39E-02     | 2.95E+01     | | FaissIVF(n\\_list=512, n\\_probe=100)  | 7.56E+01     | 1.81E-07     | **1.22E-07** | **8.94E-08** | **5.96E-08** | **1.00E+00** | 6.41E-06     | **8.36E-01** | **9.27E-01** | **7.61E-01** | 6.52E-02     | 1.53E+01     | | FaissIVF(n\\_list=1024, n\\_probe=100) | 9.22E+01     | 1.89E-07     | 1.24E-07     | **8.94E-08** | **5.96E-08** | **1.00E+00** | 2.99E-05     | 8.35E-01     | 9.26E-01     | **7.61E-01** | **3.40E-02** | **2.94E+01** | | FaissIVF(n\\_list=1024, n\\_probe=50)  | 9.22E+01     | 8.27E-04     | 1.24E-07     | **8.94E-08** | **5.96E-08** | 9.99E-01     | 1.08E-04     | 8.32E-01     | 9.23E-01     | 7.58E-01     | 1.80E-02     | 5.56E+01     |  #### Deep-image-96 k=10  | Algorithm                            | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-latency | QPS          | |--------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------| | Pinecone Vector Index                               | 7.00E+01     | **1.67E-07** | **1.15E-07** | **8.64E-08** | **5.66E-08** | **1.00E+00** | **2.08E-07** | **9.38E-01** | **9.74E-01** | **9.04E-01** | 6.88E-02     | 1.45E+01     | | FaissIVF(n\\_list=256, n\\_probe=50)   | **6.92E+01** | 2.04E-07     | 1.16E-07     | 8.72E-08     | **5.66E-08** | **1.00E+00** | 4.43E-05     | 9.36E-01     | 9.72E-01     | 9.02E-01     | 6.63E-02     | 1.51E+01     | | FaissIVF(n\\_list=256, n\\_probe=100)  | **6.92E+01** | 1.70E-07     | **1.15E-07** | **8.64E-08** | **5.66E-08** | **1.00E+00** | 8.33E-06     | 9.37E-01     | **9.74E-01** | **9.04E-01** | 1.29E-01     | 7.73E+00     | | FaissIVF(n\\_list=512, n\\_probe=50)   | 7.53E+01     | 9.99E-04     | 1.20E-07     | 8.87E-08     | 5.81E-08     | 9.99E-01     | 9.69E-05     | 9.34E-01     | 9.70E-01     | 9.00E-01     | 3.37E-02     | 2.97E+01     | | FaissIVF(n\\_list=512, n\\_probe=100)  | 7.53E+01     | 1.78E-07     | **1.15E-07** | **8.64E-08** | **5.66E-08** | **1.00E+00** | 1.69E-05     | 9.37E-01     | 9.73E-01     | 9.03E-01     | 6.48E-02     | 1.54E+01     | | FaissIVF(n\\_list=1024, n\\_probe=100) | 9.27E+01     | 3.85E-04     | 1.18E-07     | 8.79E-08     | 5.74E-08     | **1.00E+00** | 7.41E-05     | 9.35E-01     | 9.71E-01     | 9.02E-01     | 3.40E-02     | 2.94E+01     | | FaissIVF(n\\_list=1024, n\\_probe=50)  | 9.27E+01     | 3.02E-03     | 1.28E-07     | 9.09E-08     | 5.96E-08     | 9.99E-01     | 2.68E-04     | 9.28E-01     | 9.64E-01     | 8.95E-01     | **1.79E-02** | **5.59E+01** |  ---  If you would like to take advantage of the accuracy and performance of the Pinecone Vector Index, [learn more about Pinecone](/) and [try it yourself](/start). ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e340"
  },
  "content": "categories:   - Company News toc: >- author:   name: Gibbs Cullen   position: Senior Product Marketing Manager   src: /images/gibbs-cullen.jpg   href: https://www.linkedin.com/in/gibbscullen/ date: \"2022-12-23\" # Open Graph description: \"History in the making: A year of learning, growing, and connecting together\" images: [\"/images/2022-highlights-1.jpg\"] thumbnail: \"/images/2022-highlights-thumbnail.jpg\" ---  ![History in the making](/images/2022-highlights-1.jpg)  This has been an exciting year for vector search and vector databases. Embedding models got better and more accessible, the use cases for vector search expanded, and new applications powered by vector search generated a lot of buzz. While a lot happened over the past twelve months, we wanted to share a few highlights of our own.  In 2022...  ## Demand grew, and so did we  We [launched](https://techcrunch.com/2021/01/27/pinecone-lands-10m-seed-for-purpose-built-machine-learning-database/) Pinecone back in 2021 – introducing [the vector database](/learn/vector-database/) as a way to make it easy for developers to build high-performance vector search applications. The adoption of Pinecone exceeded even our own ambitious expectations.  That’s why at the beginning of 2022, we needed to make some changes to keep up with our quickly growing user base. And we did just that - starting with a $28M Series A funding [announcement](https://www.pinecone.io/learn/funding-search-ai-age/) in March. Since then, we’ve continued to observe the adoption of vector search in various applications and across new use cases. Pinecone is now being used by customers like Workday, Mem, Clubhouse, BambooHR, [Expel](/learn/expel-alert-similarity/), Course Hero, and many others for things like semantic search, anomaly detection, recommendation systems, mutli-modal search, fraud detection, and more.  To support all this growth, we also needed to grow the team. At the start of 2022, we were a company of 21 employees. Today, we more than doubled that number with employees (especially in engineering) spanning 25 cities and 6 countries and quickly growing hubs in New York City and Tel Aviv. We celebrated this amazing growth in person during [our offsite in Greece](https://www.linkedin.com/posts/pinecone-io_a-couple-of-weeks-ago-we-had-the-pleasure-activity-6984203237394366464-XVc7?utm_source=share&utm_medium=member_desktop)!  ![Team Pinecone](/images/2022-highlights-2.jpg) <small>Team Pinecone visited the Acropolis during our company offsite in Greece </small>  ## Research advancements kept us on our toes  This year was also focused a lot on learning - whether that be learning more about new use cases and the capabilities of vector search, or for creating learning opportunities for our users and the community.  We created many new learning resources around the latest technologies, including:  - Learning how to train models in [low-resource scenarios with GPL](/learn/gpl/). - Exploring multi-modal ML with [Open AI’s CLIP](/learn/clip/) and in the [Embedding Methods for Image Search](/learn/image-search/) series. - Searching through [video and audio with Whisper](/learn/openai-whisper/). - Making [Stable Diffusion more accessible](https://www.youtube.com/watch?v=YMlzhnlSAww) with “vector caching”. - Collaborating with [Deepset AI](https://www.deepset.ai) to release the [HaystackDocumentStore](https://docs.pinecone.io/docs/haystack) for Haystack users to build better, faster, and feature-rich [semantic search tools](/learn/haystack-lfqa/). - Helping users get started with semantic search in the [NLP for Semantic Search](/learn/nlp/) series.  In 2022, we attended and sponsored several conferences, including [Southern Data Science Conference](https://www.southerndatascience.com/), [SIGIR](/learn/sigir-2022/), and the [AI Summit](https://newyork.theaisummit.com/). We also hosted many webinars with guest speakers from Pinecone partners and supporters like [Yury Malkov](https://twitter.com/malkovyury) (advisor to Pinecone), [Nils Reimers](https://www.nils-reimers.de/), and [Julien Simon](https://huggingface.co/juliensimon). Subscribe to our [mailing list](/community/) and join our [meetup group](https://www.meetup.com/vector-search/) for the latest news and upcoming events.  ![Pinecone Booth at AI Summit in NYC ](/images/2022-highlights-3.jpg) <small>Happy campers at the Pinecone booth during the AI Summit in NYC</small>  ## Our users inspired us to keep innovating  Since launching Pinecone, we’ve worked hard to innovate and add features for our users. We had many product launches this year to help our users build bigger, better, and faster. Some highlights include:  - [Hybrid vector index](/learn/hybrid-search/) for keyword-aware semantic search - 10x faster searches with proprietary [graph-based vector indexes](/learn/pods-for-performance/) - Partial and real-time index updates - Combined vector search with [single-stage metadata filtering](/learn/vector-search-filtering/) - Zero-downtime [vertical scaling](/learn/faster-easier-scalable/)  [Sign-up](http://app.pinecone.io) for free, and experience the ease of building high-performance vector search applications with Pinecone.  ## What’s next?  Overall, it’s been a year full of growth, learning, coming together, and pushing the boundaries of vector search. We are very proud of the company and culture we’ve built, and can’t wait to continue growing, learning, and improving in 2023. Want to join us and continue to shape the future of AI and ML applications? [We’re hiring](/careers/#open-roles) across all functions! ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e342"
  },
  "content": "author: James Briggs intro: |   Facebook AI Similarity Search (Faiss) is one of the best open source options for similarity search. In this ebook, you will learn the essentials of vector search and how to apply them in Faiss to build powerful vector indexes. emailSubmit: true socialShare: true image: /images/faiss-ebook.png images: ['/images/faiss-ebook.png'] introChapter:      title: Introduction     text: |       Vector search has been used by tech giants like Google and Amazon for decades. It has been claimed to be a significant driver in clicks, views, and sales across several platforms. Yet, it was only with Faiss that this technology became more accessible.              In the past few years, vector search exploded in popularity. It has driven ecommerce sales, powered music and podcast search, and even recommended your next favorite shows on streaming platforms. Vector search is everywhere and in the following chapters you will discover why it has found such great success and how to apply it yourself using the Facebook AI Similarity Search (Faiss) library. chapters:   - title: Introduction to Facebook AI Similarity Search (Faiss)     text: An overview of the Faiss library and similarity search.     url: /learn/faiss-tutorial/   - title: Nearest Neighbor Indexes for Similarity Search     text: Learn how to choose the right index in Faiss.     url: /learn/vector-indexes/   - title: \"Locality Sensitive Hashing (LSH): The Illustrated Guide\"     text: Take your first steps to a deeper understanding of approximate nearest neighbor indexes with LSH.     url: /learn/locality-sensitive-hashing/   - title: Random Projection for Locality Sensitive Hashing     text: Apply LSH to modern dense vector representations using random projection.     url: /learn/locality-sensitive-hashing-random-projection/   - title: Product Quantization     text: Learn how Product Quantization (PQ) can be used to compress indexes by upto 97%.     url: /learn/product-quantization/   - title: Hierarchical Navigable Small Worlds (HNSW)     text: HNSW graphs are among the top performing indexes in similarity search.     url: /learn/hnsw/   - title: Composite Indexes and the Faiss Index Factory     url: /learn/composite-indexes/     text: Learn how to apply all we have learned so far to create multi-step composite indexes.   - title: EMAIL_SUBMIT #email submit form   - title: And more... ---  ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e344"
  },
  "title": "More",
  "description": "Vector Search in the Wild.",
  "author": "James Briggs",
  "intro": "|",
  "emailSubmit": "true",
  "socialShare": "true",
  "image": "/images/wild-ebook.png",
  "images": "['/images/wild-ebook.png']",
  "introChapter": "",
  "text": "How Nyckel automates ML workflows for image and text classification, object detection, and visual search with the help of vector search.",
  "- title": "How Nyckel Built An API for Semantic Image Search",
  "url": "/learn/nyckel-ml-automation/",
  "bonusSection": "# Bonus content / further materials",
  "content": "  - title: EMAIL_SUBMIT #email submit form   - title: How Vector Search Powers Ecommerce     text: How giants like Amazon or Shein know what you want before you do.   - title: The Future of Document Discovery     text: How organizations are using Q&A to enable a more \"human\" search experience.   - title: AI-Powered Assistants are Everywhere     text: Learn how vector search is powering the next generation of chatbots.   - title: And more... ---  ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e346"
  },
  "title": "\"Visual Guide to Applied Convolution Neural Networks\"",
  "headline": "\"Visual Guide to Applied Convolution Neural Networks\"",
  "weight": "4",
  "- name": "Laura Carnevali",
  "position": "Developer",
  "src": "/images/laura-carnevali.jpeg",
  "href": "\"https://www.linkedin.com/in/laura-carnevali-071a01b7/\"",
  "description": "A visual explainer of everything you need to know about convolutional neural networks with Python implementation details",
  "images": "['https://www.pinecone.io/images/cnn-0.png']",
  "content": "**C**onvolutional **N**eural **N**etworks (CNNs) have been the undisputed champions of **C**omputer **V**ision (CV) for almost a decade. Their widespread adoption kickstarted the world of deep learning; without them, the field of AI would look very different today.  Before deep learning with CNNs, CV applications relied on brittle edge detection algorithms, color profiles, and a plethora of manually scripted processes. These could rarely be applied across different datasets or use cases.  The result is that every dataset and every use-case required significant manual intervention and domain-specific knowledge, rarely producing performance acceptable for broader use.  Deep-layered CNNs changed this. Rather than manual feature extraction, CNNs proved capable of doing this automatically for a vast number of datasets and use cases. All they needed was training data.  Big data with deep CNNs have remained the de-facto standard in computer vision. New models using vision transformers (ViT) and multi-modality may change this in the future, but for now CNNs still dominate state-of-the-art benchmarks in vision. In this hands-on article we will learn why.  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/ZBfpkepdZlw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ---  ## What Makes a CNN?  CNNs are neural networks known for their performance on image datasets. They are characterized by something called a *convolutional layer* that can detect abstract features of an image. These images can be shifted, squashed, or rotated; if a human can still recognize the features, a CNN likely can too.  Because of their affinity to image-based applications, we find CNNs used for image classification, object detection, object recognition, and many more tasks within the realm of CV.  ![cnn](./images/cnn-1.png) <small>The typical architecture of a CNN.</small>  Deep-layered CNNs are any neural network that satisfies two conditions; (1) they have many layers (deep), and (2) they contain convolutional layers. Beyond that, networks can have many features, including pooling, normalization, and linear layers.  ### Convolutional Layers  An image is a big array of pixel activation values. These arrays are followed by more arrays of (initially random) values – the weights – that we call a *\"filter\"* or *\"kernel\"*. A convolutional layer is an element-wise multiplication between these pixel values and the filter weights – *which are then summed*.  ![scalar-product](./images/cnn-2.png)  This element-wise operation **followed by the sum of the resulting values** is often referred to as the *\"scalar product\"* because it results in a single *scalar* value [2].  ![scalar-product-ex](./images/cnn-3.png)  Considering the above example, when the 3x3 filter is applied to the input image starting from the top-left, the resulting value from the element-wise multiplication is 3.  We don't return a single scalar value because we perform many of these operations on each layer. For each convolutional layer, the filter slides (or *\"convolves\"*) over the previous layer's matrix (or image) from left-to-right and top-to-bottom.  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/cnn-4.mp4\" type=\"video/mp4\"> </video>  The filter can detect specific *local* features such as edges, shapes, and textures by sliding over the image.  ---  *This convolution has a parallel in signal processing. Given an input audio wave $f$, we \"convolve\" a filter $g$ over it to produce a modified audio wave. See the appendix for a more detailed explanation.*  ---  The convolution output is called a **\"feature map\"** or *\"activation map\"* thanks to the representation or *activations* of detected features from the input layer.  As the element-wise multiplication of the filter outputs a single value after processing multiple input values, we need to be mindful of excessive information loss via dimensionality reduction (i.e., compression).  We may want to increase or decrease the amount of compression our filters create. Compression is controlled using the filter *size* and how quickly it moves across the image (the `stride`).  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/cnn-5.mp4\" type=\"video/mp4\"> </video> <small>Larger filters create more compression.</small>  The `stride` defines the number of *pixels* a filter moves after every calculation. By increasing the stride, the filter will travel across the entire input image in fewer steps, outputting fewer values and producing a more compressed feature map.  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/cnn-6.mp4\" type=\"video/mp4\"> </video> <small>A greater `stride` produces a more compressed feature map.</small>  There are some surprising effects of image compression, and one that we must be careful of is the filter's interaction with the border areas of an input layer.  ![border](./images/cnn-7.png)  The border effects *on small images* can result in a rapid loss of information for images containing too little information. To avoid this, we either reduce compression using the previously discussed techniques (filter size and `stride`) or add `padding`.  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/cnn-8.mp4\" type=\"video/mp4\"> </video> <small>Adding padding to images/layers can help limit compression.</small>  Adding a set of zero-value pixels around the image allows us to limit or prevent compression between layers. In the example above, the operation *without* padding causes compression of *10x10* pixels into *8x8* pixels. With padding, no compression occurs (a *10x10* feature map is output). TK 10x10??  For larger images, compression is less likely to cause problems. However, when working with smaller images, `padding` acts as an easy and effective remedy to border effects,  ### Depth  We mentioned that *deep-layered* CNNs were what took CV to new heights. The AlexNet authors found the *depth* of networks to be a key component for higher performance [1]. Each successive layer extracts features from the previous layer (and previously extracted features). This chain of consecutive extractions produces ever more *\"abstract\"* features that can better represent images in more *\"human\"* terms.  A shallow network may recognize that an image contains an animal, but as we add another layer, it may identify a dog. Adding another may result in identifying specific breeds like Staffordshire Bull Terrier or Husky. More layers generally result in recognition of more abstract and specific concepts.  ### Activation Functions  Activation functions are a common feature in every type of neural network. They add *non-linearity* to networks, enabling the representation of more complex patterns.  ![act-functions](./images/cnn-9.png) <small>Some of the most popular activation functions; ReLU, tanh, and sigmoid.</small>  In the past, CNNs often used hidden layer activation functions like sigmoid or tanh. However, in 2012 a new activation function called a **Re**ctified **L**inear **U**nit was popularized through its use in AlexNet, the best-performing CNN of its time.  ![reluvstanh](./images/cnn-10.png) <small>The authors of AlexNet noted that results from a four-layer CNN with ReLU activation functions reached a 25% error rate on the CIFAR-10 dataset six times faster than the equivalent with Tanh [1].</small>  Nowadays, ReLU is still a popular choice. It is simpler than tanh and sigmoid and does not require normalization to avoid saturation (i.e., activations congregating towards the min/max values).  ### Pooling Layers  Output feature maps are highly sensitive to small changes in the location of input features [3]. To some degree, this can be useful, as it can tell us the difference between a cat's face and a dog's face. However, if an eye is two pixels further to the left than expected, the model should still be able to identify a face.  CNNs use pooling layers to handle this. Pooling layers are a downsampling method that compresses information from one layer into a smaller space in the next layer.  An effect of pooling is that information across several pixels is compressed into a single activation, essentially \"smoothing out\" variations across groups (or patches) of pixels.  The two most common pooling methods are average pooling and max pooling. Average pooling takes the average of activations in the window, whereas max pooling takes their maximum value.  ![avg-vs-max-pooling](./images/cnn-11.png)  ### Fully-Connected Layers  Fully connected linear layers are another common feature of CNNs. They are neural networks in their most stripped-down form; the dot product between inputs $X$ and layer weights $W$ with a bias term $b$ and activation function.  ![fully-connected](./images/cnn-12.png)  These layers are usually found towards the end of a CNN and handle the transformation of CNN embeddings from 3D tensors to more understandable outputs like class predictions.  Often within these final layers, we will find the most *information-rich* vector representations of the data being input to the model. In the next chapter of the ebook, we will explore these in more depth for use in content-based image retrieval (CBIR).  For now, let's focus on the task of classification. Classifiers tend to apply a *softmax* activation function that creates a probability distribution across the final output nodes — where each node represents a specific class.  ![softmax](./images/cnn-13.png)  After these final fully-connected layers, we have our predictions.  These are a few of the most common components of CNNs, but with time many different types of CNNs with different network architectures were designed. So there is no \"specific\" architecture, just a set of guideposts in the form of high-performing networks.  ## Popular Architectures  Throughout the years, there have been several hugely successful CNN architectures. We will take a high-level look at a few of the most relevant.  ### LeNet  LeNet is the earliest example of a \"deep\" CNN, developed in 1998 by Yann LeCun, et. al. [4]. Many of us have likely interacted with LeNet as Bell Labs licensed it to banks around the globe for reading the digits on handwritten cheques.  ![lenet](./images/cnn-14.png) <small>LeNet model architecture [4].</small>  This first example of a commercially successful deep CNN was surprisingly the only such example of a successful deep CNN for another 14 years.  ### AlexNet  October 2012 is widely regarded as ground zero for the birth of deep learning. The catalyst was AlexNet winning the ImageNet ILSVRC challenge [1]. AlexNet can be seen as a continuation of LeNet, using a similar architecture but adding more layers, training data, and safeguards against overfitting.  ![alexnet](./images/cnn-15.png) <small>AlexNet model architecture [1].</small>  After AlexNet, the broader community of CV researchers began focusing on training deeper models with larger datasets. The following years saw variations of AlexNet continue winning ILSVRC and reaching ever more impressive performance.  ### VGGNet  ![veggnet16](./images/cnn-16.png) <small>VGGNet model architecture [5].</small>  AlexNet was dethroned as the winner of ILSVRC in 2014 with the introduction of VGGNet, developed at Oxford University [5]. Many variants of VGGNet were developed, characterized by the number of layers they contained, such as 16 total layers (13 convolutional) for VGGNet-16, and 19 total layers for VGGNet-19.  ### ResNet  ResNet became the new champion of CV in 2015 [6]. The ResNet variants were much deeper than before, the first containing 34 layers. Since then, 50+ layer ResNet models have been developed and hold state-of-the-art results on many benchmarks.  ![resnet](./images/cnn-17.png) <small>ResNet model architecture [6].</small>  ResNet was inspired by VGGNet but added smaller filters and a less complex network architecture. Shortcut connections between different layers were also added, giving the name of **Res**idual **Net**work (ResNet).  Without these shortcuts, the greater depth of ResNet results in information loss over the many layers of transformations. Adding the shortcuts enabled information to be maintained across these greater distances.  ## Classification with CNNs  We've understood the standard components of CNNs and how these work together to extract abstract — but meaningful — features from images.  We also looked at a few of the most popular architectures. Let's now put all of this together and work through an application of a CNN for image classification.  ### Data Preprocessing  As usual, our first task is data preparation and preprocessing. We will use a popular image classification dataset called CIFAR-10 hosted on Hugging Face *datasets*.  {{< notebook file=\"cnn-get-data\" height=\"full\" >}}  Here we have specified that we want the *training* split of the dataset with `split='train'`. We return *50K* images split across ten classes from this.  Most CNNs can only accept images of a fixed size. To handle this, we will reshape all images to 32x32 pixels using `torchvision.transforms`; a pipeline built for image preprocessing.  ```python import torchvision.transforms as transforms  # image size img_size = 32  # preprocess variable, to be used ahead preprocess = transforms.Compose([     transforms.Resize((img_size,img_size)),     transforms.ToTensor() ]) ```  The `preprocess` pipeline handles the height and width of our images but not the *depth*. Every image has a number of \"color channels\" that define its depth. For RGB images, there are three color channels; **r**ed, **g**reen, and **b**lue, whereas grayscale images have just one.  Datasets commonly contain images with different color profiles, so we must convert them into a set format. We will use RGB, and as our images are all Python PIL objects, the color format is stored in an attribute called `mode`. The mode will be `RGB` for RGB images and `L` for grayscale images.  We perform the conversion to RGB and also perform the `preprocess` transformations like so:  {{< notebook file=\"cnn-process-train-data\" height=\"full\" >}}  Leaving us with *50,000* training examples, each a *3x32x32*-dimensional tensor. The tensors are normalized to a $[0, 1]$ range by the `transforms.ToTensor()` step.  Right now, this normalization does not consider the pixel values of our overall set of images. Ideally, we should normalize by the mean and standard deviation values specific to this dataset. For this dataset, these are:  ```python mean = [0.4670, 0.4735, 0.4662] std = [0.2496, 0.2489, 0.2521] ```  This normalization step is applied by another `transformers.Compose` step like so:  ```python preprocess = transforms.Compose([     transforms.Normalize(mean=mean, std=std) ])  for i in tqdm(range(len(inputs_train))):     # prepocessing     input_tensor = preprocess(inputs_train[i][0])     # replace with normalized tensor     inputs_train[i][0] = input_tensor ```  We repeat the steps above for a test set that we will use for validating our CNN classifier performance. The validation set is also downloaded from Hugging Face datasets via `load_dataset` by switching the earlier `split` parameter to `'test'`:  ```python dataset_val = load_dataset(     'cifar10',     split='test',  # test set (used as validation set)     ignore_verifications=False  # set to True if seeing splits Error ) ```  *The validation set must also be preprocessed, [find the code for it here](https://github.com/pinecone-io/examples/blob/master/learn/image-retrieval/cnn/cifar10.ipynb).*  Both train and validation splits are added into `DataLoader` objects. The data loaders shuffle, batch, and load data into the model during training or inference (validation).  ```python batch_size = 64  # add to dataloaders dloader_train = torch.utils.data.DataLoader(   \tinputs_train,     batch_size=batch_size,     shuffle=True )  dloader_val = torch.utils.data.DataLoader(   \tinputs_val,     batch_size=batch_size,     shuffle=False ) ```  With that, our data is ready, and we can move on to building and then training our CNN.  ### CNN Construction  We can start building our CNN by creating a `ConvNeuralNet` class that will contain all our network layers and define the order of transformations through the network. The network will look like this:  ![code-cnn](./images/cnn-18.png)  ```python # creating a CNN class class ConvNeuralNet(nn.Module): \t#  determine what layers and their order in CNN object      def __init__(self, num_classes):         super(ConvNeuralNet, self).__init__()         self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, padding=1)         self.relu1 = nn.ReLU()         self.max_pool1 = nn.MaxPool2d(kernel_size=3, stride=2)          self.conv_layer2 = nn.Conv2d(in_channels=64, out_channels=192, kernel_size=4, padding=1)         self.relu2 = nn.ReLU()         self.max_pool2 = nn.MaxPool2d(kernel_size=3, stride=2)          self.conv_layer3 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, padding=1)         self.relu3 = nn.ReLU()                  self.conv_layer4 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1)         self.relu4 = nn.ReLU()          self.conv_layer5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)         self.relu5 = nn.ReLU()         self.max_pool5 = nn.MaxPool2d(kernel_size=3, stride=2)                  self.dropout6 = nn.Dropout(p=0.5)         self.fc6 = nn.Linear(1024, 512)         self.relu6 = nn.ReLU()         self.dropout7 = nn.Dropout(p=0.5)         self.fc7 = nn.Linear(512, 256)         self.relu7 = nn.ReLU()         self.fc8 = nn.Linear(256, num_classes)          # progresses data across layers         def forward(self, x):         out = self.conv_layer1(x)         out = self.relu1(out)         out = self.max_pool1(out)                  out = self.conv_layer2(out)         out = self.relu2(out)         out = self.max_pool2(out)          out = self.conv_layer3(out)         out = self.relu3(out)          out = self.conv_layer4(out)         out = self.relu4(out)          out = self.conv_layer5(out)         out = self.relu5(out)         out = self.max_pool5(out)                  out = out.reshape(out.size(0), -1)                  out = self.dropout6(out)         out = self.fc6(out)         out = self.relu6(out)          out = self.dropout7(out)         out = self.fc7(out)         out = self.relu7(out)          out = self.fc8(out)  # final logits         return out ```  After designing the network architecture, we initialize it — and *if* we have access to hardware acceleration (through CUDA or MPS), we move the model to that device.  ```python import torch  device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # set the model to device model = ConvNeuralNet(num_classes).to(device) ```  Next, we set the loss and optimizer functions used during training.  ```python # set loss function loss_func = nn.CrossEntropyLoss() # set learning rate  lr = 0.008 # set optimizer as SGD optimizer = torch.optim.SGD(     model.parameters(), lr=lr )  ```  We will train the model for 50 epochs. To ensure we're not overfitting to the training set, we pass the validation set through the model for *inference only* at the end of each epoch. If we see validation set performance suddenly degrade while train set performance improves, we are likely overfitting.  The training and validation loops are written like so:  ```python num_epochs = 50 for epoch in range(num_epochs):     model.train() \t\t# load in the data in batches     for i, (images, labels) in enumerate(dloader_train):           # move tensors to the configured device         images = images.to(device)         labels = labels.to(device)                  # forward propagation         outputs = model(images)         loss = loss_func(outputs, labels)                  # backward propagation and optimize         optimizer.zero_grad()         loss.backward()         optimizer.step()              # at end of epoch check validation loss and acc     with torch.no_grad():       \t# switch model to eval (not train) model         model.eval()         correct = 0         total = 0         all_val_loss = []         for images, labels in dloader_val:             images = images.to(device)             labels = labels.to(device)             outputs = model(images)             total += labels.size(0)             # calculate predictions             predicted = torch.argmax(outputs, dim=1)             # calculate actual values             correct += (predicted == labels).sum().item()             # calculate the loss             all_val_loss.append(loss_func(outputs, labels).item())         # calculate val-loss         mean_val_loss = sum(all_val_loss) / len(all_val_loss)         # calculate val-accuracy         mean_val_acc = 100 * (correct / total)     print(         'Epoch [{}/{}], Loss: {:.4f}, Val-loss: {:.4f}, Val-acc: {:.1f}%'.format(             epoch+1, num_epochs, loss.item(), mean_val_loss, mean_val_acc         )     ) ```  After training for 50 epochs, we should reach a validation accuracy of ~80%. We can save the model to file and load it again with the following:  ```python # save to file torch.save(model, 'cnn.pt') # load from file and switch to inference mode model = torch.load('cnn.pt') model.eval() ```  ### Inference  Now that we have a fine-tuned CNN model let's look at how we can do image classification. We will use the same test set of CIFAR-10 for this step (ideally, we would not use the same data in our validation and test sets).  First, we preprocess the images using the same `preprocess` pipeline as before and stack every processed image tensor into a single tensor batch.  {{< notebook file=\"cnn-preprocess-testing\" height=\"full\" >}}  We process the tensors through the `model` and use an `argmax` function to retrieve the predictions. The predictions are all integer values, so we retrieve the textual names within the dataset `features`.  {{< notebook file=\"cnn-predict\" height=\"full\" >}}  Now let's loop through the predictions and view our results:  {{< notebook file=\"cnn-results\" height=\"full\" >}}  Almost all predictions are correct, despite being very low-resolution images that many people might struggle to classify.  ---  That's it for this introduction to the long-reigning champions of computer vision; **C**onvolutional **N**eural **N**etworks (CNNs). We've worked through the intuition of convolutions, defined the typical network components, and saw how they were used to construct several of the best-performing CNNs.  From there, we applied CNNs in practice. Building and training a network from scratch before testing it on the CIFAR-10 test set.  Going into the following chapters of the ebook, we will learn how CNNs are used in image retrieval and what may be the [successor of these models](/learn/vision-transformers/).  {{< newsletter text=\"Subscribe for more on computer vision!\" inputText=\"Email address...\" buttonText=\"Submit\">}}  ## Resources  [All Code Notebooks](https://github.com/pinecone-io/examples/tree/master/learn/image-retrieval/cnn)  [1] A. Krizhevsky et al., [ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) (2012), NeurIPS  [2] J. Brownlee, [How Do Convolutional Layers Work in Deep Learning Neural Networks?](https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/) (2019), Deep Learning for Computer Vision  [3] J. Brownlee, [A Gentle Introduction to Pooling Layers for Convolutional Neural Networks](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/) (2019), Deep Learning for Computer Vision.  [4] Y. LeCun, et. at., [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) (1998), Proc. of the IEEE.  [5] K. Simonyan et al., [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556) (2014), CVPT  [6] K. He et al., [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) (2015), CVPR.",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e348"
  },
  "title": "\"Pinecone Recognized as a 2021 Gartner Cool Vendor\"",
  "headline": ">-",
  "name": "Bryan Turriff",
  "position": "Director of Product Marketing",
  "src": "/images/company-bryan.jpeg",
  "href": "https://www.linkedin.com/in/bryanturriff/",
  "date": "\"2021-10-28\"",
  "# Date": "October 28, 2021",
  "description": "Pinecone created the category of Vector Databases to bring the power of vector similarity search to all companies. We are excited today to announce that Pinecone has been named a Gartner Cool Vendor in the October 2021 Gartner Cool Vendors&trade; in Data for Artificial Intelligence and Machine Learning.",
  "thumbnail": "\"/images/gartner-thumbnail.jpg\"",
  "content": "[Pinecone](/) created the category of [Vector Databases](/learn/vector-database/) to bring the power of vector similarity search to all companies. We are excited today to announce that Pinecone has been named a Gartner Cool Vendor in the October 2021 [Gartner Cool Vendors&trade; in Data for Artificial Intelligence and Machine Learning](https://www.gartner.com/en/documents/4006842/cool-vendors-in-data-for-artificial-intelligence-and-machine-learning)&ast;.  According to the report, “As AI and ML techniques become common in the enterprise, data is coming to the foreground. Data is what makes a difference in AI now. Data and analytics leaders want to improve the delivery of AI results with data innovations.” The report also noted that “AI teams are expanding their focus from model development to data that makes these models effective. Many of them are unaware of the proven data management solutions and are looking for AI-specific data offerings to improve and simplify their data-related efforts.”  Vector search can be more accurate and intuitive than traditional keyword search methods, which require the user to make guesses about how data is structured. Before Pinecone, only a few tech giants had the engineering resources and budgets to build their own vector databases. Pinecone’s fully-managed vector database enables organizations of any size to quickly move similarity search and recommendation engines into production without tasking a large group of ML and database engineers to build and maintain one of their own.  Vector databases often require expensive infrastructures to operate and are notoriously difficult to manage. Pinecone solves both of these challenges with a solution that was built to efficiently store and query vector data within a platform that is easy to use.  “We are honored to be recognized as a 2021 Gartner Cool Vendor which we believe is a powerful recognition of the value of vector databases and our work to expand AI-based search technology,” said Edo Liberty, Founder & CEO of Pinecone. “We introduced the vector database and we continue to work with our customers to ensure it powers the best search and recommendation experiences available.”  Gartner clients can [access the full report](https://www.gartner.com/en/documents/4006842-cool-vendors-in-data-for-artificial-intelligence-and-machine-learning).  *&ast; Gartner, “Cool Vendors in Data for Artificial Intelligence and Machine Learning,” Svetlana Sicular, Chirag Dekate, Anthony Mullen, Arun Chandrasekaran, Afraz Jaffri, 13 October 13, 2021*  ### Gartner Disclaimer  <small>GARTNER and COOL VENDORS are registered trademarks and service marks of Gartner, Inc. and/or its affiliates in the U.S. and internationally and are used herein with permission. All rights reserved.</small>  <small>Gartner does not endorse any vendor, product or service depicted in its research publications, and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of Gartner’s Research & Advisory organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose.</small>",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e34a"
  },
  "title": "\"Announcing the Pinecone Vector Database and $10M in Seed Funding\"",
  "headline": "\"Announcing the Pinecone Vector Database and $10M in Seed Funding\"",
  "name": "Edo Liberty",
  "position": "Founder and CEO",
  "src": "/images/company-edo.png",
  "href": "https://edoliberty.github.io/",
  "date": "\"2021-01-27\"",
  "# Date": "January 27, 2021",
  "description": "Machine Learning (ML) represents everything as vectors, from documents, to videos, to user behaviors. This representation makes it possible to accurately search..",
  "images": "['/images/pinecone-product-overview.png']",
  "thumbnail": "\"/images/vector-database-thumbnail.jpg\"",
  "content": "Today we are launching the [Pinecone vector database](/) as a public beta, and announcing $10M in seed funding led by Wing Venture Capital.  ## The Problems and Promises of Vectors  Machine Learning (ML) represents everything as [vectors](/learn/vector-embeddings/), from documents, to videos, to user behaviors. This representation makes it possible to accurately search, retrieve, rank, and classify different items by similarity and relevance. This is useful in many applications such as product recommendations, semantic search, image search, anomaly detection, fraud detection, face recognition, and many more.  Edo Liberty led the creation of Amazon SageMaker at AWS, when he realized the main difficulty companies were facing in leveraging machine learning wasn’t in training or deploying models. The main difficulty was in working with large amounts of vector data in real-time.  What’s so difficult about working with vector data? For starters, the vectors need to be stored and indexed somewhere. Also, the index needs to be updated every time the data is changed. Next, there needs to be a way to search the index and retrieve the most similar items. This is computationally intensive — especially if the results are needed in real-time — so it needs to run on a distributed compute system. Finally, this entire system needs to be operational which means it needs to be monitored and maintained.  There are many solutions that do this for columnar, JSON, document, and other kinds of data, but not for the dense, high-dimensional vectors used in ML and especially in Deep Learning. As a result, companies have been forced to either compromise on accuracy and speed of the application, or to build and maintain their own complex infrastructure for supporting vector data.  It was obvious to Edo this challenge would become widespread as companies launch or expand their AI/ML initiatives, so in 2019 he founded Pinecone and built the vector index &mdash; the core of the vector database.   ## Introducing the Vector Database  Pinecone is a managed database for working with vectors. It provides the infrastructure for ML applications that need to search and rank results based on similarity. With Pinecone, engineers and data scientists can build vector-based applications that are accurate, fast, and scalable, all with a simple API and zero maintenance.  ![Pinecone product overview](/images/pinecone-product-overview.png)  There are four components of the vector database:  * The **vector index** provides blazing-fast [indexing](/learn/what-is-a-vector-index/) and efficient storage for high-dimensional vectors. It uses a proprietary nearest-neighbor search algorithm that is faster and more accurate than any open-source library. (Benchmarks will be published soon.) * **Container distribution** ensures exceptional performance regardless of scale, with dynamic load balancing, replication, name-spacing, sharding, and more. * The **API** enables updating and querying vector indexes from anywhere, including Jupyter notebooks. It is also used for managing artifacts such as models, indexes, and services. * **Managed operations** provide hands-free (for users) resource allocation, observability, SLA guarantees, security, and more.  Since Pinecone is a fully managed service, there is no need to configure open-source software or set up and maintain any infrastructure.  See the [product overview](/product/) for a complete list of features.  ## Funding for Growth and Development  In addition to the product launch, we are also announcing that we raised $10M in seed funding led by [Wing Venture Capital](https://www.wing.vc/), whose founding partner Peter Wagner has joined our board. Peter is a visionary in the cloud, data, and machine learning spaces, as evidenced by his early investment in Snowflake. We can’t imagine a better partner for us than Peter, and we are beyond excited to have him onboard.  ## Try Pinecone or Join Us  Pinecone is available as a public beta starting today. [Try it free for 30 days.](https://www.pinecone.io/start/)  Following the free trial, Pinecone comes with transparent, consumption-based pricing. Companies that require additional operational control, tighter security and governance, guaranteed performance and resilience, and 24/7 on-call operational support can [contact us](/contact/) to learn more and to see a demo.  Companies are only beginning to see the potential of machine learning, and we are excited to help them achieve that potential sooner. For any engineers also excited by this: [We are hiring!](/careers/) ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e34c"
  },
  "title": "Pinecone is now available on the AWS Marketplace",
  "headline": "Pinecone is now available on the AWS Marketplace",
  "name": "Gibbs Cullen",
  "position": "Senior Product Marketing Manager",
  "src": "/images/gibbs-cullen.jpg",
  "href": "https://www.linkedin.com/in/gibbscullen/",
  "date": "\"2023-02-06\"",
  "description": "Start building faster with Pinecone through the AWS Marketplace",
  "images": "[\"/images/pinecone-aws-marketplace.png\"]",
  "thumbnail": "\"/images/pinecone-aws-thumbnail.png\"",
  "content": "We recently [announced](/learn/pinecone-gcp-marketplace/) Pinecone’s availability on the Google Cloud Platform (GCP) marketplace. Today, we are excited to announce that we are now also available on the Amazon Web Services (AWS) Marketplace. This allows AWS customers to start building AI applications on top of the Pinecone vector database within a few clicks.  The AWS Marketplace provides an extensive catalog of software solutions for users to easily explore, test, buy, and deploy on AWS. If you’re an AWS customer, here’s what this means for you:  - **Reduced procurement time**: AWS takes care of licensing and pre-configuration of Pinecone so you can skip the pre-approvals and start building immediately. - **Consolidated spend and billing**: Purchase Pinecone through the marketplace to easily add to existing spend and meet committed spend levels faster. AWS will handle billing and payments so you can view all of your AWS spend in one place.  Read our [integration guide](https://docs.pinecone.io/docs/setting-up-aws-marketplace-billing) and visit the [Marketplace listing](https://aws.amazon.com/marketplace/pp/prodview-xhgyscinlz4jk) to get started.  ## FAQs:  **Q**: Which plans and pricing are available through the AWS Marketplace?  **A**: We currently support Standard and Enterprise plans. The Starter (free) plan is not available. Learn more on our [pricing page](/pricing/).  **Q**: Can I change plans while being billed through the AWS Marketplace?  **A**: Yes, you can still upgrade and downgrade plans through the billing page in the Pinecone console.  **Q**: Where will I see how much I’m being charged for Pinecone?  **A**: All billing through marketplaces will be charged through your cloud provider. Check the billing console from your cloud provider. ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e34e"
  },
  "title": "Straightforward Guide to Dimensionality Reduction",
  "headline": "Straightforward Guide to Dimensionality Reduction",
  "weight": "12",
  "name": "Diego Lopez Yse",
  "position": "Data Scientist",
  "src": "/images/diego-lopez-yse.jpeg",
  "href": "\"https://lopezyse.medium.com/\"",
  "description": "A look at dimensionality reduction and the key techniques like PCA, t-SNE, and UMAP.",
  "images": "['/images/dimensionality-reduction-1.png']",
  "content": "There is a golden rule in Machine Learning that states: **the more data, the better**. This rule is a double-edged sword. An indiscriminate addition of data might introduce noise, reduce model performance, and slow down its training process. In this case, more data can hurt model performance, so it’s essential to understand how to deal with it.  In Machine Learning, “**dimensionality**” refers to the number of features (i.e. input variables) in a dataset. These features (represented as columns in a tabular dataset) fed into a Machine Learning model are called predictors or “*p*”, and the samples (represented as rows) “*n*“. [Most Machine Learning algorithms assume](https://machinelearningmastery.com/how-to-handle-big-p-little-n-p-n-in-machine-learning/) that there are many more samples than predictors. Still, sometimes the scenario is exactly the opposite, a situation referred to as “**big-p, little-n**” (“p” for predictors, and “n” for samples).  In industries like [Life Sciences](/learn/ml-life-sciences/), data tends to behave exactly in this way. For example, by using [high throughput screening technologies](https://www.dominodatalab.com/blog/analyzing-large-p-small-n-data-examples-from-microbiome), you can measure thousands or millions of data points for a single sample (e.g., the entire genome, the amounts of metabolites, the composition of the microbiome). Why is this a problem? Because in high dimensions, the data assumptions needed for statistical testing are not met, and [several problems](https://www.dominodatalab.com/blog/the-curse-of-dimensionality) arise:  - Data points move far away from each other in high dimensions. - Data points move far away from the center in high dimensions. - The distances between all pairs of data points become the same. - The accuracy of any predictive model approaches 100%.  This situation is referred to as “**the Curse of Dimensionality**”, which states that as data dimensionality increases, we can suffer from a significant impact on the implementation time of certain algorithms, make visualization extremely challenging, and make some Machine Learning models useless. A large number of dimensions in the feature space can mean that the [volume of that space is very large](https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/), and in turn, the points that we have in that space often represent a small and non-representative sample.  ![Dimensionality reduction diagram](/images/dimensionality-reduction-1.png) <small>With one dimension (top left), there are only ten possible positions. Therefore ten datum are required to create a representative sample that ‘covers’ the problem space. With two dimensions, there are 10² = 100 possible positions; therefore 100 datum are required to create a representative sample that ‘covers’ the problem space. With three dimensions, there are now 10³ = 1000 possible positions; therefore 1000 datum are required to create a representative sample that ‘covers’ the problem space. This exponential growth in the required number of datum continues to grow exponentially indefinitely. Source: [Turing Finance](http://www.turingfinance.com/artificial-intelligence-and-statistics-principal-component-analysis-and-self-organizing-maps/)</small>  The good news is that we can reduce data dimensionality to overcome these problems. The concept behind [dimensionality reduction](https://towardsdatascience.com/the-basics-of-data-prep-7bb5f3af77ac) is that high-dimensional data are dominated by a small number of simple variables. This way, we can find a subset of variables to represent the same level of information in the data or transform the variables into a new set of variables without losing much information.  # Main Algorithms  When facing high-dimensional data, it is helpful to reduce dimensionality by projecting the data to a lower-dimensional subspace that captures the data’s “essence.” There are several different ways in which you can achieve this algorithmically: PCA, t-SNE and UMAP.  ## Principal Component Analysis (PCA)  PCA is a **linear** dimensionality reduction algorithm that helps us extract a new set of variables from an existing high-dimensional dataset. The idea is to reduce the dimensionality of a dataset while retaining as much variance as possible.  PCA is also an **unsupervised** algorithm that creates linear combinations of the original features, called principal components. **Principal components** are learned in such a way that the first principal component explains maximum variance in the dataset, the second principal component tries to explain the remaining variance in the dataset while being uncorrelated to the first one, and so on.  Instead of simply choosing useful features and discarding others, PCA uses a linear combination of the existing features in the dataset and constructs new features that are an alternative representation of the original data.  ![Dimensionality reduction diagram](/images/dimensionality-reduction-2.png) <small>The three original variables (genes) are reduced to a lower number of two new variables termed principal components (PCs). Left: Using PCA, we can identify the two-dimensional plane that optimally describes the highest variance of the data. This two-dimensional subspace can then be rotated and presented as a two-dimensional component space (right). Source: [nlpca.org](http://www.nlpca.org/pca_principal_component_analysis.html)</small>  This way, you might keep only as many principal components as needed to reach a cumulative explained variance of 85%. But why not keep all components? What happens is that [each additional component expresses less variance and more noise](https://books.google.com.ar/books?id=Nud4EAAAQBAJ&pg=PA35&lpg=PA35&dq=each+additional+component+expresses+less+variance+and+more+noise,+so+representing+the+data+with+a+smaller+subset+of+principal+components+preserves+the+signal+and+discards+the+noise.&source=bl&ots=RhXUvO099b&sig=ACfU3U29_6RH6GZNhwWDC44mTla7vv14zw&hl=en&sa=X&ved=2ahUKEwjf7sK0rvH6AhXvpZUCHQGaCyoQ6AF6BAgbEAM#v=onepage&q=each%20additional%20component%20expresses%20less%20variance%20and%20more%20noise%2C%20so%20representing%20the%20data%20with%20a%20smaller%20subset%20of%20principal%20components%20preserves%20the%20signal%20and%20discards%20the%20noise.&f=false), so representing the data with a smaller subset of principal components preserves the signal and discards the noise.  PCA increases interpretability while minimizing information loss. It can be used to find the most significant features in a dataset and allows data visualization in 2D and 3D. At the same time, PCA is most suitable when variables have a linear relationship among them (it won’t be able to capture more complex relationships) and is susceptible to significant outliers.  ![Dimensionality reduction example](/images/dimensionality-reduction-3.png) <small>Example of dimensionality reduction of linear and nonlinear data by PCA. The same 2D points can be mapped onto a 3D space using a linear transformation (rotation) or a nonlinear transformation (spiral). When PCA is applied to the 3D datasets, the resulting 2D visualizations are strikingly different. For the linearly-transformed data, PCA is able to entirely recover the structure of the original data. However, for the nonlinear dataset, the limitation of PCA to rigid rotation of the axes causes the loss of salient information about the original data. For the linear case, PC1 and PC2 cumulatively account for 100% of the total variation, whereas they account for only 75% of the total variation in the nonlinear case. Source: [ResearchGate](https://www.researchgate.net/publication/329160047_Dimensionality_Reduction_Techniques_for_Visualizing_Morphometric_Data_Comparing_Principal_Component_Analysis_to_Nonlinear_Methods)</small>  You can find a tutorial on calculating PCA in [this link](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf) and an example coded in Python [here](http://ethen8181.github.io/machine-learning/dim_reduct/PCA.html#PCA).  ## t-Distributed Stochastic Neighbour Embedding (t-SNE)  Created for high-dimensional [visualization purposes](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf), t-SNE is a **non-linear** dimensionality reduction algorithm. This algorithm tries to [maximize the probability](https://www.sciencedirect.com/science/article/pii/S2468502X22000201) that similar points are positioned near each other in a low-dimensional map while preserving longer-distance relationships as a secondary priority. It attracts data points that are nearest neighbors to each other and simultaneously pushes all points away from each other.  Contrary to PCA, t-SNE it’s not a deterministic technique but a **probabilistic** one. The idea behind it is to [minimize the divergence between two distributions](https://builtin.com/data-science/tsne-python): a distribution that measures pairwise similarities of the input objects, and a distribution that measures pairwise similarities of the corresponding low-dimensional points in the embedding. It looks to match both distributions to determine how to best represent the original dataset using fewer dimensions.  More specifically, [t-SNE uses](http://theprofessionalspoint.blogspot.com/2019/03/what-is-t-sne-how-does-it-work-using-t.html) a **normal distribution** (in the higher dimension) and a **t-Distribution** (in the lower dimension) to reduce dimensionality. t-Distribution is a lot like a normal distribution, with the difference that it is not as tall as a normal distribution in the middle, but its tails are taller at the ends. The idea is to cluster data points in the lower dimension in a more sparse way to generate better visualizations.  ![t-distribution vs normal distribution](/images/dimensionality-reduction-4.jpg) <small>Why is t-Distribution used instead of normal distribution in lower dimensions? Because without it data clusters would clump up in the middle and will be harder to visualize. Source: [The Professionals Point](http://theprofessionalspoint.blogspot.com/2019/03/what-is-t-sne-how-does-it-work-using-t.html)</small>  There are several tuneable [hyperparameters](https://www.sciencedirect.com/science/article/pii/S2468502X22000201) to optimize t-SNE, like:  - **Perplexity**, which controls the size of the neighborhood used for attracting data points. - **Exaggeration**, which controls the magnitude of attraction. - **Learning rate**, which controls the step size for the gradient descent that seeks to minimize the error.   Changing these hyperparameters can deeply affect the accuracy and quality of t-SNE results.  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/dimensionality-reduction-5.mp4\" type=\"video/mp4\"> </video>  <small class=\"video\">t-SNE can give us a fascinating projection of the latent space. In this example from the MNIST handwritten digits dataset, the 3D projection shows dense clusters where the same digits are close to one another. Source: [Data Science Dojo](https://online.datasciencedojo.com/blogs/curse-of-dimensionality-python)</small>  t-SNE is an incredibly flexible algorithm that can find structure where others cannot. Unfortunately, it can be hard to interpret: after processing, the input features are no longer identifiable, and you cannot make any inference based only on the outputs.  While being stochastic, multiple executions with different initializations will yield different results, and whatsmore, both its computational and memory [complexity](https://towardsdatascience.com/essential-programming-time-complexity-a95bb2608cac) are [O(n2 )](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf), which can make it **quite heavy on system resources**.  Find [here](https://distill.pub/2016/misread-tsne/) an interactive explanation of t-SNE.  ## Uniform Manifold Approximation and Projection (UMAP)  [UMAP](https://umap-learn.readthedocs.io/en/latest/index.html) is another **nonlinear** dimension-reduction algorithm that overcomes some of the limitations of t-SNE. It works similarly to t-SNE in that it tries to find a low-dimensional representation that preserves relationships between neighbors in high-dimensional space, but with an increased speed and better preservation of the data’s global structure.  UMAP is a **non-parametric** algorithm that consists of two steps: (1) compute a fuzzy topological representation of a dataset, and (2) optimize the low dimensional representation to have as close a fuzzy topological representation as possible as measured by cross entropy.  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/dimensionality-reduction-6.mp4\" type=\"video/mp4\"> </video>  <small class=\"video\">Dimensionality reduction applied to the Fashion MNIST dataset. 28x28 images of clothing items in 10 categories are encoded as 784-dimensional vectors and then projected to 3 using UMAP and t-SNE. Source: [Understanding UMAP](https://pair-code.github.io/understanding-umap/)</small>  There are [two main hyperparameters](https://pair-code.github.io/understanding-umap/) in UMAP that are used to control the balance between local and global structure in the final projection:  - **The number of nearest neighbors**: which controls how UMAP balances local versus global structure - low values will push UMAP to focus more on the local structure by constraining the number of neighboring points considered when analyzing the data in high dimensions. In contrast, high values will push UMAP towards representing the big-picture structure, hence losing fine detail. - **The minimum distance between points in low-dimensional space**: which controls how tightly UMAP clumps data points together, with low values leading to more tightly packed embeddings. Larger values will make UMAP pack points together more loosely, focusing instead on the preservation of the broad topological structure.  Fine-tuning these hyperparameters can be challenging, and this is where UMAP's speed is a big advantage: by running it multiple times with a variety of hyperparameter values, you can get a better sense of how the projection is affected.  Compared to t-SNE, UMAP presents several [advantages](https://www.learndatasci.com/tutorials/applied-dimensionality-reduction-techniques-using-python/) since it:  - Achieves **comparable visualization performance** with t-SNE. - Preserves more of the **global data structure**. While the distance between the clusters formed in t-SNE does not have significant meaning, in UMAP the distance between clusters matters. - UMAP is **fast** and can **scale to Big Data**. - UMAP is not restricted for visualization-only purposes like t-SNE. It can serve as a general-purpose Dimensionality Reduction algorithm.  You can find an interactive explanation of UMAP [here](https://pair-code.github.io/understanding-umap/) and try different algorithms and datasets in the [TensorBoard Embedding Projector](https://projector.tensorflow.org/).  # Why Reduce Dimensionality?  There’s one thing we can be certain about in Machine Learning: the future will bring more data. Whatsmore, Machine Learning models will continue to evolve to highly complex architectures. In that scenario, dimensionality reduction algorithms can bring huge benefits like:  - Reducing storage needs for massive datasets. - Facilitating data visualizations by compressing information in fewer features. - Making Machine Learning models more computationally efficient.  In turn, these benefits translate into better model performance, increased interpretability, and improved data scalability. But of course, dimensionality reduction comes with data loss. No dimensionality reduction technique is perfect :  by definition, we’re [distorting the data to fit it into lower dimensions](https://pair-code.github.io/understanding-umap/). This is why it’s so critical to understand how these algorithms work and how they are used to effectively visualize and understand high-dimensional datasets. ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e350"
  },
  "content": "categories:   - Vector Search 101 toc: >- weight: 4 author:     name: Greg Kogan     position: VP Marketing     src: /images/company-greg.png     href: https://www.linkedin.com/in/gkogan/  description: A very gentle introduction to vector search and vector databases for developers. # Open Graph images: ['/images/rise-of-vector-data-5.png'] ---  There’s a lot of excitement and opportunity presented by machine learning, but it can often feel like more trouble than it’s worth. Machine learning and, by extension, data science are incredibly complicated fields. As a developer, you may only have the bandwidth to explore these fields to the extent they bring solutions to your present problems. For many developers, the present problem is vector similarity search. The solution is Pinecone.  [Pinecone](/) is a managed vector database that provides vector search (or \"similarity search\") for developers with a straightforward API and usage-based pricing. (And it's [free to try](https://app.pinecone.io).)  While it may be encouraging to hear that a SaaS solution exists for your data science needs, you still might feel lost. What is vector search? How or why would you use it? In this article, we’ll walk through everything you need to get started with vector search using Pinecone. Let’s go!  ## Vector search… what’s that?  If you’ve ever used a `LIKE` query in SQL, then you’ve performed a very simple form of [vector search](/learn/what-is-similarity-search/), though it’s unlikely that any ML theorist or practitioner would say as much. In reality though, vector search is fundamentally the same exercise.  When searching for similar text, there must be some kind of metric to check for overlap in the data. For text, this is simple: are the characters in the strings you’re searching over close to the ones you have in your search string? Depending on your background, you may have even heard technical terms like [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance) or [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance), which are precise metrics for describing the similarity (or dissimilarity) of strings.  For more complicated data sets, it’s also possible to make use of metrics like these. This is where vector search shines. In order to measure the distance between items in a particular data set, we need a programmatic way to quantify these things and their differences. Regardless of the types of objects we’re searching through, we use \"vectors\" or \"vector embeddings\" to convert the data we’re analyzing into simpler representations. Then, we check for similarity on those representations while still maintaining the deeper meaning of the objects themselves.  ## Vector embeddings?  At this point, we’ve defined an unfamiliar concept with an even more unfamiliar concept, but don’t be concerned. [Vector embeddings](/learn/vector-embeddings/) are really just a simplified numerical representation of complex data, used to make it easier to run generic machine-learning algorithms on sets of that data. By taking real-world objects and translating them to vector embeddings — numerical representations — those numbers can be fed into machine learning algorithms to determine [semantic similarity](learn/semantic-search/).  For example, let's consider the phrase \"one in a million.\" Is this phrase more similar to \"once in a lifetime\" or more similar to \"a million to one\"? You might have some intuition about which pairing is more similar. By creating a vector embedding for each of these phrases, machine learning goes beyond human intuition to generate actual metrics to quantify that similarity.  If you're thinking about how to get vector embeddings but coming from an SQL background, what might come to mind are [aggregate functions](https://www.postgresql.org/docs/9.5/functions-aggregate.html) which — if used correctly in cleverly crafted queries — could yield some sort of \"all of this data boils down to a single number\" result. At best, though, an SQL query can only perform really simple aggregations to distill out a final numerical representation.  Obtaining a vector embedding of *real* value requires some machine-learning techniques and a good understanding of the problem space.  Image data, for example, is represented numerically based on the pixel values of each image. It already has the form of a vector embedding. But what about cases where obtaining a numerical representation isn't so straightforward? Let's consider some examples.  If we wanted to determine [similarity across movies](/docs/examples/movie-recommender/), we might look at people who had watched and rated the same movies, along with what other movies they had watched and rated. To find [product similarity](/docs/examples/product-recommendation-engine/), we could look at customer purchases and see who has bought other items in the same purchase.  These examples are too complex and cumbersome to be handled manually. This data needs to be fed into some kind of neural network to reduce the number of dimensions of these vectors (length of the list of numbers). The following diagram shows how this reduction might work for simple object examples:  ![Vector embedding examples](/images/rise-of-vector-data-5.png)  Depending on the data you’re working with, there may be existing models you can use. But if you’re working with a unique dataset or use case, you’ll need to put some time into ensuring your model will capture your data well. For more generic situations — such as text data — we can use a more widely available model like [Word2vec](https://jalammar.github.io/illustrated-word2vec/). That model is trained against a wide collection of text data to determine similarities and differences between the concepts that real words represent.  Regardless of the model you’re using, though, Pinecone can help you search through the generated vector embeddings to find similar items.  ## So what do I do with my vector embedding?  Once you have a vector embedding, you need to be able to run queries against it. This is where Pinecone comes in. Rather than requiring you to learn all kinds of techniques for searching through your data, Pinecone provides managed vector search. You store vector embeddings with IDs that tie your data back to the objects they represent, allowing you to search through that data with a straightforward API and client. A store of vector embeddings and their IDs is called a \"vector index.\"  Creating a vector index is quite simple with Pinecone’s [Python client](/docs/quickstart-python/):  ```python import pinecone pinecone.init(YOUR_API_KEY) index = pinecone.create_index(\"my-new-index\") ```  (Java and Go clients are coming soon.)  You’ll need to [get an API key](/start/) and choose a name for your index. There are other parameters — such as the metric measuring similarity between vectors searching your data — but when you’re just starting, the defaults should be good enough.  Once you have an index created, you can insert your data as a tuple containing the id and vector representation of each object in your data set. Pinecone is blazing fast, able to index 10,000 tuples or more in just a second.  Querying an index is very simple. You can perform a unary query with a single vector, or you can query with a list of vectors too. All you need is a vector or a list of vectors you want to find similarities for and how many results (`integer`) you want Pinecone to return:  ```python index.query(queries=vectors, top_k=integer) ```  In the above line, `top_k` is a reference to the [k-nearest neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) to any given vector. If you’re not familiar with that algorithm, Pinecone's usage is analogous to a `LIMIT` clause from SQL. All of the k-nearest neighbors analysis is taken care of by the platform! Pinecone will return IDs that match. It also returns a score that shows its confidence in the match.  ## Your turn!  By now, you should have a better understanding of what vector search is and how Pinecone can help you integrate vector search into your application. If you’re interested in learning more, try out some techniques from [our examples](/docs/examples/) and start playing around with Pinecone yourself.  If you find you’re ready to get going with Pinecone on your own product, don’t hesitate to [sign up](/start/) or [get in touch with us](/contact/). We’ll be happy to help you get started! ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e352"
  },
  "title": "\"Data Augmentation with BERT\"",
  "content": "  - NLP for Semantic Search toc: >- weight: 10 author:   name: James Briggs   position: Developer Advocate   src: /images/james-briggs.jpeg   href: \"https://www.youtube.com/c/jamesbriggs\" description: In-domain data augmentation using AugSBERT. # Open graph images: ['/images/in-domain-augsbert-00.jpg'] ---  Many of the most significant breakthroughs of AI and ML in the 2010s were theorized and described many decades ago. A few of the greatest ingredients to the AI gold-rush of the past decade are the perceptron (1958), backpropagation (1975), and (for NLP) recurrent neural networks (1986) [1, 2, 3].  How do seemingly obscure innovations from 1958, 1975, and 1986 become critical to a science and tech revolution in the 21st century?  They were not 'rediscovered'; nothing was ever *lost*. Instead, the world at the time was not ready.  The many innovations that spurred AI forward over the last decade were ahead of their time, and their success required a few missing ingredients. The AI age was missing compute and data.  ![flops_over_time](/images/in-domain-augsbert-01.jpg) <small>Compute power (FLOPS) over time using a logarithmic scale. Source [4].</small>  Neural networks need many parameters to be effective. Look at some of the latest transformer models from the likes of OpenAI and Microsoft:  ![model_params](/images/in-domain-augsbert-02.jpg) <small>Model parameters over time, note the y-axis is using a log-scale.</small>  Models have been getting bigger and bigger. With good reason, they perform *better*. Just how big they will get is anyone's guess, but size matters, and,until very recently, there was not enough compute to train even the smallest models.  ![data_vol_over_time](/images/in-domain-augsbert-03.jpg) <small>Estimated data volume (worldwide) measured in *zeta*bytes. Estimated values for 2021 are 79ZB compared to just 2ZB in 2010. Source [5]</small>  The second missing ingredient was data. ML models are *data-hungry*. They consume massive amounts of data to identify generalized patterns and apply those learned patterns to new data.  As models get bigger, so do datasets. And although we have seen an explosion of data in the past decade, it is often not accessible or in an ML-friendly format, especially in niche domains (such as [climate-claim data](https://huggingface.co/datasets/climate_fever)) or low resource languages (Dhivehi, Navajo, etc).  Semantic search mostly requires the use of [sentence transformers](/learn/sentence-embeddings/). Thanks to the improvements to computing power, finding the compute to train these models is not usually an issue. But, they’re big models and finding enough of the right data? That is a problem.  For many niche, low-resource domains, finding or annotating a substantial dataset manually is practically impossible.  We can try [training without labeled data](/learn/unsupervised-training-sentence-transformers/) — but this only works for straight-forward semantic similarity tasks, and cannot produce as high-performing models as other supervised training methods (with labeled data).  Fortunately, we don't need to label (or even find) this new data. Instead, we can automatically generate or label data using one or more *data augmentation* techniques.  In this article, we will introduce data augmentation and its application to the field of NLP. We will focus on the 'in-domain' flavor of a particular data-augmentation strategy named augmented SBERT (AugSBERT).  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/3IPCEeh4xTg\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ## Data Augmentation  Data augmentation has been applied across the ML landscape and is not exclusive to NLP or sentence transformers. However, using data augmentation in NLP has proven to be a difficult task.  Augmentation is better developed within the field of computer vision (CV). It is relatively straightforward to apply many transformations that retain the 'essence' of an image while modifying pixel values. Images can be rotated, color graded, lightened/darkened, morphed, and more.  Language is more complex. It is more abstract and nuanced. Meaning is too easily corrupted by switching, replacing, deleting, or adding words. It is  easy to end up with nonsensical gibberish.  The AugSBERT training strategy requires that we generate new sentence pairs that are:  1. Sensible and on-topic 2. Semantically and grammatically correct.  There are several ways we can build these new pairs, and we can mix several techniques. By far, the simplest is to randomly sample new sentence pairs.  Given just three sentence pairs where each sentence is unique we can generate six new pairs:  {{< notebook file=\"3-to-9-pairs\" height=\"full\" >}}  As the number of original *source* or *gold* pairs increases, so does the number of generated *silver* pairs. Given 1,000 pairs we can generate 1,000,000 pairs. We will see later in the article that this approach is all we need to create a dataset large enough for training a sentence transformer.  *(We refer to gold data as the high-quality original data, whereas silver is artificially generated, and therefore lower quality data.)*  There are alternative techniques, and one of the more common options is the insertion or substitution of words. As mentioned, it is *hard* to do this without changing the meaning of a sentence, so we must approach insertion/substitution tactfully.  The [`nlpaug`](https://github.com/makcedward/nlpaug) library covers many data augmentation techniques for NLP, including insertion and substitution using word embeddings using embedding methods like *word2vec* and *fastText* to ensure we insert/substitute relevant words:  ![word_insert_sub](/images/in-domain-augsbert-04.jpg) <small>The original text (top) followed by a word2vec augmentation with insertion and substitution. Example  from nlpaug [6].</small>  We can even use *context* aware words embeddings with transformer models, including BERT, DistilBERT, RoBERTa, and XLNet.  ![bert_insert_sub](/images/in-domain-augsbert-05.jpg) <small>The original text (top) followed by a BERT augmentation with insertion and substitution. Example  from nlpaug [6].</small>  `nlpaug` covers many other augmentation techniques, which are reviewed in-depth in the nlpaug GitHub repository [6]. However, we will be sticking with the random sampling strategy.    ## Augmented SBERT  Fine-tuning sentence transformer models require pairs of labeled data and lots of them.  The original SBERT was trained on 1M **n**atural **l**anguage **i**nference (NLI) pairs, that is, 1M sentence pairs labeled as being highly related (entailment), contradictory, or neutral [7]. More recent models like `flax-sentence-embeddings/all_datasets_v4_mpnet-base` were trained on more than 1B sentence pairs [8].  When fine-tuning for a specific use case, it's unlikely that we'll find an existing and relevant dataset. That leaves us with two options:  1. We manually annotate 100K+ sentence pairs 2. We take 1-5K of existing (or manually annotated) sentence pairs and augment them with more data.  Under the assumption that you choose the latter option, how can we augment a dataset and generate realistic sentence pair labels? There are two steps: We use random sampling to create new pairs and then label them using a *cross-encoder*.  When comparing the semantic similarity of sentence pairs, we are not limited to sentence transformers (*bi-encoders*). We can use cross-encoder models too.  ![cross_and_bi_encoder](/images/in-domain-augsbert-06.jpg) <small>A cross-encoder (left) is a single BERT inference step that takes both sentences as a single input and outputs a similarity score. Bi-encoders (right) perform an inference step for *each* sentence and output sentence vectors.</small>  Cross-encoders are more accurate than sentence transformers and require less data to train. However, this greater accuracy comes at a cost: Cross-encoders are *much slower*.  We must pass both sentence pairs to the cross-encoder, which outputs a similarity score.  The similarity score is often more accurate, but we had to perform a full cross-encoder (let's assume we're using BERT) inference step to get that single pairwise similarity.  If we need to search across a *'small'* dataset containing 1M sentences? We need to perform 1M full-inference computations. That is *slow*.  Clustering with cross-encoders is an even more inefficient quadratic complexity [7]. For clustering, we must compare every single pair. For all but the tiniest datasets, this is too slow to be usable in most use-cases.  On the other hand, sentence transformers require us to encode each sentence to produce a *sentence vector*. We will need to run 1M full inference computations on our first run to create these vectors, but once we have them, we store them in a database/index for quicker lookups.  Given a new sentence, performing a search across that same small dataset of 1M sentence *vectors* means we encode the new sentence (one inference computation) and then calculate the Euclidean/cosine similarity between that one vector and the 1M already indexed sentence vectors.  1M cosine similarity computations are faster than 1M full BERT inference computations. Additionally, with sentence vectors, we can use **a**pproximate **n**earest **n**eighbors **s**earch (ANNS) to speed up the process even further.  A BERT cross-encoder can take 65 hours to cluster 10K sentences. The equivalent process with SBERT takes *five seconds* [7].  ### When to Use AugSBERT  We will assume one of two scenarios:  1. We have found an existing labeled dataset, but it is tiny,maybe just 1-5K pairs or 2. We have an unlabeled dataset, but it is within reason to manually label (annotate) at least 1-5K pairs.  We have a small but annotated dataset. We could try fine-tuning a sentence transformer, but it is unlikely to perform well. Instead, we can turn to augmentation to enhance our dataset and improve the potential sentence transformer performance.  There are now the two steps we mentioned earlier to create this data. First, we generate more pairs, for which we will use random sampling. Second, we label that new data with a cross-encoder fine-tuned on the original (smaller) dataset.  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/in-domain-augsbert-07.mp4\" type=\"video/mp4\"> </video>  <small class=\"video\">Random sampling is used to enlarge the number of sentence pairs in our dataset. After producing this larger dataset, we use the cross-encoder to label the new pairs.</small>  Because cross-encoders require fewer data and produce high-accuracy similarity scores, they're great for annotating our unlabeled data. With more labeled data, we can fine-tune better-performing sentence transformers.  The Augmented SBERT (AugSBERT) fine-tuning strategy is ideal for training with tiny datasets. Evaluation of the strategy showed improvements of up to 6% for in-domain tasks and up to 37% for domain adaption tasks [9].  The training process always begins with a *gold dataset*. Gold is our already labeled and (hopefully) high-quality data. If you can't get gold, the next best thing is silver. Likewise, the next best *'augmented data'* is named the *silver dataset*.  We feed the gold and unlabeled data into a BERT cross-encoder, producing our silver data. The SBERT bi-encoder is fine-tuned with this gold and silver data.  At a high level, this is how in-domain AugSBERT training works. Let's flesh out the details and work through an example.    ## In-Domain Walkthrough  To implement the in-domain AugSBERT training strategy, we need to have a small amount of labeled data within *the same* domain that we'd like to fine-tune our sentence transformer. We can then use this to generate more *in-domain* data.  ![in_domain_flow](/images/in-domain-augsbert-08.jpg) <small>Step-by-step process for fine-tuning a sentence transformer (bi-encoder) with the AugSBERT in-domain strategy.</small>  We can use this gold data to fine-tune the sentence transformer. The *problem* is that we do *not have enough data*. So, we must generate *more* data, or if there is already unlabeled data available, we can label that directly.  We will use the **S**emantic **T**extual **S**imilarity **b**enchmark (STSb) dataset. It is available via the 🤗 Datasets library. It contains just 5,749 pairs, very little for fine-tuning a sentence transformer model.  {{< notebook file=\"stsb-download\" height=\"full\" >}}  After normalizing the `label` feature to between 0 -> 1, each row in our dataset will look something like this:  ```json { 'sentence1': 'A plane is taking off.', 'sentence2': 'An air plane is taking off.', 'label': 1.0, # this value will range from 0.0 -> 1.0 'idx': 0 } ```  We have 5,749 pairs in the train set and 1,500 pairs in the validation (or *dev*) set. Fine-tuning with this core *gold* dataset produces a model that scores 0.506 using Spearman's correlation with those *dev set* labels, where 0.0 means no correlation and 1.0 is an exact match or perfect correlation.  We can improve this score using an in-domain AugSBERT training strategy, which begins by training a cross-encoder using this small *gold* dataset.  ### Fine-Tune Cross-Encoder  Before training, we must reformat our training to a list of `InputExamples` and use them to initialize a `DataLoader`.  {{< notebook file=\"setup-train-set\" height=\"full\" >}}  We then initialize and fine-tune our cross encoder.  {{< notebook file=\"bert-cross-encoder-fit\" height=\"full\" >}}  The number of warmup steps is 40% of the total training steps. It is high but helps prevent overfitting. The same could likely be achieved using a lower learning rate (the default is `2e-5`).  Evaluation of the cross-encoder model on the dev set returns a correlation score of *0.578*.  ### Create Unlabeled Data  The cross-encoder is one half of the recipe for building a silver dataset, and the other half are the unlabeled sentence pairs. There are different strategies for generating this data, but one of the simplest and most effective is to randomly sample pairs from the gold data, creating new sentence pairs.  For this, we can transform the pairs from dataset objects to Pandas DataFrames, as these provide easy-to-use sampling methods.  {{< notebook file=\"gold-load\" height=\"full\" >}}  We can then initialize a new `pairs` dataframe, loop through each unique sentence from the `sentence1` column and find new pairs from the `sentence2` column.  {{< notebook file=\"random-sample\" height=\"full\" >}}  Finally, we should drop any duplicates from the new `pairs` data.  {{< notebook file=\"drop-dupes\" height=\"full\" >}}  With that, we have 27,180 unlabeled sentence pairs; the second half needed to build a *fully labeled* silver dataset.  ### Labeling the Silver Dataset  We generate label predictions for the unlabeled data using the cross-encoder that we trained. It is this cross-encoder-labeled data that we refer to as the *silver dataset*.  ![in_domain_training](/images/in-domain-augsbert-09.jpg) <small>In-domain training strategy with AugSBERT, source [9]. We feed **unlabeled pairs** into the fine-tuned cross-encoder to create the **silver dataset**.</small>  Earlier we saved the cross-encoder to file in the local `bert-stsb-cross-encoder` directory. To load it from file we use:  {{< notebook file=\"load-cross-encoder\" height=\"full\" >}}  Then we `predict` new labels for our unlabeled data.  {{< notebook file=\"create-labels\" height=\"full\" >}}  We now have both gold *and* silver datasets to fine-tune the sentence transformer, a total of `5_749 + 27_180 == 32_929` pairs. Now, we can  fine-tune the sentence transformer.  ### Fine-Tune Sentence Transformer  Before training, we need to merge the *silver* and *gold* datasets; with both as Pandas DataFrame objects, we use `append`. As before, we also transform our data into a list of `InputExample` objects and use them to initialize a `DataLoader`.  ```python all_data = gold.append(pairs, ignore_index=True)  # format into input examples train = [] for _, row in all_data.iterrows():     train.append(         InputExample(             texts=[row['sentence1'], row['sentence2']],             label=float(row['label'])         )     )  # initialize dataloader loader = DataLoader(     train, shuffle=True, batch_size=batch_size ) ```  Our data is ready for training, so we initialize our model. The model consists of a core transformer model, in this case, `bert-base-uncased` from 🤗 Transformers. Following this, we have a *pooling layer*, which transforms the 512 token-level vectors into single sentence vectors. We will use the *mean* pooling method.  ```python from sentence_transformers import models, SentenceTransformer  # initialize model bert = models.Transformer('bert-base-uncased') pooler = models.Pooling(     bert.get_word_embedding_dimension(),     pooling_mode_mean_tokens=True ) model = SentenceTransformer(modules=[bert, pooler]) ```  We must define a loss function to optimize on. We will use a cosine similarity loss function as we have similarity scores in the dataset.  ```python from sentence_transformers import losses  loss = losses.CosineSimilarityLoss(model=model) ```  Then we begin training. We use the default learning rate and warmup for the first 15% of steps.  ```python # and training epochs = 1 # warmup for first 15% of training steps warmup_steps = int(len(loader) * epochs * 0.15)  model.fit(     train_objectives=[(loader, loss)],     epochs=epochs,     warmup_steps=warmup_steps,     output_path='bert-stsb-aug' ) ```  With that, we have fine-tuned our STSb sentence transformer using the in-domain AugSBERT training strategy. To evaluate the model, we run a [small evaluation script](https://gist.github.com/jamescalam/133a83c32642ea27a9f648bcc9297003), which returns the correlation score with pairs from the STSb dev set.  | Model | Score | Note | | ------------------------- | ----- | ------------------------------------------------------------ | | `bert-stsb-aug` | 0.691 | BERT-base uncased model fine-tuned on the gold *and* silver STSb data. | | `bert-stsb-gold` | 0.506 | BERT-base uncased model fine-tuned on the gold STSb data only (no dataset augmentation). | | `bert-stsb-cross-encoder` | 0.692 | BERT-base uncased cross-encoder model fine-tuned on the gold STSb data. Used to create the silver data. |  We return some incredible results for the sentence transformer fine-tuned using the AugSBERT strategy, returning almost 19% better performance than the model fine-tuned on the gold dataset only.  The paper introducing AugSBERT demonstrated performance increases of *up to* six points for in-domain tasks. So, it's worth assuming that the 19-point improvement here is *very high* and an atypical improvement. However, it shows just how good an AugSBERT training strategy can be.    That's it for fine-tuning using the in-domain Augmented SBERT strategy. The renaissance of ML we are currently witnessing may have been ignited by 50-70s research and enabled through massive advances in compute availability, but without the right data, we’re stuck.  It is with new techniques like AugSBERT that we are finally able to traverse the last mile and bridge those gaps in data.  We've introduced the idea of data augmentation in the field of NLP and how we can use insertion/substitution or simple random sampling techniques to generate new sentence pairs (e.g., the *silver dataset*).  We learned about cross encoders, bi-encoders, and how we can label our silver data using cross encoders.  Finally, we fine-tuned our bi-encoder *or 'sentence transformer'* using gold and silver datasets and evaluated its performance against another bi-encoder trained without the AugSBERT strategy.  With this strategy, we can apply semantic similarity models to niche domains that have not been caught in the swell of data from the past decade.Demonstrating that AugSBERT can be a convenient approach to enhancing model performance in these domains.    ## References  [1] F. Rosenblatt, [The Perceptron: A Probabilistic Model For Information Storage and Organization in the Brain](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf) (1958), PsycINFO  [2] P. Werbos, [Beyond Regression: New Tools For Prediction and Analysis in the Behavioral Sciences](https://www.researchgate.net/publication/279233597_Beyond_Regression_New_Tools_for_Prediction_and_Analysis_in_the_Behavioral_Science_Thesis_Ph_D_Appl_Math_Harvard_University) (1974), Harvard University  [3] D. Rumelhard, et al., [Learning Representations by Back-Propagating Errors](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf) (1986), Nature  [4] [Top500 Supercomputer Leaderboards](https://www.top500.org)  [5] A. Holst, [Volume of data/information created, copied, and consumed worldwide from 2010 to 2025](https://www.statista.com/statistics/871513/worldwide-data-created/) (2021), Statistica  [6] E. Ma, [nlpaug](https://github.com/makcedward/nlpaug), GitHub  [7] N. Reimers, I. Gurevych, [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084) (2019), EMNLP 2019  [8] [Model Card for `all_datasets_v4_mpnet-base`](https://huggingface.co/flax-sentence-embeddings/all_datasets_v4_mpnet-base), HuggingFace Models  [9] N. Thakur, et al., [Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks](https://arxiv.org/pdf/2010.08240.pdf) (2021), NAACL",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e354"
  },
  "content": "categories:   - Vector Search 101 toc: >- weight: 4 author:     name: Roie Schwaber-Cohen     position: Developer Advocate     src: /images/company-roie-s-c.jpeg     href: https://www.linkedin.com/in/roiecohen/  description: A basic introduction to vector embeddings for developers. # Open Graph images: ['/images/rise-of-vector-data-5.png'] ---  You might not know it yet, but vector embeddings are everywhere. They are the building blocks of many machine learning and deep learning algorithms used by applications ranging from search to AI assistants. If you’re considering building your own application in this space, you will likely run into vector embeddings at some point. In this post, we’ll try to get a basic intuition for what vector embeddings are and how they can be used.  # What problem are we trying to solve?  When you build a traditional application, your data structures are represented as objects that probably come from a database. These objects have properties (or columns in a database) that are relevant to the application you’re building.  Over time, the number of properties of these objects grows — to the point where you may need to be more intentional about which properties you need to complete a given task. You may even end up creating specialized representations of these objects to solve particular tasks without paying the overhead of having to process very “fat” objects. This process is known as feature engineering — you optimize your application by picking only the essential features relevant to the task at hand.  When you deal with **unstructured** data, you will have to go through this same feature engineering process. However, unstructured data is likely to have many more pertinent features, and performing manual feature engineering is bound to be untenable.  In those cases, we can use **vector embeddings** as a form of automatic feature engineering. Instead of manually picking the required features from our data, we apply a pre-trained machine learning model that will produce a representation of this data that is more compact while preserving what’s meaningful about the data.   # What are vector embeddings?  Before we delve into what vector embeddings are, let’s talk about vectors. A **vector** is a mathematical structure with a size and a direction. For example, we can think of the vector as a point in space, with the “direction” being an arrow from (0,0,0) to that point in the **vector space.**  <img src=\"/images/vector.png\" width=\"200\">  As developers, it might be easier to think of a vector as an array containing numerical values. For example:   ``` vector = [0,-2,...4] ```  When we look at a bunch of vectors in one space, we can say that some are closer to one another, while others are far apart. Some vectors can seem to cluster together, while others could be sparsely distributed in the space.  <img src=\"/images/multiple-vectors.png\" width=\"250\">  We’ll soon explore how these relationships between vectors can be useful.  Vectors are an ideal data structure for machine learning algorithms — modern CPUs and GPUs are optimized to perform the mathematical operations needed to process them. But our data is rarely represented as vectors. This is where vector embedding comes into play. It’s a technique that allows us to take virtually any data type and represent it as vectors.  But it isn’t as simple as just turning data into vectors. We want to ensure that we can perform tasks on this transformed data without losing the data’s original meaning. For example, if we want to compare two sentences — we don’t want just to compare the words they contain but rather whether or not they mean the same thing. To preserve the data’s **meaning**, we need to understand how to produce vectors where **relationships** between the vectors **make sense.**  To do this, we need what’s known as an **embedding model**. Many modern embedding models are built by passing a large amount of **labeled data** to a neural network. You might have heard of neural networks before — they are also a popular tool used to solve all sorts of complex problems. In very simple terms, neural networks are made of layers of nodes connected by functions. We then train these neural networks to perform all sorts of tasks.  We train neural networks by applying supervised learning —  feeding the network a large set of training data made of pairs of inputs and labeled outputs. Alternatively, we can apply self-supervised or unsupervised learning either of which doesn’t require labeled outputs. These values are transformed with each layer of network activations and operations. With every iteration of training, the neural network modifies the activations in each layer. Eventually, it can predict what an output label should be for a given input — even if it hasn’t seen that particular input before.  The embedding model is basically this neural network with the last layer removed. Instead of getting a specific labeled value for an input, we get a vector embedding.  A great example of an embedding model is the popular [word2vec](https://en.wikipedia.org/wiki/Word2vec), which is regularly used for a wide variety of text-based tasks. Let’s take a look at a visualization produced by TensorFlow’s [projector](https://projector.tensorflow.org) tool, which makes it easy to visualize embeddings.  ![Embedding Visualization](/images/embedding-visualization.png)   While this visualization represents only three dimensions of the embeddings, it can help us understand how the embedding model works. There are multiple data points highlighted in the visualization, each representing a vector embedding for a word. As the name suggests, word2vec embeds words. Words that appear close to one another are semantically similar, while far-apart words have different semantic meanings.  Once trained, an embedding model can transform our raw data into vector embeddings. That means it knows where to place new data points in the vector space.  ![Embedding Process](/images/embedding-process.png)  As we saw with word2vec, within the context of the model, vectors that are close together have a contextual similarity, whereas far-apart vectors are different from one another. That’s what gives our vector **meaning** — its relationship with other vectors in the vector space depends on how the embedding model “understands” the domain it was trained on.   # What can I do with vector embeddings?  Vector embeddings are an incredibly versatile tool and can be applied in many domains. Generally speaking, an application would use a vector embedding as its query and produce other vector embeddings which are similar to it, with their corresponding values. The difference between applications of each domain is the significance of this similarity.  Here are some examples:  * Semantic Search - search engines traditionally work by searching for overlaps of keywords. By leveraging vector embeddings, [semantic search](https://www.pinecone.io/docs/examples/semantic-text-search/) can go beyond keyword matching and deliver based on the query's semantic meaning. * Question-answering applications - by training an embedding model with pairs of questions and corresponding answers, we can create an application that would [answer questions](https://www.pinecone.io/docs/examples/extractive-question-answering/) that have not been seen before. * Image search - vector embeddings are perfectly suited to serve as the basis for image retrieval tasks. There are multiple off-the-shelf models, such as [CLIP](https://www.pinecone.io/learn/clip/), ResNet, and more. Different models handle different types of tasks like [image similarity](https://www.pinecone.io/docs/examples/image-similarity-search/), object detection, and many more. * Audio search - by converting the audio into a set of activations (an audio spectrogram), we produce vector embeddings that can be used for [audio similarity search](https://www.pinecone.io/docs/examples/audio-search/). * Recommender Systems - we can create embeddings out of structured data that correlate to different entities such as [products](https://www.pinecone.io/docs/examples/product-recommendation-engine/), articles, etc. In most cases, you’d have to create your own embedding model since it would be specific to your particular application. Sometimes this can be combined with unstructured embedding methods when images or text descriptions are found. * Anomaly detection - We can create embeddings for anomaly detection using large data sets of labeled sensor information that [identify anomalous occurrences](https://www.pinecone.io/docs/examples/it-threat-detection/).  Vector embeddings are incredibly powerful, and this is by no means an exhaustive list — head to our [example apps section](https://www.pinecone.io/docs/examples/) to go deeper. You can also read more about the basics of [vector search](https://www.pinecone.io/learn/vector-search-basics/) to see how [Pinecone](https://www.pinecone.io/) can help you wrangle vector embeddings. ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e356"
  },
  "title": "\"The Art of Asking Questions with GenQ\"",
  "headline": "\"Unsupervised Training of Retrievers Using GenQ\"",
  "weight": "12",
  "name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "Asymmetric semantic search with limited data using GenQ",
  "thumbnail": "\"/images/genq-0.jpg\"",
  "content": "Fine-tuning effective dense retrieval models is challenging. Bi-encoders (sentence transformers) are the current best models for dense retrieval in semantic search. Unfortunately, they're also notoriously data-hungry models that typically require a particular type of labeled training data.  Hard problems like this attract attention. As expected, there is plenty of attention on building ever better techniques for training retrievers.  One of the most impressive is GenQ. This approach to building bi-encoder retrievers uses the latest text generation techniques to synthetically generate training data. In short, all we need are passages of text. The generation model then augments these passages with synthetic queries, giving us the exact format we need to train an effective bi-encoder model.  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/J0cntjLKpmU\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ## GenQ Method  Let's work through the details of this training method. At a high level, there are two key steps.  1. Generate queries for pre-existing but unlabeled passages: Creating (query, passage) pairs. 2. Fine-tune a bi-encoder model using these (query, passage) pairs and **M**ultiple **N**egatives **R**anking (MNR) loss.  ![genq_overview](/images/genq-1.jpg)<small>High-level view of the *GenQ* training process.</small>  Don't worry if any (or even all) of the above doesn't make sense. We'll detail everything from start to finish.  ### Unlabeled Passages  We can describe data as either being *in-domain* or belonging to another domain. The *domain* here refers to the target data and use-case where we apply the eventual fine-tuned bi-encoder model.  For example, we may want to build a retriever model that encodes sentences (passages) for financial documents in German. In that case, any text from German financial documents is *in-domain*, and everything else is out-of-domain.  ![in_and_out_domain](/images/genq-2.jpg)<small>For our target domain of *German financial documents*, anything that fits the topic and we would expect our model to encounter is *in-domain*. Anything else is out-of-domain.</small>  To achieve good performance with a language model (LM), we need to train (fine-tune) it on in-domain data. We would typically need *a lot* of *labeled* in-domain data to fine-tune a bi-encoder.  For most domains, we can either have *a lot* of ***un**labeled* data or *a little labeled* data. It's hard to get both, and most bi-encoder training needs both.  GenQ aims to break the reliance on requiring labeled data by synthetically generating queries for otherwise unlabeled passages of text. Producing *(query, passage) pairs* from an unlabeled dataset. That means that given a large, in-domain, but unlabeled dataset, we can train with GenQ.  The task that GenQ is designed for is referred to as *asymmetric semantic search* [3]. That means the query is much shorter than the passage we would aim to retrieve. A typical query may consist of (for example) six words *\"How do I tie my shoelaces?\"*, and the relevant passage can be much longer:  *\"To tie your shoelaces, take both laces and place one over the other, pulling them tightly together...\"*  ![search_asymmetry](/images/genq-3.jpg) <small>Asymmetric semantic search is where the length of queries are typically much smaller than that of the passages/contexts being searched.</small>   It is this task, with asymmetry between queries and passages, where GenQ can be applied.  ### Generation of Queries  We need passages and a query generation model to generate the (query, passage) pairs. The model used by GenQ is the **T**ext-to-**T**ext **T**ransfer **T**ransformer (T5).  The T5 model philosophy is that all NLP tasks can be defined as a *text-to-text* problem, so they are pretrained on many different tasks with vast amounts of data.  ![T5](/images/genq-4.jpg) <small>T5 views every task as a text-to-text problem. Here are a few examples adapted from the paper that introduced T5 [4].</small>  One of these tasks is query generation. In this case, the input text, or *passage*, is fed into a special query generation T5 model that generates questions that the passage may answer [2].  Given a large corpus of passages, such as paragraphs scraped from documentation, web pages, etc. We use T5 to generate several queries for each passage.  ![t5_query_gen](/images/genq-5.jpg)  <small>Using a T5 model fine-tuned for query generation (like [BeIR/query-gen-msmarco-t5-large-v1](https://huggingface.co/BeIR/query-gen-msmarco-t5-large-v1)) we can generate sets of queries using passages of text.</small>  It's important to note that query generation is not perfect. We're using a general-purpose T5 model. The queries it generates can be noisy with plenty of randomness and nonsensical queries. Because of that, GenQ is prone to poor performance where the synthetic data is *too noisy* [1].  We have what should be a very large dataset of (query, passage) pairs. With this data, we can move on to fine-tuning the bi-encoder model.  ### Fine-Tuning the Bi-Encoder  To fine-tune the bi-encoder (sentence transformer) we use [**M**ultiple **N**egatives **R**anking (MNR) loss](/learn/fine-tune-sentence-transformers-mnr/). MNR loss is ideal for training where our dataset consists of pairs of related sentences.  For example, when training a QA retriever model, we can train with MNR loss if we have sets of (question, answer) pairs. If we have a **N**atural **L**anguage **I**nference (NLI) dataset, we can use MNR loss to train on (anchor, positive) pairs. In this case, we fine-tune on (query, passage) pairs.  MNR loss works by placing all of these pairs into batches. For each batch, the model is optimized so that pair **(Q<sub>i</sub>, P<sub>j=i</sub>)** has the highest similarity. Meaning that within a batch of 32, the similarity score between **Q<sub>i=3</sub>** and **P<sub>j=3</sub>** must be higher than the similarity between **Q<sub>i=3</sub>** and any other passage **P<sub>j≠3</sub>**.  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/genq-6.mp4\" type=\"video/mp4\"> </video>  <small class=\"video\">Similarity scores using five (query, passage) pairs. MNR loss optimizes so that **(Qi, Pi)** scores higher than any other pair **(Qi, Pj≠i)**</small>  At the end of this training process, we have a new bi-encoder fine-tuned to a specific domain. The model's performance can vary depending on the models being used, source and target domains, and many other variables. However, GenQ can sometimes achieve performances approaching models trained with supervised methods [1].  Let's move on to the implementation of GenQ.  ## Implementation Walkthrough  First, we need a dataset to train on. We will take the *context* paragraphs from the **S**tanford **Q**uestion and **A**nswering **D**ataset (SQuAD) dataset, which we will download from HuggingFace *Datasets*.  {{< notebook file=\"genq-squad\" height=\"full\" >}}  In this dataset, we already have query `'question'` and passage `'context'` pairs. However, we want to emulate the scenario in which we do *not* have queries. We will remove all but the `'context'` data to do that.  {{< notebook file=\"genq-passages\" height=\"full\" >}}  Now that we have our `passages`, we can begin **generating queries**. For this, we need a query generation model. We will use a T5 model fine-tuned for query generation as part of the BeIR project, named `BeIR/query-gen-msmarco-t5-large-v1`.  {{< notebook file=\"genq-init-model\" height=\"full\" >}}  Some layers in the model behave differently during training and inference. To ensure the model is running in \"inference mode\", we call `model.eval()`.  {{< notebook file=\"genq-generate\" height=\"full\" >}}  With this, the model will generate three queries for each passage. In this case, we generate *56,673* pairs from *18,891* passages and save them as TSV files.  We can see that the queries are generally *much smaller* than the passages; this is where the ***asymmetric** in asymmetric similarity search* comes from.  {{< notebook file=\"genq-asymmetry\" height=\"full\" >}}  <small>Example of a few generated queries given a paragraph about Python.</small>  The next step is to fine-tune a model using MNR loss. We do this easily with the `sentence-transformers` library.  We start by loading the pairs dataset we created into a list of `InputExample` objects.  {{< notebook file=\"genq-input-examples\" height=\"full\" >}}  Next, we load the pairs into a `NoDuplicatesDataLoader`. We use the *no duplicates* data loader to avoid placing duplicate passages in the same batch, as this will confuse the ranking mechanism of MNR loss.  {{< notebook file=\"genq-loader\" height=\"full\" >}}  Now we initialize the bi-encoder that we will be fine-tuning. We create the transformer-to-pooler architecture using *modules*.  {{< notebook file=\"genq-biencoder\" height=\"full\" >}}  Here we are initializing from a pretrained MPNet model, which by default outputs 512 embeddings. The second module is a mean pooling layer that takes the average activations across all of these embeddings to create a single sentence embedding.  With this, our bi-encoder is initialized. We now need to fine-tune the model, which we do using MNR loss.  {{< notebook file=\"genq-loss\" height=\"full\" >}}  Everything is now in place, and we fine-tune the model by calling the `fit` method.  {{< notebook file=\"genq-finetune\" height=\"full\" >}}  We now have a fine-tuned bi-encoder that we can use for asymmetric semantic search. Let's move on to setting up a search index and testing a few searches to see what we return.  ## Evaluation  For evaluation, we will work through a simple qualitative test. We take a few example questions from the SQuAD validation set, and we will (hopefully) see that we are returning relevant contexts.  We can use Pinecone as an ultra-fast way to store our vectors. All we need is an [API key](https://app.pinecone.io/) and to install the Pinecone client with `pip install pinecone-client`. To initialize our connection to Pinecone and create an index to store the vectors we write:  {{< notebook file=\"genq-init-index\" height=\"full\" >}}  The vector database will store all encoded contexts from the SQuAD validation set, so let's download, encode, and upsert our contexts.  To **download**, we use HuggingFace *Datasets* as before.  {{< notebook file=\"genq-load-val\" height=\"full\" >}}  We can now **encode** using our newly trained `mpnet-genq-squad` model.  {{< notebook file=\"genq-encode\" height=\"full\" >}}  And finally **upsert** to Pinecone.  {{< notebook file=\"genq-upsert\" height=\"full\" >}}  We're now ready to begin querying; we can take a few example queries from SQuAD.  {{< notebook file=\"genq-query1\" height=\"full\" >}}  We immediately return the best possible answer as the highest rated passage. Let's try with some more SQuAD queries.  {{< notebook file=\"genq-query2\" height=\"full\" >}}  Another great result; let's try one final query.  {{< notebook file=\"genq-query3\" height=\"full\" >}}  All of these great results show that our model fine-tuned with GenQ has fit well to the SQuAD domain.    That's it for this chapter covering the *GenQ* training method, a clearly powerful approach to fine-tuning models where we have limited datasets.  Using this approach, we can take passages of text, generate (query, passage) pairs, and use these pairs to train effective bi-encoder models ideal for asymmetric semantic search.  GenQ is an excellent, low-effort technique enabling projects that focus or rely on retrieving passages of text from natural language queries. Using GenQ you can begin fine-tuning models with limited data, unlocking previously inaccessible domains.  ## References  [1] J. Ma, et al., [Zero-shot Neural Passage Retrieval via Domain-targeted Synthetic Question Generation](https://arxiv.org/abs/2004.14503) (2021), ACL  [2] N. Reimers, [GenQ Page](https://www.sbert.net/examples/unsupervised_learning/query_generation/README.html), SBERT.net  [3] N. Reimers, et. al., [Semantic Search Page](https://www.sbert.net/examples/applications/semantic-search/#symmetric-vs-asymmetric-semantic-search), SBERT.net  [4] C. Raffel, et. al., [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) (2020), JMLR ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e358"
  },
  "title": "\"Zero-shot Image Classification with OpenAI's CLIP\"",
  "headline": "\"Zero-shot Image Classification with OpenAI's CLIP\"",
  "weight": "10",
  "name": "James Briggs",
  "position": "Developer Advocate",
  "src": "/images/james-briggs.jpeg",
  "href": "\"https://www.youtube.com/c/jamesbriggs\"",
  "description": "A deep dive on OpenAI's multi-modal CLIP for zero-shot image classification",
  "images": "['https://www.pinecone.io/images/zero-shot-image-classification-clip-0.png']",
  "content": "State-of-the-art (SotA) computer vision (CV) models are characterized by a *restricted* understanding of the visual world based on their training data [1].  These models can perform *very well* on specific tasks and datasets, but they do not generalize well. They cannot handle new classes or images beyond the domain they have been trained with.  This brittleness can be a problem for building niche image classification use-cases, such as defect detection in agriculture to identifying false banknotes to fight fraud. It can be extraordinarily hard to gather labeled datasets that are large enough to fine-tune CV models with traditional methods for those specialized use cases.  Ideally, a CV model should learn the contents of images without excessive focus on the specific labels it is initially trained to understand. With an image of a dog, the model should understand that a dog is in the image. But it would be a lot more useful if it could also understand that there are trees in the background, that it is daytime, and that the dog is on a grassy field.  Unfortunately, the result of classification training is the opposite. Models learn to push their internal representations of dogs into the same \"dog vector space\" and cats into the same \"cat vector space\". All that matters is the binary yes/no as to whether an image aligns with a class.  ![vector-space](./images/zero-shot-image-classification-clip-1.png)  Retraining classification models is an option, but training requires significant time and capital investment for gathering a classification dataset *and* the act of model training itself.  Fortunately, OpenAI's CLIP has proved itself as an incredibly flexible classification model that often requires *zero* retraining. In this chapter, we will explore zero-shot image classification using CLIP.  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/98POYg2HZqQ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ## N-Shot Learning  Before diving into CLIP, let's take a moment to understand what exactly \"zero-shot\" is and its significance in ML.  The concept derives from N-shot learning. Here we define *N* as the number of samples required to train a model to begin making predictions in a new domain or on a new task.  Many SotA models today are pretrained on vast amounts of data like ResNet or BERT. These pretrained models are then *fine-tuned* for a specific task and domain. For example, a ResNet model can be pretrained with [ImageNet](https://www.pinecone.io/learn/imagenet) and then fine-tuned for clothing categorization.  Models like ResNet and BERT are called *\"many-shot\"* learners because we need *many* training samples to reach acceptable performance during that final fine-tuning step.  Many-shot learning is only possible when we have compute, time, and data to allow us to fine-tune our models. Ideally, we want to maximize model performance while minimizing N-shot requirements.  Zero-shot is the natural *best-case scenario* for a model as it means we require *zero* training samples before shifting it to a new domain or task.  CLIP may not be breaking SotA performance benchmarks on specific datasets. Still, it is proving to be a massive leap forward in zero-shot performance across various tasks in both image and text modalities.  ---  *The point of CLIP is not SotA performance. However, it's worth noting that CLIP did beat the previous SotA results on the STL10 benchmark despite never being trained on that dataset.*  ---  The zero-shot adaptability of CLIP was found to work across many domains and different tasks. We will be talking about image classification in this article, but it can also be used in [multi-modal search/recommendation](https://www.pinecone.io/learn/clip), object detection, and likely many more as of yet unknown tasks.  ## How CLIP Makes Zero-Shot So Effective?  **C**ontrastive **L**anguage-**I**mage **P**retraining (CLIP) is a primarily transformer-based model released by OpenAI in 2021 [1].  CLIP consists of two models, as discussed in more depth in the [previous chapter](https://www.pinecone.io/learn/clip). The version of CLIP we use here consists of a text transformer for encoding text embeddings and a vision transformer (ViT) for encoding image embeddings.  ![clip-architecture-2](./images/zero-shot-image-classification-clip-2.png)  Both CLIP models are optimized during pretraining to align similar text and images in vector space. It does this by taking image-text pairs and pushing their output vectors nearer in vector space while separating the vectors of non-pairs.  <center><div> <img src=\"./images/zero-shot-image-classification-clip-3.png\" alt=\"CLIP pretraining\" style=\"width:500px;\"/></div> </center>  It distinguishes itself from typical classification models for several reasons. First, OpenAI trained it on a **huge dataset of 400M text-image pairs** that were scraped from across the internet.  There are three primary benefits here:  1. CLIP requires just **image-text pairs** rather than specific class labels thanks to the *contrastive* rather than *classification* focused training function. This type of data is abundant in today's social-media-centric world. 2. The large dataset size means CLIP can build a strong understanding of general textual concepts displayed within images. 3. Text descriptors often describe various features of an image, not just one. Meaning a more holistic representation of images (and text) can be built.  These benefits of CLIP are the primary factors that have led to its outstanding zero-shot performance.  The authors of CLIP draw a great example of this by comparing a ResNet-101 model trained specifically on ImageNet to CLIP when both are applied to other datasets derived from ImageNet.  ![clip-imagenet-comparison](./images/zero-shot-image-classification-clip-4.png) <small>ResNet-101 fine-tuned on ImageNet vs. zero-shot CLIP, source [1].</small>  In this comparison, we can see that despite ResNet-101 training for ImageNet, its performance on *similar* datasets is much worse than CLIP on the same tasks. CLIP outperforms a SotA model trained for ImageNet on slightly modified ImageNet tasks.  When applying a ResNet model to other domains, a standard approach is to use a \"linear probe\". That is where the ResNet learned features (from the last few layers) are fed into a linear classifier that is then fine-tuned for a specific dataset. We would regard this as few to many-shot learning.  In the CLIP paper, linear probe ResNet-50 was compared to zero-shot CLIP. In one scenario, zero-shot CLIP outperforms linear probing across many tasks.  <center><div> <img src=\"images/zero-shot-image-classification-clip-5.png\" alt=\"CLIP performance\" style=\"width:300px;\"/></div> </center> <small>Zero-shot CLIP performance compared to ResNet with linear probe, source [1].</small>  Despite CLIP not being trained for these specific tasks, it outperforms a ResNet-50 with a linear probe. However, it's worth noting that zero-shot did not outperform linear probing when given more training samples.  ## Zero-Shot Classification  How exactly does CLIP do zero-shot classification? We know that the image and text encoder creates a 512-dimensional image and text vector that map to the same vector space.  Considering this vector space alignment, what if we wrote the dataset classes as text sentences?  Given a task where we must identify whether a photo contains a car, bird, or cat: we could create and encode three text classes:  ``` \"a photo of a car\" -> T_1 \"a photo of a bird\" -> T_2 \"a photo of a cat\" -> T_3 ```  Each of these *\"classes\"* are output from the text encoder as vectors $T_1$, $T_2$, and $T_3$ respectively. Given an photo of a cat, we encode it with the ViT model to create vector $I_1$. When we calculate the similarity of these vectors with cosine similarity, we expect $sim(T_3, I_1)$ to return the highest score.  <center><div> <img src=\"images/zero-shot-image-classification-clip-6.png\" alt=\"Drawing\" style=\"width:500px;\"/></div> </center> <small>We reformat the one-word classes into sentences and encode them into label-based text vectors. When we encode and compare an image of a cat, we expect to share the highest similarity with *\"a photo of a cat\"*.</small>  We format the one-word classes into sentences because we expect CLIP saw more *sentence-like* text during pretraining. For ImageNet it was reported that a 1.3 percentage point improvement in accuracy was achieved using the same prompt template of `\"a photo of a {label}\"` \\[1\\].  Prompt templates don't always improve performance and they should be tested for each dataset. For the dataset we use below, we found it to *reduce* performance by 1.5 percentage points.  ### Python Implementation  Let's move on to an applied example of CLIP for zero-shot classification. We will use the `frgfm/imagenette` dataset via Hugging Face *Datasets*.  {{< notebook file=\"clip-classification-dataset\" height=\"full\" >}}  The dataset contains *10* labels, all stored as integer values. To perform classification with CLIP we need the *text content* of these labels. Most Hugging Face datasets include the mapping to text labels inside the the dataset `info`:  {{< notebook file=\"clip-classification-labels\" height=\"full\" >}}  Before we can compare labels and photos, we need to initialize CLIP. We will use the CLIP implementation found via Hugging Face transformers.  {{< notebook file=\"clip-classification-init-model\" height=\"full\" >}}  Text transformers cannot read text directly. Instead, they need a set of integer values known as token IDs (or `input_ids`), where each unique integer represents a word or sub-word (known as a *token*).  We create these token IDs alongside another tensor called the *attention mask* (used by the transformer's attention mechanism) using the `processor` we just initialized.  {{< notebook file=\"clip-classification-label-tokens\" height=\"full\" >}}  Using these transformer-readable tensors, we create the label text embeddings like so:  {{< notebook file=\"clip-classification-get-text-features\" height=\"full\" >}}  The vectors that CLIP outputs *are not normalized*, meaning dot product similarity will give inaccurate results *unless* the vectors are normalized beforehand. We do that like so:  {{< notebook file=\"clip-classification-normalization\" height=\"full\" >}}  *(Alternatively, you can use cosine similarity)*  All we have left is to work through the same process with the images from our dataset. We will test this with a single image first.  {{< notebook file=\"clip-classification-image-process\" height=\"full\" >}}  After processing the image, we return a single (`1`) three-color channel (`3`) image width of `224` pixels and a height of `224` pixels. We must process incoming images to normalize and resize them to fit the input size requirements of the ViT model.  We can create the image embedding with:  {{< notebook file=\"clip-classification-get-image-features\" height=\"full\" >}}  From here, all we need to do is calculate the dot product similarity between our image embedding and the ten label text embeddings. The highest score is our predicted class.  {{< notebook file=\"clip-classification-one-example\" height=\"full\" >}}  Label *2*, i.e., *\"cassette player\"* is our correctly predicted winner. We can repeat this logic over the entire `frgfm/imagenette` dataset to get the classification accuracy of CLIP.  {{< notebook file=\"clip-classification-dataset-perf\" height=\"full\" >}}  That gives us an impressive zero-shot accuracy of *98.7%*. CLIP proved to be able to accurately predict image classes with little more than some minor reformating of text labels to create sentences.  ---  Zero-shot image classification with CLIP is a fascinating use case for high-performance image classification with minimal effort and zero fine-tuning required.  Before CLIP, this was not possible. Now that we have CLIP, it is almost *too* easy. The multi-modality and contrastive pretraining techniques have enabled a technological leap forward.  From multi-modal search, zero-shot image classification, and object detection to industry-changing tools like OpenAI's Dall-E and Stable Diffusion, CLIP has opened the floodgates to many new use-cases that were previously blocked by insufficient data or compute.  ## Resources  \\[1\\] A. Radford, et. al., [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) (2021) ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e35a"
  },
  "title": "\"Detecting Similar Security Alerts at Expel\"",
  "headline": "\"Detecting Similar Security Alerts at Expel\"",
  "weight": "2",
  "- name": "Peter Silberman",
  "position": "CTO, Expel",
  "src": "/images/peter-silberman.jpg",
  "href": "\"https://www.linkedin.com/in/petersilberman/\"",
  "description": "\"How Expel built an alert similarity feature to help security analysts detect and remediate threats faster.\"",
  "images": "[\"/images/expel-1.png\"]",
  "thumbnail": "\"/images/expel-1.png\"",
  "content": "*Written by [Dan Whalen](https://www.linkedin.com/in/dan-whalen/) and [Peter Silberman](https://www.linkedin.com/in/petersilberman/) for [the Expel blog](https://expel.com/blog/how-we-built-it-alert-similarity/). Reposted with permission.*  Since the beginning of our journey here at Expel, we’ve invested in creating processes and tech that set us up for success as we grow – meaning we keep our analysts engaged (and help them avoid burnout as best we can) while maintaining the level of service our customers have come to expect from us.   One of the features we recently built and released helps us do all of this: Alert Similarity. Why did we build it and how does it benefit our analysts and customers?  Here’s a detailed look at how we approached the creation of Alert Similarity. If you’re interested in trying to develop a similar feature for your own security operations center (SOC), or learning about how to bring research to production, then read on for tips and advice.  ## Getting started   In our experience, it’s best to kick off with some research and experimentation – this is an easy way to get going and start identifying low-hanging fruit, as well as to find opportunities to make an impact without it being a massive undertaking.  We began our Alert Similarity journey by using one of our favorite research tools: a [Jupyter notebook](https://expel.com/blog/our-journey-jupyterhub-beyond/). The first task was to validate our hypothesis: we had a strong suspicion that new security alerts are similar to others we’ve seen in the past.  ![Expel Analysts Triage ~1000 Alerts/Day](/images/expel-1.png)  To test the theory, we designed an experiment in a Jupyter notebook where we:  1. Gathered a representative sample set of alerts; 2. Created vector embeddings for these alerts; 3. Generated an n:n similarity matrix comparing all alerts; and 4. Examined the results to see if our hypothesis held up.  We then gathered a sample of alerts over a few months (approximately 40,000 in total). This was a relatively easy task, as our platform stores security alerts and we have simple mechanisms in place to retrieve them regularly.   Next, we needed to decide how to create vector embeddings. For the purposes of testing our hypothesis, we decided we didn’t need to spend a ton of time perfecting how we did it. If you’re familiar with generating embeddings, you’ll know this usually turns into a never-ending process of improvement. To start, we just needed a baseline to measure our efforts against. To that end, we chose MinHash as a quick and easy way to turn our selected alerts into vector embeddings.  ### What is MinHash and how does it work?  [MinHash](https://en.wikipedia.org/wiki/MinHash) is an efficient way to approximate the [Jaccard Index](https://en.wikipedia.org/wiki/Jaccard_index) between documents. The basic principle is that the more data shared between two documents, the more similar they are. Makes sense, right?  Calculating the true Jaccard index between two documents is a simple process that looks like this:  ```python Jaccard Index = (Intersection of tokens between both documents) / (Union of tokens between both documents) ```  For example, if we have two documents:  1. The lazy dog jumped over the quick brown fox 2. The quick hare jumped over the lazy dog  We could calculate the Jaccard index like this:  ```python (the, dog, jumped, over, quick) / (the, lazy, dog, jumped, over, quick, brown, fox, hare) → 5 / 6 → 0.8333 ```  This is simple and intuitive, but at scale it presents a problem: You have to store all tokens for all documents to calculate this distance metric. In order to calculate the result, you inevitably end up using lots of storage space, memory, and CPU.   That’s where MinHash comes in. It solves the problem by approximating Jaccard similarity, yet only requires that you store a vector embedding of length K for each document. The larger K, the more accurate your approximation will be.  By transforming our input documents (alerts) into MinHash vector embeddings, we’re able to efficiently store and query against millions of alerts. This approach allows us to take any alert and ask, “What other alerts look similar to this one?” Similar documents are likely good candidates for further inspection.  ## Validating our hypothesis  Once we settled on our vectorization approach (thanks, MinHash!), we tested our hypothesis. By calculating the similarity between all alerts for a specific time period, we confirmed that 5-6% of alerts had similar neighbors (Fig 1.). Taking that even further, our metrics allowed us to estimate actual time savings for our analysts (Fig 2.).  ![Percentage of alerts with similar neighbors](/images/expel-fig1.png) <small>Fig. 1: Percentage of alerts with similar neighbors</small>  ![Estimated analyst hours saved](/images/expel-fig2.png) <small>Fig. 2: Estimated analyst hours saved (extrapolated)</small>  These metrics proved that we were onto something. Based on these results, we chose building an Alert Suggestion capability off the back of Alert Similarity as our first use case to target. This use case would allow us to improve efficiencies in our SOC and, in turn, enhance the level of service we provide to our customers.  ## Our journey to production  ### Step 1: Getting buy-in across the organization  Before moving full speed ahead into our project, we communicated our research idea and its potential benefits across the business. The TL;DR here? You can’t get your colleagues to buy into a new idea unless they understand it. Our R&D groups pride themselves on never creating “Tad-dah! It’s in production!” moments for Engineering or Product Management without them having the background on new projects first.   We created a presentation that outlined the opportunity and our research, and allowed Expletives (anyone from Product Management to Customer Success to Engineering) to review our proof of concept. In this case, we used a heavily documented notebook to walk viewers through what we did. We discussed our go-forward plan and made sure our peers across the organization understood the opportunity and were invested in our vision.  ### Step 2: Reviewing the design  Next, we created a design review document outlining a high-level design of what we wanted to build. This is a standard process at Expel and is an important part of any new project. This document doesn’t need to be a perfect representation of what you’ll end up building, nor does it need to include every last implementation detail, but it does need to give the audience an idea of the problem you’re aiming to solve and the general architectural design of the solution you’re proposing.  Here’s a quick look at the design we mocked up to guide our project:  ![Project Mock Up Design](/images/expel-3.png)  As part of this planning process, we identified the following goals and let those inform our design:  - Build new similarity-powered features with little friction - Monitor the performance and accuracy of the system - Limit complexity wherever possible (don’t reinvent the wheel) - Avoid making the feature availability mission critical (so we can move quickly without risk)  As a result of this planning exercise, we concluded that we needed to build the following components:  - Arnie (Artifact Normalization and Intelligent Encoding): A shared library to turn documents at Expel into vector embeddings - Vectorizor consumer: A worker that consumes raw documents and produces vector embeddings - Similarity API: A grpc service that provides an interface to search for similar documents  We also decided that we wouldn’t build our own vector search database and instead decided to use [Pinecone.io](/) to meet this need. This was a crucial decision that saved us a great deal of time and effort. (Remember how we said we wouldn’t reinvent the wheel?)   Why Pinecone? At this stage, we had a good sense for our technical requirements. We wanted sub-second vector search across millions of alerts, an API interface that abstracts away the complexity, and we didn’t want to have to worry about database architecture or maintenance. As we examined our options, Pinecone quicky became our preferred partner. We were really impressed by the performance we were able to achieve and how quick and easy their service was to set up and use.  ### Step 3: Implementing our Alert Similarity feature \t We’re lucky to have an extremely talented core platform team here at Expel  infrastructure capabilities we can reliably build on. Implementing our feature was as simple as using these building blocks and best practices for our use case.  ### Release day  Once the system components were built and running in staging, we needed to coordinate a release in production that didn’t introduce risk into our usual business operations. Alert Suggestion would produce suggestions in Expel Workbench like this, which could inform decisions made by our SOC analysts.   ![Release Day](/images/expel-4.png)  However, if our feature didn’t work as expected – or worse, created incorrect suggestions – we could cause confusion or defects in our process.  To mitigate these risks when moving to production, it was important to gather metrics on the performance and accuracy of our feature before we started putting suggestions in front of our analysts. We used [LaunchDarkly](https://launchdarkly.com/) and [Datadog](https://www.datadoghq.com/) to accomplish this. LaunchDarkly feature flags allowed us to deploy to production silently – meaning it runs behind the scenes and is invisible to end users. This allowed us to build a Datadog dashboard with all kinds of useful metrics like:  - How quickly we’re able to produce a suggestion - The percentage of alerts we can create suggestions for - How often our suggestions are correct (we did this by comparing what the analyst did with the alert versus what we suggested) - Model performance (accuracy, recall, F1 score) - The time it takes analysts to handle alerts with and without suggestions  To say these metrics were invaluable would be an understatement. Deploying our feature silently for a period of time allowed us to identify several bugs and correct them without having any impact on our customers. This boosted confidence in Alert Similarity before we flipped the switch. When the time came, deployment was as simple as updating a single feature flag in LaunchDarkly.  ## What we’ve learned so far  We launched Alert Similarity in February 2022, and throughout the building process we learned (or in many cases, reaffirmed) several important things:  ### Communication is key.   You can’t move an organization forward with code alone. The time we spent sharing research, reviewing design documents, and gathering feedback was crucial to the success of this project.  ### There’s nothing like real production data.   A silent release with feature flags and metrics allowed us to identify and fix bugs without affecting our analysts or customers. This approach also gave us data to feel confident that we were ready to release the feature. We’ll look to reuse this process in the future.  ### If you can’t measure it, you don’t understand it.   This whole journey from beginning to end was driven by data, allowing us to move forward based on a validated hypothesis and realistic goals versus intuition. This is how we knew our investment was worth the time and how we were able to prove the value of Alert Similarity  once it was live.  ## What’s next?  Although we targeted suggestions powered by Alert Similarity as our first feature, we anticipate an exciting road ahead filled with additional features and use cases. We’re interested in exploring other types of documents that are crucial to our success and how similarity search could unlock new value and efficiencies.   Additionally, as we alluded to above, there’s always room for improvement when transforming documents into vector embeddings. We’re already exploring new ways to represent security alerts that improve our ability to find similar neighbors for alerts. We see a whole world of opportunities where similarity search can help us, and we’ll continue experimenting, building and sharing what we learn along the way.  Interested in more engineering tips and tricks, and ideas for building your own features to enhance your service (and make your analysts’ lives easier?) [Subscribe to our blog](https://expel.com/blog/) to get the latest posts sent right to your inbox.  ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e35c"
  },
  "content": "categories:  - Embedding Methods for Image Search toc: >- weight: 3 authors:  - name: James Briggs    position: Developer Advocate    src: /images/james-briggs.jpeg    href: \"https://www.youtube.com/c/jamesbriggs\"  - name: Laura Carnevali    position: Developer    src: /images/laura-carnevali.jpeg    href: \"https://www.linkedin.com/in/laura-carnevali-071a01b7/\" description: How ImageNet and AlexNet triggered the deep learning renaissance # Open graph images: ['https://www.pinecone.io/images/imagenet-0.png'] ---  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">     <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/c_u4AHNjOpk\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  Today's deep learning revolution traces back to the 30th of September, 2012. On this day, a **C**onvolutional **N**eural **N**etwork (CNN) called AlexNet won the ImageNet 2012 challenge [1]. AlexNet didn’t just win; it *dominated*.  AlexNet was unlike the other competitors. This new model demonstrated unparalleled performance on the largest image dataset of the time, ImageNet. This event made AlexNet the first widely acknowledged, successful application of *deep learning*. It caught people's attention with a 9.8 percentage point advantage over the nearest competitor [2].  <img src=\"./images/imagenet-11.png\" style=\"width:80%;display:block;margin-left:auto;margin-right:auto;\" /> <small>The best ImageNet challenge results in 2010 and 2011, compared against all results in 2012, including AlexNet [2].</small>  Until this point, deep learning was a nice idea that most deemed as impractical. AlexNet showed that deep learning was more than a pipedream, and the authors showed the world *how* to make it practical. Yet, the surge of deep learning that followed was not fueled solely by AlexNet. Indeed, without the huge *ImageNet* dataset, there would have been no AlexNet.  ![ml-arxiv-papers](./images/imagenet-1.png) <small>Number of \"ML\" papers in ArXiv per year [3].</small>  The future of AI was to be built on the foundations set by the ImageNet challenge and the novel solutions that enabled the synergy between ImageNet and AlexNet.  ---  ## Fei-Fei Li, WordNet, and Mechanical Turks  In 2006, the world of computer vision was an underfunded discipline with little attention. Yet, many researchers were focused on building better models. Year after year saw progress, but it was slow.  Fei-Fei Li had just completed her Ph.D. in Computer Vision at Caltech [4] and started as a computer science professor at the University of Illinois Urbana-Champaign. During this time, Li noticed this focus on models and subsequent *lack of focus* on data.  Li thought that the key to better model performance could be bigger datasets that reflected the diversity of the real world.  During Li's research into datasets, she learned about professor Christiane Felbaum, a co-developer of a dataset from the 1980s called WordNet. WordNet consisted of many English-language terms organized into an ontological structure [5].  <img src=\"./images/imagenet-2.png\" style=\"width:60%;display:block;margin-left: auto;margin-right: auto;\" /> <small>Example of the ontological structure of WordNet [5].</small>  In 2007, Li and Felbaum met. Felbaum discussed her current work on adding a reference image to each word in WordNet. This inspired an idea that would shift the world of computer vision into hyperdrive. Soon after, Li put together a team to build what would become the largest image dataset of its time: ImageNet [6].  The idea behind ImageNet is that a large ontology of images – based on WordNet – could be the key to developing advanced, content-based image retrieval and understanding [7].  Two years later, the first version of ImageNet was released with 12 million images structured and labeled in line with the WordNet ontology. If one person had annotated one image/minute and did nothing else in those two years (including sleeping or eating), it would have taken 22 years and 10 months.  To do this in under two years, Li turned to Amazon Mechanical Turk, a crowdsourcing platform where anyone can hire people from around the globe to perform tasks cost-effectively.  The ImageNet team instructed \"Turkers\" to decide whether an image represents a given word (from the WordNet ontology) or not. Several measures were implemented to ensure accurate annotation, including having multiple Turker scores for each image-word pair [7].  On its release, ImageNet was the world's largest labeled dataset of images publically available. Yet, there was very little interest in the dataset. After being presented as a poster at the CVPR conference, they needed to find another way to stir interest.  ---  ## ImageNet  When the paper detailing ImageNet was released in 2009, the dataset comprised 12 million images across 22,000 categories.  ![imagenet-paper](./images/imagenet-3.png) <small>Example ontologies from WordNet used by ImageNet [7].</small>  As it used WordNet's ontological structure, these images rolled up into evermore general categories.  At the time, a few other image datasets also used an ontological structure like ImageNet's. One of the better known of these was the **E**xtra **S**ensory **P**erception (ESP) dataset, which used a similar \"crowdsourcing\" approach but via the \"ESP game\". In this game, partners would try to match words to images, creating labels [8].  ![esp-imagenet](./images/imagenet-4.png) <small>The subtree for many terms were much larger and denser for ImageNet than the public subset of ESP [7].</small>  Despite collecting a large amount of data, most of the dataset was not made public [8]. Of the 60K images that were, ImageNet offered much larger and denser coverage [7]. Additionally, ESP was found to be fundamentally flawed [9]. Beyond a relicensed version used for Google Image search [10], it did not impact the field of AI.  There was initially little interest in ImageNet or other similar datasets like ESP. At the time, very few people believed that the performance of models could be improved through more data.  Most researchers dismissed the dataset as being too large and complex. In hindsight, this seems surprising. However, at the time, models struggled on datasets with 12 categories, so ImageNet's 22,000 categories must have seemed absurd.  ### ImageNet Challenge  By the following year, the ImageNet team managed to organize the **I**mageNet **L**arge **S**cale **V**isual **R**ecognition **C**hallenge (ILSVRC). Competitors had to correctly classify images and detect different objects and scenes across a *trimmed* list of 1,000 ImageNet categories. Every year, the team that produced the model with the lowest error rate won [2].  The scale of the dataset and competition resulted in ILSVRC becoming the primary benchmark in computer vision. Researchers realized that more data could be a good thing.  2012 was not like the previous years. On September 30, 2012, the latest ILSVRC results were released. One model called AlexNet was clearly distinguished from the others [11].  <img src=\"./images/imagenet-5.png\" style=\"width:70%;display:block;margin-left:auto;margin-right:auto;\" /> <small>Results of ILSVRC 2012 [11].</small>  AlexNet was the first model to score a sub-25% error rate. The nearest competitor scored 9.8 percentage points behind [1]. AlexNet dominated the competition, and they did it with a deep-layered **C**onvolutional **N**eural **N**etwork (CNN), an architecture dismissed by most as impractical.  ### Convolutional Neural Networks  A CNN is a neural network model architecture that uses convolutional layers. These models are known today for their high performance on image data and minimal preprocessing or manual feature extraction requirements.  ![cnn](./images/imagenet-10.png) <small>Typical architecture of a CNN.</small>  CNNs use several convolutional layers stacked on top of one another. The first layers can recognize simple features, like edges, shapes, and textures. As the network gets deeper, it produces more \"abstract\" representations, eventually identifying concepts from mammals to dogs and even Siberian huskies.  ---  *Convolutional Neural Networks will be explained in more detail in the next chapter of [Embedding Methods for Image Search](https://www.pinecone.io/learn/image-search/).*  ---  These networks generally work best with many layers and large amounts of data, so they were overlooked. Shallow implementations lacked benefits over other networks, and deeper implementations were computationally unrealistic; the odds were stacked against these networks.  Despite these potential challenges, the authors of AlexNet won ILSVRC by a 9.8 percentage point margin with one of these models. It turns out they were the right people in the right place at the right time.  Several pieces came together for this to work. ImageNet provided the massive amounts of data required to train a deep CNN. A few years earlier, Nvidia had released CUDA, an API that enabled software access to highly-parallel GPU processing [12][13]. GPU power had reached a point where training AlexNet's 60 million parameters became practical with the use of multiple GPUs.  ## AlexNet  AlexNet was by no means small. To make it work, the authors had to solve many problems. The model consisted of eight layers: five convolutional layers followed by three fully-connected linear layers. To produce the 1000-label classification needed for ImageNet, the final layer used a 1000-node softmax, creating a probability distribution over the 1000 classes.  ![AlexNet architecture](./images/imagenet-6.png) <small>Network architecture of AlexNet [1].</small>  A key conclusion from AlexNet was that the depth of the network had been instrumental to its performance. That depth produced *a lot* of parameters, making training either impractically slow or simply impossible; if training on CPU. By training on GPU, training time could become practical. Still, high-end GPUs of the time were limited to ~3GB of memory, not enough to train AlexNet.  To make this work, AlexNet was distributed across two GPUs. Each GPU handled one-half of AlexNet. The two halves would communicate in specific layers to ensure they were not training two separate models.  <img src=\"./images/imagenet-7.png\" style=\"width:50%;display:block;margin-left:auto;margin-right:auto;\" /> <small>ReLU activation function.</small>  Training time was reduced further by swapping the standard sigmoid or tanh activation functions of the time for **Re**ctified **L**inear **U**nit (ReLU) activation functions.  <img src=\"./images/imagenet-8.png\" style=\"width:70%;display:block;margin-left:auto;margin-right:auto;\" /> <small>Results from a four-layer CNN with ReLU activation functions reached a 25% error rate on the CIFAR-10 dataset six times faster than the equivalent with Tanh activation functions [1].</small>  ReLU is a simpler operation and does not require normalization like other functions to avoid activations congregating towards min/max values (saturation). Nonetheless, another type of normalization called **L**ocal **R**esponse **N**ormalization (LRN) was included. Adding LRN reduced top-1 and top-5 error rates by 1.4% and 1.2% respectively [1].  Another critical component of AlexNet was the use of overlapping pooling. Pooling was already used by CNNs to summarize a group of activations in one layer to a single activation in the following layer.  ![overlapping-pooling](./images/imagenet-9.png)  Overlapping pooling performs the same operation, but, as the pooling window moves across the preceding layer, it overlaps with the previous window. AlexNet found this to improve top-1 and top-5 error rates by 0.4% and 0.3%, respectively, and reduce overfitting.  ## AlexNet in Action  While it's great to talk about all of this, it's even better to see it implemented in code. You can find the Colab notebook here, TK, if you'd like to follow along.  ### Data Preprocessing  Let's start by downloading and preprocessing our dataset. We will use a small sample from ImageNet hosted on HuggingFace.  {{< notebook file=\"imagenet-load-dataset\" height=\"full\" >}}  The `Maysee/tiny-imagenet` dataset contains 100K and 10K labeled images in the train and validation sets, respectively. All images are stored as Python PIL objects. Preprocessing of these images consists of several steps:  * Convert all images to RGB format. * Resize to fit AlexNet's expected input dimensions. * Convert to tensor format. * Normalize values. * Stack this set of tensors into a single batch.  We start with RGB; AlexNet assumes all images will have three color channels (**R**ed, **G**reen, and **B**lue). But many other formats are supported by PIL, such as *L* (grayscale), RGBA, and CMYK. We must convert any non-RGB PIL objects into RGB format.  {{< notebook file=\"imagenet-convert-rgb\" height=\"full\" >}}  AlexNet, and many other pretrained models, expect input images to be tensors of dimensions (3 x H x W), where *3* represents the three color channels. H and W are expected to have a dimensionality of at least 224 [14]. We must resize our images; this is done easily using `torchvision.transforms`.  {{< notebook file=\"imagenet-resize\" height=\"full\" >}}  Finally, we must normalize the image tensors to a range of [0, 1] using `mean = [0.485, 0.456, 0.406]` and `std = [0.229, 0.224, 0.225]` as per the implementation notes on PyTorch docs [14].  ```python # resize and crop to 224x224 preprocess = transforms.Compose([     transforms.ToTensor(),  # convert from PIL image to tensor before norm to avoid error     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ])  # final result is normalized tensor of shape (3, 224, 224) new_img = preprocess(new_img) ```  We can combine all of this into a batch of `50` images. Rather than running preprocessing on our entire dataset and keeping everything in memory, we must process it in batches. For this example, we will only test the first 50 images as we know these should all be labeled as \"goldfish\".  {{< notebook file=\"imagenet-preprocess-batch\" height=\"full\" >}}  We have preprocessed our first batch and produced tensors containing *50* input tensors of shape *(3, 224, 244)*, ready for inference with AlexNet.  ### Inference  To perform inference (e.g., make predictions) with AlexNet, we first need to download the model. We will download the pretrained AlexNet hosted by PyTorch.  {{< notebook file=\"imagenet-alexnet-download\" height=\"full\" >}}  We can see the network architecture of AlexNet here with five convolutional layers followed by three feed-forward linear layers. This represented a more efficient modification of the original AlexNet and was proposed by Krizhevsky in a later paper [15].  By default, the model is loaded to the CPU. We can run it here, but running on a CUDA-enabled GPU or MPS on Apple Silicon is more efficient. We do this by setting the device like so:  ```python # move to device if available device = torch.device(     'cuda' if torch.cuda.is_available() else (         'mps' if torch.backends.mps.is_available() else 'cpu'     ) ) ```  From this, we must always move the input tensors and model to the device *before* performing inference. Once moved, we run inference with `model(inputs)`.  {{< notebook file=\"imagenet-alexnet-inference\" height=\"full\" >}}  The model will output a set of logits (output activations) for each possible class. There are 1000 of these for every image we feed into the model. The highest activation represents the class that the model predicts for each image. We convert these logits into class predictions with an `argmax` function.  Most of the predicted values belong to class `1`. That has no meaning for us, so we cross check this with the [PyTorch AlexNet classes](https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt) like so:  {{< notebook file=\"imagenet-alexnet-classes\" height=\"full\" >}}  Clearly, the AlexNet model is predicting goldfish correctly. We can calculate the accuracy with:  ```python sum(preds == 1) / len(preds) ```  This returns an accuracy of 72% for the goldfish class. A top-1 error rate of 28% beats the reported average error rate of 37.5% from the original AlexNet paper. However, this is only for a single class, and the model performance varies from class to class.  ---  That's our overview of one of the most significant events in computer vision and machine learning. The ImageNet Challenge was hosted annually until 2017. By then, 29 of 38 contestants had an error rate of less than 5% [16], demonstrating the massive progress made in computer vision during ImageNet's active years.  AlexNet was superseded by even more powerful CNNs. Microsoft Research Asia dethroned AlexNet as the winner of ILSVRC in 2015 [17]. Since then, many more CNN architectures have come and gone. Recently, the use of another network architecture known as a [transformer](https://www.pinecone.io/learn/transformers/) has begun to disrupt CNNs domination of computer vision.  The final paragraph of the AlexNet paper proved almost prophetical for the future of AI and computer vision. They noted that they:  <div style=\"padding: 1rem;\">     <em>     \"did not use any unsupervised pre-training even though we expect it will help\",     </em>     and     <em>     \"our results have improved as we have made our network larger... we still have many orders of magnitude to go in order to match the infero-temporal pathway of the human visual system\"     </em> </div>  Unsupervised pre-training and ever larger models would later become the hallmark of ever better models.  ## Resources  [1] A. Krizhevsky, I. Sutskever, G. Hinton, [ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) (2012), NeurIPS  [2] [ImageNet Large Scale Visual Recognition Challenge (ILSVRC)](https://image-net.org/challenges/LSVRC/index.php), ImageNet  [3] J. Dean, [Machine Learning for Systems and Systems for Machine Learning](http://learningsys.org/nips17/assets/slides/dean-nips17.pdf) (2017), NeurIPS 2017  [4] F. Li, [Visual Recognition: Computational Models and Human Psychophysics](https://thesis.library.caltech.edu/2390/) (2005), Caltech  [5] G. Miller, R. Beckwith, C. Felbaum, D. Gross, K. Miller, [Introduction to WordNet: An On-line Lexical Database](https://wordnetcode.princeton.edu/5papers.pdf) (1993)  [6] D. Gershgorn, [The data that transformed AI research — and possibly the world](https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/) (2017), Quartz  [7] J. Deng, W. Dong, R. Socher, L. Li, K. Li, L. Fei-Fei, [ImageNet: A large-scale hierarchical image database](https://ieeexplore.ieee.org/document/5206848) (2009), CVPR  [8] L. Ahn, L. Dabbish, [Labeling images with a computer game](https://dl.acm.org/doi/10.1145/985692.985733) (2004), Proc. SIGCHI  [9] I. Weber, S. Robertson, M. Vojnovic, [Rethinking the ESP Game](https://www.microsoft.com/en-us/research/publication/rethinking-the-esp-game/) (2009), ACM  [10] A. Saini, [Solving the web's image problem](http://news.bbc.co.uk/1/hi/technology/7395751.stm) (2008), BBC News  [11] O. Russakovsky, J. Deng, et. al., [ImageNet Large Scale Visual Recognition Challenge](https://arxiv.org/abs/1409.0575) (2015), IJCV  [12] F. Abi-Chahla, [Nvidia's CUDA: The End of the CPU?](https://www.tomshardware.com/reviews/nvidia-cuda-gpu,1954.html) (2008), Tom's Hardware  [13] A. Krizhevsky, [cuda-convnet](https://code.google.com/archive/p/cuda-convnet/) (2011), Google Code Archive  [14] [AlexNet Implementation in PyTorch](https://pytorch.org/hub/pytorch_vision_alexnet/), PyTorch Resources  [15] A. Krizhevsky, [One weird trick for parallelizing convolutional neural networks](https://arxiv.org/abs/1404.5997) (2014)  [16] [ILSVRC2017 Results](https://image-net.org/challenges/LSVRC/2017/results) (2017)  [17] K. He, X. Zhang, S. Ren, J. Sun, [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) (2015)",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e35e"
  },
  "title": "\"Batch and Layer Normalization\"",
  "headline": "\"Build Better Deep Learning Models with Batch and Layer Normalization\"",
  "weight": "1",
  "name": "Bala Priya C",
  "position": "Technical Writer",
  "src": "/images/bala-priya.jpg",
  "href": "https://www.linkedin.com/in/bala-priya/",
  "description": "\"Batch and layer normalization are two strategies for training neural networks faster, without having to be overly cautious with initialization and other techniques.\"",
  "images": "[\"/images/batch-and-layer-normalization-strategies.png\"]",
  "content": "![Batch and layer normalization](/images/batch-and-layer-normalization-strategies.png)  Recent advances in deep learning research have revolutionized fields like medical imaging, machine vision, and [natural language processing](https://www.pinecone.io/learn/nlp/). However, it's still challenging for data scientists to choose the optimal model architecture and to tune hyperparameters for best results.  Even with the optimal model architecture, how the model is trained can make the difference between a phenomenal success or a scorching failure.  For example, take [weight initialization](https://cs231n.github.io/neural-networks-2/#init): In the process of training a neural network, we initialize the weights which are then updated as the training proceeds. For a certain random initialization, the outputs from one or more of the intermediate layers can be abnormally large. This leads to instability in the training process, which means the network will not learn anything useful during training.  Batch and layer normalization are two strategies for training neural networks faster, without having to be overly cautious with initialization and other [regularization](https://cs231n.github.io/neural-networks-2/#reg) techniques.  In this tutorial, we’ll go over the need for normalizing inputs to the neural network and then proceed to learn the techniques of batch and layer normalization.  Let's get started!  ## Why Should You Normalize Inputs in a Neural Network?  When you train a neural network on a dataset, the numeric input features could take on values in potentially different ranges. For example, if you’re working with a dataset of student loans with the age of the student and the tuition as two input features, the two values are on totally _different_ scales. While the age of a student will have a median value in the range 18 to 25 years, the tuition could take on values in the range \\\\$20K - \\\\$50K for a given academic year.  If you proceed to train your model on such datasets with input features on different scales, you’ll notice that the neural network takes significantly longer to train because the gradient descent algorithm takes longer to converge when the input features are not all on the same scale. Additionally, such high values can also propagate through the layers of the network leading to the accumulation of large error gradients that make the training process unstable, called the problem of _exploding gradients_.  To overcome the above-mentioned issues of longer training time and instability, you should consider preprocessing your input data ahead of training. Preprocessing techniques such as normalization and standardization transform the input data to be on the same scale.  ### Normalization vs Standardization  Normalization works by mapping all values of a feature to be in the range [0,1] using the transformation:  $$x_{norm} = \\frac{x-x_{min}}{x_{max}-x_{min}}$$  Suppose a particular input feature `x` has values in the range `[x_min, x_max]`. When `x` is equal to `x_min`, `x_norm` is equal to 0 and when `x` is equal to `x_max`, `x_norm` is equal to 1. So for all values of `x` between `x_min` and `x_max`, `x_norm` maps to a value between 0 and 1.  Standardization, on the other hand, transforms the input values such that they follow a normal distribution with zero mean and unit variance (unit Gaussian). Mathematically, the transformation on the data points in a distrbution with mean μ and standard deviation σ is given by:  $$x_{std} = \\frac{x-\\mu}{\\sigma}$$  In practice, this process of _standardization_ is also referred to as _normalization_ (not to be confused with the normalization process discussed above). As part of the preprocessing step, you can add a layer that applies this transform to the input features so that they all have a similar distribution. In Keras, you can add a [normalization layer](https://keras.io/api/layers/preprocessing_layers/numerical/normalization/) that applies this transform to the input features.  ## Need for Batch Normalization  In the previous section, we learned how we can normalize the input to the neural network in order to speed up training. If you look at the neural network architecture, the input layer is not the only input layer. For a network with hidden layers, the output of layer `k-1` serves as the input to layer `k`. If the inputs to a particular layer change drastically, we can again run into the problem of unstable gradients.  When working with large datasets, you’ll split the dataset into multiple batches and run the mini-batch gradient descent. The [mini-batch gradient descent](https://d2l.ai/chapter_optimization/minibatch-sgd.html) algorithm optimizes the parameters of the neural network by batchwise processing of the dataset, one batch at a time.  It’s also possible that the input distribution at a particular layer keeps changing across batches. The seminal paper titled [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167) by Sergey Ioffe and Christian Szegedy refers to this change in distribution of the input to a particular layer across batches as _internal covariate shift_. For instance, if the distribution of data at the input of layer K keeps changing across batches, the network will take longer to train.  _But why does this hamper the training process?_  For each batch in the input dataset, the mini-batch gradient descent algorithm runs its updates. It updates the weights and biases (parameters) of the neural network so as to fit to the distribution seen at the input to the specific layer for the current batch.  Now that the network has learned to fit to the current distribution, if the distribution changes substantially for the next batch, it now has to update the parameters to fit to the new distribution. This slows down the training process.  However, if we transpose the idea of _normalizing the inputs_ to the _hidden_ layers in the network, we can potentially overcome the limitations imposed by exploding activations and fluctuating distributions at the layer’s input. Batch normalization helps us achieve this, one mini-batch at a time, to accelerate the training process.  ## What is Batch Normalization?  For any hidden layer `h`, we pass the inputs through a non-linear activation to get the output. For every neuron (activation) in a particular layer, we can force the pre-activations to have zero mean and unit standard deviation. This can be achieved by subtracting the mean from each of the input features across the mini-batch and dividing by the standard deviation.  Following the output of the layer `k-1`, we can add a layer that performs this normalization operation across the mini-batch so that the pre-activations at layer `k` are unit Gaussians. The figure below illustrates this.  ![Neural Network with Batch Normalization Layer](/images/batch-normalization-layer.png) <small>Section of a Neural Network with Batch Normalization Layer (Image by the author)</small>  As an example, let's consider a mini-batch with 3 input samples, each input vector being four features long. Here's a simple illustration of how the mean and standard deviation are computed in this case. Once we compute the mean and standard deviation, we can subtract the mean and divide by the standard deviation.  ![Batch Normalization Example](/images/batch-normalization-example.png) <small>How Batch Normalization Works - An Example (Image by the author)</small>  However, forcing all the pre-activations to be zero and unit standard deviation across all batches can be too restrictive. It may be the case that the fluctuant distributions are necessary for the network to learn certain classes better.  To address this, batch normalization introduces two parameters: a scaling factor `gamma` (γ) and an offset `beta` (β). These are learnable parameters, so if the fluctuation in input distribution is necessary for the neural network to learn a certain class better, then the network learns the optimal values of `gamma` and `beta` for each mini-batch. The `gamma` and `beta` are learnable such that it’s possible to go back from the normalized pre-activations to the actual distributions that the pre-activations follow.  Putting it all together, we have the following steps for batch normalization. If `x(k)` is the pre-activation corresponding to the k-th neuron in a layer, we denote it by `x` to simplify notation.  \\begin{align} \\mu_b = \\frac{1}{B}\\sum_{i=1}^{B}x_i \\text{}\\text{ } (1)\\\\\\\\ \\sigma_b^2 = \\frac{1}{B}\\sum_{i=1}^{B}(x_i - \\mu_b)^2 \\text{}\\text{ } (2)\\\\\\\\ \\hat{x_i} = \\frac{x_i - \\mu_b}{\\sqrt{\\sigma_b^2}} \\text{}\\text{} (3)\\\\\\\\ or\\text{ }\\hat{x_i} = \\frac{x_i - \\mu_b}{\\sqrt{\\sigma_b^2 + \\epsilon}} \\text{}\\text{ } (3) \\\\\\\\ Adding\\text{ }\\epsilon\\text{ }helps\\text{ }when\\text{ }\\sigma_b^2\\text{ }is\\text{ }small\\\\\\\\ y_i = \\mathcal{BN}(x_i) = \\gamma.x_i + \\beta \\text{}\\text{ }(4) \\end{align}  ### Limitations of Batch Normalization  Two limitations of batch normalization can arise:  - In batch normalization, we use the *batch statistics*: the mean and standard deviation corresponding to the current mini-batch. However, when the batch size is small, the sample mean and sample standard deviation are not representative enough of the actual distribution and the network cannot learn anything meaningful.   - As batch normalization depends on batch statistics for normalization, it is less suited for sequence models. This is because, in sequence models, we may have sequences of potentially different lengths and smaller batch sizes corresponding to longer sequences.   Later, we’ll examine layer normalization, another technique that can be used for sequence models. For convolutional neural networks (ConvNets), batch normalization is still recommended for faster training.  ### How to Add a Batch Normalization Layer in Keras  Keras provides a `BatchNormalization` class that lets you add a batch normalization layer wherever needed in the model architecture. For a complete review of the different parameters you can use to customize the batch normalization layer, refer to the [Keras docs for BatchNormalization](https://keras.io/api/layers/normalization_layers/batch_normalization/).  The code snippet below shows how you can add batch normalization layers to an arbitrary sequential model in Keras. You can choose to selectively apply batch normalization to specific layers in the network.  ```python import keras  from keras.models import Sequential from keras.layers import Dense, Activation, BatchNormalization  model = Sequential([     Dense(units=10, input_shape=(1,4), activation='relu'),     # add batchnorm layer after activations in the previous layer     BatchNormalization(axis=1),     # pre-activations at the dense layer below are Gaussians     Dense(units=16, activation='relu'),     BatchNormalization(axis=1),     Dense(units=4, activation='softmax') ]) ```  It’s important to understand how batch normalization works under the hood during training and testing. During training, batch normalization computes the mean and standard deviation corresponding to the mini-batch.   However, at test time (inference time), we may not necessarily have a batch to compute the batch mean and variance. To overcome this limitation, the model works by maintaining a [moving average](https://mathworld.wolfram.com/MovingAverage.html) of the mean and variance at training time, called the moving mean and moving variance. These values are accumulated across batches at training time and used as mean and variance at inference time.   ## What is Layer Normalization?  [Layer Normalization](https://arxiv.org/abs/1607.06450) was proposed by researchers Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. In layer normalization, all neurons in a particular layer effectively have the same distribution across all features for a given input.   For example, if each input has `d` features, it's a d-dimensional vector. If there are `B` elements in a batch, the normalization is done along the length of the d-dimensional vector and not across the batch of size `B`.  Normalizing *across all features* but for each of the inputs to a specific layer removes the dependence on batches. This makes layer normalization well suited for sequence models such as [transformers](https://www.pinecone.io/learn/sentence-embeddings/) and [recurrent neural networks (RNNs)](https://www.ibm.com/cloud/learn/recurrent-neural-networks) that were popular in the pre-transformer era.   Here’s an example showing the computation of the mean and variance for layer normalization. We consider the example of a mini-batch containing three input samples, each with four features.  ![Layer Normalization](/images/layer-normalization.png) <small>How  Layer Normalization Works - An Example (Image by the author)</small>  \\begin{align} \\mu_l = \\frac{1}{d}\\sum_{i=1}^{d}x_i \\text{}\\text{ } (1)\\\\\\\\ \\sigma_l^2 = \\frac{1}{d}\\sum_{i=1}^{d}(x_i - \\mu_l)^2 \\text{}\\text{ } (2)\\\\\\\\ \\hat{x_i} = \\frac{x_i - \\mu_l}{\\sqrt{\\sigma_l^2}} \\text{}\\text{ } (3)\\\\\\\\ or\\text{ }\\hat{x_i} = \\frac{x_i - \\mu_l}{\\sqrt{\\sigma_l^2 + \\epsilon}} \\text{}\\text{ } (3) \\\\\\\\ Adding\\text{ }\\epsilon\\text{ }helps\\text{ }when\\text{ }\\sigma_l^2\\text{ }is\\text{ }small\\\\\\\\ y_i = \\mathcal{LN}(x_i) = \\gamma.x_i + \\beta \\text{}\\text{ }(4) \\end{align}  From these steps, we see that they’re similar to the steps we had in batch normalization. However, instead of the batch statistics, we use the mean and variance corresponding to specific input to the neurons in a particular layer, say `k`. This is equivalent to normalizing the output vector from the layer `k-1`.  ### How to Add a Layer Normalization in Keras  Similar to batch normalization, Keras also provides a `LayerNormalization` class that you can use to add layer normalization to the inputs of specific layers. The code cell below shows how you can add [layer normalization](https://keras.io/api/layers/normalization_layers/layer_normalization/) in a simple sequential model. The parameter `axis` specifies the axis along which the normalization should be done.  ```python import keras  from keras.models import Sequential from keras.layers import Dense, Activation, LayerNormalization  model = Sequential([     Dense(units=16, input_shape=(1,10), activation='relu'),     LayerNormalization(axis=1),     Dense(units=10, activation='relu'),     LayerNormalization(axis=1),     Dense(units=3, activation='softmax') ]) ```  To understand how layer normalization is used in transformers, consider reading this TensorFlow tutorial on [transformer models for language understanding](https://www.tensorflow.org/text/tutorials/transformer).  ## Batch Normalization vs Layer Normalization  So far, we learned how batch and layer normalization work. Let's summarize the key differences between the two techniques.  - Batch normalization normalizes each feature independently across the mini-batch. Layer normalization normalizes each of the inputs in the batch independently across all features. - As batch normalization is dependent on batch size, it’s not effective for small batch sizes. Layer normalization is independent of the batch size, so it can be applied to batches with smaller sizes as well. - Batch normalization requires different processing at training and inference times. As layer normalization is done along the length of input to a specific layer, the same set of operations can be used at both training and inference times.  ## Final Thoughts  In this tutorial, you learned the basics of and differences between batch and layer normalization techniques and how to implement them in Keras.  Over the past several years, batch normalization and layer normalization have emerged as the go-to normalization techniques in computer vision and natural language processing, respectively. In certain computer vision tasks, [group and instance normalization](https://www.tensorflow.org/addons/tutorials/layers_normalizations) are also used. For further reading, consider checking out the recommended resources in the section below. Happy learning!  {{< newsletter text=\"Subscribe for more deep learning walkthroughs!\" inputText=\"Email address...\" buttonText=\"Submit\">}}  ## 📚 Recommended Reading  [1] [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167), Sergey Ioffe and Christian Szegedy, 2015.  [2] [Layer Normalization](https://arxiv.org/abs/1607.06450), Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton, 2016.  [3] [How Does Batch Normalization Help Optimization?](https://arxiv.org/abs/1805.11604), Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry, NeurIPS 2018.  [4] [PowerNorm: Rethinking Batch Normalization in Transformers](https://arxiv.org/abs/2003.07845), Sheng Shen, Zhewei Yao, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer, ICML 2020.  [5] [Batch Normalization Layer in Keras](https://keras.io/api/layers/normalization_layers/batch_normalization/)  [6] [Layer Normalization Layer in Keras](https://keras.io/api/layers/normalization_layers/layer_normalization/)  [7] [Preprocessing: Normalization Layer in Keras](https://keras.io/api/layers/preprocessing_layers/numerical/normalization/)",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e360"
  },
  "title": "\"Learning Center\"",
  "headline": "\"Learn to Love Working with <span>Vector Embeddings</span>\"",
  "description": "Learn how to build semantic search systems. From machine translation to question-answering.",
  "- title": "Natural Language Processing for Semantic Search",
  "image": "\"/images/series-nlp-for-semantic-search.png\"",
  "content": "    image: \"/images/series-faiss-the-missing-manual.png\"     link: \"/learn/faiss/\"    - title: Vector Search in the Wild     description: Take a look at the hidden world of vector search and its incredible potential.     image: \"/images/series-vector-search-wild.png\"     link: \"/learn/wild/\"    - title: Embedding Methods for Image Search     description: Learn about the past, present, and future of image search, text-to-image, and more.     image: \"/images/series-image-search.png\"     link: \"/learn/image-search\" --- ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e362"
  },
  "content": "categories:   - Algorithms & Libraries toc: >- weight: 1 author:   name: James Briggs   position: Developer Advocate   src: /images/james-briggs.jpeg   href: \"https://www.youtube.com/c/jamesbriggs\" description: Overview of pre-filtering, post-filtering, and single-stage filtering. #Open Graph images: ['https://www.pinecone.io/images/vector-search-filtering-10.jpg'] ---  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/vector-search-filtering-1.mp4\" type=\"video/mp4\"> </video>  Vector similarity search makes massive datasets searchable in fractions of a second. Yet despite the brilliance and utility of this technology, often what seem to be the most straightforward problems are the most difficult to solve. Such as filtering.  Filtering takes the top place in being seemingly simple — but actually incredibly complex. Applying fast-but-accurate filters when performing a vector search (ie, nearest-neighbor search) on massive datasets is a surprisingly stubborn problem.  This article explains the two common methods for adding filters to vector search, and their serious limitations. Then we will explore [Pinecone’s solution](/) to filtering in vector search.  <p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/H_kJDHvu-v8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>  ---  ## The Filter Problem  In [vector similarity search](https://www.pinecone.io/learn/what-is-similarity-search/) we build vector representations of some data (images, text, cooking recipes, etc), storing it in an *index* (a database for vectors), and then searching through that index with another *query vector*.  If you found this article through Google, what brought you here was a semantic search identifying that the [vector representation](https://www.pinecone.io/learn/vector-embeddings/) of your search query aligned with Google’s vector representation of this page.  Netflix, Amazon, Spotify, and many others use vector similarity search to power their recommendation systems, using it to identify similar users and base their new show, product, or music recommendations on what these similar groups of people seem to like.  In search and recommender systems there is almost always a need to apply filters: Google can filter searches by category (like news or shopping) , date, or even language and region; Netflix, Amazon, and Spotify may want to focus on comparing users in similar geographies. Restricting the search scope to relevant vectors is — for many use-cases — an absolute *necessity* for providing a better customer experience.  Despite the clear need, there has been no good approach for metadata filtering — restricting the scope of a vector search based on a set of metadata conditions — that’s both accurate *and* fast.  We may want to restrict our search to a category of records or a specific date range.  For example, say we want to build a semantic search application for a large corporation to search through internal documents. Users will often want to filter their search to a specific department. It *should* be as simple as writing this pseudo-code:  ```python top-k where department == 'engineering' OR top-k where department != 'hr'  top-k where {“department”: {\"$eq\": \"engineering\"}} OR top-k where {“department”: {\"$ne\": \"hr\"}} ```  And what if we only want the last few weeks of data? Or need to see how the documents have changed over time — we would want to restrict the search to old records only!  ```python top-k where date >= 14 days ago OR top-k where date < 3 years ago  top-k where {“date”: {\"$gte\": 14}} OR top-k where {“date”: {\"$lt\": 1095}} ```  There are countless variations and combinations of queries just like these.  ```python top-k where date >= 14 days ago AND department == 'finance'  top-k where {\"date\": {\"$gte\": 14}, “department”: {\"$eq\": \"finance\"}} ```  Without filters, we restrict our ability to build powerful tools for search. Imagine Excel without data filters, SQL without a `WHERE` clause, or Python without `if...else` statements. The capabilities of these tools become massively diminished, and the same applies to vector search without filters.  But implementing such filters into a semantic/vector search application isn’t *easy*. Let’s first look at the two most common approaches — along with their advantages and disadvantages. They are **pre**-filtering and **post**-filtering.  ![Post and pre-filtering — note that for post-filtering, we search then apply the metadata filter. For pre-filtering, we apply the metadata filter then search.](/images/vector-search-filtering-10.jpg)  These approaches require *two* indexes: The *[vector index](https://www.pinecone.io/learn/vector-indexes/)* as per usual, and a *metadata index*. It is through this metadata index that we identify those vectors which satisfy our filter conditions.  ### Pre-Filtering  The first approach we could take is to pre-filter our vectors. This consists of taking our *metadata index* and applying our set of conditions. From here, we return a list of all records in our *vector index* that satisfies our filter conditions.  Now when we search, we've restricted our scope — so our *vector similarity search* will be faster, and we'll only return relevant results!  The problem is that our pre-filter disrupts how ANN engines work (ANN requires the full index), leaving us with two options:  Build an ANN index for every possible filter. Perform a brute-force kNN search across remaining vectors.  Option (1) is simply not practical, there are far too many possible filter outputs in a typical index.  That leaves us with option (2), creating the additional overhead of brute-force checking *every single* vector embedding remaining after the metadata filter.   <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/vector-search-filtering-4.mp4\" type=\"video/mp4\"> </video>  Pre-filtering is excellent for returning relevant results, but significantly slows-down our search due to the exhaustive, brute-force search that’s required.  While slow for smaller datasets — the problem is exacerbated for larger datasets. This brute-force search is simply *not manageable* for datasets in the millions or billions.  Maybe **post**-filtering will work better?  ### Post-Filtering  We can't rely on a brute-force search every time we apply a filter, so what if we applied the filter *after* our vector search?  Working through it, we have our vector index and a query — we perform a similarity search as usual. We return `k` of the top nearest matches. We'll set `k == 10`.  We've now got our top `10` best matches, but there's plenty of vectors in here that would *not* satisfy our filter conditions.  We go ahead and apply our post-search filter to remove those irrelevant results. That filter has removed *six* of our results, leaving us with just *four* results. We wanted ten results, and we’ve returned four. That's not so great.  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/vector-search-filtering-6.mp4\" type=\"video/mp4\"> </video>  Unfortunately, it gets worse... What if we didn't return *any results* in the top `k` that satisfy our filter conditions? That leaves us with no results *at all*.  By applying our metadata filter post-search, we avoid the speed issues of an initial brute-force search. Still, we risk returning very few or even *zero* results, even though there may be relevant records in the dataset. So much for improving customer experience...  We can eliminate the risk of returning too few (or zero) results by increasing `k` to a high number. But now we have slower search times thanks to our excessively high `k` searches, and so we’re back to the slow search times of *pre*-filtering.  Slow search with pre-filtering, or unreliable results with post-filtering. Neither of these approaches sounds particularly attractive, so what can we do?  ## Single-Stage Filtering  Pinecone's single-stage filtering produces the accurate results of pre-filtering at *even faster* speeds than post-filtering.  It works by *merging the vector and metadata indexes into a single index* — resulting in a single-stage filter as opposed to the two-stage filter-and-search method of pre- and post-filtering.  This gives us pre-filtering accuracy benefits *without being restricted* to small datasets. It can even increase search speeds whether you have a dataset of 1 million or 100 *billion*.  Exactly how this works will be covered in a later article. For now, let's explore some of the new single-stage filtering features and test its performance.  ### Testing the New Filtering  Filtering in Pinecone is effortless to use. All we need are some vectors, metadata, and an [API key](https://www.pinecone.io/start/).  In this example we use the [Stanford Question and Answering Dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer/) for both English and Italian languages.  The SQuAD datasets are a set of paragraphs, questions, and answers. Each of those paragraphs is sourced from different Wikipedia pages, and the SQuAD data includes the topic title:  ```python {  'id': '573387acd058e614000b5cb5',  'title': 'University_of_Notre_Dame',  'context': 'One of the main driving forces in the growth of the University was its football team, the ... against the New York Giants in New York City.',  'question': 'In what year did the team lead by Knute Rockne win the Rose Bowl?',  'answers': {  'text': '1925',  'answer_start': 354  } } ```  From this, we have *two* metadata tags:  * `lang` — the source text's language (either `en` or `it`). * `title` — the topic title of the source text.  We pre-process the datasets to create a new format that looks like:  ```python {  'id': '573387acd058e614000b5cb5en',  'context': 'One of the main driving forces in the growth of the University was its football team, the ... against the New York Giants in New York City.',  'metadata': {  'lang': 'en',  'title': 'University_of_Notre_Dame'  } } ```  For Pinecone, we need three items: `id`, `vector`, and `metadata`. We have two of those, but we're missing our `vector` field.  The `vector` field will contain the dense vector embedding of the `context`. We can build these embeddings using models from `sentence-transformers`.   We will search in either English or Italian in our use case and return the most similar English/Italian paragraphs. For this, we will need to use a [multi-lingual vector embedding model](/learn/multilingual-transformers/). We can use the `stsb-xlm-r-multilingual` transformer model from `sentence-transformers` to do this.  Let's go ahead and encode our `context` values:   {{< notebook file=\"encode\" height=\"full\" >}}  The new format we produce includes our sample `id`, `text`, `vector`, and `metadata`, which we can go ahead and `upsert` to Pinecone:  ```python {  'id': '573387acd058e614000b5cb5en',  'vector': [0.1, 0.8, 0.2, ..., 0.7],  'metadata': {  'lang': 'en',  'title': 'University_of_Notre_Dame'  } } ```  We can add as many `metadata` tags as we'd like. They can even contain numerical values.  #### Creating the Index  Now, all we need to do is build our Pinecone index. We can use the Python client — which we install using `pip install pinecone`.  Using an [API key](https://www.pinecone.io/start/) we can initialize a new index for our SQuAD data with:   {{< notebook file=\"create-index-1\" height=\"full\" >}}  And we're ready to `upsert` our data! We'll perform the upload in batches of `100`:   {{< notebook file=\"upsert-batch\" height=\"full\" >}}  That's it — our vectors and metadata are ready to go!  #### Searching  Let's start with an unfiltered search. We'll search for paragraphs that are similar to `\"Early engineering courses provided by American Universities in the 1870s\"`. We encode this using the `sentence-transformer` model, then query our index with `index.query`.  {{< notebook file=\"encode-and-query\" height=\"full\" >}}  We're returning the three `top_k` (most similar) results — Pinecone returns our IDs and their respective similarity scores. As we have our contexts stored locally, we can map these IDs back to our contexts using a dictionary (`get_sample`) like so:   {{< notebook file=\"get-sample\" height=\"full\" >}}  The first Italian paragraph reads:  ``` The College of Engineering was established in the 1920s, however, the first courses in civil and mechanical engineering have been a part of the College of Sciences since the 1870s. Today the college, housed in the Fitzpatrick, Cushing and Stinson-Remick Halls of Engineering, comprises five study departments - aerospace and mechanical engineering, chemical and biomolecular engineering, civil engineering and geological sciences, computer science and engineering, electronic engineering - with eight B. S bachelor's offers. In addition, the college offers five years of dual degree programs with the College of Arts and Letters and an additional B. ```  So we're getting great results for both English *and* Italian paragraphs. However, what if we'd prefer to return English results *only*?  We can do that using Pinecone's single-stage filtering. Filters can be built using a set of operators that can be applied to *strings* and/or *numbers*:  | Operator | Function | String | Number | | -------- | ------------------------ | ------ | ------ | | `$eq` | Equal to | ✅ | ✅ | | `$ne` | Not equal to | ✅ | ✅ | | `$gt` | Greater than | | ✅ | | `$gte` | Greater than or equal to | | ✅ | | `$lt` | Less than | | ✅ | | `$lte` | Less than or equal to | | ✅ | | `$in` | In array | ✅ | | | `$nin` | Not in array | ✅ | |  We're looking to filter based on if `lang` is *equal to* (`$eq`) `en`. All we do is add this filter condition to our `query`:  {{< notebook file=\"lang-filter\" height=\"full\" >}}  And we're now returning English-only paragraphs, with a search-time equal to (or even *faster*) than our unfiltered query.  Another cool filter feature is the ability to add multiple conditions. We could exclude the titles *'University_of_Kansas'* and *'University_of_Notre_Dame'* while retaining the `lang` filter:  {{< notebook file=\"nin-filter\" height=\"full\" >}}  Now we're returning English-only results that are *not* from the Universities of Kansas or Notre Dame topics.  There are also numeric filters. The SQuAD data doesn’t contain any numerical metadata — so we'll generate a set of random date values and `upsert` those to our existing vectors.  First, we use a `date_maker` function to randomly generate our dates (we could use POSIX format too!), then we'll use the `Dataset.map` method to add `date` to our metadata. After this, we `upsert` as we did before.  {{< notebook file=\"dates-metadata\" height=\"full\" >}}  Before we filter by date, let's see the dates we generated for our previous set of results:  {{< notebook file=\"view-dates\" height=\"full\" >}}  We have dates from *2006* and *2016*. Let's swap this out for a specific date range of *2020* only.  {{< notebook file=\"multi-filter\" height=\"full\" >}}  We've built some detailed metadata filters with multiple conditions and even numeric ranges. All with relentlessly fast search speeds. But we’re using a small dataset of 40K records, which means we’re missing out on one of the best features of single-stage filtering — let’s experiment with something bigger.  #### Filtering and Search Speeds  Earlier, we mentioned that applying the single-stage filter can speed up our search while still maintaining the accuracy of a pre-filter. This speedup exists with a small 40K dataset, but it's masked by the network latency.  So we're going to use a new dataset. The vectors have been randomly generated using `np.random.rand`. We're still using a vector size of `768`, but our index contains 1.2M vectors this time.  We will test the metadata filtering through a single tag, `tag1`, consisting of an integer value between `0` and `100`.  Without any filter, we start with a search time of 79.2ms:  {{< notebook file=\"speedtest-no-filter\" height=\"full\" >}}  Using a greater than `$gt` condition and steadily increasing the filter scope — as expected, we return faster search times the more we restrict our search scope.  {{< notebook file=\"speedtests\" height=\"full\" >}}  ![Filter speed test](/images/vector-search-filtering-9.jpg) *<small>As we increase our `tag1` `$gt` value, the search scope is reduced, making our search with the single-stage filter *even faster*.</small>*  And that's it! We explored the flexible filter queries and tested the impressive search speeds of the single-stage filter!  ---  Thanks for following with us through the three approaches to metadata filtering in vector similarity search; pre-filtering, post-filtering, and Pinecone’s single-stage filtering.  We hope the article has been helpful! Now you can [try our single-stage filtering](https://www.pinecone.io/start/) for your own vector search application. ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e364"
  },
  "content": "categories:   - Algorithms & Libraries toc: >- weight: 1 author:   name: Bala Priya C   position: Technical Writer   src: /images/bala-priya.jpg   href: https://www.linkedin.com/in/bala-priya/ description: \"Cross-Entropy Loss Function: Everything You Need to Know\" images: ['/images/cross-entropy-loss.png'] ---  ![Cross-Entropy Loss](/images/cross-entropy-loss.png)  Have you ever wondered what happens under the hood when you train a neural network? You’ll run the [gradient descent optimization](https://cs231n.github.io/optimization-1/#opt3) algorithm to find the optimal parameters (weights and biases) of the network. In this process, there’s a *loss* function that tells the network how good or bad its current prediction is. The goal of optimization is to find those parameters that *minimize* the loss function: the lower the loss, the better the model.   In classification problems, the model predicts the class label of an input. In such problems, you need metrics beyond accuracy. While accuracy tells the model whether or not a particular prediction is correct, **cross-entropy loss** gives information on *how* correct a particular prediction is. When training a classifier neural network, minimizing the cross-entropy loss during training is equivalent to helping the model learn to predict the correct labels with higher confidence.  In this tutorial, we'll go over binary and categorical cross-entropy losses, used for binary and multiclass classification, respectively. We’ll learn how to interpret cross-entropy loss and implement it in Python. As the loss function’s derivative drives the gradient descent algorithm, we’ll learn to compute the derivative of the cross-entropy loss function.  Let’s begin!  ## What is Cross Entropy?  Before we proceed to learn about cross-entropy loss, it'd be helpful to review the definition of cross entropy. In the context of [information theory](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html), the cross entropy between two discrete probability distributions is related to [KL divergence](http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf), a metric that captures how close the two distributions are.  Given a true distribution **t** and a predicted distribution **p**, the cross entropy between them is given by the following equation.  $$H(\\textbf{t},\\textbf{p}) = - \\sum_{s \\in S} \\textbf{t}(s).log(\\textbf{p}(s))$$   Here, both **t** and **p** are distributed on the same support S, but could take potentially different values. For a three-element support S, if **t** = [t1, t2, t3] and **p** = [p1, p2, p3], it’s not necessary that `t_i = p_i` for i in {1,2,3}.  **Note:** `log(x)` refers to log to the base `e` (or natural logarithm), also written as `ln(x)`.  *So how is cross entropy relevant in neural networks?*  Recall that in binary classification, the [sigmoid activation](https://www.pinecone.io/learn/softmax-activation/#can-you-use-sigmoid-or-argmax-activations-instead) is used in the output layer, and the neural network outputs a probability score (p) between 0 and 1; the true label (t) being one of {0, 1}.  In case of multiclass classification, we use the [softmax activation](https://www.pinecone.io/learn/softmax-activation/#the-softmax-activation-function-explained) at the output layer to get a vector of predicted probabilities **p**. The true distribution **t** contains all of the probability mass (1)  at the index of the correct class, and 0 everywhere else. For example, in a classification problem with N classes, the true distribution corresponding to class `i` is a vector that's `N` classes long, with 1 at the index of the class label `i` and 0 at all other indices.  We’ll discuss this in greater detail in the coming sections.  ## Cross-Entropy Loss for Binary Classification  Let's start this section by reviewing the `log` function in the interval (0,1].  ▶️ Run the following code snippet to plot the values of `log(x)` and `-log(x)` in the range 0 to 1. As `log(0)` is -∞, we add a small offset, and start with 0.001 as the smallest value in the interval.  ```python import numpy as np import seaborn as sns from matplotlib import pyplot as plt sns.set()  x_arr = np.linspace(0.001,1) log_x = np.log(x_arr) fig, axes = plt.subplots(1, 2,figsize=(8,4)) sns.lineplot(ax=axes[0],x=x_arr,y=log_x) axes[0].set_title('Plot of log(x) in the interval (0,1]') axes[0].set(xlabel='x', ylabel='log(x)') sns.lineplot(ax=axes[1],x=x_arr,y=-log_x) axes[1].set_title('Plot of -log(x) in the interval (0,1]') axes[1].set(xlabel='x', ylabel='-log(x)') ```  ![Sigmoid Function Equation](/images/log-x-plot.png) <small>Plot of log x and -log x in the interval (0,1]</small>  As seen in the plots above, in the interval (0,1], `log(x)` and `-log(x)` are negative and positive, respectively. Observe how `-log(x)` approaches 0 as `x` approaches 1. This observation will be helpful when we parse the expression for cross-entropy loss.  In binary classification, the raw output of the neural network is passed through the sigmoid function, which outputs a probability score `p = σ(z)`, as shown below.  ![Binary Classification](/images/binary-classification.png) <small>A Simple Binary Classification Model (Image by the author)</small>  The true value, or the true label, is one of {0, 1} and we’ll call it `t`. The binary cross-entropy loss, also called the log loss, is given by:  $$\\mathcal{L}(t,p) = -(t.log(p) + (1-t).log(1-p))$$  As the true label is either 0 or 1, we can rewrite the above equation as two separate equations.  When `t = 1`, the second term in the above equation goes to zero, and the equation reduces to the following:  $$When \\text{ } t=1,\\mathcal{L}(t,p) = -log(p)$$  Therefore, when `t =1`, the binary cross-entropy loss is equal to the negative logarithm of the predicted probability `p`.  Similarly, when the true label `t=0`, the term `t.log(p)` vanishes, and the expression for binary cross-entropy loss reduces to:  $$When \\text{ } t=0,\\mathcal{L}(t,p) = -log(1-p)$$  Now, let's plot the binary cross-entropy loss for different values of the predicted probability `p`.  ```python bce_1 = -np.log(p) bce_0 = -np.log(1-p) plot1 = sns.lineplot(x=p,y=bce_1,label='True value:1').set(ylim=(0,4)) plot2 = sns.lineplot(x=p,y=bce_0,label='True value:0').set(ylim=(0,4)) plt.xlabel('p') plt.ylabel('Binary Cross-Entropy Loss') ```  ![Binary Cross-Entropy Loss](/images/binary-cross-entropy-loss.png)  From the plots above, we can make the following observations:  - When the true label `t` is 1, the cross-entropy loss approaches 0 as the predicted probability `p` approaches 1 and - When the true label `t` is 0, the cross-entropy loss approaches 0 as the predicted probability `p` approaches 0.  In essence, the cross-entropy loss attains its *minimum* when the predicted probability `p` is close to the true value and is substantially *higher* when the predicted probability is far away from the true label.  *But which predictions does cross-entropy loss penalize the most?*  Recall that `log(0)` → -∞; so `-log(0)` →  ∞. As seen from the plots of the binary cross-entropy loss, this happens when the network outputs `p=1` or a value close to 1 when the true class label is 0, and outputs `p=0` or a value close to 0 when the true label is 1.  Putting it all together, cross-entropy loss increases drastically when the network makes *incorrect* predictions with *high* confidence.  If there are `S` samples in the dataset, then the total cross-entropy loss is the sum of the loss values over all the samples in the dataset.  $$\\mathcal{L}(t,p) = -\\sum_{i=1}^{S}(t_i.log(p_i) + (1-t_i).log(1-p_i))$$  ### Binary Cross-Entropy Loss in Python  Let's define a Python function to compute the binary cross-entropy loss.  ```python def binary_cross_entropy(t,p):     t = np.float_(t)     p = np.float_(p)     # binary cross-entropy loss     return -np.sum(t * np.log(p) + (1 - t) * np.log(1 - p)) ```  Next, let’s call the function `binary_cross_entropy`with arrays of true and predicted values as the arguments.  ```python     t = [0,1,1,0,0,1,1] p = [0.07,0.91,0.74,0.23,0.85,0.17,0.94]  binary_cross_entropy(t,p) 4.460303459760249 ```  To get a better idea of how the loss varies with `p`, let's modify the function definition to print out the values of the loss for each of the samples, as shown below.  ```python def binary_cross_entropy(t,p):     t = np.float_(t)     p = np.float_(p)     for tt, pp in zip(t,p):       print(f'true_val = {tt}, predicted_val = {pp}, loss = {-(tt * np.log(pp) + (1 - tt) * np.log(1 - pp))}')     return -np.sum(t * np.log(p) + (1 - t) * np.log(1 - p)) ```  Now that we’ve modified the function, let’s call the function yet again to check the outputs.  ```python binary_cross_entropy(t,p)  # Output true_val = 0.0, predicted_val = 0.07, loss = 0.0725706928348355 true_val = 1.0, predicted_val = 0.91, loss = 0.09431067947124129 true_val = 1.0, predicted_val = 0.74, loss = 0.3011050927839216 true_val = 0.0, predicted_val = 0.23, loss = 0.2613647641344075 true_val = 0.0, predicted_val = 0.85, loss = 1.897119984885881 true_val = 1.0, predicted_val = 0.17, loss = 1.7719568419318752 true_val = 1.0, predicted_val = 0.94, loss = 0.06187540371808753 4.460303459760249 ```  In the above output, when the true and predicted values are closer, the cross-entropy loss is lower; the loss increases when the true and predicted values are different.   The highest value of the loss, 1.897, occurs when the network predicts a probability score of 0.85 corresponding to a true value of 0. Suppose the problem is to classify whether the given image is that of a seal or not. The model, in this case, is 85% confident that the image is a seal when it actually isn't.  Similarly, the second highest value of the binary cross-entropy loss, 1.771 occurs when the network predicts a score of 0.17, significantly lower than the true value of 1. In the image classification example, this means that the model is only about 17% confident of the input image being a seal, when it actually is a seal.  This validates our earlier observation that the loss is higher when the predictions are away from the true values.  In the next section, let's explore an extension of cross-entropy loss to the multiclass classification case.  ## Categorical Cross-Entropy Loss for Multiclass Classification  Let's formalize the setting we'll consider. In a multiclass classification problem over `N` classes, the class labels are 0, 1, 2 through N - 1. The labels are one-hot encoded with 1 at the index of the correct label, and 0 everywhere else.  For example, in an image classification problem where the input image is one of {panda, seal, duck}, the class labels and the corresponding one-hot vectors are shown below.  ![Multiclass Classification Encoding](/images/multiclass-classification-encoding.png) <small>One-hot encoding of class labels in multiclass classification (Image by the author)</small>  In multiclass classification, the raw outputs of the neural network are passed through the [softmax activation](https://www.pinecone.io/learn/softmax-activation/), which then outputs a vector of predicted probabilities over the input classes.  ![Multiclass Classification Model](/images/multiclass-classification-model.png) <small>A Simple Multiclass Classification Model (Image by the author)</small>  The categorical cross-entropy loss between the true distribution **t** and the predicted distribution **p** in a multiclass classification problem with N classes is given by:  $$\\mathcal{L}(\\textbf{t},\\textbf{p}) = - \\sum_{j=1}^{N}t_j log(p_j)$$  This expression may seem daunting, but we’ll parse this and arrive at a much simpler expression.  Recall that the true distribution **t** is a one-hot vector that has 1 at one of the indices and zero everywhere else. If a given image belongs to the class `k`, in the true distribution vector, `t_k = 1`, and all other indices are zero.  Substituting as follows, $$t_k=1 \\text{ } and \\text{ } t_j = 0 \\text{ } for \\text{ } j \\neq k$$ We see that `N - 1` terms in the summation go to zero, and you’ll have the following simplified expression:  $$\\mathcal{L}(\\textbf{t},\\textbf{p}) = - t_k log(p_k) = -log(p_k)\\\\\\\\ for \\text{ } an\\text{ } input \\text{ } \\in \\text{ } Class \\text{ }k$$  The loss, therefore, reduces to the negative logarithm of the predicted probability for the correct class. The loss approaches zero, as `p_k` → 1.  In the figure below, we present some examples of true and predicted distributions. In our image classification example, if the target class is *seal*, the categorical cross-entropy loss is minimized when the network predicts a probability score close to 1 for the correct class (*seal*). This works similarly for the other target classes, *panda* and *duck*.  ![Predicted Loss Probability](/images/predicted-loss-probability.png) <small>Cross-entropy loss decreases as the predicted probability for the target class approaches 1 (Image by the author)</small>  For a dataset with `S` samples in all, the categorical cross-entropy loss is given by:  $$\\mathcal{L}(\\textbf{t},\\textbf{p}) = - \\sum_{i=1}^{S}\\sum_{j=1}^{N}t_{ij} log(p_{ij})$$  In practice, you could also use the average cross-entropy loss across all samples in the dataset. Next, let's code the categorical cross-entropy loss in Python.  ### Categorical Cross-Entropy Loss in Python  The code snippet below contains the definition of the function `categorical_cross_entropy`.The function accepts two lists as arguments: `t_list` and `p_list` containing lists of true and predicted distributions, respectively. It then computes the cross-entropy loss over each set of predicted and true values.  ```python def categorical_cross_entropy(t_list,p_list):     t_list = np.float_(t_list)     p_list = np.float_(p_list)     losses = []     for t,p in zip(t_list,p_list):       loss = -np.sum(t * np.log(p))       losses.append(loss)       print(f't:{t}, p:{p},loss:{loss}\\n')     return np.sum(losses) ```  Now, let’s make a call to the function with the lists of true and predicted distribution vectors as the arguments and check the outputs.  ```python    t_list = [[1,0,0],[0,1,0],[0,0,1],[1,0,0]] p_list = [[0.91,0.04,0.05],[0.11,0.8,0.09],[0.3,0.1,0.6],[0.25,0.4,0.35]] categorical_cross_entropy(t_list,p_list)  t:[1. 0. 0.], p:[0.91 0.04 0.05],loss:0.09431067947124129  t:[0. 1. 0.], p:[0.11 0.8  0.09],loss:0.2231435513142097  t:[0. 0. 1.], p:[0.3 0.1 0.6],loss:0.5108256237659907  t:[1. 0. 0.], p:[0.25 0.4  0.35],loss:1.3862943611198906  2.214574215671332 ```  From the output above, we see that the loss is lower when the model predicts a higher probability corresponding to the correct class label.  ## Derivative of the Softmax Cross-Entropy Loss Function   One of the [limitations of the argmax function](https://www.pinecone.io/learn/softmax-activation/#limitations-of-the-argmax-function) as the output layer activation is that it doesn’t support the backpropagation of gradients through the layers of the neural network. However, when using the softmax function as the output layer activation, along with cross-entropy loss, you can compute gradients that facilitate backpropagation. The gradient evaluates to a simple expression, easy to interpret and intuitive.  If you’d like to know how softmax activation and cross-entropy loss yield a gradient that can be used in backpropagation, please read ahead. The preceding sections do not necessitate the use of the following section but this will provide interesting and potentially helpful insight.   The following subsections assume you have some familiarity with [differential calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr). To follow along, you should be able to apply the chain rule for differentiation and compute partial derivatives.   ### Derivative of the Softmax Function  Recall that if **z** is the output of the neural network, then `softmax(z)` outputs a vector **p** of probabilities. Let's start with the expression for softmax activation.   <div> $$softmax(\\textbf{z})_i = p_i = \\frac{e^{z_i}}{\\sum_{j = 1}^{N} e^{z_j}}\\\\\\\\ Let \\text{ } \\sum_{j = 1}^{N} e^{z_j} = \\Sigma N \\\\\\\\ softmax(\\textbf{z})_i = \\frac{e^{z_i}}{ \\Sigma N}$$ </div>  Now, let’s compute the derivative of the softmax output `p_i` with respect to the raw output `z_i` of the neural network.  Here, the computation of derivatives can be handled under two different cases, as shown below.  \\begin{align} Case\\text{ }1: \\text{ }When\\text{ } i = j: \\\\\\\\ \\frac{\\partial p_i}{\\partial z_i} = \\frac{\\partial \\frac{e^{z_i}}{\\Sigma N}}{\\partial z_i}\\\\\\\\ = \\frac{\\Sigma N.\\frac{\\partial e^{z_i}}{\\partial z_i} - e^{z_i}.\\frac{\\partial \\Sigma N}{\\partial z_i}}{(\\Sigma N)^{2}}\\\\\\\\ \\frac{\\partial \\Sigma N}{\\partial z_i} = \\frac{\\partial \\sum_{j \\neq i} e^{z_j}}{\\partial z_i} + \\frac{\\partial e^{z_i}}{\\partial z_i} = e^{z_i}\\\\\\\\ \\frac{\\partial e^{z_i}}{\\partial z_i} = e^{z_i}\\\\\\\\ \\frac{\\partial p_i}{\\partial z_i} = \\frac{\\Sigma N.e^{z_i} - e^{z_i}.e^{z_i}}{(\\Sigma N)^{2}} = \\frac{e^{z_i}}{\\Sigma N}\\left[1- \\frac{e^{z_i}}{\\Sigma N}\\right] = p_i(1-p_i) \\end{align}  \\begin{align} Case\\text{ }2: \\text{ }When\\text{ } i \\neq j:\\\\\\\\ \\frac{\\partial p_i}{\\partial z_j} = \\frac{\\partial \\frac{e^{z_i}}{\\Sigma N}}{\\partial z_j}\\\\\\\\ = \\frac{\\Sigma N.\\frac{\\partial e^{z_i}}{\\partial z_j} - e^{z_i}.\\frac{\\partial \\Sigma N}{\\partial z_j}}{(\\Sigma N)^{2}}\\\\\\\\ = 0 -  \\frac{e^{z_i}.e^{z_j}}{(\\Sigma N)^{2}}\\\\\\\\ = 0 -  \\frac{e^{z_i}}{\\Sigma N}.\\frac{e^{z_j}}{\\Sigma N}\\\\\\\\ \\implies \\frac{\\partial p_i}{\\partial z_j} = -p_ip_j \\end{align}  ### Derivative of the Cross-Entropy Loss Function  Next, let's compute the derivative of the cross-entropy loss function with respect to the output of the neural network. We’ll apply the [chain rule](https://cs231n.github.io/optimization-2/#backprop) and substitute the derivatives of the softmax activation function.   \\begin{align}\\frac{\\partial{L}}{\\partial{z_i}} = - \\sum_{j = 1}^{N} \\frac{\\partial{(t_j log p_j)}}{\\partial{z_i}}\\\\\\\\ = - \\sum_{j = 1}^{N} t_j\\frac{\\partial{(log p_j)}}{\\partial{z_i}}\\\\\\\\ = - \\sum_{j = 1}^{N} t_j\\frac{1}{p_j}\\frac{\\partial{p_j}}{\\partial{z_i}}\\\\\\\\ = - \\frac{t_i}{p_i}\\frac{\\partial{p_i}}{\\partial{z_i}} - \\sum_{j \\neq i} \\frac{t_j}{p_j} \\frac{\\partial{p_j}}{\\partial{z_i}}\\\\\\\\ = - \\frac{t_i}{p_i}p_i(1-p_i) - \\sum_{j \\neq i} \\frac{t_j}{p_j} (-p_jp_i)\\\\\\\\ = -t_j + t_ip_i + \\sum_{j \\neq i} t_jp_i \\\\\\\\ = -t_i + \\sum_{j = 1}^{N} t_jp_i\\\\\\\\ = -t_i + p_i\\sum_{j = 1}^{N} t_j\\\\\\\\ \\implies \\frac{\\partial{L}}{\\partial{z_i}} = p_i - t_i\\end{align}   As seen above, the gradient works out to the difference between the predicted and true probability values.  ## Wrapping Up  In this tutorial, you’ve learned how binary and categorical cross-entropy losses work. They impose a penalty on *predictions* that are significantly different from the *true* value. You’ve learned to implement both the binary and categorical cross-entropy losses from scratch in Python. In addition, we covered how using the cross-entropy loss, in conjunction with the softmax activation, yields a simple gradient expression in backpropagation.   As a next step, you may try spinning up a simple [image classification model](https://keras.io/examples/vision/mnist_convnet/) using softmax activation and cross-entropy loss function. Until the next tutorial!  {{< newsletter text=\"Subscribe for more ML tutorials!\" inputText=\"Email address...\" buttonText=\"Submit\">}}  ## 📚 Resources  [1] [Softmax Activation Function](https://www.pinecone.io/learn/softmax-activation/), pinecone.io  [2] [Essence of Calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr), YouTube playlist by Grant Sanderson of 3Blue1Brown  [3] [Gradient Descent Optimization](https://cs231n.github.io/optimization-1/), CS231n  [4] [Backpropagation](https://cs231n.github.io/optimization-2/), CS231n  [5] [Simple MNIST Convnet](https://keras.io/examples/vision/mnist_convnet/), keras.io  [6] [Information Theory](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html), Dive into Deep Learning ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e366"
  },
  "title": "\"Introducing the Enterprise Plan\"",
  "headline": "\"Introducing the Enterprise Plan for Mission-Critical Applications\"",
  "name": "Greg Kogan",
  "position": "VP Marketing",
  "src": "/images/company-greg.png",
  "href": "https://www.linkedin.com/in/gkogan/",
  "date": "\"2022-04-06\"",
  "# Date": "April 6, 2022",
  "description": "Introducing the Enterprise plan for mission-critical applications.",
  "images": "[\"/images/enterprise-plan.png\"]",
  "thumbnail": "\"/images/enterprise-thumbnail.png\"",
  "content": "The most enjoyable thing about building a product in the cutting-edge [vector search](/learn/vector-search-basics/) space is seeing what users do with it. The second most enjoyable thing is using that information to make improvements that help them do it easier, better, and faster.  One thing we’re seeing more often is customers using Pinecone in mission-critical applications without needing it to run in a dedicated, single-tenant cloud environment. Those customers want the enterprise-grade support, availability, and observability that comes with our single-tenant Dedicated plan. Yet they also want the fast deployment, lower overhead, and lower cost of our self-serve, multi-tenant Standard plan.  Today, we’re introducing a new plan that offers the best of both: Enterprise-grade features and support, with self-serve convenience.  ![Introducing the new Enterprise plan](/images/enterprise-plan.png)  With this update we also:  - Made our support commitments more clear, so you can rest easy knowing we have your back. - Renamed the “Free” plan to “Starter” (it’s still free to use), and the “Dedicated” plan to “Enterprise Dedicated.” - Made the usage estimator more accurate by letting you specify the size of your metadata.  Continue reading or jump to the [pricing page](/pricing/) for full details.  ## The New Enterprise plan  The new Enterprise plan gives you the features and support you need for mission-critical applications, in our fully managed cloud with self-service and usage-based billing. Previously, the maximum level of features and support was only available on the Dedicated plan, which required more time to deploy and cost more to run.  If you have a mission-critical application that does not _need_ to be in a private environment, you can now get the availability, reliability, observability, and support you need without having to wait or pay for a dedicated deployment. It’s now easy to start and scale with Pinecone whether you’re just experimenting or launching a large enterprise application into production.  The Enterprise plan has all the features of the Standard plan, plus:  - Multiple availability zones for greater availability: When you [add replicas](/docs/api/operation/scale_index/#tag/Index-Operations/operation/scale_index) to your index, they are automatically distributed across availability zones within the same cloud region. - Promethesus metrics for greater observability: [Monitor](/docs/monitoring/) your Pinecone indexes by ingesting performance metrics into your own Prometheus instances, or into Prometheus- and OpenMetrics-compatible monitoring tools. - _And more coming soon to give you operational visibility and controls._  It also comes with Premium-level support for peace of mind — with availability SLAs, response-time SLAs, and 24/7 access to support by email or Slack.  The new plan is available today. Learn more and see complete details on our [pricing page](/pricing/), or [contact us](/contact/) with questions. To upgrade to the new plan, [go to your account](https://app.pinecone.io) and then to Billing.  ![Manage plans and billing in the Pinecone console](/images/enterprise-plan-screenshot.png) ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e368"
  },
  "title": "\"Zero Shot Object Detection with OpenAI's CLIP\"",
  "headline": "\"Zero Shot Object Detection with OpenAI's CLIP\"",
  "weight": "11",
  "- name": "Laura Carnevali",
  "position": "Developer",
  "src": "/images/laura-carnevali.jpeg",
  "href": "\"https://www.linkedin.com/in/laura-carnevali-071a01b7/\"",
  "description": "Zero shot object detection and localization using OpenAI's CLIP",
  "images": "['https://www.pinecone.io/images/zero-shot-object-detection-clip-0.png']",
  "content": "The **I**magenet **L**arge **S**cale **V**isual **R**ecognition **C**hallenge (ILSVRC)<sup>[1]</sup> was a world-changing competition hosted annually from 2010 until 2017. During this time, the competition acted as the catalyst for the explosion of deep learning<sup>[2]</sup> and was the place to find state-of-the-art image classification, object localization, and object detection.  Researchers fine-tuned better-performance computer vision (CV) models to achieve ever more impressive results year-after-year. But there was an unquestioned assumption causing problems.  We assumed that every new task required model fine-tuning, this required *a lot* of data, and this needed both time and capital.  It wasn't until very recently that this assumption was questioned and proven wrong.  The astonishing rise of multi-modal models has made the impossible possible across various domains and tasks. One of those is zero-shot object detection and localization.  \"Zero-shot\" means applying a model without the need for fine-tuning. Meaning we take a multi-modal model and use it to detect images in one domain, then switch to another entirely different domain *without* the model seeing a single training example from the new domain.   Not needing a single training example means we completely skip the hard part of data annotation and model training. We can focus solely on application of our models.  In this chapter, we will explore how to apply OpenAI's CLIP to this task—using CLIP for localization and detection across domains with *zero* fine-tuning.  <div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/i3OYlaoj-BM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> </div>  ---  ## Classification, Localization, and Detection  [**Image classification**](https://www.pinecone.io/learn/zero-shot-image-classification-clip/) is one of the most straightforward tasks in visual recognition and the first step on the way to object detection. It consists of assigning a categorical label to an image.  <img src=\"./images/zero-shot-object-detection-clip-1.png\" alt=\"image classification of dog image\" style=\"width:100%;\" center /> <small>Image classified as (1) \"dog\".</small>  We could have an image classification model that identifies animals and could classify images of dogs, cats, mice, etc. If we pass the above image into this model, we'd expect it to return the class *\"dog\"*.  **Object localization** takes this one step further by *\"localizing\"* the identified object.  <img src=\"./images/zero-shot-object-detection-clip-2.png\" alt=\"Drawing\" style=\"width:100%;\"/> <small>(1) Image classified as \"dog\" and (2) object localized.</small>  When we *localize* the object, we identify the object's coordinates on the image. That typically includes a set of patches where the object is located or a bounding box defined by $(x, y)$ coordinates, box width, and box height.  **Object detection** can be thought of as the next step. With detection, we are *localizing* multiple object instances within the same image.  <img src=\"./images/zero-shot-object-detection-clip-3.png\" alt=\"dog image passed through object localization model/algorithm\" style=\"width:100%;\"/> <small>(1) Object localized and classified as \"cat\" and (2) object localized and classified as \"dog\".</small>  In the example above, we are detecting two different objects within the image, a cat *and* a dog. Both objects are localized, and the results are returned.  Object detection can also identify multiple instances of the *same* object in a single image. If we added another dog to the previous image, an object detection algorithm could detect two dogs and a single cat.  ## Zero Shot CLIP  [OpenAI's CLIP](/learn/clip/) is a *multi-modal* model pretrained on a massive dataset of text-image pairs <sup>[3]</sup>. It can identify text and images with similar meanings by encoding both modalities into a shared vector space.  ![multi-modal-similarity](./images/zero-shot-object-detection-clip-4.png) <small>CLIP is able to encode different text and images into the same vector space.</small>  CLIP's broad pretraining means it can perform effectively across many domains. We can adjust the task being performed (i.e. from classification to detection) with just a few lines of code. A big part of this flexibility if thanks to the multi-modal vector embeddings built by CLIP.  These vector embeddings allow us to switch from [text-to-image search](/learn/clip), [image classification](/learn/zero-shot-image-classification-clip), and object detection. We simply adjust how we preprocess data being fed into CLIP, or how we interpret the similarity scores between the CLIP embeddings. The model itself requires no modification.  For classification, we need to give CLIP a list of our class labels, and it will encode them into a vector space:  ![finding-class-images](./images/zero-shot-object-detection-clip-5.png)  <small>By encoding both images and class labels into the same vector space, we can identify each image's most similar class label.</small>  From there, we give CLIP the images we'd like to classify. CLIP will encode them in the same vector space, and we find which of the class label embeddings is nearest to our image embeddings.  ### Object Localization  We can apply similar logic to using CLIP in a zero-shot object localization setting. As before, we create a class label embedding like `\"a fluffy cat\"`. But, unlike before, we don't feed the entire image into CLIP.  To localize an object, we break the image into many small patches. We then pass a `window` over these patches, moving across the entire image and generating an image embedding for a unique window.  We can calculate the similarity between these patch image embeddings and our class label embeddings — returning a score for each patch.  After calculating the similarity scores for every patch, we collate them into a map of relevance across the entire image. We use that \"map\" to identify the location of the object of interest.  ![patches](./images/zero-shot-object-detection-clip-6.png) <small>We split images into small patches, which we can use to create mini-images that are encoded and compared to each encoded label. Producing a set of scores for each part of the image.</small>  From there, we can recreate the traditional approach of creating a \"bounding box\" around the object.  <center><img src=\"./images/zero-shot-object-detection-clip-7.png\" alt=\"bounding box\" width=\"50%\" /></center>  <small>We can use the scored patches of the image to find a bounding box that encapsulates the object of interest.</small>  Both of these visuals capture the same information but displays them in different ways.  #### Occlusion Algorithm  Occlusion is another method of localization where we slide a black patch across the image. The idea being that we dentify similarity by the \"absence\" of an object <sup>\\[4\\]\\[5\\]</sup>.  <video autoplay loop muted playsinline class=\"responsive\">     <source src=\"./images/zero-shot-object-detection-clip-8.mp4\" type=\"video/mp4\"> </video> <small>The occlusion algorithm works by sliding a black square to hide parts of the image at each time step.</small>  If the black patch covers the object we are looking for, the similarity score will drop. We then take that position as the assumed location of our object.  ### Object Detection  There is a fine line between object localization and object detection. With object localization, we perform a \"classification\" of a single object followed by the localization of that object. With object detection, we perform localization for multiple classes and/or objects.  With our cat and butterfly image, we could search for two objects; `\"a fluffy cat\"` and `\"a butterfly\"`. We use object localization to identify each *individual* object, but by iteratively identifying multiple objects, this becomes *object detection*.  <center><img src=\"./images/zero-shot-object-detection-clip-9.png\" alt=\"object detection with multiple classes\" style=\"width:50%;\" /></center>  <small>Object detection differs from localization by allowing the detection of multiple classes and multiple objects belonging to each class.</small>  We stick with the bounding box visualizations for object detection, as the other method makes it harder to visualize multiple objects within the same image.  We have covered the idea behind object localization and detection in a zero-shot setting with CLIP. Now let's take a look at how to implement it.  ### Detection with CLIP  Before we move on to any classification, localization, or detection task, we need images to process. We will use a small demo dataset named `jamescalam/image-text-demo` hosted on Hugging Face *datasets*.  {{< notebook file=\"clip-detection-dataset\" height=\"full\" >}}  The dataset contains the image of a butterfly landing on a cat's nose. We can view it in a Jupyter notebook with the following:  ```python data[2]['image'] ```  <center><img alt=\"image of a butterfly landing on the nose of a cat\" src=\"./images/zero-shot-object-detection-clip-10.png\" width=\"50%;\" /></center>  <small>The image we will be using for image localization and detection.</small>  We have downloaded the image, but it is not in the format we need for localization. For that, we must break the image into smaller patches.  #### Creating Patches  To create the patches, we must first convert our PIL image object into a PyTorch tensor. We can do this using `torchvision.transforms`.  {{< notebook file=\"clip-detection-to-tensor\" height=\"full\" >}}  Our tensor has `3` color channels (RGB), a height of `5184` pixels, and width of `3456` pixels.  Assuming each patch has an equal height and width of 256 pixels, we must reshape this tensor into a tensor of shape `(1, 20, 13, 3, 256, 256)` where *20* and *13* of the number of patches in height and width of the image and *1* represents the batch dimension.  We first add the batch dimension and move the color channels' dimension behind the height and width dimensions.  {{< notebook file=\"clip-detection-add-batch-reshape\" height=\"full\" >}}  Following this, we broke up the image into horizontal patches first. All patches will be square with dimensionalities of *256x256*, so the horizontal patch height equals *256* pixels.  {{< notebook file=\"clip-detection-horizontal-patches\" height=\"full\" >}}  <center><img alt=\"horizontal patches applied to cat image\" src=\"./images/zero-shot-object-detection-clip-11.png\" width=\"40%;\" /></center>  <small>Cat image split into *256* pixel high strips.</small>  We need one more unfold to create the vertical space between patches.  {{< notebook file=\"clip-detection-vertical-patches\" height=\"full\" >}}  <center><img alt=\"horizontal patches applied to cat image\" src=\"./images/zero-shot-object-detection-clip-12.png\" width=\"40%;\" /></center>  <small>Cat image split into small *256x256* pixel patches.</small>  Every patch is tiny, and looking at a single patch gives us little-to-no information about the image's content. Rather than feeding single patches to CLIP, we merge multiple patches to create a big patch passed to CLIP.  <center><img alt=\"horizontal patches applied to cat image\" src=\"./images/zero-shot-object-detection-clip-13.png\" width=\"40%;\" /></center>  <small>The first *6x6* window viewed by CLIP.</small>  We call this grouping of patches a `window`. A larger `window` size captures more global views of the image, whereas a smaller `window` can produce a more precise map at the risk of missing larger objects. To slide across the image and create a `big_batch` at each step, we do the following:  ```python window = 6 stride = 1  # window slides from top to bottom for Y in range(0, patches.shape[1]-window+1, stride):     # window slides from left to right     for X in range(0, patches.shape[2]-window+1, stride):         # initialize an empty big_patch array         big_patch = torch.zeros(patch*window, patch*window, 3)         # this gets the current batch of patches that will make big_batch         patch_batch = patches[0, Y:Y+window, X:X+window]         # loop through each patch in current batch         for y in range(patch_batch.shape[1]):             for x in range(patch_batch.shape[0]):                 # add patch to big_patch                 big_patch[                     y*patch:(y+1)*patch, x*patch:(x+1)*patch, :                 ] = patch_batch[y, x].permute(1, 2, 0)         # display current big_patch         plt.imshow(big_patch)         plt.show() ```  <video autoplay loop muted playsinline class=\"responsive\">     <source src=\"./images/zero-shot-object-detection-clip-14.mp4\" type=\"video/mp4\"> </video> <small>Sliding window as it progresses through the image. The small image to the bottom right is what CLIP sees.</small>  We will re-use this logic later when creating our patch image embeddings. Before we do that, we must initialize CLIP.  #### CLIP and Localization  The Hugging Face *transformers* library contains an implementation of CLIP named [`openai/clip-vit-base-patch32`](https://huggingface.co/openai/clip-vit-base-patch32). We can download and initialize it like so:  ```python from transformers import CLIPProcessor, CLIPModel import torch  # define processor and model model_id = \"openai/clip-vit-base-patch32\"  processor = CLIPProcessor.from_pretrained(model_id) model = CLIPModel.from_pretrained(model_id)  # move model to device if possible device = 'cuda' if torch.cuda.is_available() else 'cpu'  model.to(device) ```  Note that we also move to model to a CUDA-enabled GPU *if possible* to reduce inference times.  With CLIP initialized, we can rerun the patch sliding logic, but this time we will calculate the similarity between each `big_patch` and the text label `\"a fluffy cat\"`.  ```python window = 6 stride = 1  scores = torch.zeros(patches.shape[1], patches.shape[2]) runs = torch.ones(patches.shape[1], patches.shape[2])  for Y in range(0, patches.shape[1]-window+1, stride):     for X in range(0, patches.shape[2]-window+1, stride):         big_patch = torch.zeros(patch*window, patch*window, 3)         patch_batch = patches[0, Y:Y+window, X:X+window]         for y in range(window):             for x in range(window):                 big_patch[                     y*patch:(y+1)*patch, x*patch:(x+1)*patch, :                 ] = patch_batch[y, x].permute(1, 2, 0)         # we preprocess the image and class label with the CLIP processor         inputs = processor(             images=big_patch,  # big patch image sent to CLIP             return_tensors=\"pt\",  # tell CLIP to return pytorch tensor             text=\"a fluffy cat\",  # class label sent to CLIP             padding=True         ).to(device) # move to device if possible          # calculate and retrieve similarity score         score = model(**inputs).logits_per_image.item()         # sum up similarity scores from current and previous big patches         # that were calculated for patches within the current window         scores[Y:Y+window, X:X+window] += score         # calculate the number of runs on each patch within the current window         runs[Y:Y+window, X:X+window] += 1 ```  Here we have also added `scores` and `runs` that we will use to calculate the *mean* score for each patch. We calculate the `scores` tensor as the sum of every `big_patch` score calculated while the patches were within the `window`.  Some patches will be seen more often than others (for example, the top-left patch is seen once), so the scores will be much greater for patches viewed more frequently. That is why we use the `runs` tensor to keep track of the \"visit frequency\" for each patch. With both tensors populated, we calculate the mean score:  ```python scores /= runs ```  The `scores` tensor typically contains a smooth gradient of values as a byproduct of the scoring function sliding over each window. This means the scores gradually fade to `0.0` the further they are from the object of interest.  We cannot accurately visualize the object location with the current scores. Ideally, we should push low scores to zero while maintaining a range of values for higher scores. We can do this by clipping our outputs and normalizing the remaining values.  ```python # clip the scores scores = np.clip(scores-scores.mean(), 0, np.inf)  # normalize scores scores = (     scores - scores.min()) / (scores.max() - scores.min() ) ```  ![clipping](./images/zero-shot-object-detection-clip-15.png) <small>After clipping and normalization we return a more useful visual (right).</small>  With that, our patch scores are ready, and we can move on to visualizing the results.  #### Visualize Localization  Each patch in the $(20, 13)$ patches tensor is assigned a similarity score within the range of $0$ (not similar) to $1$ (perfect match).  If we can align the scores with the original image pixels, we can multiply each pixel by its corresponding similarity score. Those near $0$ will be dark, and near $1$ will maintain their original brightness.  The only problem is that these two tensors are *not* the same shape:  ```python scores.shape, patches.shape ```  ``` [Out]: (torch.Size([20, 13]), torch.Size([1, 20, 13, 3, 256, 256])) ```  We need to reshape `patches` to align with scores. To do that, we use `squeeze` to remove the batch dimension at position `0` and then re-order the dimensions using `permute`.  ```python # transform the patches tensor adj_patches = patches.squeeze(0).permute(3, 4, 2, 0, 1) adj_patches.shape ```  ``` [Out]: torch.Size([256, 256, 3, 20, 13]) ```  From there, we multiply the adjusted patches and `scores` to return the brightness-adjusted patches. These need to be permuted again to be visualized with `matplotlib`.  ```python # multiply patches by scores adj_patches = adj_patches * scores  # rotate patches to visualize adj_patches = adj_patches.permute(3, 4, 2, 0, 1) adj_patches.shape  ```  ``` [Out]: torch.Size([20, 13, 3, 256, 256]) ```  Now we're ready to visualize:  ```python Y = adj_patches.shape[0] X = adj_patches.shape[1]  fig, ax = plt.subplots(Y, X, figsize=(X*.5, Y*.5)) for y in range(Y):     for x in range(X):         ax[y, x].imshow(adj_patches[y, x].permute(1, 2, 0))         ax[y, x].axis(\"off\")         ax[y, x].set_aspect('equal') plt.subplots_adjust(wspace=0, hspace=0) plt.show() ```  <center><img alt=\"object localization applied for 'a fluffy cat'\" src=\"./images/zero-shot-object-detection-clip-16.png\" width=\"40%;\" /></center>  <small>Object localization for *\"a fluffy cat\"*.</small>  That works well. We can repeat the same but with the prompt `\"a butterfly\"` to return:  <center><img alt=\"object localization applied for 'a fluffy cat'\" src=\"./images/zero-shot-object-detection-clip-17.png\" width=\"40%;\" /></center>  <small>Object localization for *\"a butterfly\"*.</small>  CLIP shows another good result and demonstrates how easy it is to add new labels to classification and localization tasks with CLIP.  #### Bounding Box  Before moving on to object detection, we need to rework the visualization to handle multiple objects.  The standard way to outline objects for localization and detection is to use a bounding box. We will do the same using the `scores` calculated previously for the `\"a butterfly\"` prompt.  The bounding box requires a defined edge, unlike our previous visual, which had a more continuous fade to black. To do this, we need to set a threshold for what is positive or negative, and we will use `0.5`.  ```python # scores higher than 0.5 are positive detection = scores > 0.5 ```  We can now detect the non-zero positions with the `np.nonzero` function. The output values represent the $x, y$ coordinates of patches with `scores > 0.5`.  {{< notebook file=\"clip-detection-non-zero\" height=\"full\" >}}  The first column represents the *x-coordinates* of non-zero positions, and the second column represents the respective *y-coordinates*.  <center><img alt=\"true patch detection coordinates\" src=\"./images/zero-shot-object-detection-clip-18.png\" width=\"60%;\" /></center>  <small>Detection coordinates created by `detection = scores > 0.5`.</small>  Our bounding box will take each of the edges produced by these non-zero coordinates.  <center><img alt=\"bounding box\" src=\"./images/zero-shot-object-detection-clip-22.png\" width=\"60%;\" /></center>  We need the minimum and maximum $x$ and $y$ coordinates to find the box corners.  {{< notebook file=\"clip-detection-min-max-coords\" height=\"full\" >}}  <center><img alt=\"bounding box using coordinates\" src=\"./images/zero-shot-object-detection-clip-19.png\" width=\"60%;\" /></center>  <small>The *min* and *max* values of $x, y$ coordinates give us our bounding box corners within the patches array.</small>  These give us the bounding box coordinates based on patches rather than pixels. To get the pixel coordinates (for the visual), we multiply the coordinates by `patch`. After that, we calculate the box `height` and `width`.  {{< notebook file=\"clip-detection-pixel-coords\" height=\"full\" >}}  <center><img alt=\"bounding box using coordinates\" src=\"./images/zero-shot-object-detection-clip-20.png\" width=\"60%;\" /></center>  With the `x_min`, `y_min`, `width`, and `height` values we can use `matplotlib.patches` to create the bounding box. Before we do that, we convert the original PIL image into a `matplotlib`-friendly format.  {{< notebook file=\"clip-detection-move-color-channel\" height=\"full\" >}}  Now we visualize everything together:  {{< notebook file=\"clip-detection-show-bounding-box\" height=\"full\" >}}  There we have our bounding box visual.  ### Object Detection  We finally have everything we need to perform **object detection** for multiple object classes within the same image. The logic is a loop over what we have already built, and we can package it into a neater function like so:  ```python def detect(prompts, img, patch_size=256, window=6, stride=1, threshold=0.5):     # build image patches for detection     img_patches = get_patches(img, patch_size)     # convert image to format for displaying with matplotlib     image = np.moveaxis(img.data.numpy(), 0, -1)     # initialize plot to display image + bounding boxes     fig, ax = plt.subplots(figsize=(Y*0.5, X*0.5))     ax.imshow(image)     # process image through object detection steps     for i, prompt in enumerate(tqdm(prompts)):         scores = get_scores(img_patches, prompt, window, stride)         x, y, width, height = get_box(scores, patch_size, threshold)         # create the bounding box         rect = patches.Rectangle((x, y), width, height, linewidth=3, edgecolor=colors[i], facecolor='none')         # add the patch to the Axes         ax.add_patch(rect)     plt.show() ```  *(Find the [full code here](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/image-retrieval/clip-object-detection/zero-shot-object-detection-clip.ipynb))*  Now we pass a list of class labels and the image to `detect`. The function will return our image with each detected object annotated with a bounding box.  ```python detect([\"a cat\", \"a butterfly\"], img, window=4, stride=1) ```  <center><img alt=\"bounding box using coordinates\" src=\"./images/zero-shot-object-detection-clip-21.png\" width=\"60%;\" center /></center>  The current implementation is limited to displaying a single object from each class, but this can be solved with a small amount of additional logic.  ---  That's it for this walkthrough of *zero-shot* object localization and detection with OpenAI's CLIP. Zero-shot opens the doors to many organizations and domains that could not perform good object detection due to a lack of training data or compute resources — which is the case for the vast majority of companies.  Multi-modality and CLIP are just part of a trend towards more broadly applicable ML with a much lower barrier to entry. Zero-to-few-shot learning unlocks those previously inaccessible projects and presents us with what will undoubtedly be a giant leap forward in ML capability and adoption across the globe.  ## Resources  [**Colab Notebook**](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/image-retrieval/clip-object-detection/zero-shot-object-detection-clip.ipynb)  [1] O. Russakovsky et al., [ImageNet Large Scale Visual Recognition Challenge](https://arxiv.org/abs/1409.0575) (2014)  [2] A. Krizhevsky et al., [ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) (2012), NeurIPS  [3] A. Radford, J. Kim, et al., [Learning Transferable Visual Models From Natural Language Supervision](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language.pdf) (2021)  [4] F. Bianchi, [Domain-Specific Multi-Modal Machine Learning with CLIP](https://youtu.be/uqRSc-KSA1Y?t=1841) (2022), Pinecone Workshop  [5] R. Pisoni, [Searching Across Images and Text: Intro to OpenAI's CLIP](https://youtu.be/W11lSifqJDs?t=1690) (2022), Pinecone Workshop ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e36a"
  },
  "title": "\"Ludicrous BERT Search Speeds\"",
  "headline": "\"Ludicrous <span>BERT Search Speeds</span>\"",
  "weight": "3",
  "name": "Rajat Tripathi",
  "position": "Software Engineer",
  "src": "/images/company-rajat.jpeg",
  "href": "https://www.linkedin.com/in/rajat-tripathi-08/",
  "description": "Semantic search applications require quality embeddings and fast queries, making BERT and Pinecone the perfect combination.",
  "images": "['/images/bert-search-speed-3.png']",
  "content": "*This is a comparison of Approximate Nearest Neighbor (ANN) speeds between Pinecone and several flavors of Elasticsearch. Thanks to [Dmitry Kan](https://dmitry-kan.medium.com/), whose article is referenced here, for reviewing drafts of this post and providing valuable suggestions.*  ---  Semantic search applications have two core ingredients: Text documents represented as vector embeddings that capture meaning, and a search algorithm to retrieve semantically similar items.  BERT is a popular word embedding model because its pretrained versions provide high-quality embeddings without any additional training. However, those embeddings are high-dimensional (768 in our experiment), making it unbearably slow to search with k-nearest-neighbor (KNN) algorithms.  Fortunately, there are ways to speed things up considerably. [One article](https://towardsdatascience.com/speeding-up-bert-search-in-elasticsearch-750f1f34f455) shows how to search through BERT (or any other) embeddings progressively faster in Elasticsearch, starting with native, then with the Elastiknn plugin, then with Approximate k-NN in Open Distro for Elasticsearch, and finally with a GSI Associative Processing Unit (APU).  In the end, the author accelerated a million-document search from 1,500ms with Elasticsearch to 93ms with the GSI APU. An improvement of 6.1x — not bad! Not to mention the reduced index sizes and indexing times.  We wanted to take this experiment even further. Using [Pinecone vector search](/) **we improved search speeds by 2.4x compared to Elasticsearch with GSI APU, and by 4.4x compared to Open Distro for Elasticsearch**. All while maintaining a rank@k recall of 0.991.  Here’s how we did it.  ## ANN and Vector Indexes  Pinecone uses approximate-nearest-neighbor (ANN) search instead of k-nearest-neighbor (kNN). ANN is a method of performing nearest neighbor search where we trade off some accuracy for a massive boost in performance. Instead of going through the entire list of objects and finding the exact neighbors, it retrieves a “good guess” of an object’s neighbors.  A [vector index](/learn/vector-database/) is a specialized data structure tailor-made for performing a nearest neighbor search (approximate or exact) on vectors. An index allows us to insert, fetch and query our vectors efficiently. Pinecone employs its search algorithm and index that scales up to billions of vectors with extremely high throughput.  ![How BERT works with Pinecone](/images/bert-search-speed-3.png)  ## The Setup  The environment is not too relevant since Pinecone is a distributed service and your query and index times depend on the network rather than the machine itself. Though a beefier notebook instance means less time to compute embeddings. I used a GCP Notebook instance (n1-standard-8 US-West) for this experiment.  We used the same dataset, model, and approach as the reference article, using Pinecone instead for performing the search. We used the [DBpedia dataset](http://downloads.dbpedia.org/wiki-archive/data-set-37.html) containing full abstracts of Wikipedia articles and [SBERT](https://www.sbert.net/) from SentenceTransformers for creating embeddings.  You can find code for pre-processing the data in the GitHub repository from the reference article. Since Pinecone is a managed service, we can create and work with an index using its python client directly from the notebook. The code is [available publicly](https://github.com/pinecone-io/examples/blob/master/bert_search_test/bertcomparison.ipynb).  Once we have the embeddings ready, searching with Pinecone is a three-step process: Create the index, upsert the embeddings, and query the index.  Here are a few snippets from the code to see how we used Pinecone for this experiment.   ```python !pip install --quiet -U pinecone-client  import pinecone pinecone.init(api_key=api_key)  index_name = 'bert-stats-test'  pinecone.create_index(index_name,metric='cosine', shards=shards)  def upload_items(items_to_upload: List, batch_size: int) -> float:     print(f\"\\nUpserting {len(items_to_upload)} vectors...\")     start = time.perf_counter()     upsert_cursor = index.upsert(items=items_to_upload,batch_size=batch_size)     end = time.perf_counter()     return (end - start) / 60.0 ```  ## The Test  Pinecone runs in the cloud and not on your local machine. We changed how we query from the original article to make the comparisons better. Since my instance was on GCP (and Pinecone on AWS), querying an index across the internet adds overhead like network, authentication, parsing, etc. To estimate the speeds better, we queried the index in two ways: * Send 10 single queries and average the speed. * Send 10 batched queries with batch size = 1000 and calculate the average.  With batched queries, we lower the overheads per query. One way to ensure lower latencies is deploying your application in the same region and cloud service provider as Pinecone. Currently, our production servers run on AWS in US-West, but we can deploy Pinecone in any region on AWS or GCP.  Batching queries also has other practical applications apart from improving per query latency. Most notably, when you have a set of queries that you know you want to make beforehand, it’s better to send them in a batch.  Here’s our query function:  ```python def query(test_items: List, index):     print(f\"\\nQuerying...\")     times = []     #For single queries, we pick 10 queries     for test_item in test_items[:10]:         start = time.perf_counter()         #test_item is an array of [id,vector]         query_results = index.query(queries=[test_item[1]],disable_progress_bar=True) # querying vectors with top_k=10         end = time.perf_counter()         times.append((end-start))     #For batch queries, we pick 1000 vectors at perform 10 queries     print(f\"\\n Batch Querying...\")     batch_times = []     for i in range(0,10000, 1000):         start = time.perf_counter()         batch_items = test_items[i:i+1000]         vecs = [item[1] for item in batch_items]         query_results = index.query(queries=vecs,disable_progress_bar=True)         end = time.perf_counter()         batch_times.append((end-start))     return mean(times)*1000,mean(batch_times)*1000 ```  Using the setup and query method above, we saw the following results:  ![Summary of BERT indexing and search speeds with Pinecone.](/images/bert-search-speed-4.png)  Indexing time in minutes:  ![Average BERT indexing speeds with Pinecone.](/images/bert-search-speed-2.png)  Search speed for single queries:  ![Average BERT search speeds with Pinecone.](/images/bert-search-speed-7.png)  Total search speed for 1,000 queries (batched):  ![Average BERT search speeds with Pinecone.](/images/bert-search-speed-6.png)  ### Estimating Per-Query Speed  As mentioned earlier, the results don’t accurately represent the Pinecone engine’s search speed due to network overheard dominating a large part of the final results. However, we can try to estimate the search speed per query without the overheads.  We can view the returned search speeds as:  ``` network_overhead + [num_queries] * [search_speed_per_query] ```  The network_overhead may change a bit for every call but can be seen as a constant no matter the index size and batch size.  For 1 million documents and assuming the overhead is constant:  Single query: ```   network_overhead + 1 * search_speed_per_query = 35.68ms ``` Batched query: ```   network_overhead + 1000 * search_speed_per_query = 7020.4ms ```  Solving for search_speed_per_query: ```   999 * search_speed_per_query = 7020.4 - 35.68   search_speed_per_query = 6.98 ms ```  Using this logic let's look at the estimated search speed per query on Pinecone's engine  ![Average BERT search speeds per query with Pinecone.](/images/bert-search-speed-1.png)  ### Calculating Recall  Recall is an important metric whenever we talk about ANN algorithms since they trade accuracy for speed. Since we do not have ground truth in terms of nearest neighbors for our queries, one way to calculate recall is to compare results between [approximate and exact searches](/docs/create-index/#parameters).  We calculated recall and approximation-loss as described in our [benchmark guide](/docs/examples/benchmark-vector-similarity-search/):  > \"Rank-k recall is widespread due to its usage in a standard approximated nearest neighbor search benchmarking tool. It calculates the fraction of approximated (top-k) results with a score (i.e., 'metric' score) at least as high as the optimal, exact search, rank-k score. Usually, we robustify the threshold by adding a small 'epsilon' number to the rank-k score.\"  > Approximation-loss computes the median top-k ordered score differences between the optimal and approximate-search scores for each test query. In other words, the differences between the optimal and approximate-search top-1 scores, top-2 scores, up to the top-k scores. The final measure is the average of these median values over a set of test queries.  These were the recall and approximation-loss results for 1 million vectors:  ```   Accuracy results over 10 test queries:   The average recall @rank-k is 0.991   The median approximation loss is 1.1920928955078125e-07 ```  We saw an average recall of 0.991, which means Pinecone maintains high recall while providing a big boost in search speeds.  ## Compared Results  Compared to results in the original test, using Pinecone for SBERT embeddings is a way to further improve index sizes, indexing speed, and search speeds.  **Pinecone vs. Elasticsearch (vanilla), 1M SBERT embeddings:**  * 32x faster indexing * 5x smaller index size * 40x faster single-query (non-batched) searches  **Pinecone vs. Open Distro for Elasticsearch from AWS, 200k SBERT embeddings:**  * 54x faster indexing * 6x smaller index size * Over 100ms faster average search speed in batched searches * 4.4x faster single-query searches  However, note that the [HNSW algorithm](/learn/hnsw/) inside Open Distro can be fine-tuned for higher throughput than seen in the reference article.  **Pinecone vs. Elasticsearch with GSI APU plugin, 1M SBERT embeddings:** * 2.4x faster single-query searches  ---  **1M SBERT Embeddings**  <table class=\"table table-responsive table-dark\">   <tr>     <th></th>     <th>Indexing Speed (min)</th>     <th>Index Size (GB)</th>     <th>Search Speed (ms)</th>   </tr>   <tr>     <td>Pinecone (batch)</td>     <td>1.36</td>     <td>3</td>     <td>6.8</td>   </tr>   <tr>     <td>Pinecone (single)</td>     <td>1.36</td>     <td>3</td>     <td>38.3</td>   </tr>   <tr>     <td>Elasticsearch</td>     <td>44</td>     <td>15</td>     <td>1519.7</td>   </tr>   <tr>     <td>Elasticsearch + Elastiknn</td>     <td>51</td>     <td>15</td>     <td>663.8</td>   </tr>   <tr>     <td>Elasticsearch + GSI</td>     <td>44</td>     <td>15</td>     <td>92.6</td>   </tr> </table>  **200k SBERT Embeddings, Approximate Nearest Neighbors**  <table class=\"table table-responsive table-dark\">   <tr>     <th></th>     <th>Indexing Speed (min)</th>     <th>Index Size (GB)</th>     <th>Search Speed (ms)</th>   </tr>   <tr>     <td>Pinecone (batch)</td>     <td>0.21</td>     <td>0.6</td>     <td>1.1</td>   </tr>   <tr>     <td>Pinecone (single)</td>     <td>0.21</td>     <td>0.6</td>     <td>24.0</td>   </tr>   <tr>     <td>Open Distro</td>     <td>11.25</td>     <td>3.6</td>     <td>105.7</td>   </tr> </table>  If you would like to benchmark Pinecone for your use case and dataset — for both speed and accuracy — follow our [benchmarking guide for vector search services](/docs/examples/benchmark-vector-similarity-search/). ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e36c"
  },
  "title": "\"K-Nearest Neighbor (KNN) Explained\"",
  "headline": "\"K-Nearest Neighbor (KNN) Explained\"",
  "weight": "4",
  "name": "Diego Lopez Yse",
  "position": "Data Scientist",
  "src": "/images/diego-lopez-yse.jpeg",
  "href": "\"https://lopezyse.medium.com/\"",
  "description": "K-Nearest Neighbor (KNN) Explained.",
  "images": "[\"https://www.pinecone.io/images/knn-proximity.png\"]",
  "content": "Have you ever heard of the Gestalt Principles? These are part of a theory of perception that examines how humans group similar objects in order to interpret the complex world around them.  According to the Gestalt principle of proximity, elements that are closer together are perceived to be more related than elements that are farther apart, helping us understand and organize information faster and more efficiently.  ![KNN Proximity](/images/knn-proximity.jpg) <small>Our brains perceive these sets of closely placed elements as groups. Proximity is essential to our perception. Source: [UXMISFIT](https://uxmisfit.com/2019/04/23/ui-design-in-practice-gestalt-principles/)</small>  Likewise, in Machine Learning, the concepts of proximity and similarity are tightly linked. Meaning that the closer two data points are, the more similar they are to one another than to other data points. The content recommendation systems you use every day for movies, texts, songs, and more, rely on this principle.  One Machine Learning algorithm that relies on the concepts of proximity and similarity is **K-Nearest Neighbor (KNN)**. KNN is a supervised learning algorithm capable of performing both classification and regression tasks.  ---  **Note: As you'll see in this article, doing KNN-search or even ANN-search at scale can be slow and expensive. That's why we made [Pinecone](/) — an API for fast, fresh, filtered, and low-cost ANN search at any scale.**  ---  As opposed to unsupervised learning — where there is no output variable to guide the learning process and where data is explored by algorithms to find patterns — in supervised learning your existing data is already labeled and you know which behavior you want to predict in the new data you obtain.  The KNN algorithm predicts responses for new data (testing data) based upon its similarity with other known data (training) samples. It assumes that data with similar traits sit together and uses distance measures at its core.  KNN belongs to the class of non-parametric models as it doesn't learn parameters from the training dataset to come up with a discriminative function to predict new unseen data. It operates by memorizing the training dataset.  Given its flexibility, KNN is used in industries such as:  - Agriculture: to perform climate forecasting, estimating soil water parameters, or predicting crop yields. - Finance: to predict bankruptcies, understanding and managing financial risk. - Healthcare: to identify cancer risk factors, predict heart attacks, or analyze gene expression data. - Internet: using clickstream data from websites, to provide automatic recommendations to users on additional content.  ## Distance Measures  How near are two data points? It depends on how you measure it, and in Machine Learning, there isn’t just one way of measuring distances.  A distance measure is an objective score that summarizes the relative difference between two objects in a problem domain and plays an important role in most algorithms. Some of the main measures are:  - **Euclidean** is probably the most intuitive one and represents the shortest distance between two points. It’s calculated using the well-known Pythagorean theorem. Conceptually, it should be [used whenever we are comparing observations with continuous features](https://aigents.co/data-science-blog/publication/distance-metrics-for-machine-learning), like height, weight, or salaries. This distance measure is often the “default” distance used in algorithms like KNN.  ![Euclidean distance](/images/knn-euclidean-distance.png) <small>Euclidean distance between two points. Source: [VitalFlux](https://vitalflux.com/different-types-of-distance-measures-in-machine-learning/)</small>  - **Manhattan** is used to estimate the distance to get from one data point to another if a grid-like path is taken. Unlike Euclidean distance, the Manhattan distance calculates the sum of the absolute values of the difference of the coordinates of two points. This way, instead of estimating a straight line between two points, we “walk” through available paths. The Manhattan distance is useful when our observations are distributed along a grid, like in chess or city blocks (when the [features of our observations are entire integers](https://aigents.co/data-science-blog/publication/distance-metrics-for-machine-learning) with no decimal parts).  ![Manhattan distance](/images/knn-manhattan-distance.png) <small>Manhattan distance between two points A and B. Source: [VitalFlux](https://vitalflux.com/different-types-of-distance-measures-in-machine-learning/)</small>  - **Cosine** is employed to calculate similarity between two [vectors](https://www.pinecone.io/learn/vector-embeddings/). Through this measure, data objects in a dataset are treated as vectors, and similarity is calculated by the cosine of the angle between two vectors. Vectors which are most similar will have a value of 0 degrees between them (the value of cos = 0 is 1), while vectors that are most dissimilar will have a value of -1. The smaller the angle, the higher the similarity. This different perspective about distance can provide novel insights which might not be found using the previous distance metrics.  ![Cosign vectors](/images/knn-cosign-vectors.png) <small>Two cosine vectors that are aligned in the same orientation will have a similarity measurement of 1, whereas two vectors aligned perpendicularly will have a similarity of 0. If two vectors are diametrically opposed, meaning they are oriented in exactly opposite directions (i.e. back-to-back), then the similarity measurement is -1. Source: [DeepAI](https://deepai.org/machine-learning-glossary-and-terms/cosine-similarity)</small>  - **Hamming** represents the number of points at which two corresponding pieces of data can be different. While comparing two vectors of equal length, Hamming distance is the number of bit positions in which they differ. This metric is generally used when comparing texts or binary vectors.  ![Hamming distance](/images/knn-hamming-distance.png) <small>How many bits are different between Data A and Data B? You can easily figure out the difference just by comparing bit by bit. In this case, you would figure out only one bit difference , ande: that’s the Hamming difference between both vectors. Source: [ShareTechnote](https://www.sharetechnote.com/html/Handbook_Communication_HammingDistance.html)</small>  Here is a [complete review](https://www.liebertpub.com/doi/10.1089/big.2018.0175) of different distance measures applied to KNN.  ## How KNN works  KNN performs classification or regression tasks for new data by calculating the distance between the new example and all the existing examples in the dataset. But how?  Here’s the secret: The algorithm stores the entire dataset and classifies each new data point based on the existing data points that are similar to it. KNN makes predictions based on the training or “known” data only.  After the user defines a distance function, like the ones we mentioned earlier, KNN calculates the distance between data points in order to find the closest data points from our training data for any new data point. The existing data points closest to the new data point using the defined distance will become the “k-neighbors”. For a classification task, KNN will use the most frequent of all values from the k-neighbors to predict the new data label. For a regression task, the algorithm will use the average of all values from the k-neighbors to predict the new data value.  <video autoplay loop muted playsinline class=\"responsive\">   <source src=\"/images/knn.mp4\" type=\"video/mp4\"> </video>  <small class=\"video\">In a classification task with K=3, an unlabelled data point is labeled as pink due to the majority vote of the 3 most similar labeled examples. Source: [Towards Data Science](https://towardsdatascience.com/beginners-guide-to-k-nearest-neighbors-in-r-from-zero-to-hero-d92cd4074bdb)</small>  All KNN does is store the complete dataset, and without doing any calculation or modeling on top of it, measure the distance between the new data point and its closest data points. For this reason, and since there’s not really a learning process happening, KNN is called a “lazy” algorithm (as opposed to “eager” algorithms like Decision Trees that build generalized models before performing predictions on new data points).  ## How to find k?  With KNN, in order to make a classification/regression task, you need to define a number of neighbors, and that number is given by the parameter “k”. In other words, “k” determines the number of neighbors the algorithm looks at when assigning a value to any new observation.  This number can go from 1 (in which case the algorithm only looks at the closest neighbor for each prediction) to the total number of data points of the dataset (in which case the algorithm would predict the majority class of the complete dataset).  ![Finding K](/images/knn-k.png) <small>In the above example, if k=3 then the new data point will be classified as B. But if k=6, then it will be classified as A because the majority of points are from class A. Source: [ShareTechnote](https://www.sharetechnote.com/html/Handbook_Communication_HammingDistance.html)</small>  So how can you know the optimum value of “k”? We can decide based on the error calculation of a training and testing set. Separating the data into training and test sets allows for an objective model evaluation.  One popular approach is testing different numbers of “k” and measuring the resulting error, choosing the “k” value at which an increase will cause a very small decrease in the error sum, while a decrease will sharply increase the error sum. This point that defines the optimal number is known as the “elbow point”.  ![Error rates vs K value](/images/knn-error-rates.png) <small>The “k” value best suited for a dataset is selected by using the error rate (this is, the difference between the real and predicted values). After a certain point, the error rate becomes almost constant. Source: [AI in Plain English](https://ai.plainenglish.io/k-nearest-neighbors-knn-algorithm-for-machine-learning-ab6f17df4a7f)</small>  Above, we can see that after “k” > 23, the error rate stabilizes and becomes almost constant. For this reason, “k” = 23 seems to be the optimal value.  Another alternative for deciding the value of “k” is using grid search to find the best value. Grid search is a process that searches exhaustively through a specified numerical space for the algorithm to optimize a given parameter (which in this case is the error rate). Tools like Python’s [GridSearchCV](https://realpython.com/knn-python/) can automate the fit of KNN on the training set while validating the performance on the testing set in order to identify the optimal value of “k”.  ## Why KNN  In contrast to many other Machine Learning algorithms, KNN is simple to implement and intuitive. It’s flexible (has many distance metrics to choose from), evolves as new data is revealed, and has one single hyperparameter to tune (the value of “k”). It can detect linear or nonlinear distributed data, and since it is non-parametric, there are no assumptions to be met to implement it (i.e. [as opposed to linear regression models](https://www.fromthegenesis.com/pros-and-cons-of-k-nearest-neighbors/) that have plenty of assumptions to be met by the data before they can be employed).  Like every algorithm, it has some downsides. The choice of “k” and the distance metric are critical to its performance and need to be carefully tuned. KNN is also very sensitive to outliers and imbalanced datasets. Also, since all the algorithm does is store the training data to perform its predictions, the memory needs grow linearly with the number of data points you provide for training.  KNN is definitely an algorithm worth learning. Whether it’s for dealing with [missing data](https://www.analyticsvidhya.com/blog/2021/05/20-questions-to-test-your-skills-on-k-nearest-neighbour/) or [predicting image similarity](https://meesho.io/blog/predicting-visual-similarities-in-product-images-using-knn), this is a tool that you can’t miss! ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e36e"
  },
  "title": "\"What is Cosine Similarity?\"",
  "headline": "\"What is Cosine Similarity?\"",
  "description": "\"Cosine similarity is a measure of similarity between two vectors in a multi-dimensional space.\"",
  "content": "Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. Two vectors with the same orientation have a cosine similarity of $1$, two vectors at $90°$ have a similarity of $0$, and two vectors diametrically opposed have a similarity of $-1$, independent of their magnitude.  The formula for cosine similarity is:  $$\\text{Cosine similarity} = (\\text{Dot product of two vectors}) / (\\text{Product of Euclidean lengths of two vectors})$$  Cosine similarity is a commonly used measure in information retrieval and text mining, where it is often used to measure the similarity of documents. It is also used in collaborative filtering, where it is used to measure the similarity of users or items. ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e370"
  },
  "title": "\"What is the Distance Between Two Vectors?\"",
  "headline": "\"What is the Distance Between Two Vectors?\"",
  "description": "\"The distance between two vectors is a measure of the difference between the two vectors.\"",
  "content": "The distance between two vectors is a measure of how different they are from each other. It is a numerical value that can be calculated using a variety of methods, such as Euclidean distance, dot product, or cosine similarity.   This measure of distance is used in many machine learning and data science applications, such as clustering, classification, and recommendation systems.   For example, in a clustering algorithm, the distance between two vectors can be used to determine which cluster they should be assigned to. In a classification algorithm, the distance between two vectors can be used to determine which class they should be assigned to. In a recommendation system, the distance between two vectors can be used to determine which items should be recommended to a user. ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e372"
  },
  "title": "\"What is Maximum a Posteriori Estimation?\"",
  "headline": "\"What is Maximum a Posteriori Estimation?\"",
  "description": "\"Maximum a posteriori (MAP) estimation is a method for estimating the parameters of a statistical model based on a set of observations.\"",
  "content": "Maximum a posteriori (MAP) estimation is a method of estimating the parameters of a statistical model. It is a type of Bayesian estimation, which uses Bayes' theorem to update the probability for a hypothesis as more evidence or information becomes available.   MAP estimation can be used in machine learning and data science to estimate the parameters of a model, such as the weights of a neural network or the coefficients of a linear regression model. MAP estimation can also be used to estimate the probability of a given hypothesis, such as the probability of a given data point belonging to a certain class. ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e374"
  },
  "title": "\"What is an Inner Product?\"",
  "headline": "\"What is an Inner Product?\"",
  "description": "\"An inner product is a mathematical operation that takes two vectors as input and produces a scalar as output.\"",
  "content": "An inner product is a mathematical operation that takes two vectors of the same size and produces a scalar value. It is calculated by multiplying each element of one vector with the corresponding element of the other vector and then summing up the results.  It is used in many areas of mathematics, including linear algebra, calculus, and geometry. In machine learning and data science, inner products are used to measure the similarity between two vectors.  For example, in a neural network, the inner product of two vectors can be used to measure the similarity between two input vectors, or between an input vector and a weight vector.   Inner products can also be used to measure the similarity between two images, or between two text documents. ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e376"
  },
  "title": "\"What is the Tanimoto Similarity?\"",
  "headline": "\"What is the Tanimoto Similarity?\"",
  "description": "\"The Tanimoto similarity, also called Tanimoto similarity coefficient, is a measure of the similarity between two sets of data.\"",
  "content": "Tanimoto similarity is a measure of similarity between two sets of data. It is a metric used to compare the similarity of two sets of data, and is often used in machine learning and data science.   The Tanimoto similarity is calculated by taking the intersection of two sets and dividing it by the sum of the sizes of the two sets. This gives a value between 0 and 1, where 0 indicates no similarity and 1 indicates perfect similarity.  Tanimoto similarity is often used in machine learning and data science to compare the similarity of two sets of data. For example, it can be used to compare the similarity of two sets of images, or two sets of text documents. It can also be used to compare the similarity of two sets of data points, such as two sets of customer data. In this case, the Tanimoto similarity can be used to identify customers who are similar in terms of their purchase history or other characteristics.  ### Tanimoto Similarity vs. Jaccard Coefficient  It is similar to the Jaccard coefficient, which is the ratio of the intersection of two sets to the union of two sets.   The Tanimoto similarity and Jaccard coefficient are both measures of similarity between two sets of data. The main difference between the two is that the Tanimoto similarity takes into account the size of the sets, while the Jaccard Index does not. The Tanimoto similarity is calculated by dividing the number of elements that are common to both sets by the total number of elements in both sets, while the Jaccard Index is calculated by dividing the number of elements that are common to both sets by the number of elements that are unique to either set.",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e378"
  },
  "title": "\"What is a Sparse Vector?\"",
  "headline": "\"What is a Sparse Vector?\"",
  "description": "\"A sparse vector is a vector with many elements that are zero or close to zero, and only a few elements that are non-zero.\"",
  "content": "A sparse vector is a vector that contains mostly zeros, with only a few non-zero elements. It is a useful data structure for representing data that is mostly empty or has a lot of zeros. For example, if you have a vector of length 10,000 and only 10 elements are non-zero, then it is a sparse vector.  It is used in machine learning and data science when dealing with large datasets, as it can reduce the amount of memory needed to store the data. For example, if you have a dataset with millions of features where only a few features are important for each data point, you can represent it as a sparse vector, which will take up much less memory than a dense vector.   Sparse vectors can also be used in algorithms such as linear regression, where they can help reduce the computational complexity of the algorithm. ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e37a"
  },
  "title": "\"What is the Euclidean Distance?\"",
  "headline": "\"What is the Euclidean Distance?\"",
  "description": "\"Euclidean distance is a measure of the straight-line distance between two points in Euclidean space.\"",
  "content": "The Euclidean Distance is a measure of the distance between two points in a Euclidean space. It is calculated by taking the square root of the sum of the squared differences between the coordinates of the two points. For example, if two points have coordinates (x1, y1) and (x2, y2), then the Euclidean Distance between them is given by:   $$d = sqrt((x2 - x1)^2 + (y2 - y1)^2)$$  The Euclidean Distance is a useful measure for many applications, such as clustering, classification, and regression. It is also used in machine learning algorithms such as k-means clustering and k-nearest neighbors. ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e37c"
  },
  "title": "\"What is the Jaccard Similarity?\"",
  "headline": "\"What is the Jaccard Similarity?\"",
  "description": "\"The Jaccard similarity is a measure of similarity between two sets of data.\"",
  "content": "The Jaccard similarity coefficient, or Jaccard Index, is a measure of similarity between two sets of data. It is calculated by taking the size of the intersection of the two sets and dividing it by the size of the union of the two sets. This gives us a value between 0 and 1, where 0 indicates no similarity and 1 indicates perfect similarity.  The Jaccard similarity coefficient can be used in machine learning and data science in a variety of ways. For example, it can be used to measure the similarity between two documents, or to measure the similarity between two sets of data points. It can also be used to measure the similarity between two clusters of data points, or to measure the similarity between two sets of features. In addition, it can be used to measure the similarity between two sets of words or phrases. ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e37e"
  },
  "title": "\"What is Inductive Learning?\"",
  "headline": "\"What is Inductive Learning?\"",
  "description": "\"Inductive learning is a type of learning that involves using observations or examples to induce general rules or principles.\"",
  "content": "Inductive learning is a type of machine learning that uses data to make predictions or generalizations about a given problem. It is based on the idea that if a set of data points have certain characteristics, then future data points will also have those characteristics.   This type of learning is used in many areas of machine learning and data science, such as supervised learning, unsupervised learning, and reinforcement learning. For example, in supervised learning, inductive learning can be used to build a model that can predict the outcome of a given problem based on the data it has been trained on.   In unsupervised learning, inductive learning can be used to identify patterns in data and make predictions about future data points. In reinforcement learning, inductive learning can be used to identify the best action to take in a given situation. ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e380"
  },
  "title": "Introduction",
  "description": "Roughly Explained",
  "author": "Pinecone",
  "intro": "|",
  "emailSubmit": "true",
  "socialShare": "true",
  "image": "/images/roughly-explained-ebook.png",
  "images": "['/images/roughly-explained-ebook.png']",
  "introChapter": "",
  "text": "|",
  "content": "",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e382"
  },
  "title": "Learn More",
  "url": "/ai-search-use-case",
  "description": "Launch, use, and scale your vector search service with our easy API, without worrying about infrastructure or algorithms. We'll keep it running smoothly and securely.",
  "href": "/contact",
  "text": "Search by  keyword quickly across millions of images in a catalog.",
  "src": "/images/ai-search.svg",
  "alt": "Deepset AI",
  "width": "190",
  "eyebrow": "Use cases",
  "- title": "Multi modal search",
  "icon": "'/images/multi-modal-search2.svg'",
  "author": "Ohad Parush",
  "position": "Chief R&D Officer",
  "logo": "/images/gong-logo.png",
  "content": "      image: /images/ebooks-icon.png       url: /learn/nlp/     - title: The Rise of Vector Data       # text: Lorem ipsum dolor sit amet consectetuer elit gravidas.       image: /images/blogs-icon.png       url: /learn/rise-vector-data/     - title: What is a vector database?        # text: Lorem ipsum dolor sit amet consectetuer elit gravidas rrerum adipliscit elit.       image: /images/blogs-icon.png       url: /learn/vector-database/     - title: \"Webinar: Semantic Search with NLP and a Vector Database\"       # text: Lorem ipsum dolor sit amet consectetuer elit gravidas.       image: /images/webinars-icon.png       url: https://www.youtube.com/watch?v=7RF03_WQJpQ  # custom class for styling customClass: ai-search --- ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e384"
  },
  "content": "",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e386"
  },
  "title": "Partners and integrations",
  "url": "https://perception-point.io",
  "description": "\"Pinecone made it fast and easy to take vector search from research to production, without DevOps. Thanks to their performance and scalability we can provide accurate, reliable, and real-time threat detection to our customers.\"",
  "href": "/contact",
  "text": "Detect similar security alerts to help your developer triage events faster.",
  "src": "/images/security-use-case.svg",
  "alt": "Deepset AI",
  "width": "190",
  "eyebrow": "Use cases",
  "- title": "Observability",
  "icon": "'/images/observability.svg'",
  "logo": "/images/perception-point-logo.svg",
  "authorName": "Oded K.",
  "authorTitle": "R&D Group Lead",
  "authorImage": "/images/oded-kalev.jpeg",
  "authorUrl": "https://www.linkedin.com/in/oded-kalev-9a4667ba/",
  "content": "# Learn More learnMoreSection:   title: Learn More   cards:      - title: Expel case study       # text: Lorem ipsum dolor sit amet consectetuer elit gravidas.       image: /images/blogs-icon.png       url: /learn/expel-alert-similarity/      - title: Threat detection example app       # text: Lorem ipsum dolor sit amet consectetuer elit gravidas.       image: /images/documents-icon.png       url: https://docs.pinecone.io/docs/it-threat-detection      - title: Security webinar/panel recording        # text: Lorem ipsum dolor sit amet consectetuer elit gravidas rrerum adipliscit elit.       image: /images/videos-icon.png       url: https://www.youtube.com/watch?v=mWplVuntklI      # - title: \"Webinar: Semantic Search with NLP and a Vector Database\"     #   # text: Lorem ipsum dolor sit amet consectetuer elit gravidas.     #   image: /images/search-like-you-mean-it-thumbnail.jpg     #   url: https://www.youtube.com/watch?v=7RF03_WQJpQ  # custom class for styling  customClass: security-use-case --- ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e388"
  },
  "title": "Perception Point",
  "intro": "\"The #1 vector database. Search through billions of items for similar matches to any object, in milliseconds. It’s the next generation of search, an API call away.\"",
  "description": "\"Pinecone made it fast and easy to take vector search from research to production, without DevOps. Thanks to their performance and scalability we can provide accurate, reliable, and real-time threat detection to our customers.\"",
  "href": "/contact/",
  "text": "Pinecone is SOC 2 Type II certified, GDPR-ready, and built to keep data secure.",
  "src": "/images2/pinecone-diagram.svg",
  "alt": "\"co:here\"",
  "width": "447",
  "height": "548",
  "eyebrow": "Managed Vector Search",
  "- title": "Secure",
  "icon": "'/images/secure.svg'",
  "logo": "/images/perception-point-logo.svg",
  "url": "https://perception-point.io",
  "authorName": "Oded K.",
  "authorTitle": "R&D Group Lead",
  "authorImage": "/images/oded-kalev.jpeg",
  "authorUrl": "https://www.linkedin.com/in/oded-kalev-9a4667ba/",
  "- logo": "/images/cohere.png",
  "content": "",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e38a"
  },
  "title": "Talk to the <span>vector search experts</span>",
  "intro": "\"Search through billions of items for similar matches to any object, in milliseconds. It’s the next generation of search, an API call away.\"",
  "description": "Send us your questions about choosing between Pinecone and Weaviate. We’ll schedule a time to learn and share more with you.",
  "href": "/contact/",
  "text": "Pinecone is SOC 2 Type II certified, GDPR-ready, and built to keep data secure.",
  "src": "/images2/pinecone-diagram.svg",
  "alt": "\"co:here\"",
  "width": "447",
  "height": "548",
  "eyebrow": "Production-Ready Managed Service",
  "- title": "Secure",
  "icon": "'/images/secure.svg'",
  "logo": "/images/perception-point-logo.svg",
  "url": "https://perception-point.io",
  "authorName": "Oded K.",
  "authorTitle": "R&D Group Lead",
  "authorImage": "/images/oded-kalev.jpeg",
  "authorUrl": "https://www.linkedin.com/in/oded-kalev-9a4667ba/",
  "- logo": "/images/cohere.png",
  "info": "|",
  "content": "",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e38c"
  },
  "title": "Perception Point",
  "intro": "\"The #1 vector database. Search through billions of items for similar matches to any object, in milliseconds. It’s the next generation of search, an API call away.\"",
  "description": "\"Pinecone made it fast and easy to take vector search from research to production, without DevOps. Thanks to their performance and scalability we can provide accurate, reliable, and real-time threat detection to our customers.\"",
  "href": "/contact/",
  "text": "Pinecone is SOC 2 Type II certified, GDPR-ready, and built to keep data secure.",
  "src": "/images2/pinecone-diagram.svg",
  "alt": "\"co:here\"",
  "width": "447",
  "height": "548",
  "eyebrow": "Managed Vector Database",
  "- title": "Secure",
  "icon": "'/images/secure.svg'",
  "logo": "/images/perception-point-logo.svg",
  "url": "https://perception-point.io",
  "authorName": "Oded K.",
  "authorTitle": "R&D Group Lead",
  "authorImage": "/images/oded-kalev.jpeg",
  "authorUrl": "https://www.linkedin.com/in/oded-kalev-9a4667ba/",
  "- logo": "/images/cohere.png",
  "content": "",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e38e"
  },
  "title": "Talk to the <span>vector search experts</span>",
  "intro": "\"Search through billions of items for similar matches to any object, in milliseconds. It’s the next generation of search, an API call away.\"",
  "description": "Send us your questions about choosing between Pinecone and OpenSearch on AWS. We’ll schedule a time to learn and share more with you.",
  "href": "/contact/",
  "text": "Pinecone is SOC 2 Type II certified, GDPR-ready, and built to keep data secure.",
  "src": "/images2/pinecone-diagram.svg",
  "alt": "\"co:here\"",
  "width": "447",
  "height": "548",
  "eyebrow": "Scalable Managed Service",
  "- title": "Secure",
  "icon": "'/images/secure.svg'",
  "logo": "/images/perception-point-logo.svg",
  "url": "https://perception-point.io",
  "authorName": "Oded K.",
  "authorTitle": "R&D Group Lead",
  "authorImage": "/images/oded-kalev.jpeg",
  "authorUrl": "https://www.linkedin.com/in/oded-kalev-9a4667ba/",
  "- logo": "/images/cohere.png",
  "info": "|",
  "content": "",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e390"
  },
  "title": "Talk to the <span>vector search experts</span>",
  "intro": "\"Search through billions of items for similar matches to any object, in milliseconds. It’s the next generation of search, an API call away.\"",
  "description": "Send us your questions about choosing between Pinecone and Milvus. We’ll schedule a time to learn and share more with you.",
  "href": "/contact/",
  "text": "Pinecone is SOC 2 Type II certified, GDPR-ready, and built to keep data secure.",
  "src": "/images2/pinecone-diagram.svg",
  "alt": "\"co:here\"",
  "width": "447",
  "height": "548",
  "eyebrow": "Production-Ready Managed Service",
  "- title": "Secure",
  "icon": "'/images/secure.svg'",
  "logo": "/images/perception-point-logo.svg",
  "url": "https://perception-point.io",
  "authorName": "Oded K.",
  "authorTitle": "R&D Group Lead",
  "authorImage": "/images/oded-kalev.jpeg",
  "authorUrl": "https://www.linkedin.com/in/oded-kalev-9a4667ba/",
  "- logo": "/images/cohere.png",
  "info": "|",
  "content": "",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e392"
  },
  "title": "Talk to the <span>vector search experts</span>",
  "intro": "\"Search through billions of items for similar matches to any object, in milliseconds. It’s the next generation of search, an API call away.\"",
  "description": "Send us your questions about choosing between Pinecone and Elastic. We’ll schedule a time to learn and share more with you.",
  "href": "/contact/",
  "text": "Pinecone is SOC 2 Type II certified, GDPR-ready, and built to keep data secure.",
  "src": "/images2/pinecone-diagram.svg",
  "alt": "\"co:here\"",
  "width": "447",
  "height": "548",
  "eyebrow": "Scalable Managed Service",
  "- title": "Secure",
  "icon": "'/images/secure.svg'",
  "logo": "/images/perception-point-logo.svg",
  "url": "https://perception-point.io",
  "authorName": "Oded K.",
  "authorTitle": "R&D Group Lead",
  "authorImage": "/images/oded-kalev.jpeg",
  "authorUrl": "https://www.linkedin.com/in/oded-kalev-9a4667ba/",
  "- logo": "/images/cohere.png",
  "info": "|",
  "content": "",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e394"
  },
  "title": "Dev-friendly alternative to Google Vertex AI Matching Engine",
  "intro": "\"Search through billions of items for similar matches to any object, in milliseconds. It’s the next generation of search, an API call away.\"",
  "description": "Pinecone makes it easy to build high-performance vector search applications. It’s a managed, cloud-native vector database with a simple API that you can start using and scaling within minutes. Developers love the simple API, full metadata filtering, extensive documentation, and low cost compared to Vertex AI Matching Engine from Google.",
  "href": "/contact/",
  "text": "or contact us",
  "src": "/images2/pinecone-diagram.svg",
  "alt": "Pinecone Diagram",
  "width": "447",
  "height": "548",
  "content": "  heading2: Pinecone   list:     - category: Cost by index size       list:         - item: Data processing                      selfHosted: \"$3 per GiB\"           pinecone: Free         - item: 100k vectors                      selfHosted: $1.064           pinecone: $0.070         - item: 500k vectors                      selfHosted: $1.064           pinecone: $0.070         - item: 1M vectors                      selfHosted: $1.064           pinecone: $0.070         - item: 5M vectors                      selfHosted: $1.064           pinecone: $0.075         - item: 10M vectors                      selfHosted: $1.064           pinecone: $0.150     - category: Features       list:         - item: Vector search (ANN)                      selfHosted: true           pinecone: true         - item: Exact vector search (kNN)                      selfHosted: true           pinecone: true         - item: Real-time Index Updates                      selfHosted: false           pinecone: true         - item: Full metadata filtering                      selfHosted: false           pinecone: true         - item: Namespacing                      selfHosted: false           pinecone: true         - item: Query diversification                      selfHosted: true           pinecone: Coming soon         - item: Hybrid search (sparse + dense vectors)                      selfHosted: false           pinecone: Coming soon         - item: Run in GCP                      selfHosted: true           pinecone: true         - item: Run in AWS                      selfHosted: false           pinecone: true     - category: Support       list:         - item: Premium support                      selfHosted: Costs $29–$12.5k/month + 3–4% of monthly charges           pinecone: Included   ctaBtn:     url: https://app.pinecone.io     text: Get Started  # Markdown section markdownSection: |   ### Vertex AI    [Vertex AI Matching Engine](https://cloud.google.com/vertex-ai/docs/matching-engine/overview) is a subset of Vertex AI services for semantic search. It includes (1) a set of model architectures for training embeddings, and (2) an approximate nearest neighbor (ANN) service. The two parts can be used together or separately. For instance, it is possible to use the Matching Engine’s model training capabilities to generate embeddings, and then use these embeddings elsewhere (eg, in Pinecone). [The Vertex AI Matching Engine ANN service](https://cloud.google.com/vertex-ai/docs/matching-engine/ann-service-overview) is based on ScaNN, Google’s ANN algorithm.    Besides the comparatively high cost and limited features, a common complaint from those who’ve tried it is the developer experience of using Vertex AI and its documentation. For instance, managing indexes is more complex than other similar services because it requires three steps: Creating the index (45+ minutes), creating index endpoints, and deploying the index (45+ minutes).     Quotes from users who’ve tried Matching Enginge:   “The [Matching Engine] API is painful to use.”   “[Matching Enginge] takes several hours to build a new index after every update.”   “For 2 indexes of 10M 768-dims, [Matching Engine] is $1,700/month. Pinecone is $216/month.”   “It's hard to set up an experiment”   “... Pinecone is much easier to use than Google Vertex Matching.”    ### Pinecone    [Pinecone](https://www.pinecone.io) makes it easy to build high-performance vector search applications. It’s a managed, cloud-native vector database with a simple API and no infrastructure hassles.    Pinecone does not host or run embeddings models. You need to run your embedding models and perform your transformations outside of Pinecone, and only then you can upload your embeddings into Pinecone.    Pinecone is cost-effective and provides great performance at scale. However, customers say **developer experience** is the biggest reason they chose Pinecone. The simple API, web console, and documentation make it easy and quick to deploy and manage the vector database.  # Use cases usecases:   eyebrow: Use Cases   title: What can you do with <span>vector search</span>?   description: |     Once you have <a href=\"/learn/vector-embeddings/\">vector embeddings</a>, manage and search through them in Pinecone to power <a href=\"/semantic-search/\">semantic search</a>, recommenders, and other applications that rely on relevant information retrieval.   list:     - title: Search       icon: \"/images/semantic-text-search.svg\"       children:         - Semantic search         - Product search         - Multi-modal search         - Question-Answering      - title: Generation       icon: \"/images/deduplication2.svg\"       children:         - Chatbots         - Text generation         - Image generation      - title: Security       icon: \"/images/fraud-detection.svg\"       children:         - Anomaly Detection         - Fraud Detection         - Bot/threat detection         - Identity verification      - title: Personalization       icon: \"/images/personalization-alt.svg\"       children:         - Recommendations         - Feed ranking         - Ad targeting         - Candidate selection      - title: Analytics & ML       icon: \"/images/model-training.svg\"       children:         - Data labeling         - Model training         - Molecular search         - Generative AI      - title: Data Management       icon: \"/images/data-labeling.svg\"       children:         - Pattern matching         - Deduplication         - Grouping         - Tagging  # Why Pinecone features:   eyebrow: Why Pinecone   title: Fast, fresh, and filtered vector search.   list:     - title: Fast       text:       - Ultra-low query latency, even with billions of items. Give users a great experience.     - title: Fresh       text:       - Live index updates when you add, edit, or delete data. Your data is ready right away.     - title: Filtered       text:       - Combine vector search with metadata filters for more relevant and faster results.  # Features features2:   eyebrow: Managed Service   title: |     Fully managed,</br><span>production-ready</span>   text: Launch, use, and scale your vector search service with our easy API. No worrying about managing Google infrastructure or setting up API endpoints in Vertex.   list:     - title: Easy to use       text: Get started on the free plan with an easy-to-use API or the Python client.       icon: '/images/easy-to-use.svg'     - title: Scalable       text: Scale from zero to billions of items, with no downtime and minimal latency impact.       icon: '/images/scalable.svg'     - title: Pay for what you use       text: Start free, then pay only for what you use with usage-based pricing.       icon: '/images/usage.svg'     - title: No ops overhead       text: No need to maintain infrastructure, monitor services, or troubleshoot algorithms.       icon: '/images/gear.svg'     - title: Reliable       text: Choose a cloud provider and region — we'll take care of uptime, consistency, and the rest.       icon: '/images/cloud.svg'     - title: Secure       text: Pinecone is SOC 2 Type II certified, GDPR-ready, and built to keep data secure.       icon: '/images/secure.svg'  # Customers customers:   testimonial:     title: Perception Point     logo: /images/perception-point-logo.svg     url: https://perception-point.io     description: \"Pinecone made it fast and easy to take vector search from research to production, without DevOps. Thanks to their performance and scalability we can provide accurate, reliable, and real-time threat detection to our customers.\"     authorName: Oded K.     authorTitle: R&D Group Lead     authorImage: /images/oded-kalev.jpeg     authorUrl: https://www.linkedin.com/in/oded-kalev-9a4667ba/   logos:     - logo: /images/gong-logo.png       alt: Gong     - logo: /images/clubhouse-logo.png       alt: Clubhouse     - logo: /images/klue-logo.svg       alt: Klue     - logo: /images/expel-logo.png       alt: Expel     - logo: /images/course-hero.png       alt: Course Hero     - logo: /images/circleup-logo.svg       alt: CircleUp     - logo: /images/xandr-logo.png       alt: Xandr     - logo: /images/bamboohr-logo.svg       alt: BambooHR     - logo: /images/mem-logo.svg       alt: Mem     - logo: /images/cohere.png       alt: \"co:here\"  # Contact hero2:   title: Talk to our experts about <span>Pinecone vs. Vertex AI Matching Engine from Google</span>   description: Send us your questions about Pinecone and why developers choose us over Vertex AI Matching Engine from Google. We’ll schedule a time to learn and share more with you. contact:     enabled: true     info: |         Want to try Pinecone?<br>         [Start here.](/start/)          Press and general inquiries:<br>         info@pinecone.io          Support:<br>         support@pinecone.io<br>         [Read the Docs](/docs/) --- ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e396"
  },
  "title": "Talk to the <br><span>Vector Search</span> Experts",
  "intro": "\"Search through billions of items for similar matches to any object, in milliseconds. It’s the next generation of search, an API call away.\"",
  "description": "Send us your questions about Pinecone or details about your vector search needs. We’ll schedule a time to learn and share more with you.",
  "href": "/contact/",
  "text": "Pinecone is SOC 2 Type II certified, GDPR-ready, and built to keep data secure.",
  "src": "/images2/pinecone-diagram.svg",
  "alt": "\"co:here\"",
  "width": "447",
  "height": "548",
  "url": "https://perception-point.io",
  "eyebrow": "Managed Service",
  "- title": "Secure",
  "icon": "'/images/secure.svg'",
  "logo": "/images/perception-point-logo.svg",
  "authorName": "Oded K.",
  "authorTitle": "R&D Group Lead",
  "authorImage": "/images/oded-kalev.jpeg",
  "authorUrl": "https://www.linkedin.com/in/oded-kalev-9a4667ba/",
  "- logo": "/images/cohere.png",
  "info": "|",
  "content": "",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e398"
  },
  "title": "Perception Point",
  "intro": "\"The #1 vector database. Search through billions of items for similar matches to any object, in milliseconds. It’s the next generation of search, an API call away.\"",
  "description": "\"Pinecone made it fast and easy to take vector similarity search from research to production, without DevOps. Thanks to their performance and scalability we can provide accurate, reliable, and real-time threat detection to our customers.\"",
  "href": "/contact/",
  "text": "Pinecone is SOC 2 Type II certified, GDPR-ready, and built to keep data secure.",
  "src": "/images2/pinecone-diagram.svg",
  "alt": "\"co:here\"",
  "width": "447",
  "height": "548",
  "eyebrow": "Managed Vector Similarity Search",
  "- title": "Secure",
  "icon": "'/images/secure.svg'",
  "logo": "/images/perception-point-logo.svg",
  "url": "https://perception-point.io",
  "authorName": "Oded K.",
  "authorTitle": "R&D Group Lead",
  "authorImage": "/images/oded-kalev.jpeg",
  "authorUrl": "https://www.linkedin.com/in/oded-kalev-9a4667ba/",
  "- logo": "/images/cohere.png",
  "content": "",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e39a"
  },
  "title": "Talk to the <br><span>Vector Search</span> Experts",
  "intro": "\"Search through billions of items for similar matches to any object, in milliseconds. It’s the next generation of search, an API call away.\"",
  "description": "Send us your questions about Pinecone or details about your vector search needs. We’ll schedule a time to learn and share more with you.",
  "href": "/contact/",
  "text": "Pinecone is SOC 2 Type II certified, GDPR-ready, and built to keep data secure.",
  "src": "/images2/pinecone-diagram.svg",
  "alt": "\"co:here\"",
  "width": "447",
  "height": "548",
  "url": "https://perception-point.io",
  "eyebrow": "Managed Service",
  "- title": "Secure",
  "icon": "'/images/secure.svg'",
  "logo": "/images/perception-point-logo.svg",
  "authorName": "Oded K.",
  "authorTitle": "R&D Group Lead",
  "authorImage": "/images/oded-kalev.jpeg",
  "authorUrl": "https://www.linkedin.com/in/oded-kalev-9a4667ba/",
  "- logo": "/images/cohere.png",
  "info": "|",
  "content": "",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e39c"
  },
  "title": "OpenAI ML QA",
  "description": "A Q&A tool that takes discussions and docs from some of the best Python ML libraries and collates their content into a natural language search and Q&A tool.",
  "images": "[\"/images/apps-image2.jpg\"]",
  "content": "<iframe     src=\"https://pinecone-openai-ml-qa.hf.space\"     frameborder=\"0\"     width=\"100%\"     height=\"700\" ></iframe> ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e39e"
  },
  "title": "Find Your Celebrity Match",
  "description": "An image-based vector search application which can be used to discover celebrities with similar facial features.",
  "images": "[\"/images/apps-image1.jpg\"]",
  "content": "<iframe \tsrc=\"https://pinecone-find-your-celebrity-match.hf.space\" \tallow=\"camera\" \tframeborder=\"0\" \twidth=\"100%\" \theight=\"700\" ></iframe> ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e3a0"
  },
  "title": "Diffusion Image Search",
  "description": "Search through stable diffusion images using natural language prompts.",
  "images": "[\"/images/apps-diffusion-image-search.png\"]",
  "content": "<iframe \tsrc=\"https://pinecone-diffusion-image-search.hf.space\" \tframeborder=\"0\" \twidth=\"100%\" \theight=\"700\" ></iframe> ",
  "source": "www"
},{
  "_id": {
    "$oid": "63e26bb3f2e6c738d0f7e3a2"
  },
  "title": "Application <span>Showcase</span>",
  "description": "A discovery engine to explore topics that intrigue you.",
  "- title": "Mood Surf",
  "image": "/images/apps-moodsurf.jpg",
  "url": "https://mood.surf/",
  "content": "",
  "source": "www"
}]