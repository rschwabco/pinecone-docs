[{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccb8"
  },
  "filename": "examples.md",
  "title": "Examples",
  "category": "630fc5235d91a70054705fb7",
  "content": "---\ntitle: Examples\ncategory: 630fc5235d91a70054705fb7\n---\nThese examples demonstrate how you might build vector search into your\napplications with Pinecone. You can view their source code to jumpstart your\nown application.\n\nOur [Learn section](https://pinecone.io/learn) explains the basics of vector search and vector databases.\n\n# Semantic search\n\n<div class=\"example-cards\">\n\n  <!-- Semantic text search -->\n  <a href=\"/docs/semantic-text-search\">\n    <span>Semantic text search</span>\n    <p>How to create a simple semantic text search using Pinecone's similarity search service.</p>\n  </a>\n\n  <!-- Basic hybrid search -->\n  <a href=\"/docs/basic-hybrid-search\">\n    <span>Basic hybrid search</span>\n    <p>How to pair semantic search with a basic keyword filter.</p>\n  </a>\n\n  <!-- Generative QA with OpenAI -->\n  <a href=\"/docs/gen-qa-openai\">\n    <span>Generative question answering (OpenAI)</span>\n    <p>How to build retrieval enhanced generative QA systems with OpenAI.</p>\n  </a>\n\n  <!-- Extractive question answering -->\n  <a href=\"/docs/extractive-question-answering\">\n    <span>Extractive question answering</span>\n    <p>How to build an extractive QA application with similarity search.</p>\n  </a>\n\n  <!-- Abstractive question answering -->\n  <a href=\"/docs/abstractive-question-answering\">\n    <span>Abstractive question answering</span>\n    <p>How to build an abstractive (generative) QA application with similarity search.</p>\n  </a>\n\n  <!-- Tabular question answering -->\n  <a href=\"/docs/table-qa\">\n    <span>Tabular question answering</span>\n    <p>How to build a question-answering application for extracting answers from tables with similarity search.</p>\n  </a>\n\n  <!-- NER search -->\n  <a href=\"/docs/ner-search\">\n    <span>NER search</span>\n    <p>How to automatically extract entities from text and use them to improve search results.</p>\n  </a>\n\n  <!-- Video transcription search -->\n  <a href=\"/docs/video-search\">\n    <span>Video transcription search</span>\n    <p>How to create an app that searches video transcription data.</p>\n  </a>\n\n  <!-- GIF description search -->\n  <a href=\"/docs/gif-search\">\n    <span>GIF description search</span>\n    <p>How to create a GIF search app.</p>\n  </a>\n</div>\n\n# Image, audio, and video search\n\n<div class=\"example-cards\">\n  <!-- Image similarity search -->\n  <a href=\"/docs/image-similarity-search\">\n    <span>Image similarity search</span>\n    <p>How to build advanced image search applications.</p>\n  </a>\n\n  <!-- Facial similarity search -->\n  <a href=\"/docs/facial-similarity-search\">\n    <span>Facial similarity search</span>\n    <p>Find your celebrity doppelganger with facial similarity search.</p>\n  </a>\n\n  <!-- Audio similarity search -->\n  <a href=\"/docs/audio-search\">\n    <span>Audio similarity search</span>\n    <p>How to build advanced audio search applications.</p>\n  </a>\n</div>\n\n# Recommendation systems\n\n<div class=\"example-cards\">\n\n  <!-- Personalized content recommendations -->\n  <a href=\"/docs/personalized-content-recommendations\">\n    <span>Personalized content recommendations</span>\n    <p>How to use Pinecone to create a simple personalized article or content recommender. </p>\n  </a>\n\n  <!-- Movie recommender -->\n  <a href=\"/docs/movie-recommender\">\n    <span>Movie recommender</span>\n    <p>How to create a movie recommendation system with the MovieLens dataset.</p>\n  </a>\n</div>\n\n# Other examples\n\n<div class=\"example-cards\">\n\n  <!-- Document deduplication -->\n  <a href=\"/docs/document-deduplication\">\n    <span>Document deduplication</span>\n    <p>How to create a simple application for identifying duplicate documents.</p>\n  </a>\n\n  <!-- IT threat detection -->\n  <a href=\"/docs/it-threat-detection\">\n    <span>IT threat detection</span>\n    <p>How to build an application for detecting rare events in IT threat detection.</p>\n  </a>\n\n  <!-- Extreme classification -->\n  <a href=\"/docs/extreme-classification\">\n    <span>Extreme classification</span>\n    <p>How to label new texts automatically when there is an enormous number of potential labels.</p>\n  </a>\n\n  <!-- Time series similarity search -->\n  <a href=\"/docs/time-series\">\n    <span>Time series similarity search</span>\n    <p>How to perform time-series \"pattern\" matching using a similarity search service.</p>\n  </a>\n</div>\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccb9"
  },
  "filename": "manage-indexes.md",
  "title": "Manage indexes",
  "category": "630fc5235d91a70054705fb6",
  "content": "---\ntitle: Manage indexes\ncategory: 630fc5235d91a70054705fb6\n---\n\nIn this section, we explain how you can get a list of your indexes, create an index, delete an index, and describe an index.\n\nTo learn about the concepts related to indexes, see [Indexes](indexes).\n\n> ⚠️  Warning\n>\n> Indexes on the Starter (free) plan are deleted after 7 days of inactivity. To\n> prevent this, send any API request or log into the console. This will count\n> as activity.\n\n<html><iframe width=\"450\" height=\"253\" src=\"https://www.youtube.com/embed/DCQrrnFbLt8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></html>\n\n## Getting information on your indexes\n\nList all your Pinecone indexes:\n\n```python\npinecone.list_indexes()\n```\n```shell curl\ncurl -i https://controller.YOUR_ENVIRONMENT.pinecone.io/databases \\\n  -H 'Api-Key: YOUR_API_KEY'\n```\n\nGet the configuration and current status of an index named \"pinecone-index\":\n\n```python\npinecone.describe_index(\"pinecone-index\")\n```\n```shell curl\ncurl -i -X GET https://controller.YOUR_ENVIRONMENT.pinecone.io/databases/example-index \\\n  -H 'Api-Key: YOUR_API_KEY'\n```\n\n\n## Creating an index\n\nThe simplest way to create an index is as follows. This gives you an index with a single pod that will perform approximate nearest neighbor (ANN) search using cosine similarity:\n\n```python\npinecone.create_index(\"example-index\", dimension=128)\n```\n```shell curl\ncurl -i -X POST https://controller.YOUR_ENVIRONMENT.pinecone.io/databases \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"name\": \"example-index\",\n    \"dimension\": 128\n  }'\n```\n\nA more complex index can be created as follows. This creates an index that measures similarity by Euclidean distance and runs on 4 s1 (storage-optimized) pods of size `x1`:\n\n[//]: # \"TODO: Add these code samples to Docs Code Samples Test.ipynb\"\n\n\n```python\npinecone.create_index(\"example-index\", dimension=128, metric=\"euclidean\", pods=4, pod_type=\"s1.x1\")\n```\n```shell curl\ncurl -i -X POST https://controller.YOUR_ENVIRONMENT.pinecone.io/databases \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"name\": \"example-index\",\n    \"dimension\": 128,\n    \"metric\": \"euclidean\",\n    \"pods\": 4,\n    \"pod_type\": \"p1.x1\"\n  }'\n```\n\n### Create an index from a collection\n\nTo create an index from a [collection](collections), use the [`create_index`](/reference/create_index/) operation and provide a [`source_collection`](/reference/create_index/#!path=source_collection&t=request) parameter containing the name of the collection from which you wish to create an index. The new index is queryable and writable.\n\nCreating an index from a collection generally takes about 10 minutes. Creating a p2 index from a collection can take several hours when the number of vectors is on the order of 1M.\n\n**Example**\n\nThe following example creates an index named `example-index` with 128 dimensions from a collection named `example-collection`.\n\n```python\npinecone.create_index(\"example-index\", dimension=128, source_collection=\"example-collection\")\n```\n```shell curl\ncurl -i -X POST https://controller.us-west1-gcp.pinecone.io/databases \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"name\": \"example-index\",\n    \"source_collection\":\"example-collection\"}\n  }'\n```\n\nFor more information about each pod type and size, see [Indexes](indexes).\n\nFor the full list of parameters available to customize an index, see the [create_index API reference](/reference/create_index/).\n\n### Create an index from a public collection\n\nTo create an index from a [public collection](collections#public-collections-contain-real-world-data), follow these steps:\n\n1. Open the [Pinecone console](https://app.pinecone.io).\n1. Click the name of the project in which you want to create the index.\n1. In the left menu, click **Public Collections**.\n1. Find the public collection from which you want to create an index. Next to that public collection, click **CREATE INDEX**.\n1. When index creation is complete, the console redirects you to view the new index.\n\nTo learn more about using specific public collections, see the example documentation for the [OpenAPI Trec](/integrations/openai), [Cohere Trec](/integrations/cohere), and [SQuAD](extractive-question-answering/) collections.\n\n## Changing pod sizes\n\nThe default pod size is `x1`. After index creation, you can increase the pod size for an index.\n\nIncreasing the pod size of your index does not result in downtime. Reads and writes continue uninterrupted during the scaling process. Currently, you cannot reduce the pod size of your indexes. Your number of replicas and your total number of pods remain the same, but each pod changes size. Resizing completes in about 10 minutes.\n\nTo learn more about pod sizes, see [Indexes](indexes/#pods-pod-types-and-pod-sizes).\n\n### Increasing the pod size for an index\n\nTo change the pod size of an existing index, use the [configure_index](/reference/configure_index/) operation and append the new size to the `pod_type` parameter, separated by a period (.).\n\n**Example**\n\nThe following example assumes that `my_index` has size `x1` and changes the size to `x2`.\n\n```python\npinecone.configure_index(\"my_index\", pod_type=\"s1.x2\")\n```\n```shell curl\ncurl -i -X PATCH https://controller.us-west1-gcp.pinecone.io/databases/example-index \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n                \"pod_type\": \"s1.x2\"\n    }\n  }'\n```\n\n### Checking the status of a pod size change\n\nTo check the status of a pod size change, use the [describe_index](/reference/describe_index/) operation. The `status` field in the results contains the key-value pair `\"state\":\"ScalingUp\"` or `\"state\":\"ScalingDown\"` during the resizing process and the key-value pair `\"state\":\"Ready\"` after the process is complete.\n\nThe index fullness metric provided by [`describe_index_stats`](/reference/describe_index_stats) may be inaccurate until the resizing process is complete.\n\n**Example**\n\nThe following example uses `describe_index` to get the index status of the index `example-index`. The `status` field contains the key-value pair `\"state\":\"ScalingUp\"`, indicating that the resizing process is still ongoing.\n\n```python\npinecone.describe_index(\"example-index\")\n```\n```shell curl\ncurl -i -X GET https://controller.us-west1-gcp.pinecone.io/databases/example-index \\\n  -H 'Api-Key: YOUR_API_KEY'\n```\n\nResults:\n\n```json\n{\n\"database\": {\n\"name\": \"example-index\",\n\"dimensions\": \"768\",\n\"metric\": \"cosine\",\n\"pods\": 6,\n\"replicas\": 2,\n\"shards\": 3,\n\"pod_type\": \"p1.x2\",\n\"index_config\": {},\n\"status\": {\n    \"ready\": true,\n    \"state\": \"ScalingUp\"\n\t}\n}\n}\n```\n\n## Replicas\n\nYou can increase the number of replicas for your index to increase throughput (QPS). All indexes start with replicas=1.\n\n**Example**\n\nThe following example uses the [`configure_index`](/reference/configure_index/) operation to set the number of replicas for the index `example-index` to 4.\n\n```python\npinecone.configure_index(\"example-index\", replicas=4)\n```\n```shell curl\ncurl -i -X PATCH https://controller.us-west1-gcp.pinecone.io/databases/example-index \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"replicas\": 4\n  }'\n```\n\nSee the [configure_index API reference](/referenceconfigure_index/) for more details.\n\n## Selective metadata indexing\n\nBy default, Pinecone indexes all [metadata](metadata-filtering). When you index metadata fields, you can filter vector search queries using those fields. When you store metadata fields without indexing them, you [keep memory utilization low](metadata-filtering/#supported-metadata-types), especially when you have many unique metadata values, and therefore can fit more vectors per pod.\n\nWhen you create a new index, you can specify which metadata fields to index using the `metadata_config` parameter.\n\n```python\nmetadata_config = {\n    \"indexed\": [\"metadata-field-name\"]\n}\n\npinecone.create_index(\"example-index\", dimension=128,\n                      metadata_config=metadata_config)\n```\n```shell curl\ncurl -i -X POST https://controller.YOUR_ENVIRONMENT.pinecone.io/databases \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"name\": \"example-index\",\n    \"dimension\": 128,\n    \"metadata_config\": {\n      \"indexed\": [\"metadata-field-name\"]\n    }\n  }'\n```\n\nThe value for the `metadata_config` parameter is a JSON object containing the names of the metadata fields to index.\n\n```JSON\n{\n    \"indexed\": [\n        \"metadata-field-1\",\n        \"metadata-field-2\",\n        \"metadata-field-n\"\n    ]\n}\n```\n\nWhen you provide a `metadata_config` object, Pinecone only indexes the metadata fields present in that object: any metadata fields absent from the `metadata_config` object are not indexed.\n\nWhen a metadata field is indexed, you can [filter your queries](metadata-filtering) using that metadata field; if a metadata field is not indexed, metadata filtering ignores that field.\n\n### Examples\n\nThe following example creates an index that only indexes the `genre` metadata field. Queries against this index that filter for the `genre` metadata field may return results; queries that filter for other metadata fields behave as though those fields do not exist.\n \n```python\nmetadata_config = {\n    \"indexed\": [\"genre\"]\n}\n\npinecone.create_index(\"example-index\", dimension=128,\n                      metadata_config=metadata_config)\n```\n```shell curl\ncurl -i -X POST https://controller.us-west1-gcp.pinecone.io/databases \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"name\": \"example-index\",\n    \"dimension\": 128,\n    \"metadata_config\": {\n      \"indexed\": [\"genre\"]\n    }\n  }'\n```\n\n## Deleting an index\n\nThis operation will delete all of the data and the computing resources associated with the index.\n\n> ℹ️  Note\n>\n> When you create an index, it runs as a service until you delete it. Users are\n> billed for running indexes, so we recommend you delete any indexes you're not\n> using. This will minimize your costs.\n\nDelete a Pinecone index named \"pinecone-index\":\n\n```python\npinecone.delete_index(\"example-index\")\n```\n\n```shell curl\ncurl -i -X DELETE https://controller.YOUR_ENVIRONMENT.pinecone.io/databases/example-index \\\n  -H 'Api-Key: YOUR_API_KEY'\n```\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccba"
  },
  "filename": "insert-data.md",
  "title": "Insert data",
  "category": "630fc5235d91a70054705fb6",
  "content": "---\ntitle: Insert data\ncategory: 630fc5235d91a70054705fb6\n---\n\nAfter creating a Pinecone index, you can start inserting vector embeddings and metadata into the index.\n\n<html><iframe width=\"450\" height=\"253\" src=\"https://www.youtube.com/embed/HjeW6ed2dmI\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></html>\n\n## Inserting the vectors\n\n1. Connect to the index:\n\n```python\nindex = pinecone.Index(\"pinecone-index\")\n```\n```shell curl\n# Not applicable\n```\n\n2. Insert the data as a list of `(id, vector)` tuples. Use the `Upsert` operation to write vectors into a namespace:\n\n```python\n# Insert sample data (5 8-dimensional vectors)\nindex.upsert([\n    (\"A\", [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]),\n    (\"B\", [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]),\n    (\"C\", [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]),\n    (\"D\", [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]),\n    (\"E\", [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n])\n```\n```shell curl\ncurl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/upsert \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"vectors\": [\n      {\n        \"id\": \"A\",\n        \"values\": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n      },\n      {\n        \"id\": \"B\",\n        \"values\": [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n      },\n      {\n        \"id\": \"C\",\n        \"values\": [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]\n      },\n      {\n        \"id\": \"D\",\n        \"values\": [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\n      },\n      {\n        \"id\": \"E\",\n        \"values\": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n      }\n    ]\n  }'\n```\n\nImmediately after the upsert response is received, vectors may not be visible to queries yet. In most situations, you can check if the vectors have been received by checking for the vector counts returned by `describe_index_stats()` to be updated. This technique may not work if the index has multiple replicas. The database is eventually consistent.\n\n## Batching upserts\n\nFor clients upserting larger amounts of data, you should insert data into an index in batches of 100 vectors or fewer over multiple upsert requests.\n\n**Example**\n\n```python\nimport random\nimport itertools\n\ndef chunks(iterable, batch_size=100):\n    \"\"\"A helper function to break an iterable into chunks of size batch_size.\"\"\"\n    it = iter(iterable)\n    chunk = tuple(itertools.islice(it, batch_size))\n    while chunk:\n        yield chunk\n        chunk = tuple(itertools.islice(it, batch_size))\n\nvector_dim = 128\nvector_count = 10000\n\n# Example generator that generates many (id, vector) pairs\nexample_data_generator = map(lambda i: (f'id-{i}', [random.random() for _ in range(vector_dim)]), range(vector_count))\n\n# Upsert data with 100 vectors per upsert request\nfor ids_vectors_chunk in chunks(example_data_generator, batch_size=100):\n    index.upsert(vectors=ids_vectors_chunk)  # Assuming `index` defined elsewhere\n```\n\n## Sending upserts in parallel\n\nBy default, all vector operations block until the response has been received. But using our client they can be made asynchronous. For the [Batching Upserts](#batching-upserts) example this can be done as follows:\n\n```python\n# Upsert data with 100 vectors per upsert request asynchronously\n# - Create pinecone.Index with pool_threads=30 (limits to 30 simultaneous requests)\n# - Pass async_req=True to index.upsert()\nwith pinecone.Index('example-index', pool_threads=30) as index:\n    # Send requests in parallel\n    async_results = [\n        index.upsert(vectors=ids_vectors_chunk, async_req=True)\n        for ids_vectors_chunk in chunks(example_data_generator, batch_size=100)\n    ]\n    # Wait for and retrieve responses (this raises in case of error)\n    [async_result.get() for async_result in async_results]\n```\n```shell\n# Not applicable\n```\n\nPinecone is thread-safe, so you can launch multiple read requests and multiple write requests in parallel. Launching multiple requests can help with improving your throughput. However, reads and writes can’t be performed in parallel, therefore writing in large batches might affect query latency and vice versa.\n\nIf you experience slow uploads, see [Performance tuning](performance-tuning) for advice.\n\n## Partitioning an index into namespaces\n\nYou can organize the vectors added to an index into partitions, or \"namespaces,\" to limit queries and other vector operations to only one such namespace at a time. For more information, see: [Namespaces](namespaces).\n\n## Inserting vectors with metadata\n\nYou can insert vectors that contain metadata as key-value pairs.\n\nYou can then use the metadata to filter for those criteria when sending the query. Pinecone will search for similar vector embeddings only among those items that match the filter. For more information, see: [Metadata Filtering](metadata-filtering).\n\n[//]: # (This sample code is a copy from metadata-filtering)\n```python\nindex.upsert([\n    (\"A\", [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], {\"genre\": \"comedy\", \"year\": 2020}),\n    (\"B\", [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2], {\"genre\": \"documentary\", \"year\": 2019}),\n    (\"C\", [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3], {\"genre\": \"comedy\", \"year\": 2019}),\n    (\"D\", [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4], {\"genre\": \"drama\"}),\n    (\"E\", [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], {\"genre\": \"drama\"})\n])\n```\n```shell curl\ncurl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/upsert \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"vectors\": [\n      {\n        \"id\": \"A\",\n        \"values\": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n        \"metadata\": {\"genre\": \"comedy\", \"year\": 2020}\n      },\n      {\n        \"id\": \"B\",\n        \"values\": [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],\n        \"metadata\": {\"genre\": \"documentary\", \"year\": 2019}\n      },\n      {\n        \"id\": \"C\",\n        \"values\": [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],\n        \"metadata\": {\"genre\": \"comedy\", \"year\": 2019}\n      },\n      {\n        \"id\": \"D\",\n        \"values\": [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4],\n        \"metadata\": {\"genre\": \"drama\"}\n      },\n      {\n        \"id\": \"E\",\n        \"values\": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n        \"metadata\": {\"genre\": \"drama\"}\n      }\n    ]\n  }'\n```\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccbb"
  },
  "filename": "troubleshooting.md",
  "title": "Troubleshooting",
  "category": "630fc5235d91a70054705fb6",
  "content": "---\ntitle: Troubleshooting\ncategory: 630fc5235d91a70054705fb6\n---\n\nThis section describes common issues and how to solve them. Need help? [Ask your question in our support forum.](https://community.pinecone.io/c/support/) Standard, Enterprise, and Dedicated customers can also [contact support](https://support.pinecone.io) for help.\n\n## Unable to pip install\n\nVersion 3 of Python uses pip3. Use the following commands at the command line (the terminal):\n\n```\npip3 install -U pinecone-client\n```\n\n## Index is missing after inactivity\n\nIndexes on the Starter (free) plan are deleted after 14 days of inactivity. To prevent this, you can send any API request to Pinecone and the counter will reset.\n\n## Slow uploads or high latencies\n\nTo minimize latency when accessing Pinecone:\n\n- Switch to a cloud environment. For example: EC2, GCE, [Google Colab](https://colab.research.google.com), [GCP AI Platform Notebook](https://cloud.google.com/ai-platform-notebooks), or [SageMaker Notebook](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html). If you experience slow uploads or high query latencies, it might be because you are accessing Pinecone from your home network.\n- Consider deploying your application in the same environment as your Pinecone service. For users on the Starter (free) plan, the environment is GCP US-West (Oregon).\n- See [performance tuning](performance-tuning) for more tips.\n\n## High query latencies with batching\n\nIf you're batching queries, try reducing the number of queries per call to 1 query vector. You can make these calls in parallel and expect roughly the same performance as with batching.\n\n## Upsert throttling when using the gRPC client\n\nIt's possible to get write-throttled sooner when upserting using the gRPC index. If you see this often, then we recommend using a backoff algorithm while upserting.\n\n## Pods are full\n\nThere is a limit to how much vector data a single pod can hold. Create an index with more pods to hold more data. Check [the usage estimator](https://www.pinecone.io/pricing/#cost) to determine the *minimum* number of pods, or [contact us](https://www.pinecone.io/contact/) for help estimating and configuring larger indexes.\n\nIf your metadata has high cardinality, such as having a unique value for every vector in a large index, the index will take up more memory than estimated. This could result in the pods being full sooner than you expected. Consider only indexing metadata to be used for filtering, and storing the rest in a separate key-value store.\n\nSee the [Manage Indexes documentation](manage-indexes) for information on how to specify the number of pods for your index.\n\n\n## Security concerns\n\nWe work hard to earn and maintain trust by treating security and reliability as a cornerstone of our company and product. Pinecone is SOC 2 Type II compliant and GDPR-ready. See the [Trust & Security page](https://www.pinecone.io/security/) for more information. [Contact us](https://www.pinecone.io/contact/) to report any security concerns.\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccbc"
  },
  "filename": "manage-data.md",
  "title": "Manage data",
  "category": "630fc5235d91a70054705fb6",
  "content": "---\ntitle: Manage data\ncategory: 630fc5235d91a70054705fb6\n---\n\nIn addition to [inserting](insert-data) and [querying](query-data) data, there are other ways you can interact with vector data in a Pinecone index. This section walks through the various vector operations available.\n\n<html><iframe width=\"450\" height=\"253\" src=\"https://www.youtube.com/embed/cqzWyNWU8oo\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></html>\n\n## Connect to an index\n\nIf you're using a Pinecone client library to access an index, you'll need to open a session with the index:\n\n```python\n# Connect to the index\nindex = pinecone.Index(\"pinecone-index\")\n```\n```shell curl\n# Not applicable\n```\n\n## Specify an index endpoint\n\nPinecone indexes each have their own DNS endpoint. For cURL and other direct\nAPI calls to a Pinecone index, you'll need to know the dedicated endpoint for\nyour index.\n\nIndex endpoints take the following form:\n\n`https://{index-name}-{project-name}.svc.YOUR_ENVIRONMENT.pinecone.io`\n\n+ `{index-name}` is the name you gave your index when you created it.\n+ `{project-name}` is the Pinecone project name that your API key is associated\n  with. This can be retrieved using the `whoami` operation below.\n+ `YOUR_ENVIRONMENT` is the [cloud region for your Pinecone project](projects#project-environment)..\n\n### Call `whoami` to retrieve your project name.\n\nThe following command retrieves your Pinecone project name.\n\n```python\npinecone.whoami()\n```\n```shell curl\ncurl -i https://controller.YOUR_ENVIRONMENT.pinecone.io/actions/whoami -H 'Api-Key: YOUR_API_KEY'\n```\n\n## Describe index statistics\n\nGet statistics about an index, such as vector count per namespace:\n\n```python\nindex.describe_index_stats()\n```\n```shell curl\ncurl -i -X GET https://YOUR_INDEX-PROJECT_NAME.svc.YOUR_ENVIRONMENT.pinecone.io/describe_index_stats \\\n  -H 'Api-Key: YOUR_API_KEY'\n```\n\n## Fetching vectors\n\nThe `Fetch` operation looks up and returns vectors, by id, from an index. The returned vectors include the vector data and/or metadata. Typical fetch latency is under 5ms.\n\nFetch items by their ids:\n\n```python\nindex.fetch([\"id-1\", \"id-2\"])\n\n# Returns:\n# {'namespace': '',\n#  'vectors': {'id-1': {'id': 'id-1',\n#                       'values': [0.568879, 0.632687092, 0.856837332, ...]},\n#              'id-2': {'id': 'id-2',\n#                       'values': [0.00891787093, 0.581895, 0.315718859, ...]}}}\n```\n```shell curl\ncurl -i -X GET \"https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/fetch?ids=id-1&ids=id-2\" \\\n  -H 'Api-Key: YOUR_API_KEY'\n# Output:\n# {\n#   \"vectors\": {\n#     \"id-1\": {\n#       \"id\": \"id-1\",\n#       \"values\": [0.568879, 0.632687092, 0.856837332, ...]\n#     },\n#     \"id-2\": {\n#       \"id\": \"id-2\",\n#       \"values\": [0.00891787093, 0.581895, 0.315718859, ...]\n#     }\n#   },\n#   \"namespace\": \"\"\n# }\n```\n\n## Updating vectors\n\nThere are two methods for updating vectors and metadata, using *full* or *partial* updates.\n\n### Full update\n\nFull updates modify the entire item, that is vectors and metadata. Updating an item by id is done the same way as [inserting items](insert-data). (Write operations in Pinecone are [idempotent](https://en.wikipedia.org/wiki/Idempotence).)\n\nThe `Upsert` operation writes vectors into an index.\n\n> ℹ️  Note\n>\n> If a new value is upserted for an existing vector id, it will overwrite the\n> previous value.\n\n1. Update the value of the item `(\"id-3\", [3.3, 3.3])`:\n\n```python\nindex.upsert([(\"id-3\", [3.3, 3.3])])\n```\n```shell curl\ncurl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/upsert \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"vectors\": [\n      {\n        \"id\": \"id-0\",\n        \"values\": [3.3, 3.3]\n      }\n    ]\n  }'\n```\n2.  Fetch the item again. We should get `(\"id-3\", [3.3, 3.3])`:\n\n```python\nindex.fetch([\"id-3\"])\n```\n```shell curl\ncurl -i -X GET https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/fetch?ids=id-3 \\\n  -H 'Api-Key: YOUR_API_KEY'\n```\n\n### Partial update\n\nThe `Update` operation performs partial updates that allow changes to *part* of an item. Given an id, we can update the vector value with the `values` argument or update metadata with the `set_metadata` argument.\n\n> ⚠️  Warning\n>\n> The `Update` operation does not validate the existence of ids within an\n> index. If a non-existent id is given then no changes are made and a `200 OK`\n> will be returned.\n\nTo update the value of item `(\"id-3\", [3., 3.], {\"type\": \"doc\", \"genre\": \"drama\"})`:\n\n```python\nindex.update(id=\"id-3\", values=[4., 2.])\n```\n```shell curl\ncurl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/update \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n       \"id\": \"id-3\",\n       \"values\": [\n          3.3,\n          3.3\n       ]\n     }'\n```\n\nThe updated item would now be `(\"id-3\", [4., 2.], {\"type\": \"doc\", \"genre\": \"drama\"})`.\n\nWhen updating metadata only specified fields will be modified. If a specified field does not exist, it is added.\n\n> ℹ️  Note\n>\n> Metadata updates apply *only* to fields passed to the `set_metadata`\n> argument. Any other fields will remain unchanged.\n\nTo update the metadata of item `(\"id-3\", [4., 2.], {\"type\": \"doc\", \"genre\": \"drama\"})`:\n\n```python\nindex.update(id=\"id-3\", set_metadata={\"type\": \"web\", \"new\": \"true\"})\n```\n```shell curl\ncurl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/update \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n       \"id\": \"id-3\",\n       \"setMetadata\": {\n          \"type\": \"web\",\n          \"new\": \"true\"\n       }\n     }'\n```\n\nThe updated item would now be `(\"id-3\", [4., 2.], {\"type\": \"web\", \"genre\": \"drama\", \"new\": \"true\"})`.\n\nBoth vector and metadata can be updated at once by including both `values` and `set_metadata` arguments. To update the `\"id-3\"` item we write:\n\n```python\nindex.update(id=\"id-3\", values=[1., 2.], set_metadata={\"type\": \"webdoc\"})\n```\n```shell curl\ncurl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/update \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n        \"id\": \"id-3\",\n        \"values\": [1., 2.],\n        \"set_metadata\": {\"type\": \"webdoc\"}\n      }\n  }'\n```\n\nThe updated item would now be `(\"id-3\", [1., 2.], {\"type\": \"webdoc\", \"genre\": \"drama\", \"new\": \"true\"})`.\n\n## Deleting vectors\n\nThe `Delete` operation deletes vectors, by ID, from an index.\n\nAlternatively, it can also delete all vectors from an index or [namespace](namespaces).\n\nWhen deleting large numbers of vectors, limit the scope of delete operations to hundreds of vectors per operation.\n\nInstead of deleting all vectors in an index, [delete the index](https://www.pinecone.io/docs/manage-indexes/#deleting-an-index) and [recreate it](https://www.pinecone.io/docs/manage-indexes/#creating-an-index).\n\n### Delete vectors by ID\n\nTo delete vectors by their IDs, specify an `ids` parameter to `delete`. The `ids` parameter is an array of strings containing vector IDs.\n\n**Example**\n\n```python\nindex.delete(ids=[\"id-1\", \"id-2\"], namespace='example-namespace')\n```\n```shell curl\ncurl -i -X DELETE \"https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io.pinecone.io/vectors/delete?ids=id-1&ids=id-2&namespace=example-namespace\" \\\n  -H 'Api-Key: YOUR_API_KEY'\n```\n\n### Delete vectors by namespace\n\nTo delete all vectors from a namespace, specify `delete_all=True` and provide a\n`namespace` parameter.\n\n> ℹ️  Note\n>\n> If you delete all vectors from a single namespace, it will also delete the\n> namespace.\n\n**Example**:\n\n```python\nindex.delete(delete_all=True, namespace='example-namespace')\n```\n```shell curl\ncurl -i -X DELETE \"https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/delete?delete_all=true&namespace=example-namespace\" \\\n  -H 'Api-Key: YOUR_API_KEY'\n```\n\n### Delete vectors by metadata\n\nTo delete vectors by metadata, [pass a metadata filter expression to the delete operation](https://www.pinecone.io/docs/metadata-filtering/#deleting-vectors-by-metadata-filter).\n\n\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccbd"
  },
  "filename": "performance-tuning.md",
  "title": "Performance tuning",
  "category": "630fc5235d91a70054705fb6",
  "content": "---\ntitle: Performance tuning\ncategory: 630fc5235d91a70054705fb6\n---\n\nThis section provides some tips for getting the best performance out of Pinecone.\n\n## Basic performance checklist\n\n- **Switch to a cloud environment.** For example: EC2, GCE, [Google Colab](https://colab.research.google.com), [GCP AI Platform Notebook](https://cloud.google.com/ai-platform-notebooks), or [SageMaker Notebook](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html). If you experience slow uploads or high query latencies, it might be because you are accessing Pinecone from your home network.\n- **Deploy your application and your Pinecone service in the same region.** For users on the Free plan, Pinecone runs in GCP US-West (Oregon). [Contact us](https://www.pinecone.io/contact/) if you need a dedicated deployment.\n- **Reuse connections.** We recommend you reuse the same `pinecone.Index()` instance when you are upserting and querying the same index.\n- **Operate within known [limits](limits).**\n\n## How to increase throughput\n\nTo increase throughput (QPS), increase the number of replicas for your index.\n\n**Example**\n\nThe following example increases the number of replicas for `example-index` to 4.\n\n```python\npinecone.configure_index(\"example-index\", replicas=4)\n```\n```shell curl\ncurl -i -X PATCH https://controller.us-west1-gcp.pinecone.io/databases/example-index \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"replicas\": 4\n  }'\n```\n\nSee the [configure_index API reference](/reference/configure_index/) for more details.\n\n<embed src=\"/snippets/_test.md\" />\n\n## Using the gRPC client to get higher upsert speeds\n\nPinecone has a gRPC flavor of the standard client ([installation](installation#installing-the-grpc-flavor-of-the-standard-client)) that can provide higher upsert speeds for multi-pod indexes.\n\nTo connect to an index via the gRPC client:\n\n```python\n\nindex = pinecone.GRPCIndex(\"index-name\")\n\n```\n\nThe syntax for upsert, query, fetch, and delete with the gRPC client remain the same as the standard client.\n\nWe recommend you use parallel upserts to get the best performance.\n\n```python\n\nindex = pinecone.GRPCIndex('example-index')\ndef chunker(seq, batch_size):\n  return (seq[pos:pos + batch_size] for pos in range(0, len(seq), batch_size))\nasync_results = [\n        index.upsert(vectors=chunk, async_req=True)\n        for chunk in chunker(data, batch_size=100)\n    ]\n# Wait for and retrieve responses (in case of error)\n[async_result.result() for async_result in async_results]\n\n```\n\nWe recommend you use the gRPC client for multi-pod indexes only. The performance of the standard and gRPC clients are similar in a single-pod index.\n\nIt's possible to get write throttled faster when upserting using the gRPC index. If you see this often, we recommend you use a backoff algorithm while upserting.\n\nPinecone is thread-safe, so you can launch multiple read requests and multiple write requests in parallel. Launching multiple requests can help with improving your throughput. However, reads and writes can’t be performed in parallel, therefore writing in large batches might affect query latency and vice versa.\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccbe"
  },
  "filename": "moving-to-production.md",
  "title": "Moving to production",
  "category": "630fc5235d91a70054705fb6",
  "content": "---\ntitle: Moving to production\ncategory: 630fc5235d91a70054705fb6  \n---\n\n## Introduction\n\nThe goal of this document is to prepare users to begin using their Pinecone indexes in production by anticipating production issues and identifying best practices for production indexes. Because these issues are highly workload-specific, the recommendations here are general.\n\n## Overview\n\nOnce you have become familiar with Pinecone and experimented with creating indexes and queries that reflect your intended workload, you may be planning to use your indexes to serve production queries. Before you do, there are several steps you can take that can prepare your project for production workloads, anticipate production issues, and enable reliability and growth. \n\nConsider the following areas before moving your indexes to production:\n\n## Prepare your project structure\n\nOne of the first steps towards a production-ready Pinecone index is configuring your project correctly. Consider [creating a separate project](https://www.pinecone.io/docs/manage-projects/#creating-a-new-project) for your development and production indexes, to allow for testing changes to your index before deploying them to production. Ensure that you have properly [configured user access](https://www.pinecone.io/docs/manage-projects/#user-roles) to your production environment so that only those users who need to access the production index can do so. Consider how best to manage the API key associated with your production project.\n\n## Test your query results\n\nBefore you move your index to production, make sure that your index is returning accurate results in the context of your application. Consider [identifying the appropriate metrics](https://www.pinecone.io/learn/offline-evaluation/) for evaluating your results. \n\n## Estimate the appropriate number and size of pods and replicas\n\nDepending on your data and the types of workloads you intend to run, your project may require a different [number and size of pods](https://www.pinecone.io/docs/indexes/#pods-pod-types-and-pod-sizes) and [replicas](https://www.pinecone.io/docs/manage-indexes/#replicas). Factors to consider include the number of vectors, the dimensions per vector, the amount and cardinality of metadata, and the acceptable queries per second (QPS). Use the [index fullness metric](/reference/describe_index_stats_post) to identify how much of your current resources your indexes are using. You can [use collections to create indexes](https://www.pinecone.io/docs/manage-indexes/#create-an-index-from-a-collection) with different pod types and sizes to experiment.\n\n## Load test your indexes\n\nBefore moving your project to production, consider determining whether your index configuration can serve the load of queries you anticipate from your application. You can write load tests in Python from scratch or using a load testing framework like [Locust](https://locust.io/).\n\n## Back up your indexes\n\nIn order to enable long-term retention, compliance archiving, and deployment of new indexes, consider backing up your production indexes by [creating collections](https://www.pinecone.io/docs/back-up-indexes/). \n\n## Tune for performance\n\nBefore serving production workloads, identify ways to [improve latency](https://www.pinecone.io/docs/performance-tuning/) by making changes to your deployment, project configuration, or client. \n\n## Configure monitoring \n\nPrepare to observe production performance and availability by [configuring monitoring](https://www.pinecone.io/docs/monitoring/) with Prometheus or OpenMetrics on your production indexes.\n\n## Plan for scaling\n\nBefore going to production, consider planning ahead for how you might scale your indexes when the need arises. Identify metrics that may indicate the need to scale, such as [index fullness](/reference/describe_index_stats_post) and [average request latency](https://www.pinecone.io/docs/monitoring/#average-request-latency). Plan for increasing the number of pods, changing to a more performant [pod type](https://www.pinecone.io/docs/indexes/#pods-pod-types-and-pod-sizes), [vertically scaling](https://www.pinecone.io/learn/testing-p2-collections-scaling/#vertical-scaling-on-p1-and-s1) the [size of your pods](https://www.pinecone.io/docs/indexes/#pods-pod-types-and-pod-sizes), increasing the number of [replicas](https://www.pinecone.io/docs/manage-indexes/#replicas), or increasing storage capacity with a [storage-optimized pod type](https://www.pinecone.io/docs/indexes/#pods-pod-types-and-pod-sizes).\n\n## Know how to get support\n\nIf you need help, visit [support.pinecone.io](https://support.pinecone.io), or talk to the [Pinecone community](https://www.pinecone.io/community/). Ensure that your [plan tier](https://www.pinecone.io/pricing/) matches the support and availability SLAs you need. This may require you to upgrade to Enterprise.\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccbf"
  },
  "filename": "namespaces.md",
  "title": "Using namespaces",
  "category": "630fc5235d91a70054705fb8",
  "content": "---\ntitle: Using namespaces\ncategory: 630fc5235d91a70054705fb8\n---\n\nPinecone allows you to partition the vectors in an index into **namespaces**. Queries and other operations are then limited to one namespace, so different requests can search different subsets of your index.\n\nFor example, you might want to define a namespace for indexing articles by **content**, and another for indexing articles by **title**. For a complete example, see: [Semantic Text Search (Example)](semantic-text-search/).\n\nEvery index is made up of one or more namespaces. Every vector exists in exactly one namespace.\n\nNamespaces are uniquely identified by a namespace name, which almost all operations accept as a parameter to limit their work to the specified namespace. When you don't specify a namespace name for an operation, Pinecone uses the default namespace name of `\"\"` (the empty string).\n\n## Creating a namespace\n\nA destination namespace can be specified when vectors are upserted. If the namespace doesn't exist, it is created implicitly.\n\nThe example below will create a \"my-first-namespace\" namespace if it doesn’t already exist:\n\n```python\n# Upsert vectors while creating a new namespace\nindex.upsert(vectors=[('id-1', [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])],\n             namespace='my-first-namespace')\n```\n```shell curl\ncurl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/upsert \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"vectors\": [\n      {\n        \"id\": \"id-1\",\n        \"values\": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n      }\n    ],\n    \"namespace\": \"my-first-namespace\"\n  }'\n```\n\nThen you can submit queries and other operations specifying that namespace as a parameter. For example, to query the vectors in namespace \"my-first-namespace\":\n\n```python\nindex.query(vector=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n            top_k=1,\n            namespace='my-first-namespace')\n```\n```shell curl\ncurl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/query \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"topK\": 1,\n    \"vector\": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n    \"namespace\": \"my-first-namespace\"\n  }'\n```\n\n## Creating more than one namespace\n\nYou can create more than one namespace. For example, insert data into separate namespaces:\n\n```python\nimport numpy as np\n\n# Create three sets of 8-dimensional vectors\nvectors_a = np.random.rand(15, 8).tolist()\nvectors_b = np.random.rand(20, 8).tolist()\nvectors_c = np.random.rand(30, 8).tolist()\n\n# Create ids\nids_a = map(str, np.arange(15).tolist())\nids_b = map(str, np.arange(20).tolist())\nids_c = map(str, np.arange(30).tolist())\n\n# Insert into separate namespaces\nindex.upsert(vectors=zip(ids_a,vectors_a),namespace='namespace_a')\nindex.upsert(vectors=zip(ids_b,vectors_b),namespace='namespace_b')\n\n# if no namespaces are specified, the index uses the default namespace\nindex.upsert(vectors=zip(ids_c,vectors_c))\n\n# At this point, index.describe_index_stats() returns:\n# {'dimension': 8,\n#  'namespaces': {'': {'vector_count': 30},\n#                 'namespace_a': {'vector_count': 15},\n#                 'namespace_b': {'vector_count': 20}}}\n```\n```shell curl\n# No example\n```\n\n## Operations across all namespaces\n\nAll vector operations apply to a single namespace, with one exception:\n\nThe `DescribeIndexStatistics` operation returns per-namespace statistics about the contents of **all** namespaces in an index. [More details](/reference/describe_index_stats_post)\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccc0"
  },
  "filename": "query-data.md",
  "title": "Query data",
  "category": "630fc5235d91a70054705fb6",
  "content": "---\ntitle: Query data\ncategory: 630fc5235d91a70054705fb6\n---\n\nAfter your data is [indexed](insert-data), you can start sending queries to Pinecone.\n\nThe `Query` operation searches the index using a query vector. It retrieves the IDs of the most similar vectors in the index, along with their similarity scores. It can optionally include the result vectors' values and metadata too. You specify the number of vectors to retrieve each time you send a query. They are always ordered by similarity from most similar to least similar.\n\n## Sending a query\n\nWhen you send a query, you provide a `vector` and retrieve the `top-k` most similar vectors for each query. For example, this example sends a query vector and retrieves three matching vectors:\n\n```python\nindex.query(\n  vector=[0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],\n  top_k=3,\n  include_values=True\n)\n\n# Returns:\n# {'matches': [{'id': 'C',\n#               'score': -1.76717265e-07,\n#               'values': [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]},\n#                   {'id': 'B',\n#                    'score': 0.080000028,\n#                    'values': [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]},\n#                   {'id': 'D',\n#                    'score': 0.0800001323,\n#                    'values': [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]}],\n#               'namespace': ''}\n```\n```shell curl\ncurl -i -X POST https://hello-pinecone-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/query \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"vector\":[0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],\n    \"topK\": 3,\n    \"includeValues\": true\n  }'\n\n# Output:\n# {\n#  \"matches\":[\n#      {\n#       \"id\": \"C\",\n#       \"score\": -1.76717265e-07,\n#       \"values\": [0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3]\n#      },\n#      {\n#       \"id\": \"B\",\n#       \"score\": 0.080000028,\n#       \"values\": [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n#      },\n#      {\n#       \"id\": \"D\",\n#       \"score\": 0.0800001323,\n#       \"values\": [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\n#      }\n#  ],\n#  \"namespace\": \"\"\n# }\n```\n\nDepending on your data and your query, you may not get top_k results. This happens when top_k is larger than the number of possible matching vectors for your query.\n\n## Querying by namespace\n\nYou can organize the vectors added to an index into partitions, or \"namespaces,\" to limit queries and other vector operations to only one such namespace at a time. For more information, see: [Namespaces](namespaces).\n\n## Using metadata filters in queries\n\nYou can add metadata to document embeddings within Pinecone, and then filter for those criteria when sending the query. Pinecone will search for similar vector embeddings only among those items that match the filter. For more information, see: [Metadata Filtering](metadata-filtering).\n\n[//]: # (This sample code is a copy from metadata-filtering)\n```python\nindex.query(\n    vector=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n    filter={\n        \"genre\": {\"$eq\": \"documentary\"},\n        \"year\": 2019\n    },\n    top_k=1,\n    include_metadata=True\n)\n```\n```shell curl\ncurl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/query \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"vector\": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n    \"filter\": {\"genre\": {\"$in\": [\"comedy\", \"documentary\", \"drama\"]}},\n    \"topK\": 1,\n    \"includeMetadata\": true\n  }'\n```\n\n\n## Limitations\n\nAvoid returning vector data and metadata when top_k>1000. This means queries with top_k over 1000 should not contain: `include_metadata=True`  or `include_data=True`. For more limitations, see: [Limits](limits).\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccc1"
  },
  "filename": "back-up-indexes.md",
  "title": "Back up indexes",
  "category": "630fc5235d91a70054705fb6",
  "content": "---\ntitle: Back up indexes\ncategory: 630fc5235d91a70054705fb6\n---\n\n## Overview\n\nThis document describes how to make backup copies of your indexes using [collections](collections).\n\nTo learn how to create an index from a collection, see [Manage indexes](manage-indexes/#creating-an-index).\n\n\n> ⚠️  Warning\n>\n> This document uses [collections](collections). This is a **public preview**\n> feature. Test thoroughly before using this feature with production workloads.\n\n## Create a backup using a collection\n\nTo create a backup of your index, use the [`create_collection`](/reference/create_collection) operation. A collection is a static copy of your index that only consumes storage.\n\n**Example**\n\nThe following example creates a collection named `example-collection` from an index named `example-index`.\n\n```python\npinecone.create_collection(\"example-collection\", \"example-index\")\n```\n```shell curl\ncurl -i -X POST https://controller.us-west1-gcp.pinecone.io/collections \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n        \"name\": \"example-collection\",\n        \"source\": \"example-index\"\n  }'\n```\n\n## Check the status of a collection\n\nTo retrieve the status of the process creating a collection and the size of the collection, use the [`describe_collection`](/reference/describe_collection) operation. Specify the name of the collection to check. You can only call `describe_collection` on a collection in the current project.\n\nThe `describe_collection` operation returns an object containing key-value pairs representing the name of the collection, the size in bytes, and the creation status of the collection.\n\n**Example**\n\nThe following example gets the creation status and size of a collection named `example-collection`.\n\n```python\npinecone.describe_collection(\"example-collection\")\n```\n```shell curl\ncurl -i -X GET https://controller.us-west1-gcp.pinecone.io/collections/example-collection \\\n  -H 'Api-Key: YOUR_API_KEY'\n```\n\nResults:\n```shell\nCollectionDescription(name='test-collection', size=3818809, status='Ready')\n```\n\n## List your collections\n\nTo get a list of the collections in the current project, use the [`list_collections`](/reference/list_collections) operation.\n\n**Example**\n\nThe following example gets a list of all collections in the current project.\n\n```python\npinecone.list_collections()\n```\n```shell curl\ncurl -i -X GET https://controller.us-west1-gcp.pinecone.io/collections \\\n  -H 'Api-Key: YOUR_API_KEY'\n```\n\nResults\n```shell\nexample-collection\n```\n\n## Delete a collection\n\nTo delete a collection, use the [`delete_collection`](/reference/delete_collection) operation. Specify the name of the collection to delete.\n\nDeleting the collection takes several minutes. During this time, the `describe_collection` operation returns the status `\"deleting\"`.\n\n**Example**\n\nThe following example deletes the collection `example-collection`.\n\n```python\npinecone.delete_collection(\"example-collection\")\n```\n```shell curl\ncurl -i -X DELETE https://controller.us-west1-gcp.pinecone.io/collections/example-collection \\\n  -H 'Api-Key: YOUR_API_KEY'\n```\n\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccc2"
  },
  "filename": "manage-billing.md",
  "title": "Manage billing",
  "category": "630fc5235d91a70054705fb6",
  "content": "---\ntitle: Manage billing\ncategory: 630fc5235d91a70054705fb6\n---\n\n## Overview\n\nThis category contains guides for tasks related to Pinecone billing.\n\n## Tasks\n\n+ [Setting up GCP Marketplace billing](setting-up-gcp-marketplace-billing)\n+ [Changing your billing plan](changing-your-billing-plan)\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccc3"
  },
  "filename": "metadata-filtering.md",
  "title": "Metadata filtering",
  "category": "630fc5235d91a70054705fb6",
  "content": "---\ntitle: Metadata filtering\ncategory: 630fc5235d91a70054705fb6\n---\n\nYou can limit your vector search based on metadata. Pinecone lets you attach metadata key-value pairs to vectors in an index, and specify filter expressions when you query the index.\n\nSearches with metadata filters retrieve exactly the number of nearest-neighbor results that match the filters. For most cases, the search latency will be even lower than unfiltered searches.\n\nFor more background information on metadata filtering, see: [The Missing WHERE Clause in Vector Search](https://www.pinecone.io/learn/vector-search-filtering/).\n\n<html><iframe width=\"450\" height=\"253\" src=\"https://www.youtube.com/embed/tn_Y19oB5bs\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></html>\n\n## Supported metadata types\n\nYou can associate a metadata payload with each vector in an index, as key-value pairs in a JSON object where keys are strings and values are one of:\n\n* String\n* Number (integer or floating point, gets converted to a 64 bit floating point)\n* Booleans (true, false)\n* List of String\n\n> ℹ️  Note\n>\n> **High cardinality consumes more memory:** Pinecone indexes metadata to allow\n> for filtering. If the metadata contains many unique values — such as a unique\n> identifier for each vector — the index will consume significantly more\n> memory. Consider using [selective metadata\n> indexing](manage-indexes/#selective-metadata-indexing) to avoid indexing\n> high-cardinality metadata that is not needed for filtering.\n\n> ⚠️  Warning\n>\n> Null metadata values are not supported. Instead of setting a key to hold a\n> null value, we recommend you remove that key from the metadata payload.\n\nFor example, the following would be valid metadata payloads:\n```json\n{\n    \"genre\": \"action\",\n    \"year\": 2020,\n    \"length_hrs\": 1.5\n}\n\n{\n    \"color\": \"blue\",\n    \"fit\": \"straight\",\n    \"price\": 29.99,\n    \"is_jeans\": true\n}\n```\n\n## Metadata query language\n\n> ℹ️  Note\n>\n> Pinecone's filtering query language is based on [MongoDB's query and\n> projection\n> operators](https://docs.mongodb.com/manual/reference/operator/query/). We\n> currently support a subset of those selectors.\n\nThe metadata filters can be combined with AND and OR:\n\n- `$eq` - Equal to *(number, string, boolean)*\n- `$ne` - Not equal to *(number, string, boolean)*\n- `$gt` - Greater than *(number)*\n- `$gte` - Greater than or equal to *(number)*\n- `$lt` - Less than *(number)*\n- `$lte` - Less than or equal to *(number)*\n- `$in` - In array *(string or number)*\n- `$nin` - Not in array *(string or number)*\n\n### Using arrays of strings as metadata values or as metadata filters\n\nA vector with metadata payload...\n```json\n{\"genre\":[\"comedy\",\"documentary\"]}\n```\n\n...means the `\"genre\"` takes on both values.\n\nFor example, queries with the following filters will match the vector:\n```json\n{\"genre\":\"comedy\"}\n\n{\"genre\": {\"$in\":[\"documentary\",\"action\"]}}\n\n{\"$and\": [{\"genre\": \"comedy\"}, {\"genre\":\"documentary\"}]}\n```\n\nQueries with the following filter will **not** match the vector:\n```json\n{\"$and\": [{\"genre\": \"comedy\"}, {\"genre\":\"drama\"}]}\n```\n\nAnd queries with the following filters will **not** match the vector because they are invalid. They will result in a query compilation error:\n```\n# INVALID QUERY:\n{\"genre\": [\"comedy\", \"documentary\"]}\n```\n```\n# INVALID QUERY:\n{\"genre\": {\"$eq\": [\"comedy\", \"documentary\"]}}\n```\n\n## Inserting metadata into an index\nMetadata can be included in upsert requests as you insert your vectors.\n\nFor example, here's how to insert vectors with metadata representing movies into an index:\n```python\nimport pinecone\n\npinecone.init(api_key=\"your-api-key\", environment=\"us-west1-gcp\")\nindex = pinecone.Index(\"example-index\")\n\nindex.upsert([\n    (\"A\", [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], {\"genre\": \"comedy\", \"year\": 2020}),\n    (\"B\", [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2], {\"genre\": \"documentary\", \"year\": 2019}),\n    (\"C\", [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3], {\"genre\": \"comedy\", \"year\": 2019}),\n    (\"D\", [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4], {\"genre\": \"drama\"}),\n    (\"E\", [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], {\"genre\": \"drama\"})\n])\n```\n```shell curl\ncurl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/upsert \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"vectors\": [\n      {\n        \"id\": \"A\",\n        \"values\": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n        \"metadata\": {\"genre\": \"comedy\", \"year\": 2020}\n      },\n      {\n        \"id\": \"B\",\n        \"values\": [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],\n        \"metadata\": {\"genre\": \"documentary\", \"year\": 2019}\n      },\n      {\n        \"id\": \"C\",\n        \"values\": [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],\n        \"metadata\": {\"genre\": \"comedy\", \"year\": 2019}\n      },\n      {\n        \"id\": \"D\",\n        \"values\": [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4],\n        \"metadata\": {\"genre\": \"drama\"}\n      },\n      {\n        \"id\": \"E\",\n        \"values\": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n        \"metadata\": {\"genre\": \"drama\"}\n      }\n    ]\n  }'\n```\n\n## Querying an index with metadata filters\n\nMetadata filter expressions can be included with queries to limit the search to only vectors matching the filter expression.\n\nFor example, we can search the previous movies index for documentaries from the year 2019. This also uses the `include_metadata` flag so that vector metadata is included in the response.\n\n> ⚠️  Warning\n>\n> For performance reasons, do not return vector data and metadata when\n> `top_k>1000`. Queries with `top_k` over 1000 should not contain\n> `include_metadata=True` or `include_data=True`.\n\n```python\nindex.query(\n    vector=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n    filter={\n        \"genre\": {\"$eq\": \"documentary\"},\n        \"year\": 2019\n    },\n    top_k=1,\n    include_metadata=True\n)\n\n# Returns:\n# {'matches': [{'id': 'B',\n#               'metadata': {'genre': 'documentary', 'year': 2019.0},\n#               'score': 0.0800000429,\n#               'values': []}],\n#  'namespace': ''}\n```\n```shell curl\ncurl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/query \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"vector\": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n    \"filter\": {\"genre\": {\"$in\": [\"comedy\", \"documentary\", \"drama\"]}},\n    \"topK\": 1,\n    \"includeMetadata\": true\n  }'\n\n# Output:\n# {\n#       \"matches\": [\n#         {\n#           \"id\": \"B\",\n#           \"score\": 0.0800000429,\n#           \"values\": [],\n#           \"metadata\": {\n#             \"genre\": \"documentary\",\n#             \"year\": 2019\n#           }\n#         }\n#       ],\n#       \"namespace\": \"\"\n#     }\n```\n\n### More example filter expressions\n\nA comedy, documentary, or drama:\n```json\n{\n    \"genre\": {\"$in\": [\"comedy\", \"documentary\", \"drama\"]}\n}\n```\n\nA drama from 2020:\n```json\n{\n    \"genre\": {\"$eq\": \"drama\"},\n    \"year\": {\"$gte\": 2020}\n}\n```\n\nA drama from 2020 (equivalent to the previous example):\n```json\n{\n    \"$and\": [\n        {\"genre\": {\"$eq\": \"drama\"}},\n        {\"year\": {\"$gte\": 2020}}\n    ]\n}\n```\n\nA drama or a movie from 2020:\n```json\n{\n    \"$or\": [\n        {\"genre\": {\"$eq\": \"drama\"}},\n        {\"year\": {\"$gte\": 2020}}\n    ]\n}\n```\n\n## Deleting vectors by metadata filter\nTo specify vectors to be deleted by metadata values, pass a metadata filter expression to the delete operation. This deletes all vectors matching the metadata filter expression.\n\n**Example**\n\nThis example deletes all vectors with genre \"documentary\" and year 2019 from an index.\n```python\nindex.delete(\n    filter={\n        \"genre\": {\"$eq\": \"documentary\"},\n        \"year\": 2019\n    }\n)\n```\n```shell curl\ncurl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/delete \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"filter\": {\"genre\": {\"$in\": [\"comedy\", \"documentary\", \"drama\"]}}\n  }'\n```\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccc4"
  },
  "filename": "monitoring.md",
  "title": "Monitoring",
  "category": "630fc5235d91a70054705fb6",
  "content": "---\ntitle: Monitoring\ncategory: 630fc5235d91a70054705fb6\n---\n\nThis document describes how to configure monitoring for your Pinecone index using Prometheus or compatible tools.\n\n## Overview\n\nYou can ingest performance metrics from Pinecone indexes into your own Prometheus instances, or into Prometheus- and OpenMetrics-compatible monitoring tools. The Prometheus metric endpoint is for users who want to monitor and store system health metrics using their own Prometheus metrics logger.\n\n> ⚠️  Warning\n>\n> This feature is in public preview and is only available to Enterprise or\n> Enterprise Dedicated users.\n\n## Connect\n\nMetrics are available at a URL like the following:\n\n`https://metrics.YOUR_ENVIRONMENT.pinecone.io/metrics`\n\nYour API key must be passed via the Authorization header as a bearer token like the following:\n\n`Authorization: Bearer \\<api-key\\>`\n\nOnly the metrics for the project associated with the API key are available at this URL.\n\nFor Prometheus, configure prometheus.yml as follows:\n\n```yaml\nscrape_configs:\n  - job_name: pinecone-job-1\n    authorization:\n      credentials: <api-key-here>\n    scheme: https\n    static_configs:\n      - targets: ['metrics.YOUR_ENVIRONMENT.pinecone.io']\n```\n\n[See Prometheus docs](https://prometheus.io/docs/prometheus/latest/configuration/configuration/) for more configuration details.\n\n## Available Metrics\n\nThe metrics available are as follows:\n\n<div>\n <table class=\"table table-responsive\">\n  <thead>\n   <tr>\n    <th>\n     Name\n    </th>\n    <th>\n     Type\n    </th>\n    <th>\n     Description\n    </th>\n    <th>\n     Labels\n    </th>\n   </tr>\n  </thead>\n  <tbody>\n   <tr>\n    <td>\n     pinecone_vector_count\n    </td>\n    <td>\n     gauge\n    </td>\n    <td>\n     pinecone_vector_count gives the number of items per pod in the index.<br/>\n     <i>\n     Labels:<br/>\n     - pid: Process identifier<br/>\n     - index_name: Name of the index<br/>\n     - project_name: Pinecone project name<br/>\n     </i>\n    </td>\n   </tr>\n   <tr>\n    <td>\n     pinecone_request_count_total\n    </td>\n    <td>\n     counter\n    </td>\n    <td>\n     pinecone_request_count_total gives the number of data plane calls made by clients.<br/>\n     <i>\n     Labels:<br/>\n     - pid: Process identifier<br/>\n     - index_name: Name of the index<br/>\n     - project_name: Pinecone project name<br/>\n     - request_type: One of upsert, delete, fetch, query, describe_index_stats<br/>\n     </i>\n    </td>\n   </tr>\n   <tr>\n    <td>\n     pinecone_request_error_count_total\n    </td>\n    <td>\n     counter\n    </td>\n    <td>\n     pinecone_request_error_count_total gives the number of data plane calls made by clients that resulted in errors.<br/>\n     <i>\n     Labels:<br/>\n     - pid: Process identifier<br/>\n     - index_name: Name of the index<br/>\n     - project_name: Pinecone project name<br/>\n     - request_type: One of upsert, delete, fetch, query, describe_index_stats<br/>\n     </i>\n    </td>\n   </tr>\n   <tr>\n    <td>\n     pinecone_request_latency_seconds\n    </td>\n    <td>\n     histogram\n    </td>\n    <td>\n     pinecone_request_latency_seconds gives the distribution of server-side processing latency for pinecone data plane calls.<br/>\n     <i>\n     Labels:<br/>\n     - pid: Process identifier<br/>\n     - index_name: Name of the index<br/>\n     - project_name: Pinecone project name<br/>\n     - request_type: One of upsert, delete, fetch, query, describe_index_stats<br/>\n     </i>\n    </td>\n   </tr>\n   <tr>\n    <td>\n     pinecone_index_fullness\n    </td>\n    <td>\n     gauge\n    </td>\n    <td>\n     pinecone_index_fullness gives the fullness of the index on a scale of 0 to 1.<br/>\n     <i>\n     Labels:<br/>\n     - pid: Process identifier<br/>\n     - index_name: Name of the index<br/>\n     - project_name: Pinecone project name<br/>\n     </i>\n    </td>\n   </tr>\n  </tbody>\n </table>\n</div>\n\n## Example queries\n\nThe following Prometheus queries gather information about your Pinecone index.\n\n### Average request latency\n\nThe following query returns the average latency in seconds for all requests\nagainst the Pinecone index `example-index`.\n\n```\navg by (request_type) (pinecone_request_latency_seconds{index_name=\"example-index\"})\n```\n\nThe following query returns the vector count for the Pinecone index `example-index`.\n\n```\nsum ((avg by (app) (pinecone_vector_count{index_name=\"example-index\"})))\n```\n\nThe following query returns the total number of requests against the Pinecone index `example-index` over one minute.\n\n```\nsum by (request_type)(increase(pinecone_request_count_total{index_name=\"example-index\"}[60s]))\n```\n\nThe following query returns the total number of upsert requests against the Pinecone index `example-index` over one minute.\n\n```\nsum by (request_type)(increase(pinecone_request_count_total{index_name=\"example-index\", request_type=\"upsert\"}[60s]))\n```\n\nThe following query returns the total errors returned by the Pinecone index `example-index` over one minute.\n\n```\nsum by (request_type) (increase(pinecone_request_error_count{\n      index_name=\"example-index\"}[60s]))\n```\n\nThe following query returns the index fullness metric for the Pinecone index `example-index`.\n\n```\nround(max (pinecone_index_fullness{index_name=\"example-index\"} * 100))\n```\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccc5"
  },
  "filename": "manage-projects.md",
  "title": "Manage projects",
  "category": "630fc5235d91a70054705fb6",
  "content": "---\ntitle: Manage projects\ncategory: 630fc5235d91a70054705fb6\n---\n\n## Overview\n\nThis category contains guides for tasks related to Pinecone projects.\n\n## Tasks\n\n+ [Create a project](create-project)\n+ [Add users to a project](add-users-to-projects-and-organizations)\n+ [Change the pod limit for a project](change-project-pod-limit)\n+ [Rename a project](rename-project)\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccc6"
  },
  "filename": "create-project.md",
  "title": "Create a project",
  "category": "630fc5235d91a70054705fb6",
  "content": "---\ntitle: Create a project\ncategory: 630fc5235d91a70054705fb6\n---\n\n## Overview\n\n> ℹ️ Info\n>\n> Starter (free) users can only have 1 owned project. To create a new project, Starter users must upgrade to the Standard or Enterprise plan or delete their default project.\n\nFollow these steps to create a new project:\n\n1. Access the [Pinecone Console](https;//app.pinecone.io).\n\n1. Click **Organizations** in the left menu.\n\n1. In the **Organizations** view, click the **PROJECTS** tab.\n\n1. Click the **+CREATE PROJECT** button.\n\n1. Enter the **Project Name**.\n\n1. Select a [cloud provider and region](projects#project-environment).\n\n1. Enter the [project pod limit](projects/#project-pod-limit).\n\n1. Click **CREATE PROJECT**.\n\n## Next steps\n\n* [Add users to your project](add-users-to-projects-and-organizations).\n* [Create an index](manage-indexes#creating-an-index).\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccc7"
  },
  "filename": "change-project-pod-limit.md",
  "title": "Change project pod limit",
  "category": "630fc5235d91a70054705fb6",
  "content": "---\ntitle: Change project pod limit\ncategory: 630fc5235d91a70054705fb6\n---\n\n## Overview\n\nIf you are a [project owner](projects#project-roles), follow these steps to change the maximum total number of [pods](indexes#pods-pod-types-and-pod-sizes) in your project.\n\n## Change project pod limit in console\n\n1. Access the [Pinecone Console](https://app.pinecone.io).\n1. Click **Settings** in the left menu.\n1. In the **Settings** view, click the **PROJECTS** tab.\n1. Next to the project you want to update, click <img src=\"https://raw.githubusercontent.com/pinecone-io/img/main/edit-icon.png\" alt=\"pencil icon\" height=\"20\"/>.\n1. Under **Pod Limit**, enter the new number of pods.\n1. Click **SAVE CHANGES**.\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccc8"
  },
  "filename": "add-users-to-projects-and-organizations.md",
  "title": "Add users to projects and organizations",
  "category": "630fc5235d91a70054705fb6",
  "content": "---\ntitle: Add users to projects and organizations\ncategory: 630fc5235d91a70054705fb6\n---\n\n## Overview \n\nIf you are a [project](/manage-projects) or [organization](/organizations) owner, follow these steps to add users to organizations and projects. \n\n## Add users to projects and organizations\n\n1. Access the [Pinecone Console](https://app.pinecone.io).\n\n1. Click **Settings** in the left menu.\n\n1. In the **Settings** view, click the **USERS** tab.\n\n1. Click **+INVITE USER**.\n\n1. (Organization owner only) Select an [organization role](organizations#organization-roles).\n\n1. Select one or more projects.\n\n1. Select a [project role](projects#project-roles).\n\n1. Enter the user's email address.\n\n1. Click **+INVITE USER**.\n\nWhen you invite another user to join your organization or project, Pinecone sends them an email containing a link that enables them to gain access to the organization or project. If they already have a Pinecone account, they still receive an email, but they can also immediately view the project.\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccc9"
  },
  "filename": "rename-project.md",
  "title": "Rename a project",
  "category": "630fc5235d91a70054705fb6",
  "content": "---\ntitle: Rename a project\ncategory: 630fc5235d91a70054705fb6\n---\n\n## Overview\n\nIf you are a [project owner](projects#project-roles), follow these steps to change the name ofyour project.\n\n1. Access the [Pinecone Console](https://app.pinecone.io).\n\n1. Click **Settings** in the left menu.\n\n1. In the **Settings** view, click the **PROJECTS** tab.\n\n1. Next to the project you want to update, click ![edit icon](https://raw.githubusercontent.com/pinecone-io/img/main/edit-icon.png).\n\n1. Under **Project Name**, enter the new project name.\n\n1. Click **SAVE CHANGES**.\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccca"
  },
  "filename": "changing-your-billing-plan.md",
  "title": "Changing your billing plan",
  "category": "630fc5235d91a70054705fb6",
  "content": "---\ntitle: Changing your billing plan\ncategory: 630fc5235d91a70054705fb6\n---\n\nThis document describes how to change the billing plan for your Pinecone organization through the Pinecone console..\n\n> ℹ️  Note\n>\n> Accounts created by [signing up through GCP Marketplace](setting-up-gcp-marketplace-billing) must change billing plans through the Pinecone console using this workflow. \n\nTo change your billing plan, you must be the [organization owner](organizations#organization-owners) for your organization.\n\nTo change your billing plan through the Pinecone console, follow these steps:\n\n1. Log in to the [Pinecone console](https://app.pinecone.io).\n1. In the left menu, click **Organizations**.\n1. Click the **Billing** tab.\n1. Under the plan you want, click **Upgrade**.\n\n\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1cccb"
  },
  "filename": "setting-up-gcp-marketplace-billing.md",
  "title": "Setting up billing through GCP Marketplace",
  "category": "630fc5235d91a70054705fb6",
  "content": "---\ntitle: Setting up billing through GCP Marketplace\ncategory: 630fc5235d91a70054705fb6\n---\n\nThis document describes how to configure pay-as-you-go billing for your Pinecone organization through Google Cloud Platform (GCP) Marketplace. To commit to annual spending, [contact Pinecone](https://www.pinecone.io/contact).\n\n> ℹ️  Note\n>\n> This workflow creates a new Pinecone [organization](organizations). If you already have an organization, signing up through GCP Marketplace creates an additional organization. \n\nTo configure Pinecone billing through the GCP Marketplace, follow these steps:\n\n1. Log in to the [GCP Marketplace](https://console.cloud.google.com/marketplace). Your project must be enabled for purchase by your billing administrator.\n1. Search for the [Pinecone listing](https://console.cloud.google.com/marketplace/product/pinecone-public/pinecone).\n1. Click **Subscribe**.\n1. Read and agree to the terms and conditions.\n1. Click **Subscribe**.\n1. On the **Your order request has been sent to Pinecone** dialog, click **Sign up with Pinecone**.\n1. On the **You're leaving Google** dialog, click **OK**. This takes you to the Pinecone sign-in page.\n1. Sign in to Pinecone.io.\n\nWhen you sign in, Pinecone creates a new organization linked to your GCP billing.\n\nIf you already have a Pinecone organization, you can select the new \"GCP Linked\" organization in the top-left drop-down menu in the console. After creating the new organization, contact [Pinecone support](https://support.pinecone.io) for help migrating to the new organization.\n\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1cccc"
  },
  "filename": "elastic.md",
  "title": "Elasticsearch",
  "category": "630fc5235d91a70054705fb4",
  "content": "---\ntitle: Elasticsearch\ncategory: 630fc5235d91a70054705fb4\n---\n\n[Elasticsearch](https://www.elastic.co/) is a powerful open-source search engine and analytics platform that is widely used as a document store for keyword-based text search.\n\nPinecone is a [vector database](https://www.pinecone.io/learn/vector-database/) widely used for production applications — such as semantic search, recommenders, and threat detection — that require fast and fresh vector search at the scale of tens or hundreds of millions (or even billions) of embeddings. Although Pinecone offers [hybrid search](https://docs.pinecone.io/docs/hybrid-search) for keyword-aware semantic search, Pinecone is not a document store and does not replace Elasticsearch for keyword-only retrieval.\n\nIf you already use Elasticsearch and want to add Pinecone’s low-latency and large-scale vector search to your applications, this guide will show you how. You will see how to:\n\n- Add an embedding model to Elasticsearch\n- Transform text data into vector embeddings within Elasticsearch\n- Load those vector embeddings into Pinecone, with corresponding IDs and metadata.\n\n## Uploading the embedding model\n\nWe first need to upload the embedding model to our Elastic instance. To do so, we’ll use the `[eland](https://github.com/elastic/eland)` Elastic client. We’ll have to clone the \"eland\" repository and build the docker image before running it:\n\n```bash\ngit clone git@github.com:elastic/eland.git\ncd eland\ndocker build -t elastic/eland .\n```\n\nIn this example, we’ll use the `[sentence-transformers/msmarco-MiniLM-L-12-v3](https://huggingface.co/sentence-transformers/msmarco-MiniLM-L-12-v3)` model from [Hugging Face](https://huggingface.co/) — although you could use any model you’d like. To upload the model to your Elasticsearch deployment, run the following command:\n\n```bash\ndocker run -it --rm elastic/eland \\\n   eland_import_hub_model \\\n   --url https://<user>:<password>@<host>:<port>/ \\\n   --hub-model-id sentence-transformers/msmarco-MiniLM-L-12-v3 \\\n   --task-type text_embedding \\\n   --start\n```\n\nNote that you’ll have to replace the placeholders with your Elasticsearch instance user, password, host, and port. If you set up your own Elasticsearch instance, you would have already set the username and password when initially setting up the instance. If you’re using the hosted Elastic Stack, you can find the username and password in the \"Security\" section of the Elastic Stack console.\n\nWe can quickly test the uploaded model by running the following command in the Elasticsearch developer console:\n\n```\nPOST /_ml/trained_models/sentence-transformers__msmarco-minilm-l-12-v3/deployment/_infer\n{\n \"docs\": {\n   \"text_field\": \"Hello World!\"\n }\n}\n```\n\nWe should get the following result:\n\n```json\n{\n \"predicted_value\": [\n   -0.06176435202360153,\n   -0.008180409669876099,\n   0.3309500813484192,\n   0.38672536611557007,\n   ...\n ]\n}\n```\n\nThis is the vector embedding for our query. We’re now ready to upload our dataset and apply the model to produce the vector embeddings.\n\n## Uploading the dataset\n\nNext, upload a dataset of documents to Elasticsearch. In this example, we’ll use a subset of the MSMacro dataset. You can [download the file](https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-passagetest2019-top1000.tsv.gz) or run the following command:\n\n```bash\ncurl -O https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-passagetest2019-top1000.tsv.gz\ngunzip msmarco-passagetest2019-top1000.tsv\n```\n\nIn this example, we’ll be using the hosted Elastic Stack, which makes it easier to use various integrations. We’ll use the \"Upload\" integration to load the data into an Elasticsearch index.\n\n![add-data](https://raw.githubusercontent.com/pinecone-io/img/main/elastic_1.png)\n\nWe’ll drag the unzipped TSV file. The Upload integration will sample the data for us and show the following:\n\n![data-preview](https://raw.githubusercontent.com/pinecone-io/img/main/elastic_2.png)\n\nWe’ll click the \"Import\" button and continue to name the index:\n\n![import](https://raw.githubusercontent.com/pinecone-io/img/main/elastic_3.png)\n\nOnce the import is complete, you’ll see the following:\n\n![import-complete](https://raw.githubusercontent.com/pinecone-io/img/main/elastic_4.png)\n\nClicking \"View index in Discover\" will reveal the index view where we can look at the uploaded data:\n\n![discover-index](https://raw.githubusercontent.com/pinecone-io/img/main/elastic_5.png)\n\n## Creating the embeddings\n\nWe’ve now created an index for our data. Next, we’ll create a pipeline to produce a vector embedding for each document. We’ll head to the Elasticsearch developer console and issue the following command to create the pipeline:\n\n```\nPUT _ingest/pipeline/produce-embeddings\n{\n \"description\": \"Vector embedding pipeline\",\n \"processors\": [\n   {\n     \"inference\": {\n       \"model_id\": \"sentence-transformers__msmarco-minilm-l-12-v3\",\n       \"target_field\": \"text_embedding\",\n       \"field_map\": {\n         \"text\": \"text_field\"\n       }\n     }\n   }\n ],\n \"on_failure\": [\n   {\n     \"set\": {\n       \"description\": \"Index document to 'failed-<index>'\",\n       \"field\": \"_index\",\n       \"value\": \"failed-{{{_index}}}\"\n     }\n   },\n   {\n     \"set\": {\n       \"description\": \"Set error message\",\n       \"field\": \"ingest.failure\",\n       \"value\": \"{{_ingest.on_failure_message}}\"\n     }\n   }\n ]\n}\n```\n\nThe \"processor\" definition tells Elasticsearch which model to use and which field to read from. The \"on_failure\" definition defines the failure behavior that Elasticsearch will apply &mdash; specifically, which error message to write and which file to write them into.\n\nOnce the embedding pipeline is created, we’ll re-index our \"msmacro-raw\" index, applying the embedding pipeline to produce the new embeddings. In the developer console, execute the following command:\n\n```\nPOST _reindex?wait_for_completion=false\n{\n \"source\": {\n   \"index\": \"msmacro-raw\"\n },\n \"dest\": {\n   \"index\": \"msmacro-with-embeddings\",\n   \"pipeline\": \"text-embeddings\"\n }\n}\n```\n\nThis will kick off the embedding pipeline. We’ll get a task id which we can track with the following command:\n\n```\nGET _tasks/<task_id>\n```\n\nLooking at the index, we can see that the embeddings have been created in an object called \"text_embeddings\" under the field \"predicted_value\".\n\nTo make the loading process a bit easier, we’re going to pluck the \"predicted_value\" field and add it as its own column:\n\n```\nPOST _reindex?wait_for_completion=false\n{\n \"source\": {\n   \"index\": \"msmacro-with-embeddings\"\n },\n \"dest\": {\n   \"index\": \"msmacro-with-embeddings-flat\"\n },\n \"script\": {\n   \"source\": \"ctx._source.predicted_value = ctx._source.text_embedding.predicted_value\"\n }\n}\n```\n\nNext, we’ll load the embeddings into Pinecone. Since the index size is considerable, we’ll use Apache Spark to parallelize the process.\n\n## Moving the Elasticsearch index to Pinecone\n\nIn this example, we’ll be using Databricks to handle the process of loading Elasticsearch index to Pinecone. We’ll add the Elasticsearch Spark from Maven by navigating to the “Libraries” tab in the cluster settings view, and clicking “Install new”:\n\n![cluster-setup](https://raw.githubusercontent.com/pinecone-io/img/main/elastic_6.png)\n\nUse the following Maven coordinates:\n\n`org.elasticsearch:elasticsearch-spark-30_2.12:8.5.2`\n\nWe’ll add the Pinecone Databricks connectors from S3:\n\n`s3://pinecone-jars/spark-pinecone-uberjar.jar`\n\nRestart the cluster if needed. Next, we’ll create a new notebook, attach it to the cluster and import the required dependencies:\n\n```scala\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.elasticsearch.spark._\n```\n\nWe’ll initialize the Spark context:\n\n```scala\nval spark = SparkSession.builder.appName(\"elasticSpark\").master(\"local[*]\").getOrCreate()\n```\n\nNext, we’ll read the index from Elasticsearch:\n\n```scala\nval df = (spark.read\n     .format( \"org.elasticsearch.spark.sql\" )\n     .option( \"es.nodes\",   \"<ELASTIC_URL>\" )\n     .option( \"es.net.http.auth.user\", \"<ELASTIC_USER>\" )\n     .option( \"es.net.http.auth.pass\", \"<ELASTIC_PASSWORD>\" )\n     .option( \"es.port\",    443     )\n     .option( \"es.nodes.wan.only\", \"true\" )\n     .option(\"es.net.ssl\", \"true\")\n     .option(\"es.read.field.as.array.include\",\"predicted_value:1\")\n     .load( \"msmacro-with-embeddings\")\n )\n```\n\nNote that to ensure the index is read correctly into the dataframe, we must specify that the “predicted_value” field is an array with a depth of 1, as shown below:\n\n```scala\n  .option(\"es.read.field.as.array.include\",\"predicted_value:1\")\n```\n\nNext, we’ll use the Pinecone Spark connector to load this dataframe into a Pinecone index. We’ll start by creating an index in the [Pinecone console](https://app.pinecone.io/). Log in to the console and click “Create Index”. Then, name your index, and configure it to use 384 dimensions.\n\n![create-index](https://raw.githubusercontent.com/pinecone-io/img/main/elastic_7.png)\n\nWhen you’re done configuring the index, click “Create Index”.\n\nWe have to do some prep work to get the dataframe ready for indexing. In order to index the original document with the embeddings we’ve created, we’ll create the following UDF which will encode the original document as a Base64 string. This will ensure the metadata object will remain a valid JSON object regardless of the content of the document.\n\n```scala\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions.udf\nimport java.util.Base64\n\nval text_to_metadata = udf((text: String) => \"{ \\\"document\\\" : \\\"\" +  Base64.getEncoder.encodeToString(text.getBytes(\"UTF-8\")) + \"\\\" }\")\n```\n\nWe’ll apply the UDF and get rid of some unnecessary columns:\n\n```scala\nval clean_df = df.drop(\"text_embedding\").withColumnRenamed(\"predicted_value\", \"vector\").withColumn(\"metadata\", text_to_metadata(col(\"text_field\"))).withColumn(\"namespace\", lit(\"\")).drop(\"text_field\")\n```\n\nNext, we’ll use the Pinecone Spark connector:\n\n```scala\nval pineconeOptions = Map(\n  \"pinecone.apiKey\" -> \"<PINECONE_API_KEY>\",\n  \"pinecone.environment\" -> \"us-west1-gcp\",\n  \"pinecone.projectName\" -> \"<PROJECT_IDENTIFIER>\",\n  \"pinecone.indexName\" -> \"elastic-index\"\n)\n\nclean_df.write\n  .options(pineconeOptions)\n  .format(\"io.pinecone.spark.pinecone.Pinecone\")\n  .mode(SaveMode.Append)\n  .save()\n```\n\nOur vectors have been added to our Pinecone index!\n\nTo query the index, we’ll need to generate a vector embedding for our query first, using the `sentence-transformers/msmarco-MiniLM-L-12-v3` model. Then, we’ll use the Pinecone client to issue the query. We'll do this in a Python notebook.\n\nWe’ll start by installing the required dependencies:\n\n```\n!pip install -qU pinecone-client sentence-transformers pandas\n```\n\nNext, we’ll set up the client:\n\n```python\nimport pinecone\n\n# connect to pinecone environment\npinecone.init(\n   api_key=\"<PINECONE API KEY>\",\n   environment=\"us-west1-gcp\"\n)\n```\n\nWe’ll set up the index:\n\n```python\nindex_name = \"elastic-index\"\nindex = pinecone.Index(index_name)\n```\n\nWe’ll create a helper function that will decode the encoded documents we get:\n\n```python\ndef decode_entries(entries):\n   return list(map(lambda entry: {\n       \"id\": entry[\"id\"],\n       \"score\": entry[\"score\"],\n       \"document\": base64.b64decode(entry[\"metadata\"][\"document\"]).decode(\"UTF-8\"),\n   }, entries))\n```\n\nNext, we’ll create a function that will encode our query, query the index and convert the display the data using Pandas:\n\n```python\ndef queryIndex(query, num_results):\n vector = model.encode(query).tolist()\n result = index.query(vector, top_k=num_results, include_metadata=True)\n return pd.DataFrame(decode_entries(result.matches))\n```\n\nFinally, we’ll test our index:\n\n```python\ndisplay(queryIndex(\"star trek\", 10))\n```\n\nShould yield the results:\n![results](https://raw.githubusercontent.com/pinecone-io/img/main/elastic_8.png)\n\n## Summary\n\nIn conclusion, by following the steps outlined in this post, you can easily upload an embedding model to Elasticsearch, ingest raw textual data, create the embeddings, and load them into Pinecone. With this approach, you can take advantage of the benefits of integrating Elasticsearch and Pinecone. As mentioned, while Elasticsearch is optimized for indexing documents, Pinecone provides vector storage and search capabilities that can handle hundreds of millions and even billions of vectors.\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1cccd"
  },
  "filename": "openai.md",
  "title": "OpenAI",
  "category": "630fc5235d91a70054705fb4",
  "content": "---\ntitle: OpenAI\ncategory: 630fc5235d91a70054705fb4\n---\n\n<div class=\"source\">\n  <!-- Source Link -->\n  <a href=\"https://github.com/pinecone-io/examples/blob/master/integrations/openai/\" class=\"source-link\"><img src=\"../images/source.svg\" /> View Source</a>\n  <!-- Colab Link -->\n  <a href=\"https://colab.research.google.com/github/pinecone-io/examples/blob/master/integrations/openai/semantic_search_openai.ipynb\" class=\"source-link\"><img src=\"../images/colab.svg\" /> Open in Colab</a>\n</div>\n\nIn this guide you will learn how to use the [OpenAI Embedding API](https://beta.openai.com/docs/guides/embeddings) to generate language embeddings, and then index those embeddings in the [Pinecone vector database](https://www.pinecone.io) for fast and scalable vector search.\n\nThis is a powerful and common combination for building semantic search, question-answering, threat-detection, and other applications that rely on NLP and search over a large corpus of text data.\n\nThe basic workflow looks like this:\n* Embed and index\n  * Use the OpenAI Embedding API to generate vector embeddings of your documents (or any text data).\n  * Upload those vector embeddings into Pinecone, which can store and index millions/billions of these vector embeddings, and search through them at ultra-low latencies.\n* Search\n  * Pass your query text or document through the OpenAI Embedding API again.\n  * Take the resulting vector embedding and send it as a [query](/docs/query-data) to Pinecone.\n  * Get back semantically similar documents, even if they don't share any keywords with the query.\n\n![Basic workflow of OpenAI and Pinecone](https://files.readme.io/6a3ea5a-pinecone-openai-overview.png)\n\nLet's get started...\n\n## Environment Setup\n\nWe start by installing the OpenAI and Pinecone clients, we will also need HuggingFace *Datasets* for downloading the TREC dataset that we will use in this guide.\n\n```bash\npip install -U openai pinecone-client datasets\n```\n\n## Creating Embeddings\n\nTo create embeddings we must first initialize our connection to OpenAI Embeddings, we sign up for an API key at [OpenAI](https://beta.openai.com/signup).\n\n```python\nimport openai\n\nopenai.organization = \"<<YOUR_ORG_KEY>>\"\n# get this from top-right dropdown on OpenAI under organization > settings\nopenai.api_key = \"<<YOUR_API_KEY>>\"\n# get API key from top-right dropdown on OpenAI website\n\nopenai.Engine.list()  # check we have authenticated\n```\n\nThe `openai.Engine.list()` function should return a list of models that we can use. We will use OpenAI's Babbage model.\n\n```python\nMODEL = \"text-similarity-babbage-001\"\n\nres = openai.Embedding.create(\n    input=[\n        \"Sample document text goes here\",\n        \"there will be several phrases in each batch\"\n    ], engine=MODEL\n)\n```\n\nIn `res` we should find a JSON-like object containing two 2048-dimensional embeddings, these are the vector representations of the two inputs provided above. To access the embeddings directly we can write:\n\n```python\n# extract embeddings to a list\nembeds = [record['embedding'] for record in res['data']]\n```\n\nWe will use this logic when creating our embeddings for the **T**ext **RE**trieval **C**onference (TREC) question classification dataset later.\n\n## Initializing a Pinecone Index\n\nNext, we initialize an index to store the vector embeddings. For this we need a Pinecone API key, [sign up for one here](https://app.pinecone.io).\n\n```python\nimport pinecone\n\n# initialize connection to pinecone (get API key at app.pinecone.io)\npinecone.init(\n    api_key=\"<<YOUR_API_KEY>>\",\n    environment=\"us-west1-gcp\"\n)\n# check if 'openai' index already exists (only create index if not)\nif 'openai' not in pinecone.list_indexes():\n    pinecone.create_index('openai', dimension=len(embeds[0]))\n# connect to index\nindex = pinecone.Index('openai')\n```\n\n## Populating the Index\n\nWith both OpenAI and Pinecone connections initialized, we can move onto populating the index. For this, we need the TREC dataset.\n\n```python\nfrom datasets import load_dataset\n\n# load the first 1K rows of the TREC dataset\ntrec = load_dataset('trec', split='train[:1000]')\n```\n\nThen we create a vector embedding for each question using OpenAI (as demonstrated earlier), and `upsert` the ID, vector embedding, and original text for each phrase to Pinecone.\n\n> ⚠️  Warning\n>\n> High-cardinality metadata values (like the unique text values we use here)\n> can reduce the number of vectors that fit on a single pod. See\n> [Limits](/limits/) for more.\n\n```python\nfrom tqdm.auto import tqdm  # this is our progress bar\n\nbatch_size = 32  # process everything in batches of 32\nfor i in tqdm(range(0, len(trec['text']), batch_size)):\n    # set end position of batch\n    i_end = min(i+batch_size, len(trec['text']))\n    # get batch of lines and IDs\n    lines_batch = trec['text'][i: i+batch_size]\n    ids_batch = [str(n) for n in range(i, i_end)]\n    # create embeddings\n    res = openai.Embedding.create(input=lines_batch, engine=MODEL)\n    embeds = [record['embedding'] for record in res['data']]\n    # prep metadata and upsert batch\n    meta = [{'text': line} for line in lines_batch]\n    to_upsert = zip(ids_batch, embeds, meta)\n    # upsert to Pinecone\n    index.upsert(vectors=list(to_upsert))\n```\n\n## Querying\n\nWith our data indexed, we're now ready to move onto performing searches. This follows a similar process to indexing. We start with a text `query`, that we would like to use to find similar sentences. As before we encode this with OpenAI's text similarity Babbage model to create a *query vector* `xq`. We then use `xq` to query the Pinecone index.\n\n```python\nquery = \"What caused the 1929 Great Depression?\"\n\nxq = openai.Embedding.create(input=query, engine=MODEL)['data'][0]['embedding']\n```\n\nNow we query.\n\n```python\nres = index.query([xq], top_k=5, include_metadata=True)\n```\n\nThe response from Pinecone includes our original text in the `metadata` field, let's print out the `top_k` most similar questions and their respective similarity scores.\n\n```python\nfor match in res['matches']:\n    print(f\"{match['score']:.2f}: {match['metadata']['text']}\")\n```\n\n```\n[Out]:\n0.95: Why did the world enter a global depression in 1929 ?\n0.87: When was `` the Great Depression '' ?\n0.86: What crop failure caused the Irish Famine ?\n0.82: What caused the Lynmouth floods ?\n0.79: What caused Harry Houdini 's death ?\n```\n\nLooks good, let's make it harder and replace *\"depression\"* with the incorrect term *\"recession\"*.\n\n```python\nquery = \"What was the cause of the major recession in the early 20th century?\"\n\n# create the query embedding\nxq = openai.Embedding.create(input=query, engine=MODEL)['data'][0]['embedding']\n\n# query, returning the top 5 most similar results\nres = index.query([xq], top_k=5, include_metadata=True)\n\nfor match in res['matches']:\n    print(f\"{match['score']:.2f}: {match['metadata']['text']}\")\n```\n\n```\n[Out]:\n0.92: Why did the world enter a global depression in 1929 ?\n0.85: What crop failure caused the Irish Famine ?\n0.83: When was `` the Great Depression '' ?\n0.82: What are some of the significant historical events of the 1990s ?\n0.82: What is considered the costliest disaster the insurance industry has ever faced ?\n```\n\nLet's perform one final search using the definition of depression rather than the word or related words.\n\n```python\nquery = \"Why was there a long-term economic downturn in the early 20th century?\"\n\n# create the query embedding\nxq = openai.Embedding.create(input=query, engine=MODEL)['data'][0]['embedding']\n\n# query, returning the top 5 most similar results\nres = index.query([xq], top_k=5, include_metadata=True)\n\nfor match in res['matches']:\n    print(f\"{match['score']:.2f}: {match['metadata']['text']}\")\n```\n\n```\n[Out]:\n0.93: Why did the world enter a global depression in 1929 ?\n0.83: What crop failure caused the Irish Famine ?\n0.82: When was `` the Great Depression '' ?\n0.82: How did serfdom develop in and then leave Russia ?\n0.80: Why were people recruited for the Vietnam War ?\n```\n\nIt's clear from this example that the semantic search pipeline is clearly able to identify the meaning between each of our queries. Using these embeddings with Pinecone allows us to return the most semantically similar questions from the already indexed TREC dataset.\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccce"
  },
  "filename": "databricks.md",
  "title": "Databricks",
  "category": "630fc5235d91a70054705fb4",
  "content": "---\ntitle: Databricks\ncategory: 630fc5235d91a70054705fb4\n---\n\n**Using Databricks and Pinecone to create and index vector embeddings at scale**\n\nDatabricks, built on top of [Apache Spark](https://spark.apache.org/), is a powerful platform for data processing and analytics, known for its ability to efficiently handle large datasets. In this guide, we will show you how to use Spark (with [Databricks](https://www.databricks.com/)) to create [vector embeddings](https://www.pinecone.io/learn/vector-embeddings/) and load them into [Pinecone](https://pinecone.io/).\n\nFirst, let’s discuss why using Databricks and Pinecone is necessary in this context. When you process less than a million records, using a single machine might be sufficient. But when you work with hundreds of millions of records, you have to start thinking about how the operation scales. We need to consider two things:\n\n1. How efficiently can we generate the embeddings at scale?\n2. How efficiently would we be able to ingest and update these embeddings, at scale?\n\nDatabricks is a great tool for creating embeddings at scale: it allows us to parallelize the process over multiple machines and leverage GPUs to accelerate the process.\n\nPinecone lets us efficiently ingest, update and query hundreds of millions or even billions of embeddings. As a managed service, Pinecone can guarantee a very high degree of reliability and performance when it comes to datasets of this size.\n\nPinecone provides a specialized connector for Databricks that is optimized to ingest data from Databricks and into Pinecone. That allows the ingestion process to be completed much faster than it would have if we were to use Pinecone’s REST or gRPC APIs on a large-scale dataset.\n\nTogether, Pinecone and Databricks make a great combination for managing the entire lifecycle of vector embeddings at scale.\n\n\n## Why Databricks?\n\nDatabricks is a Unified Analytics Platform on top of Apache Spark. The primary advantage of using Spark is its ability to distribute the workload across a cluster of machines, allowing it to process large amounts of data quickly and efficiently. By adding more machines or increasing the number of cores on each machine, it is easy to horizontally scale the cluster as needed to handle larger workloads.\n\nAt the core of Spark is the map-reduce pattern, where data is divided into partitions and a series of transformations is applied to each partition in parallel. The results from each partition are then automatically collected and aggregated into the final result. This approach makes Spark both fast and fault-tolerant, as it can retry failed tasks without requiring the entire workload to be reprocessed.\n\nIn addition to its parallel processing capabilities, Spark allows developers to write code in popular languages like Python and Scala, which are then optimized for parallel execution under the covers. This makes it easier for developers to focus on the data processing itself, rather than worrying about the details of distributed computing.\n\nVector embedding is a computationally intensive task, where parallelization can save many hours of precious computation time and resources. Leveraging GPUs with Spark can produce even better results — enjoying the benefits of the fast computation of a GPU combined with parallelization will ensure optimal performance.\n\nDatabricks makes it easier to work with Apache Spark: it provides easy set-up and tear-down of clusters, dependency management, compute allocation, storage solution integrations, and more.\n\n\n## Why Pinecone?\n\nPinecone is a vector database that makes it easy to build high-performance vector search applications. It offers a number of key benefits for dealing with vector embeddings at scale, including ultra-low query latency at any scale, live index updates when you add, edit, or delete data, and the ability to combine vector search with metadata filtering or [keyword search](https://docs.pinecone.io/docs/hybrid-search) for more relevant results. As mentioned before, Pinecone can easily handle very large scales of hundreds of millions and even billions of vector embeddings. Additionally, Pinecone is fully managed, so it's easy to use and scale.\n\nWith Pinecone, you can easily index and search through vector embeddings. It is ideal for a variety of use cases such as semantic text search, question-answering, visual search, recommendation systems, and more.\n\n\nIn this example, we'll create embeddings based on the [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) model from [Hugging Face](https://huggingface.co/). We'll then use a dataset with a large volume of documents to produce the embeddings and upsert them into Pinecone. Note that the actual model and dataset we'll use are immaterial for this example. This method should work on any embeddings you may want to create, with whatever dataset you may choose.\n\nIn order to create embeddings at scale, we need to do four things:\n\n1. Set up a Spark cluster\n2. Load the dataset into partitions\n3. Apply an embedding model on each entry to produce the embedding\n4. Save the results\n\nLet's get started!\n\n## Setting up a Spark Cluster\n\nUsing Databricks makes it easy to speed up the creation of our embedding even more by using GPUs instead of CPUs in our cluster. To do this, navigate to the \"Compute\" section in your Databricks console, and select the following options:\n\n![cluster-setup](https://raw.githubusercontent.com/pinecone-io/img/main/databricks_1.png)\n\nNext, we'll add the Pinecone Spark connector to our cluster. Navigate to the \"Libraries\" tab and click \"Instal\" new”.\n\n![install](https://raw.githubusercontent.com/pinecone-io/img/main/databricks_2.png)\n\nSelect \"DBF\"/S3” and paste the following S3 URI:\n\n`s3://pinecone-jars/spark-pinecone-uberjar.jar`\n\n![s3-install](https://raw.githubusercontent.com/pinecone-io/img/main/databricks_3.png)\n\nTo complete the installation, click \"Install\". To use the new cluster, create a new notebook and attach it to the newly created cluster.\n\n## Environment Setup\n\nWe'll start by installing some dependencies:\n\n```\n%pip install datasets transformers pinecone-client torch\n```\n\nNext, we'll set up the connection to Pinecone. You'll have to retrieve the following information from [your Pinecone console](https://app.pinecone.io):\n\n1. API Key: navigate to your project and click the \"API Keys\" button on the sidebar\n2. Environment: check the browser url to fetch the environment. `https://app.pinecone.io/organizations/[org-id]/projects/[environment]:[project_name]/indexes`\n\nYour index name will be the same index name used when we initialized the index (in this case, news).\n\n```python\nimport pinecone\n\napi_key = # <YOUR_PINECONE_API_KEY>\nenvironment = 'us-west1-gcp'\npinecone.init(api_key=api_key, environment=environment)\n```\n\nNext, we'll create a new index in Pinecone, where our vector embeddings will be saved:\n\n```python\nindex_name = 'news'\n\nif index_name in pinecone.list_indexes():\n   pinecone.delete_index(index_name)\npinecone.create_index(name=index_name, dimension=384)\nindex = pinecone.Index(index_name=index_name)\n```\n\n## Load the dataset into partitions\n\nIn this example, we'll use a collection of news articles as our example dataset. We'll use Hugging Face's datasets library and load the data into our environment:\n\n```python\nfrom datasets import list_datasets, load_dataset\n\ndataset_name = \"allenai/multinews_sparse_max\"\ndataset = load_dataset(dataset_name, split=\"train\")\n```\n\nNext, we'll convert the dataset from the Hugging Face format and repartition it:\n\n```python\ndataset.to_parquet('/dbfs/tmp/dataset_parquet.pq')\nnum_workers = 10\ndataset_df = spark.read.parquet('/tmp/dataset_parquet.pq').repartition(num_workers)\n```\n\nOnce the repartition is complete, we get back a DataFrame, which is a distributed collection of the data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. As mentioned above, each partition in the dataframe has an equal amount of the original data.\n\nThe dataset doesn't have identifiers associated with each document, so let's add them:\n\n```python\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.functions import monotonically_increasing_id\n\ndataset_df = dataset_df.withColumn('id', monotonically_increasing_id().cast(StringType()))\n```\n\nAs its name suggests, withColumn adds a column to the dataframe, containing a simple increasing identifier that we cast to a string. Great! Now we have identifiers for each document. Let's move on to creating the embeddings for each document.\n\n## Create a function for transforming text into embeddings\n\nIn this example, we will create a UDF (User Defined Function) to create the embeddings, using the AutoTokenizer and AutoModel classes from the Hugging Face transformers library. The UDF will be applied to each partition in a dataframe. When applied to a partition, a UDF is executed on each row in the partition. The UDF will tokenize the document using AutoTokenzier and then pass the result to the model (in this case we're using [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)). Finally, we'll produce the embeddings themselves by extracting the last hidden layer from the result.\n\nOnce the UDF is created, it can be applied to a dataframe to transform the data in the specified column. The Python UDF will be sent to the Spark workers, where it will be used to transform the data. After the transformation is complete, the results will be sent back to the driver program and stored in a new column.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ndef create_embeddings(partitionData):\n   tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n   model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n\n   for row in partitionData:\n       document = str(row.document)\n       inputs = tokenizer(document, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n       result = model(**inputs)\n       embeddings = result.last_hidden_state[:, 0, :].cpu().detach().numpy()\n       lst = embeddings.flatten().tolist()\n       yield [row.id, lst, '', '{}']\n```\n\n## Applying the UDF to the data\n\nA dataframe in Spark is a higher-level abstraction built on top of a more fundamental building block called an RDD - or Resilient Distributed Dataset. We're going to use the `mapPartitions` function that gives us finer control over the execution of our UDF, by explicitly applying it to each partition of the RDD.\n\n```python\nembeddings = dataset_df.rdd.mapPartitions(create_embeddings)\n```\n\nNext, we’ll convert the resulting RDD back into a dataframe with the schema required by Pinecone:\n\n```python\nfrom pyspark.sql.types import StructType,StructField, ArrayType, FloatType\n\nschema = StructType([\n    StructField(\"id\",StringType(),True),\n    StructField(\"vector\",ArrayType(FloatType()),True),\n    StructField(\"namespace\",StringType(),True),\n    StructField(\"metadata\", StringType(), True),\n  ])\n\nembeddings_df = spark.createDataFrame(data=embeddings,schema=schema)\n```\n\n## Upserting the embeddings\n\nLastly, we'll use the Pinecone Spark connector to save the embeddings to our index.\n\n```python\n\n(\n    df.write\n    .option(\"pinecone.apiKey\", api_key)\n    .option(\"pinecone.environment\", environment)\n    .option(\"pinecone.projectName\", pinecone.whoami().projectname)\n    .option(\"pinecone.indexName\", index_name)\n    .format(\"io.pinecone.spark.pinecone.Pinecone\")\n    .mode(\"append\")\n    .save()\n)\n\n```\n\nThe process of writing the embeddings to Pinecone should take approximately 15 seconds. When it completes, you’ll see the following:\n\n```\nspark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@41638051\n\npineconeOptions: scala.collection.immutable.Map[String,String] = Map(pinecone.apiKey -><YOUR API KEY>, pinecone.environment -> us-west1-gcp, pinecone.projectName -><YOUR PROJECT NAME>, pinecone.indexName -> \"news\")\n```\n\nThis means the process was completed successfully and the embeddings have been stored in Pinecone.\n\n\n## Summary\n\nCreating vector embeddings for large datasets can be challenging, but Databricks a great tool to accomplish the task. Databricks makes it easy to set up a GPU cluster and handle the required dependencies, allowing for efficient creation of embeddings at scale.\n\nDatabricks and Pinecone are the perfect combination for working with very large vector datasets. Pinecone provides a way to efficiently store and retrieve the vectors created by Databricks, making it easy and performant to work with a huge number of vectors. Overall, the combination of Databricks and Pinecone provides a powerful and effective solution for creating embeddings for very large datasets. By parallelizing the embedding generation and the data ingestion processes, we can create a fast and resilient pipeline that will be able to index and update large volumes of vectors.\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1cccf"
  },
  "filename": "cohere.md",
  "title": "Cohere",
  "category": "630fc5235d91a70054705fb4",
  "content": "---\ntitle: Cohere\ncategory: 630fc5235d91a70054705fb4\n---\n\n<div class=\"source\">\n  <!-- Source Link -->\n  <a href=\"https://github.com/pinecone-io/examples/blob/master/integrations/cohere/\" class=\"source-link\"><img src=\"../images/source.svg\" /> View Source</a>\n  <!-- Colab Link -->\n  <a href=\"https://colab.research.google.com/github/pinecone-io/examples/blob/master/integrations/cohere/semantic_search_trec.ipynb\" class=\"source-link\"><img src=\"../images/colab.svg\" /> Open in Colab</a>\n</div>\n\nIn this guide you will learn how to use the [Cohere Embed API endpoint](https://docs.cohere.ai/reference/embed) to generate language embeddings, and then index those embeddings in the [Pinecone vector database](https://www.pinecone.io) for fast and scalable vector search.\n\nThis is a powerful and common combination for building semantic search, question-answering, threat-detection, and other applications that rely on NLP and search over a large corpus of text data.\n\nThe basic workflow looks like this:\n* Embed and index\n  * Use the Cohere Embed API endpoint to generate vector embeddings of your documents (or any text data).\n  * Upload those vector embeddings into Pinecone, which can store and index millions/billions of these vector embeddings, and search through them at ultra-low latencies.\n* Search\n  * Pass your query text or document through the Cohere Embed API endpoint again.\n  * Take the resulting vector embedding and send it as a [query](/docs/query-data) to Pinecone.\n  * Get back semantically similar documents, even if they don't share any keywords with the query.\n\n![Basic workflow of Cohere with Pinecone](https://files.readme.io/fd0ba7b-pinecone-cohere-overview.png)\n\nLet's get started...\n\nhttps://files.readme.io/fd0ba7b-pinecone-cohere-overview.png\n\n## Environment Setup\n\nWe start by installing the Cohere and Pinecone clients, we will also need HuggingFace *Datasets* for downloading the TREC dataset that we will use in this guide.\n\n```bash\npip install -U cohere pinecone-client datasets\n```\n\n## Creating Embeddings\n\nTo create embeddings we must first initialize our connection to Cohere, we sign up for an API key at [Cohere](https://os.cohere.ai/).\n\n```python\nimport cohere\n\nco = cohere.Client(\"<<YOUR_API_KEY>>\")\n```\n\nWe will load the **T**ext **RE**trieval **C**onference (TREC) question classification dataset which contains 5.5K labeled questions. We will take the first 1K samples for this walkthrough, but this can be scaled to millions or even billions of samples.\n\n```python\nfrom datasets import load_dataset\n\n# load the first 1K rows of the TREC dataset\ntrec = load_dataset('trec', split='train[:1000]')\n```\n\nEach sample in `trec` contains two label features and the *text* feature, which we will be using. We can pass the questions from the *text* feature to Cohere to create embeddings.\n\n```python\nembeds = co.embed(\n    texts=trec['text'],\n    model='small',\n    truncate='LEFT'\n).embeddings\n```\n\nWe can check the dimensionality of the returned vectors, for this we will convert it from a list of lists to a Numpy array. We will need to save the embedding dimensionality from this to be used when initializing our Pinecone index later.\n\n```python\nimport numpy as np\n\nshape = np.array(embeds).shape\nprint(shape)\n```\n\n```\n[Out]:\n(1000, 1024)\n```\n\nHere we can see the `1024` embedding dimensionality produced by Cohere's small model, and the `1000` samples we built embeddings for.\n\n## Storing the Embeddings\n\nNow that we have our embeddings we can move on to indexing them in the Pinecone vector database. For this we need a Pinecone API key, [sign up for one here](https://app.pinecone.io).\n\nWe first initialize our connection to Pinecone, and then create a new index for storing the embeddings (we will call it `\"cohere-pinecone-trec\"`). When creating the index we specify that we would like to use the cosine similarity metric to align with Cohere's embeddings, and also pass the embedding dimensionality of `1024`.\n\n```python\nimport pinecone\n\npinecone.init(\"<<YOUR_API_KEY>>\", environment='us-west1-gcp')\n\nindex_name = 'cohere-pinecone-trec'\n\n# if the index does not exist, we create it\nif index_name not in pinecone.list_indexes():\n    pinecone.create_index(\n        index_name,\n        dimension=shape[1],\n        metric='cosine'\n    )\n\n# connect to index\nindex = pinecone.Index(index_name)\n```\n\nNow we can begin populating the index with our embeddings. Pinecone expects us to provide a list of tuples in the format *(id, vector, metadata)*, where the *metadata* field is an optional extra field where we can store anything we want in a dictionary format. For this example, we will store the original text of the embeddings.\n\n> ⚠️  Warning\n>\n> High-cardinality metadata values (like the unique text values we use here)\n> can reduce the number of vectors that fit on a single pod. See\n> [Limits](/limits/) for more.\n\nWhile uploading our data, we will batch everything to avoid pushing too much data in one go.\n\n```python\nbatch_size = 128\n\nids = [str(i) for i in range(shape[0])]\n# create list of metadata dictionaries\nmeta = [{'text': text} for text in trec['text']]\n\n# create list of (id, vector, metadata) tuples to be upserted\nto_upsert = list(zip(ids, embeds, meta))\n\nfor i in range(0, shape[0], batch_size):\n    i_end = min(i+batch_size, shape[0])\n    index.upsert(vectors=to_upsert[i:i_end])\n\n# let's view the index statistics\nprint(index.describe_index_stats())\n```\n\n```\n[Out]:\n{'dimension': 1024,\n 'index_fullness': 0.0,\n 'namespaces': {'': {'vector_count': 1000}}}\n```\n\nWe can see from `index.describe_index_stats` that we have a *1024-dimensionality* index populated with *1000* embeddings. The `indexFullness` metric tells us how full our index is, at the moment it is empty. Using the default value of one *p1* pod we can fit around 750K embeddings before the `indexFullness` reaches capacity. The [Usage Estimator](https://www.pinecone.io/pricing/) can be used to identify the number of pods required for a given number of *n*-dimensional embeddings.\n\n## Semantic Search\n\nNow that we have our indexed vectors we can perform a few search queries. When searching we will first embed our query using Cohere, and then search using the returned vector in Pinecone.\n\n```python\nquery = \"What caused the 1929 Great Depression?\"\n\n# create the query embedding\nxq = co.embed(\n    texts=[query],\n    model='small',\n    truncate='LEFT'\n).embeddings\n\nprint(np.array(xq).shape)\n\n# query, returning the top 5 most similar results\nres = index.query(xq, top_k=5, include_metadata=True)\n```\n\nThe response from Pinecone includes our original text in the `metadata` field, let's print out the `top_k` most similar questions and their respective similarity scores.\n\n```python\nfor match in res['matches']:\n    print(f\"{match['score']:.2f}: {match['metadata']['text']}\")\n```\n\n```\n[Out]:\n0.83: Why did the world enter a global depression in 1929 ?\n0.75: When was `` the Great Depression '' ?\n0.50: What crop failure caused the Irish Famine ?\n0.34: What war did the Wanna-Go-Home Riots occur after ?\n0.34: What were popular songs and types of songs in the 1920s ?\n```\n\nLooks good, let's make it harder and replace *\"depression\"* with the incorrect term *\"recession\"*.\n\n```python\nquery = \"What was the cause of the major recession in the early 20th century?\"\n\n# create the query embedding\nxq = co.embed(\n    texts=[query],\n    model='small',\n    truncate='LEFT'\n).embeddings\n\n# query, returning the top 5 most similar results\nres = index.query(xq, top_k=5, include_metadata=True)\n\nfor match in res['matches']:\n    print(f\"{match['score']:.2f}: {match['metadata']['text']}\")\n```\n\n```\n[Out]:\n0.66: Why did the world enter a global depression in 1929 ?\n0.61: When was `` the Great Depression '' ?\n0.43: What are some of the significant historical events of the 1990s ?\n0.43: What crop failure caused the Irish Famine ?\n0.37: What were popular songs and types of songs in the 1920s ?\n```\n\nLet's perform one final search using the definition of depression rather than the word or related words.\n\n```python\nquery = \"Why was there a long-term economic downturn in the early 20th century?\"\n\n# create the query embedding\nxq = co.embed(\n    texts=[query],\n    model='small',\n    truncate='LEFT'\n).embeddings\n\n# query, returning the top 10 most similar results\nres = index.query(xq, top_k=10, include_metadata=True)\n\nfor match in res['matches']:\n    print(f\"{match['score']:.2f}: {match['metadata']['text']}\")\n```\n\n```\n[Out]:\n0.71: Why did the world enter a global depression in 1929 ?\n0.62: When was `` the Great Depression '' ?\n0.40: What crop failure caused the Irish Famine ?\n0.38: What are some of the significant historical events of the 1990s ?\n0.38: When did the Dow first reach ?\n```\n\nIt's clear from this example that the semantic search pipeline is clearly able to identify the meaning between each of our queries. Using these embeddings with Pinecone allows us to return the most semantically similar questions from the already indexed TREC dataset.\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccd0"
  },
  "filename": "hugging-face-endpoints.md",
  "title": "Hugging Face Inference Endpoints",
  "category": "630fc5235d91a70054705fb4",
  "content": "---\ntitle: Hugging Face Inference Endpoints\ncategory: 630fc5235d91a70054705fb4\n---\n\nHugging Face Inference Endpoints allows access to straightforward model inference. Coupled with Pinecone we can generate and index high-quality vector embeddings with ease.\n\nLet's get started by initializing an Inference Endpoint for generating vector embeddings.\n\n## Endpoints\n\nWe start by heading over to the [Hugging Face Inference Endpoints homepage](https://ui.endpoints.huggingface.co/endpoints) and signing up for an account if needed. After, we should find ourselves on this page:\n\n![endpoints 0](https://github.com/pinecone-io/examples/blob/master/integrations/hugging-face/endpoints/assets/hf-endpoints-0.png?raw=true)\n\nWe click on **Create new endpoint**, choose a model repository (eg name of the model), endpoint name (this can be anything), and select a cloud environment. Before moving on it is *very important* that we set the **Task** to **Sentence Embeddings** (found within the *Advanced configuration* settings).\n\n![endpoints 1](https://github.com/pinecone-io/examples/blob/master/integrations/hugging-face/endpoints/assets/hf-endpoints-1.png?raw=true)\n\n![endpoints 2](https://github.com/pinecone-io/examples/blob/master/integrations/hugging-face/endpoints/assets/hf-endpoints-2.png?raw=true)\n\nOther important options include the *Instance Type*, by default this uses CPU which is cheaper but also slower. For faster processing we need a GPU instance. And finally, we set our privacy setting near the end of the page.\n\nAfter setting our options we can click **Create Endpoint** at the bottom of the page. This action should take use to the next page where we will see the current status of our endpoint.\n\n![endpoints 3](https://github.com/pinecone-io/examples/blob/master/integrations/hugging-face/endpoints/assets/hf-endpoints-3.png?raw=true)\n\nOnce the status has moved from **Building** to **Running** (this can take some time), we're ready to begin creating embeddings with it.\n\n## Creating Embeddings\n\nEach endpoint is given an **Endpoint URL**, it can be found on the endpoint **Overview** page. We need to assign this endpoint URL to the `endpoint_url` variable.\n\n![endpoints 4](https://github.com/pinecone-io/examples/blob/master/integrations/hugging-face/endpoints/assets/hf-endpoints-4.png?raw=true)\n\n\n```python\nendpoint = \"<<ENDPOINT_URL>>\"\n```\n\nWe will also need the organization API token, we find this via the organization settings on Hugging Face (`https://huggingface.co/organizations/<ORG_NAME>/settings/profile`). This is assigned to the `api_org` variable.\n\n![endpoints 5](https://github.com/pinecone-io/examples/blob/master/integrations/hugging-face/endpoints/assets/hf-endpoints-5.png?raw=true)\n\n\n```python\napi_org = \"<<API_ORG_TOKEN>>\"\n```\n\nNow we're ready to create embeddings via Inference Endpoints. Let's start with a toy example.\n\n\n```python\nimport requests\n\n# add the api org token to the headers\nheaders = {\n    'Authorization': f'Bearer {api_org}'\n}\n# we add sentences to embed like so\njson_data = {\"inputs\": [\"a happy dog\", \"a sad dog\"]}\n# make the request\nres = requests.post(\n    endpoint,\n    headers=headers,\n    json=json_data\n)\n```\n\nWe should see a `200` response.\n\n\n```python\nres\n```\n\n\n\n\n    <Response [200]>\n\n\n\nInside the response we should find two embeddings...\n\n\n```python\nlen(res.json()['embeddings'])\n```\n\n\n\n\n    2\n\n\n\nWe can also see the dimensionality of our embeddings like so:\n\n\n```python\ndim = len(res.json()['embeddings'][0])\ndim\n```\n\n\n\n\n    768\n\n\n\nWe will need more than two items to search through, so let's download a larger dataset. For this we will use Hugging Face datasets.\n\n\n```python\nfrom datasets import load_dataset\n\nsnli = load_dataset(\"snli\", split='train')\nsnli\n```\n\n    Downloading: 100%|██████████| 1.93k/1.93k [00:00<00:00, 992kB/s]\n    Downloading: 100%|██████████| 1.26M/1.26M [00:00<00:00, 31.2MB/s]\n    Downloading: 100%|██████████| 65.9M/65.9M [00:01<00:00, 57.9MB/s]\n    Downloading: 100%|██████████| 1.26M/1.26M [00:00<00:00, 43.6MB/s]\n\n    Dataset({\n        features: ['premise', 'hypothesis', 'label'],\n        num_rows: 550152\n    })\n\n\n\nSNLI contains 550K sentence pairs, many of these include duplicate items so we will take just one set of these (the *hypothesis*) and deduplicate them.\n\n\n```python\npassages = list(set(snli['hypothesis']))\nlen(passages)\n```\n\n\n\n\n    480042\n\n\n\nWe will drop to 50K sentences so that the example is quick to run, if you have time, feel free to keep the full 480K.\n\n\n```python\npassages = passages[:50_000]\n```\n\n## Vector DB\n\nWith our endpoint and dataset ready, all that we're missing is a vector database. For this, we need to initialize our connection to Pinecone, this requires a [free API key](https://app.pinecone.io/).\n\n\n```python\nimport pinecone\n\n# connect to pinecone environment\npinecone.init(\n    api_key=\"<<YOUR_API_KEY>>\",\n    environment=\"us-west1-gcp\"\n)\n```\n\nNow we create a new index called `'hf-endpoints'`, the name isn't important *but* the `dimension` must align to our endpoint model output dimensionality (we found this in `dim` above) and the model metric (typically `cosine` is okay, but not for all models).\n\n\n```python\nindex_name = 'hf-endpoints'\n\n# check if the hf-endpoints index exists\nif index_name not in pinecone.list_indexes():\n    # create the index if it does not exist\n    pinecone.create_index(\n        index_name,\n        dimension=dim,\n        metric=\"cosine\"\n    )\n\n# connect to hf-endpoints index we created\nindex = pinecone.Index(index_name)\n```\n\n## Create and Index Embeddings\n\nNow we have all of our components ready; endpoints, dataset, and Pinecone. Let's go ahead and create our dataset embeddings and index them within Pinecone.\n\n\n```python\nfrom tqdm.auto import tqdm\n\n# we will use batches of 64\nbatch_size = 64\n\nfor i in tqdm(range(0, len(passages), batch_size)):\n    # find end of batch\n    i_end = min(i+batch_size, len(passages))\n    # extract batch\n    batch = passages[i:i_end]\n    # generate embeddings for batch via endpoints\n    res = requests.post(\n        endpoint,\n        headers=headers,\n        json={\"inputs\": batch}\n    )\n    emb = res.json()['embeddings']\n    # get metadata (just the original text)\n    meta = [{'text': text} for text in batch]\n    # create IDs\n    ids = [str(x) for x in range(i, i_end)]\n    # add all to upsert list\n    to_upsert = list(zip(ids, emb, meta))\n    # upsert/insert these records to pinecone\n    _ = index.upsert(vectors=to_upsert)\n\n# check that we have all vectors in index\nindex.describe_index_stats()\n```\n\n    100%|██████████| 782/782 [11:02<00:00,  1.18it/s]\n\n\n    {'dimension': 768,\n     'index_fullness': 0.1,\n     'namespaces': {'': {'vector_count': 50000}},\n     'total_vector_count': 50000}\n\n\n\nWith everything indexed we can begin querying. We will take a few examples from the *premise* column of the dataset.\n\n\n```python\nquery = snli['premise'][0]\nprint(f\"Query: {query}\")\n# encode with HF endpoints\nres = requests.post(endpoint, headers=headers, json={\"inputs\": query})\nxq = res.json()['embeddings']\n# query and return top 5\nxc = index.query(xq, top_k=5, include_metadata=True)\n# iterate through results and print text\nprint(\"Answers:\")\nfor match in xc['matches']:\n    print(match['metadata']['text'])\n```\n\n    Query: A person on a horse jumps over a broken down airplane.\n    Answers:\n    The horse jumps over a toy airplane.\n    a lady rides a horse over a plane shaped obstacle\n    A person getting onto a horse.\n    person rides horse\n    A woman riding a horse jumps over a bar.\n\n\nThese look good, let's try a couple more examples.\n\n\n```python\nquery = snli['premise'][100]\nprint(f\"Query: {query}\")\n# encode with HF endpoints\nres = requests.post(endpoint, headers=headers, json={\"inputs\": query})\nxq = res.json()['embeddings']\n# query and return top 5\nxc = index.query(xq, top_k=5, include_metadata=True)\n# iterate through results and print text\nprint(\"Answers:\")\nfor match in xc['matches']:\n    print(match['metadata']['text'])\n```\n\n    Query: A woman is walking across the street eating a banana, while a man is following with his briefcase.\n    Answers:\n    A woman eats a banana and walks across a street, and there is a man trailing behind her.\n    A woman eats a banana split.\n    A woman is carrying two small watermelons and a purse while walking down the street.\n    The woman walked across the street.\n    A woman walking on the street with a monkey on her back.\n\n\nAnd one more...\n\n\n```python\nquery = snli['premise'][200]\nprint(f\"Query: {query}\")\n# encode with HF endpoints\nres = requests.post(endpoint, headers=headers, json={\"inputs\": query})\nxq = res.json()['embeddings']\n# query and return top 5\nxc = index.query(xq, top_k=5, include_metadata=True)\n# iterate through results and print text\nprint(\"Answers:\")\nfor match in xc['matches']:\n    print(match['metadata']['text'])\n```\n\n    Query: People on bicycles waiting at an intersection.\n    Answers:\n    A pair of people on bikes are waiting at a stoplight.\n    Bike riders wait to cross the street.\n    people on bicycles\n    Group of bike riders stopped in the street.\n    There are bicycles outside.\n\n\nAll of these results look excellent. If you are not planning on running your endpoint and vector DB beyond this tutorial, you can shut down both.\n\n**Once the index is deleted, you cannot use it again.**\n\nShut down the endpoint by navigating to the Inference Endpoints **Overview** page and selecting **Delete endpoint**. Delete the Pinecone index with:\n\n\n```python\npinecone.delete_index(index_name)\n```\n\n---\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccd1"
  },
  "filename": "haystack.md",
  "title": "Haystack",
  "category": "630fc5235d91a70054705fb4",
  "content": "---\ntitle: Haystack\ncategory: 630fc5235d91a70054705fb4\n---\n\nIn this guide we will see how to integrate Pinecone and the popular [Haystack library](https://github.com/deepset-ai/haystack) for *Question-Answering*.\n\n## Installing Haystack\n\nWe start by installing the latest version of Haystack with all dependencies required for the `PineconeDocumentStore`.\n\n```python\n!pip install -U farm-haystack>=1.3.0 pinecone-client datasets\n```\n\n## Initializing the PineconeDocumentStore\n\nWe initialize a `PineconeDocumentStore` by providing an API key and environment name. [Create an account](https://app.pinecone.io) to get your free API key.\n\n```python\nfrom haystack.document_stores import PineconeDocumentStore\n\ndocument_store = PineconeDocumentStore(\n    api_key='<<YOUR_API_KEY>>',\n    index='haystack-extractive-qa',\n    similarity=\"cosine\",\n    embedding_dim=384\n)\n```\n\n    INFO - haystack.document_stores.pinecone -  Index statistics: name: haystack-extractive-qa, embedding dimensions: 384, record count: 0\n\n## Data Preparation\n\nBefore adding data to the document store, we must download and convert data into the Document format that Haystack uses.\n\nWe will use the SQuAD dataset available from Hugging Face Datasets.\n\n```python\nfrom datasets import load_dataset\n\n# load the squad dataset\ndata = load_dataset(\"squad\", split=\"train\")\n```\n\nNext, we remove duplicates and unecessary columns.\n\n```python\n# convert to a pandas dataframe\ndf = data.to_pandas()\n# select only title and context column\ndf = df[[\"title\", \"context\"]]\n# drop rows containing duplicate context passages\ndf = df.drop_duplicates(subset=\"context\")\ndf.head()\n```\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>context</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>University_of_Notre_Dame</td>\n      <td>Architecturally, the school has a Catholic cha...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>University_of_Notre_Dame</td>\n      <td>As at most other universities, Notre Dame's st...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>University_of_Notre_Dame</td>\n      <td>The university is the major seat of the Congre...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>University_of_Notre_Dame</td>\n      <td>The College of Engineering was established in ...</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>University_of_Notre_Dame</td>\n      <td>All of Notre Dame's undergraduate students are...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\nThen convert these records into the Document format.\n\n```python\nfrom haystack import Document\n\ndocs = []\nfor d in df.iterrows():\n    d = d[1]\n    # create haystack document object with text content and doc metadata\n    doc = Document(\n        content=d[\"context\"],\n        meta={\n            \"title\": d[\"title\"],\n            'context': d['context']\n        }\n    )\n    docs.append(doc)\n```\n\nThis `Document` format contains two fields; *'content'* for the text content or paragraphs, and *'meta'* where we can place any additional information that can later be used to apply metadata filtering in our search.\n\nNow we upsert the documents to Pinecone.\n\n```python\n# upsert the data document to pinecone index\ndocument_store.write_documents(docs)\n```\n\n## Initialize Retriever\n\nThe next step is to create embeddings from these documents. We will use Haystacks `EmbeddingRetriever` with a SentenceTransformer model (`multi-qa-MiniLM-L6-cos-v1`) which has been designed for question-answering.\n\n\n```python\nfrom haystack.retriever.dense import EmbeddingRetriever\n\nretriever = EmbeddingRetriever(\n    document_store=document_store,\n    embedding_model=\"multi-qa-MiniLM-L6-cos-v1\",\n    model_format=\"sentence_transformers\"\n)\n```\n\nThen we run the `PineconeDocumentStore.update_embeddings` method with the `retriever` provided as an argument. GPU acceleration can greatly reduce the time required for this step.\n\n```python\ndocument_store.update_embeddings(\n    retriever,\n    batch_size=16\n)\n```\n\n## Inspect Documents and Embeddings\n\nWe can get documents by their ID with the `PineconeDocumentStore.get_documents_by_id` method.\n\n```python\nd = document_store.get_documents_by_id(ids=['49091c797d2236e73fab510b1e9c7f6b'], return_embedding=True)[0]\n```\n\nFrom here we return can view document content with `d.content` and the document embedding with `d.embedding`.\n\n## Initializing an Extractive QA Pipeline\n\nAn `ExtractiveQAPipeline` contains three key components by default:\n\n* a document store (`PineconeDocumentStore`)\n* a retriever model\n* a reader model\n\nWe use the `deepset/electra-base-squad2` model from the HuggingFace model hub as our reader model. \n\n```python\nfrom haystack.nodes import FARMReader\n\nreader = FARMReader(\n    model_name_or_path='deepset/electra-base-squad2', \n    use_gpu=True\n)\n```\n\nWe are now ready to initialize the `ExtractiveQAPipeline`.\n\n\n```python\nfrom haystack.pipelines import ExtractiveQAPipeline\n\npipe = ExtractiveQAPipeline(reader, retriever)\n```\n\n## Asking Questions\n\nUsing our QA pipeline we can begin querying with `pipe.run`.\n\n\n```python\nfrom haystack.utils import print_answers\n\nquery = \"What was Albert Einstein famous for?\"\n# get the answer\nanswer = pipe.run(\n    query=query,\n    params={\n        \"Retriever\": {\"top_k\": 1},\n    }\n)\n# print the answer(s)\nprint_answers(answer)\n```\n\n\n    Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.53 Batches/s]\n\n    \n    Query: What was Albert Einstein famous for?\n    Answers:\n    [   <Answer {\n        'answer': 'his theories of special relativity and general relativity', 'type': 'extractive', 'score': 0.993550717830658,\n        'context': 'Albert Einstein is known for his theories of special relativity and general relativity. He also made important contributions to statistical mechanics,',\n        'offsets_in_document': [{'start': 29, 'end': 86}],\n        'offsets_in_context': [{'start': 29, 'end': 86}], \n        'document_id': '23357c05e3e46bacea556705de1ea6a5',\n        'meta': {\n            'context': 'Albert Einstein is known for his theories of special relativity and general relativity. He also made important contributions to statistical mechanics, especially his mathematical treatment of Brownian motion, his resolution of the paradox of specific heats, and his connection of fluctuations and dissipation. Despite his reservations about its interpretation, Einstein also made contributions to quantum mechanics and, indirectly, quantum field theory, primarily through his theoretical studies of the photon.', 'title': 'Modern_history'\n        }\n    }>]\n\n\n```python\nquery = \"How much oil is Egypt producing in a day?\"\n# get the answer\nanswer = pipe.run(\n    query=query,\n    params={\n        \"Retriever\": {\"top_k\": 1},\n    }\n)\n# print the answer(s)\nprint_answers(answer)\n```\n\n    Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.81 Batches/s]\n\n    \n    Query: How much oil is Egypt producing in a day?\n    Answers:\n    [   <Answer {\n        'answer': '691,000 bbl/d', 'type': 'extractive', 'score': 0.9999906420707703,\n        'context': 'Egypt was producing 691,000 bbl/d of oil and 2,141.05 Tcf of natural gas (in 2013), which makes Egypt as the largest oil producer not member of the Or',\n        'offsets_in_document': [{'start': 20, 'end': 33}],\n        'offsets_in_context': [{'start': 20, 'end': 33}],\n        'document_id': '57ed9720050a17237e323da5e3969a9b',\n        'meta': {\n            'context': 'Egypt was producing 691,000 bbl/d of oil and 2,141.05 Tcf of natural gas (in 2013), which makes Egypt as the largest oil producer not member of the Organization of the Petroleum Exporting Countries (OPEC) and the second-largest dry natural gas producer in Africa. In 2013, Egypt was the largest consumer of oil and natural gas in Africa, as more than 20% of total oil consumption and more than 40% of total dry natural gas consumption in Africa. Also, Egypt possesses the largest oil refinery capacity in Africa 726,000 bbl/d (in 2012). Egypt is currently planning to build its first nuclear power plant in El Dabaa city, northern Egypt.', 'title': 'Egypt'\n        }\n    }>]\n\n\n    \n\n\n\n```python\nquery = \"What are the first names of the youtube founders?\"\n# get the answer\nanswer = pipe.run(\n    query=query,\n    params={\n        \"Retriever\": {\"top_k\": 1},\n    }\n)\n# print the answer(s)\nprint_answers(answer)\n```\n\n\n    Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.83 Batches/s]\n\n    \n    Query: What are the first names of the youtube founders?\n    Answers:\n    [   <Answer {\n        'answer': 'Hurley and Chen', 'type': 'extractive', 'score': 0.9998972713947296,\n        'context': 'According to a story that has often been repeated in the media, Hurley and Chen developed the idea for YouTube during the early months of 2005, after ',\n        'offsets_in_document': [{'start': 64, 'end': 79}],\n        'offsets_in_context': [{'start': 64, 'end': 79}],\n        'document_id': 'bd1cbd61ab617d840c5f295e21e80092',\n        'meta': {\n            'context': 'According to a story that has often been repeated in the media, Hurley and Chen developed the idea for YouTube during the early months of 2005, after they had experienced difficulty sharing videos that had been shot at a dinner party at Chen\\'s apartment in San Francisco. Karim did not attend the party and denied that it had occurred, but Chen commented that the idea that YouTube was founded after a dinner party \"was probably very strengthened by marketing ideas around creating a story that was very digestible\".', 'title': 'YouTube'\n        }\n    }>]\n\n\n\nWe can return multiple answers by setting the `top_k` parameter.\n\n\n```python\nquery = \"Who was the first person to step foot on the moon?\"\n# get the answer\nanswer = pipe.run(\n    query=query,\n    params={\n        \"Retriever\": {\"top_k\": 3},\n    }\n)\n# print the answer(s)\nprint_answers(answer)\n```\n\n\n\n\n    Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.71 Batches/s]\n    Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.78 Batches/s]\n    Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.88 Batches/s]\n\n    \n    Query: Who was the first person to step foot on the moon?\n    Answers:\n    [   <Answer {\n        'answer': 'Armstrong', 'type': 'extractive', 'score': 0.9998227059841156, \n        'context': 'The trip to the Moon took just over three days. After achieving orbit, Armstrong and Aldrin transferred into the Lunar Module, named Eagle, and after ', \n        'offsets_in_document': [{'start': 71, 'end': 80}], \n        'offsets_in_context': [{'start': 71, 'end': 80}], \n        'document_id': 'f74e1bf667e68d72e45437a7895df921', \n        'meta': {\n            'context': 'The trip to the Moon took just over three days. After achieving orbit, Armstrong and Aldrin transferred into the Lunar Module, named Eagle, and after a landing gear inspection by Collins remaining in the Command/Service Module Columbia, began their descent. After overcoming several computer overload alarms caused by an antenna switch left in the wrong position, and a slight downrange error, Armstrong took over manual flight control at about 180 meters (590 ft), and guided the Lunar Module to a safe landing spot at 20:18:04 UTC, July 20, 1969 (3:17:04 pm CDT). The first humans on the Moon would wait another six hours before they ventured out of their craft. At 02:56 UTC, July 21 (9:56 pm CDT July 20), Armstrong became the first human to set foot on the Moon.', 'title': 'Space_Race'\n            }\n        }>, <Answer {\n        'answer': 'Frank Borman', 'type': 'extractive', 'score': 0.7770257890224457, \n        'context': 'On December 21, 1968, Frank Borman, James Lovell, and William Anders became the first humans to ride the Saturn V rocket into space on Apollo 8. They ', \n        'offsets_in_document': [{'start': 22, 'end': 34}], \n        'offsets_in_context': [{'start': 22, 'end': 34}], \n        'document_id': '2bc046ba90d94fe201ccde9d20552200', \n        'meta': {\n            'context': \"On December 21, 1968, Frank Borman, James Lovell, and William Anders became the first humans to ride the Saturn V rocket into space on Apollo 8. They also became the first to leave low-Earth orbit and go to another celestial body, and entered lunar orbit on December 24. They made ten orbits in twenty hours, and transmitted one of the most watched TV broadcasts in history, with their Christmas Eve program from lunar orbit, that concluded with a reading from the biblical Book of Genesis. Two and a half hours after the broadcast, they fired their engine to perform the first trans-Earth injection to leave lunar orbit and return to the Earth. Apollo 8 safely landed in the Pacific ocean on December 27, in NASA's first dawn splashdown and recovery.\", 'title': 'Space_Race'\n            }\n        }>, <Answer {\n        'answer': 'Aldrin', 'type': 'extractive', 'score': 0.6680101901292801, \n        'context': ' were, \"That\\'s one small step for [a] man, one giant leap for mankind.\" Aldrin joined him on the surface almost 20 minutes later. Altogether, they spe', \n        'offsets_in_document': [{'start': 240, 'end': 246}], \n        'offsets_in_context': [{'start': 72, 'end': 78}], \n        'document_id': 'ae1c366b1eaf5fc9d32a8d81f76bd795', \n        'meta': {\n            'context': 'The first step was witnessed by at least one-fifth of the population of Earth, or about 723 million people. His first words when he stepped off the LM\\'s landing footpad were, \"That\\'s one small step for [a] man, one giant leap for mankind.\" Aldrin joined him on the surface almost 20 minutes later. Altogether, they spent just under two and one-quarter hours outside their craft. The next day, they performed the first launch from another celestial body, and rendezvoused back with Columbia.', 'title': 'Space_Race'\n            }\n        }>\n    ]\n\n\n\n\n---\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccd2"
  },
  "filename": "overview.md",
  "title": "Overview",
  "category": "630fc5235d91a70054705fb5",
  "content": "---\ntitle: Overview\ncategory: 630fc5235d91a70054705fb5\n---\n\nPinecone makes it easy to build high-performance **vector search** applications. It’s a managed, cloud-native vector database with a **simple API** and no infrastructure hassles.\n\nKey benefits of Pinecone:\n\n* Fast: Ultra-low query latency at any scale, even with billions of items.\n* Fresh: Live index updates when you add, edit, or delete data.\n* Filtered: Combine vector search with metadata filters for more relevant, faster results.\n* Fully managed: Easy to start, use, and scale, while we keep things running smoothly and securely.\n\n\n## Key concepts\n\n#### Vector search\n\nUnlike traditional search methods that revolve around keywords, it is done by indexing and searching through ML-generated representations of data — vector embeddings — to find items most similar to the query.\n\n#### Vector embeddings\n\n[Vector embeddings](https://www.pinecone.io/learn/vector-embeddings/), or “vectors,” are sets of floating-point numbers that represent objects. They are generated by [embedding models](https://www.pinecone.io/learn/sentence-embeddings/) trained to capture the semantic similarity of objects in a given set.\n\nYou need to have vector embeddings to use Pinecone.\n\n#### Vector database\n\nA [vector database](https://www.pinecone.io/learn/vector-database/) indexes and stores vector embeddings for efficient management and fast retrieval. Unlike a standalone [vector index](https://www.pinecone.io/learn/vector-indexes/), a vector database like Pinecone provides additional capabilities such as index management, data management, metadata storage and filtering, and horizontal scaling.\n\n## Example use cases\n\nWant to see more and start with working example notebooks? See: [Example Applications](/docs/examples)\n\n* Semantic text search: Convert text data into vector embeddings using an [NLP](https://www.pinecone.io/learn/nlp/) transformer (eg, [a sentence embedding model](https://www.pinecone.io/learn/sentence-embeddings/)), then index and search through those vectors using Pinecone.\n* Question-answering: Index a set of questions as vectors and retrieve the most similar question’s answer for any new question.\n* Image similarity search: Transform image data into vector embeddings and build an index with Pinecone. Then convert query images into vectors and retrieve similar images.\n* Product recommendations: Generate product recommendations for ecommerce based on vectors representing users.\n\n\n## Overview of the workflow\n\n![workflow](https://raw.githubusercontent.com/pinecone-io/img/main/workflow.png)\n\nFollow these guides to set up your index::\n\n1. [Create an index](https://docs.pinecone.io/docs/manage-indexes)\n2. [Connect to an index](https://docs.pinecone.io/docs/manage-data#connect)\n3. [Insert the data](https://docs.pinecone.io/docs/insert-data) (and vectors) into the index\n\nOnce you have an index with data, follow these guides to start using your index:\n\n- [Query the data](https://docs.pinecone.io/docs/query-data)\n  - [Filter the data](https://docs.pinecone.io/docs/metadata-filtering)\n- [Fetch data](https://docs.pinecone.io/docs/manage-data#fetching-an-item)\n- [Insert more data](https://docs.pinecone.io/docs/insert-data) or update existing vectors\n- [Manage the index](https://docs.pinecone.io/docs/manage-indexes)\n- [Manage data](https://docs.pinecone.io/docs/manage-data)\n\n## Pricing and deployment options\n\n[Visit the pricing page](https://www.pinecone.io/pricing/) for pricing and deployment options.\n\nGet started with Pinecone\n---------------\n\n[Go to the quickstart guide](https://docs.pinecone.io/docs/quickstart) to get a production-ready vector search service up and running in minutes.\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccd3"
  },
  "filename": "quickstart.md",
  "title": "Quickstart",
  "category": "630fc5235d91a70054705fb5",
  "content": "---\ntitle: Quickstart\ncategory: 630fc5235d91a70054705fb5\n---\n\n\nThis guide explains how to set up a Pinecone client and get a production-ready similarity search service up and running in minutes.\n\n## 1. Install Pinecone (optional)\n\nThis step is optional. Do this step only if you want to use the Python client.\n\nUse the following shell command to install Pinecone:\n\n```python\npip install pinecone-client\n```\n\nFor more information on how to install Pinecone clients, including troubleshooting, see [Installation](installation).\n\n## 2. Get and verify your Pinecone API key\n\nTo use Pinecone, you must have an API key. To find your API key, open the [Pinecone console](https://app.pinecone.io), click your project name, and click **API Keys**. This view also displays the environment for your project. Note both your API key and your environment.\n\nTo verify that your Pinecone API key works, use the following commands:\n\n```python\nimport pinecone\n\npinecone.init(api_key=\"YOUR_API_KEY\", environment=\"YOUR_ENVIRONMENT\")\n```\n```shell curl\ncurl -i https://controller.YOUR_ENVIRONMENT.pinecone.io/actions/whoami -H 'Api-Key: YOUR_API_KEY'\n```\n. \n\nIf you don't receive an error message, then your API key is valid.\n\n## 3. Hello, Pinecone!\n\nYou can complete the remaining steps in three ways:\n\n- Use the [\"Hello, Pinecone!\" colab notebook](https://colab.research.google.com/github/pinecone-io/examples/blob/master/quick_tour/hello_pinecone.ipynb) to write and execute Python in your browser.\n- Copy the commands below into your local installation of Python.\n- Use the cURL API commands below.\n\n1\\. Initialize Pinecone\n\n```python\nimport pinecone\npinecone.init(api_key=\"YOUR_API_KEY\", environment=\"YOUR_ENVIRONMENT\")\n```\n```shell curl\n# Not applicable\n```\n\n2\\. Create an index.\n\nThe commands below create an index named \"quickstart\" that performs approximate nearest-neighbor search using the [Euclidean distance metric](https://www.pinecone.io/docs/manage-indexes/#distance-metrics) for 8-dimensional vectors.\n\nIndex creation takes roughly a minute.\n\n```python\npinecone.create_index(\"quickstart\", dimension=8, metric=\"euclidean\", pod_type=\"p1\")\n```\n```shell curl\ncurl -i -X POST \\\n  -H 'Content-Type: application/json' \\\n  -H 'Api-Key: YOUR_API_KEY_HERE' \\\n  https://controller.YOUR_ENVIRONMENT.pinecone.io/databases \\\n  -d '{\n    \"name\": \"quickstart\",\n    \"dimension\": 8,\n    \"metric\": \"euclidean\"\n  }'\n```\n\n\n> ⚠️  Warning\n> Indexes on the Starter (free) plan are deleted after 14 days of inactivity. To prevent this, send any API request or log into the console. This will count as activity.\n\n3\\. Retrieve a list of your indexes.\n\n Once your index is created, its name appears in the index list.\n\n Use the following commands to return a list of your indexes.\n\n```python\npinecone.list_indexes()\n# Returns:\n# ['quickstart']\n```\n```shell curl\ncurl -i https://controller.YOUR_ENVIRONMENT.pinecone.io/databases \\\n  -H \"Api-Key: YOUR_API_KEY\"\n# Output:\n# [\"quickstart\"]\n```\n\n4\\. Connect to the index (Client only).\n\nBefore you can query your index using a client, you must connect to the index.\n\nUse the following commands to connect to your index.\n\n```python\nindex = pinecone.Index(\"quickstart\")\n```\n```shell curl\n# Not applicable\n```\n\n5\\. Insert the data.\n\nTo ingest vectors into your index, use the [`upsert`](https://www.pinecone.io/docs/api/operation/upsert/) operation. \n\nThe upsert operation inserts a new vector in the index or updates the vector if a vector with the same ID is already present.\n\nThe following commands upsert 5 8-dimensional vectors into your index.\n\n```python\n# Upsert sample data (5 8-dimensional vectors)\nindex.upsert([\n    (\"A\", [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]),\n    (\"B\", [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]),\n    (\"C\", [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]),\n    (\"D\", [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]),\n    (\"E\", [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n])\n```\n```shell curl\ncurl -i -X POST https://quickstart-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/upsert \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"vectors\": [\n      {\n        \"id\": \"A\",\n        \"values\": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n      },\n      {\n        \"id\": \"B\",\n        \"values\": [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n      },\n      {\n        \"id\": \"C\",\n        \"values\": [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]\n      },\n      {\n        \"id\": \"D\",\n        \"values\": [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\n      },\n      {\n        \"id\": \"E\",\n        \"values\": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n      }\n    ]\n  }'\n```\n\nThe cURL command above uses the [endpoint](manage-data/#specify-an-index-endpoint) for your Pinecone index. \n\n> ℹ️  Note\n> When upserting larger amounts of data, [upsert data in batches](insert-data/#batching-upserts) of 100 vectors or fewer over multiple upsert requests.\n\n\n6\\. Get statistics about your index.\n\nThe following commands return statistics about the contents of your index.\n\n```python\nindex.describe_index_stats()\n# Returns:\n# {'dimension': 8, 'index_fullness': 0.0, 'namespaces': {'': {'vector_count': 5}}}\n```\n```shell curl\ncurl -i https://quickstart-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/describe_index_stats \\\n  -H 'Api-Key: YOUR_API_KEY'\n\n# Output:\n# {\n#   \"namespaces\": {\n#     \"\": {\n#       \"vectorCount\": 5\n#     }\n#   },\n#   \"dimension\": 8\n# }\n```\n\n7\\. Query the index and get similar vectors.\n\nThe following example queries the index for the three (3) vectors that are most similar to an example 8-dimensional vector using the Euclidean distance metric specified in step 2 (\"Create an index.\") above.\n\n```python\nindex.query(\n  vector=[0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],\n  top_k=3,\n  include_values=True\n)\n# Returns:\n# {'matches': [{'id': 'C',\n#               'score': 0.0,\n#               'values': [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]},\n#              {'id': 'D',\n#               'score': 0.0799999237,\n#               'values': [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]},\n#              {'id': 'B',\n#               'score': 0.0800000429,\n#               'values': [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]}],\n#  'namespace': ''}\n```\n```shell curl\ncurl -i -X POST https://quickstart-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/query \\\n  -H 'Api-Key: YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"vector\": [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],\n    \"topK\": 3,\n    \"includeValues\": true\n  }'\n\n# Output:\n# {\n#       \"matches\":[\n#         {\n#           \"id\": \"C\",\n#           \"score\": -1.76717265e-07,\n#           \"values\": [0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3]\n#         },\n#         {\n#           \"id\": \"B\",\n#           \"score\": 0.080000028,\n#           \"values\": [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n#         },\n#         {\n#           \"id\": \"D\",\n#           \"score\": 0.0800001323,\n#           \"values\": [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\n#         }\n#       ],\n#       \"namespace\": \"\"\n#     }\n```\n\n8\\. Delete the index.\n\nOnce you no longer need the index, use the [`delete_index`](https://www.pinecone.io/docs/api/operation/delete_index/) operation to delete it. \n\nThe following commands delete the index.\n\n```python\npinecone.delete_index(\"quickstart\")\n```\n```shell curl\ncurl -i -X DELETE https://controller.YOUR_ENVIRONMENT.pinecone.io/databases/quickstart \\\n  -H 'Api-Key: YOUR_API_KEY'\n```\n\n> ⚠️ Warning\n> After you delete an index, you cannot use it again.\n\n## Next steps\n\nNow that you’re successfully making indexes with your API key, you can [start inserting data](insert-data) or view [more examples](examples).\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccd4"
  },
  "filename": "choosing-index-type-and-size.md",
  "title": "Choosing index type and size",
  "category": "630fc5235d91a70054705fb5",
  "content": "---\ntitle: Choosing index type and size\ncategory: 630fc5235d91a70054705fb5\n---\n\n## Introduction\n\nWhen planning your Pinecone deployment, it is important to understand the approximate storage requirements of your vectors to choose the appropriate pod type and number. This page will give guidance on sizing to help you plan accordingly.\n\nAs with all guidelines, these considerations are general and may not apply to your specific use case. We caution you to always test your deployment and ensure that the index configuration you are using is appropriate to your requirements.\n\n[Collections](/docs/collections/) make it easy to create new versions of your index with different pod types and sizes, and we encourage you to take advantage of that feature to test different configurations. This guide is merely an overview of sizing considerations and should not be taken as a definitive guide. \n\nUsers on the Standard, Enterprise, and Enterprise Dedicated plans can [contact support](https://support.pinecone.io) for further help with sizing and testing.\n\n# Overview\n\nThere are five main considerations when deciding how to configure your Pinecone index:\n\n+ Number of vectors\n+ Dimensionality of your vectors\n+ Size of metadata on each vector\n+ QPS throughput\n+ Cardinality of indexed metadata\n\nEach of these considerations comes with requirements for index size, pod type, and replication strategy. \n\n## Number of vectors\n\nThe most important consideration in sizing is the [number of vectors](/docs/insert-data/) you plan on working with. As a rule of thumb, a single p1 pod can store approximately 1M vectors, while a s1 pod can store 5M vectors. However, this can be affected by other factors, such as dimensionality and metadata, which are explained below. \n\n## Dimensionality of vectors\n\nThe rules of thumb above for how many vectors can be stored in a given pod assumes a typical configuration of 768 [dimensions per vector](/docs/manage-indexes/#creating-an-index). As your individual use case will dictate the dimensionality of your vectors, the amount of space required to store them may necessarily be larger or smaller. \n\nEach dimension on a single vector consumes 4 bytes of memory and storage per dimension, so if you expect to have 1M vectors with 768 dimensions each, that’s about 3GB of storage without factoring in metadata or other overhead. Using that reference, we can estimate the typical pod size and number needed for a given index. Table 1 below gives some examples of this.\n\n**Table 1: Estimated number of pods per 1M vectors by dimensionality**\n\n| Pod type | Dimensions | Estimated max vectors per shard | \n| -------- | ---------- | ------------------------------- |  \n| p1       | 512        | 1,250,000                       | \n|          | 768        | 1,000,000                       | \n|          | 1024       | 675,000                         | \n| p2       | 512        | 1,2500,000                      | \n|          | 768        | 1,100,000                       | \n|          | 1024       | 1,000,000                       | \n| s1\t   | 512        | 8,000,000                       | \n|          | 768        | 5,000,000                       | \n|          | 1024       | 4,000,000                       | \n\nPinecone does not support fractional pod deployments, so always round up to the next nearest whole number when choosing your pods. \n\n# Queries per second (QPS)\n\nQPS speeds are governed by a combination of the [pod type](https://www.pinecone.io/docs/indexes/#pods-pod-types-and-pod-sizes) of the index, the number of [replicas](/docs/manage-indexes/#replicas), and the `top_k` value of queries. The pod type is the primary factor driving QPS, as the different pod types are optimized for different approaches.\n\nThe [p1 pods](/docs/indexes/#p1-pods) are performance-optimized pods which provide very low query latencies, but hold fewer vectors per pod than [s1 pods](/docs/indexes/#s1-pods). They are ideal for applications with low latency requirements (<100ms). The s1 pods are optimized for storage and provide large storage capacity and lower overall costs with slightly higher query latencies than p1 pods. They are ideal for very large indexes with moderate or relaxed latency requirements. \n\nThe [p2 pod type](/docs/indexes/#p2-pods) provides greater query throughput with lower latency. They support 200 QPS per replica and return queries in less than 10ms. This means that query throughput and latency are better than s1 and p1, especially for low dimension vectors (<512D).\n\nAs a rule, a single p1 pod with 1M vectors of 768 dimensions each and no replicas can handle about 20 QPS. It’s possible to get greater or lesser speeds, depending on the size of your metadata, number of vectors, the dimensionality of your vectors, and the `top_K` value for your search. See Table 2 below for more examples.\n\n**Table 2: QPS by pod type and `top_k` value**\\*\n\n| Pod type | `top_k` 10 | `top_k` 250 | `top_k` 1000 |\n| -------- | ---------- | ----------- | ------------ |\n| p1       | 30         | 25          | 20           |\n| p2       | 150        | 50          | 20           |\n| s1       | 10         | 10          | 10           |\n\n\\*The QPS values in Table 2 represent baseline QPS with 1M vectors and 768 dimensions.\n\nAdding replicas is the simplest way to [increase your QPS](/docs/performance-tuning/#how-to-increase-throughput). Each replica increases the throughput potential by roughly the same QPS, so aiming for 150 QPS using p1 pods means using the primary pod and 5 replicas. Using threading or multiprocessing in your application is also important, as issuing single queries sequentially still subjects you to delays from any underlying latency. The [Pinecone gRPC client](/docs/performance-tuning/#using-the-grpc-client-to-get-higher-upsert-speeds) can also be used to increase throughput of upserts.\n\n## Metadata cardinality and size\n\nThe last consideration when planning your indexes is the cardinality and size of your [metadata](/docs/insert-data/#inserting-vectors-with-metadata). While the increases are small when talking about a few million vectors, they can have a real impact as you grow to hundreds of millions or billions of vectors. \n\nIndexes with very high cardinality, like those storing a unique user ID on each vector, can have significant memory requirements, resulting in fewer vectors fitting per pod. Also, if the size of the metadata per vector is larger, the index requires more storage. Limiting which metadata fields are indexed using [selective metadata indexing](/docs/manage-indexes/#selective-metadata-indexing) can help lower memory usage.\n\n## Pod sizes\n\nYou can also start with one of the larger [pod sizes](/docs/indexes/#pod-size-and-performance), like p1.x2. Each step up in pod size doubles the space available for your vectors. We recommend starting with x1 pods and scaling as you grow. This way, you don’t start with too large a pod size and have nowhere else to go up, meaning you have to migrate to a new index before you’re ready.\n\n## Example applications\n\nThe following examples will showcase how to use the sizing guidelines above to choose the appropriate type, size, and number of pods for your index. \n\n### Example 1: Semantic search of news articles\n\nIn our first example, we’ll use the [demo app for semantic search](semantic-text-search) from our documentation. In this case, we’re only working with 204,135 vectors. The vectors use 300 dimensions each, well under the general measure of 768 dimensions. Using the rule of thumb above of up to 1M vectors per p1 pod, we can run this app comfortably with a single p1.x1 pod. \n\n### Example 2: Facial recognition\n\nFor this example, suppose you’re building an application to identify customers using facial recognition for a secure banking app. Facial recognition can work with as few as 128 dimensions, but in this case, because the app will be used for access to finances, we want to make sure we’re certain that the person using it is the right one. We plan for 100M customers and use 2048 dimensions per vector.\n\nWe know from our rules of thumb above that 1M vectors with 768 dimensions fit nicely in a p1.x1 pod. We can just divide those numbers into the new targets to get the ratios we’ll need for our pod estimate:\n\n    100M / 1M = 100 base p1 pods\n    2048 / 768 = 2.667 vector ratio\n    2.667 * 100 = 267 rounding up\n\nSo we need 267 p1.x1 pods. We can reduce that by switching to s1 pods instead, sacrificing latency by increasing storage availability. They hold five times the storage of p1.x1, so the math is simple:\n\n    267 / 5 = 54 rounding up\n\nSo we estimate that we need 54 s1.x1 pods to store very high dimensional data for the face of each of the bank’s customers. \n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccd5"
  },
  "filename": "installation.md",
  "title": "Installation",
  "category": "630fc5235d91a70054705fb5",
  "content": "---\ntitle: Installation\ncategory: 630fc5235d91a70054705fb5\n---\n\n## Installing Pinecone client libraries\n\nUse the following shell command to install the [Pinecone Python client](https://github.com/pinecone-io/pinecone-python-client) for use with Python versions 3.6+:\n\n```python\npip3 install -U pinecone-client\n```\n\n## Installing Pinecone in a Jupyter notebook\n\nAlternatively, you can install Pinecone in a Jupyter notebook:\n\n```python\n!pip3 install -U pinecone-client\n```\n\nWe strongly recommend installing Pinecone in a virtual environment. For more information on using Python virtual environments, see:\n\n* [PyPA Python Packaging User Guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)\n* [Python Virtual Environments: A Primer](https://realpython.com/python-virtual-environments-a-primer/)\n\n\n## Installing the gRPC flavor of the standard client\n\nPinecone also comes with a gRPC flavor of the standard client. To install it, use the following command:\n\n```python\npip3 install -U \"pinecone-client[grpc]\"\n```\n\nThe gRPC client comes with more dependencies compared to the standard client. To get faster upsert speeds, we recommend you use the gRPC client with multi-pod indexes. See the [performance tuning](performance-tuning) section for more details.\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccd6"
  },
  "filename": "extractive-question-answering.md",
  "title": "Extractive Question Answering",
  "category": "630fc5235d91a70054705fb7",
  "content": "---\ntitle: Extractive Question Answering\ncategory: 630fc5235d91a70054705fb7\n---\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/search/question-answering/extractive-question-answering.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/search/question-answering/extractive-question-answering.ipynb) [![Open Github](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/github-shield.svg)](https://github.com/pinecone-io/examples/tree/master/search/question-answering/extractive-question-answering.ipynb)\n\nThis notebook demonstrates how Pinecone helps you build an extractive question-answering application. To build an extractive question-answering system, we need three main components:\n\n- A vector index to store and run semantic search\n- A retriever model for embedding context passages\n- A reader model to extract answers\n\nWe will use the SQuAD dataset, which consists of **questions** and **context** paragraphs containing question **answers**. We generate embeddings for the context passages using the retriever, index them in the vector database, and query with semantic search to retrieve the top k most relevant contexts containing potential answers to our question. We then use the reader model to extract the answers from the returned contexts.\n\n# Install Dependencies\n\n```python\n!pip install -qU datasets pinecone-client sentence-transformers torch\n``` \n\n# Load Dataset\n\nNow let's load the SQuAD dataset from the HuggingFace Model Hub. We load the dataset into a pandas dataframe and filter the title and context columns, and we drop any duplicate context passages.\n\n```python\nfrom datasets import load_dataset\n\n# load the squad dataset into a pandas dataframe\ndf = load_dataset(\"squad\", split=\"train\").to_pandas()\n# select only title and context column\ndf = df[[\"title\", \"context\"]]\n# drop rows containing duplicate context passages\ndf = df.drop_duplicates(subset=\"context\")\ndf\n```\n<div id=\"df-5935beba-26a2-40e4-bfca-a3dda53e101c\">\n  <div class=\"colab-df-container\">\n    <table border=\"1\" class=\"dataframe\">\n      <thead>\n        <tr style=\"text-align: right;\">\n          <th></th>\n          <th>title</th>\n          <th>context</th>\n        </tr>\n      </thead>\n      <tbody>\n        <tr>\n          <th>0</th>\n          <td>University_of_Notre_Dame</td>\n          <td>Architecturally, the school has a Catholic cha...</td>\n        </tr>\n        <tr>\n          <th>5</th>\n          <td>University_of_Notre_Dame</td>\n          <td>As at most other universities, Notre Dame's st...</td>\n        </tr>\n        <tr>\n          <th>10</th>\n          <td>University_of_Notre_Dame</td>\n          <td>The university is the major seat of the Congre...</td>\n        </tr>\n        <tr>\n          <th>15</th>\n          <td>University_of_Notre_Dame</td>\n          <td>The College of Engineering was established in ...</td>\n        </tr>\n        <tr>\n          <th>20</th>\n          <td>University_of_Notre_Dame</td>\n          <td>All of Notre Dame's undergraduate students are...</td>\n        </tr>\n        <tr>\n          <th>...</th>\n          <td>...</td>\n          <td>...</td>\n        </tr>\n        <tr>\n          <th>87574</th>\n          <td>Kathmandu</td>\n          <td>Institute of Medicine, the central college of ...</td>\n        </tr>\n        <tr>\n          <th>87579</th>\n          <td>Kathmandu</td>\n          <td>Football and Cricket are the most popular spor...</td>\n        </tr>\n        <tr>\n          <th>87584</th>\n          <td>Kathmandu</td>\n          <td>The total length of roads in Nepal is recorded...</td>\n        </tr>\n        <tr>\n          <th>87589</th>\n          <td>Kathmandu</td>\n          <td>The main international airport serving Kathman...</td>\n        </tr>\n        <tr>\n          <th>87594</th>\n          <td>Kathmandu</td>\n          <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n        </tr>\n      </tbody>\n    </table>\n    <p>18891 rows × 2 columns</p>\n  </div>\n</div>\n\n# Initialize Pinecone Index\n\nThe Pinecone index stores vector representations of our context passages which we can retrieve using another vector (query vector). We first need to initialize our connection to Pinecone to create our vector index. For this, we need a free [API key](https://app.pinecone.io/) and an environment value. You can find your environment value in the [Pinecone console](https://app.pinecone.io) under **API Keys**.,\n\nWe initialize the connection like so:\n\n```python\nimport pinecone\n\n# connect to pinecone environment\npinecone.init(\n    api_key=\"YOUR_API_KEY\",\n    environment=\"YOUR_ENVIRONMENT\"\n)\n```\n\nNow we create a new index called \"question-answering\" — we can name the index anything we want. We specify the metric type as \"cosine\" and dimension as 384 because the retriever we use to generate context embeddings is optimized for cosine similarity and outputs 384-dimension vectors.\n\n```python\nindex_name = \"extractive-question-answering\"\n\n# check if the extractive-question-answering index exists\nif index_name not in pinecone.list_indexes():\n    # create the index if it does not exist\n    pinecone.create_index(\n        index_name,\n        dimension=384,\n        metric=\"cosine\"\n    )\n\n# connect to extractive-question-answering index we created\nindex = pinecone.Index(index_name)\n```\n\n# Initialize Retriever\n\nNext, we need to initialize our retriever. The retriever will mainly do two things:\n\n- Generate embeddings for all context passages (context vectors/embeddings)\n- Generate embeddings for our questions (query vector/embedding)\n\nThe retriever will generate embeddings in a way that the questions and context passages containing answers to our questions are nearby in the vector space. We can use cosine similarity to calculate the similarity between the query and context embeddings to find the context passages that contain potential answers to our question.\n\nWe will use a SentenceTransformer model named ``multi-qa-MiniLM-L6-cos-v1`` designed for semantic search and trained on 215M (question, answer) pairs from diverse sources as our retriever.\n\n```python\nimport torch\nfrom sentence_transformers import SentenceTransformer\n\n# set device to GPU if available\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n# load the retriever model from huggingface model hub\nretriever = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1', device=device)\nretriever\n```\n    SentenceTransformer(\n      (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n      (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n      (2): Normalize()\n    )\n\n# Generate Embeddings and Upsert\n\nNext, we need to generate embeddings for the context passages. We will do this in batches to help us more quickly generate embeddings and upload them to the Pinecone index. When passing the documents to Pinecone, we need an id (a unique value), context embedding, and metadata for each document representing context passages in the dataset. The metadata is a dictionary containing data relevant to our embeddings, such as the article title, context passage, etc.\n\n```python\nfrom tqdm.auto import tqdm\n\n# we will use batches of 64\nbatch_size = 64\n\nfor i in tqdm(range(0, len(df), batch_size)):\n    # find end of batch\n    i_end = min(i+batch_size, len(df))\n    # extract batch\n    batch = df.iloc[i:i_end]\n    # generate embeddings for batch\n    emb = retriever.encode(batch[\"context\"].tolist()).tolist()\n    # get metadata\n    meta = batch.to_dict(orient=\"records\")\n    # create unique IDs\n    ids = [f\"{idx}\" for idx in range(i, i_end)]\n    # add all to upsert list\n    to_upsert = list(zip(ids, emb, meta))\n    # upsert/insert these records to pinecone\n    _ = index.upsert(vectors=to_upsert)\n\n# check that we have all vectors in index\nindex.describe_index_stats()\n```\n    100%|██████████| 296/296 [02:57<00:00, 1.99it/s]\n\n    {'dimension': 384,\n     'index_fullness': 0.0,\n     'namespaces': {'': {'vector_count': 18891}},\n     'total_vector_count': 18891}\n\n# Initialize Reader\n\nWe use the `deepset/electra-base-squad2` model from the HuggingFace model hub as our reader model. We load this model into a \"question-answering\" pipeline from HuggingFace transformers and feed it our questions and context passages individually. The model gives a prediction for each context we pass through the pipeline.\n\n```python\nfrom transformers import pipeline\n\nmodel_name = \"deepset/electra-base-squad2\"\n# load the reader model into a question-answering pipeline\nreader = pipeline(tokenizer=model_name, model=model_name, task=\"question-answering\", device=device)\n```\n\nNow all the components we need are ready. Let's write some helper functions to execute our queries. The `get_context` function retrieves the context embeddings containing answers to our question from the Pinecone index, and the `extract_answer` function extracts the answers from these context passages.\n\n```python\n# gets context passages from the pinecone index\ndef get_context(question, top_k):\n    # generate embeddings for the question\n    xq = retriever.encode([question]).tolist()\n    # search pinecone index for context passage with the answer\n    xc = index.query(xq, top_k=top_k, include_metadata=True)\n    # extract the context passage from pinecone search result\n    c = [x[\"metadata\"][\"context\"] for x in xc[\"matches\"]]\n    return c\n\nquestion = \"How much oil is Egypt producing in a day?\"\ncontext = get_context(question, top_k = 1)\ncontext\n```\n    ['Egypt was producing 691,000 bbl/d of oil and 2,141.05 Tcf of natural gas (in 2013), which makes Egypt as the largest oil producer not member of the Organization of the Petroleum Exporting Countries (OPEC) and the second-largest dry natural gas producer in Africa. In 2013, Egypt was the largest consumer of oil and natural gas in Africa, as more than 20% of total oil consumption and more than 40% of total dry natural gas consumption in Africa. Also, Egypt possesses the largest oil refinery capacity in Africa 726,000 bbl/d (in 2012). Egypt is currently planning to build its first nuclear power plant in El Dabaa city, northern Egypt.']\n\n\nAs we can see, the retiever is working and returns the context passage that contains the answer to our question. Now let's use the reader to extract the exact answer from the context passage.\n\n```python\nfrom pprint import pprint\n\n# extracts answer from the context passage\ndef extract_answer(question, context):\n    results = []\n    for c in context:\n        # feed the reader the question and contexts to extract answers\n        answer = reader(question=question, context=c)\n        # add the context to answer dict for printing both together\n        answer[\"context\"] = c\n        results.append(answer)\n    # sort the result based on the score from reader model\n    sorted_result = pprint(sorted(results, key=lambda x: x[\"score\"], reverse=True))\n    return sorted_result\n\nextract_answer(question, context)\n```\n    [{'answer': '691,000 bbl/d',\n      'context': 'Egypt was producing 691,000 bbl/d of oil and 2,141.05 Tcf of '\n                 'natural gas (in 2013), which makes Egypt as the largest oil '\n                 'producer not member of the Organization of the Petroleum '\n                 'Exporting Countries (OPEC) and the second-largest dry natural '\n                 'gas producer in Africa. In 2013, Egypt was the largest consumer '\n                 'of oil and natural gas in Africa, as more than 20% of total oil '\n                 'consumption and more than 40% of total dry natural gas '\n                 'consumption in Africa. Also, Egypt possesses the largest oil '\n                 'refinery capacity in Africa 726,000 bbl/d (in 2012). Egypt is '\n                 'currently planning to build its first nuclear power plant in El '\n                 'Dabaa city, northern Egypt.',\n      'end': 33,\n      'score': 0.9999852180480957,\n      'start': 20}]\n    \nThe reader model predicted with 99% accuracy the correct answer *691,000 bbl/d* as seen from the context passage. Let's run few more queries.\n\n```python\nquestion = \"What are the first names of the men that invented youtube?\"\ncontext = get_context(question, top_k=1)\nextract_answer(question, context)\n```\n    [{'answer': 'Hurley and Chen',\n      'context': 'According to a story that has often been repeated in the media, '\n                 'Hurley and Chen developed the idea for YouTube during the early '\n                 'months of 2005, after they had experienced difficulty sharing '\n                 \"videos that had been shot at a dinner party at Chen's apartment \"\n                 'in San Francisco. Karim did not attend the party and denied that '\n                 'it had occurred, but Chen commented that the idea that YouTube '\n                 'was founded after a dinner party \"was probably very strengthened '\n                 'by marketing ideas around creating a story that was very '\n                 'digestible\".',\n      'end': 79,\n      'score': 0.9999276399612427,\n      'start': 64}]\n    \n```python\nquestion = \"What is Albert Eistein famous for?\"\ncontext = get_context(question, top_k=1)\nextract_answer(question, context)\n```\n\n    [{'answer': 'his theories of special relativity and general relativity',\n      'context': 'Albert Einstein is known for his theories of special relativity '\n                 'and general relativity. He also made important contributions to '\n                 'statistical mechanics, especially his mathematical treatment of '\n                 'Brownian motion, his resolution of the paradox of specific '\n                 'heats, and his connection of fluctuations and dissipation. '\n                 'Despite his reservations about its interpretation, Einstein also '\n                 'made contributions to quantum mechanics and, indirectly, quantum '\n                 'field theory, primarily through his theoretical studies of the '\n                 'photon.',\n      'end': 86,\n      'score': 0.9500371217727661,\n      'start': 29}]\n    \nLet's run another question. This time for top 3 context passages from the retriever.\n\n```python\nquestion = \"Who was the first person to step foot on the moon?\"\ncontext = get_context(question, top_k=3)\nextract_answer(question, context)\n```\n\n    [{'answer': 'Armstrong',\n      'context': 'The trip to the Moon took just over three days. After achieving '\n                 'orbit, Armstrong and Aldrin transferred into the Lunar Module, '\n                 'named Eagle, and after a landing gear inspection by Collins '\n                 'remaining in the Command/Service Module Columbia, began their '\n                 'descent. After overcoming several computer overload alarms '\n                 'caused by an antenna switch left in the wrong position, and a '\n                 'slight downrange error, Armstrong took over manual flight '\n                 'control at about 180 meters (590 ft), and guided the Lunar '\n                 'Module to a safe landing spot at 20:18:04 UTC, July 20, 1969 '\n                 '(3:17:04 pm CDT). The first humans on the Moon would wait '\n                 'another six hours before they ventured out of their craft. At '\n                 '02:56 UTC, July 21 (9:56 pm CDT July 20), Armstrong became the '\n                 'first human to set foot on the Moon.',\n      'end': 80,\n      'score': 0.9998037815093994,\n      'start': 71},\n     {'answer': 'Aldrin',\n      'context': 'The first step was witnessed by at least one-fifth of the '\n                 'population of Earth, or about 723 million people. His first '\n                 \"words when he stepped off the LM's landing footpad were, \"\n                 '\"That\\'s one small step for [a] man, one giant leap for '\n                 'mankind.\" Aldrin joined him on the surface almost 20 minutes '\n                 'later. Altogether, they spent just under two and one-quarter '\n                 'hours outside their craft. The next day, they performed the '\n                 'first launch from another celestial body, and rendezvoused back '\n                 'with Columbia.',\n      'end': 246,\n      'score': 0.6958656907081604,\n      'start': 240},\n     {'answer': 'Frank Borman',\n      'context': 'On December 21, 1968, Frank Borman, James Lovell, and William '\n                 'Anders became the first humans to ride the Saturn V rocket into '\n                 'space on Apollo 8. They also became the first to leave low-Earth '\n                 'orbit and go to another celestial body, and entered lunar orbit '\n                 'on December 24. They made ten orbits in twenty hours, and '\n                 'transmitted one of the most watched TV broadcasts in history, '\n                 'with their Christmas Eve program from lunar orbit, that '\n                 'concluded with a reading from the biblical Book of Genesis. Two '\n                 'and a half hours after the broadcast, they fired their engine to '\n                 'perform the first trans-Earth injection to leave lunar orbit and '\n                 'return to the Earth. Apollo 8 safely landed in the Pacific ocean '\n                 \"on December 27, in NASA's first dawn splashdown and recovery.\",\n      'end': 34,\n      'score': 0.49247056245803833,\n      'start': 22}]\n    \nWe return the correct answer first, followed by relevant answers on similar parallel topics. We have recieved great results.\n\n# Example Application \n\nTo try out an application like this one, see this [example application](https://huggingface.co/spaces/pinecone/extractive-question-answering).\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccd7"
  },
  "filename": "metadata-filtered-search.md",
  "title": "Metadata Filtered Search",
  "category": "630fc5235d91a70054705fb7",
  "content": "---\ntitle: Metadata Filtered Search\ncategory: 630fc5235d91a70054705fb7\n---\n\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/search/metadata-filtered-search/metadata-filtered-search.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/search/metadata-filtered-search/metadata-filtered-search.ipynb) [![Open github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/pinecone-io/examples/blob/master/search/metadata-filtered-search/metadata-filtered-search.ipynb)\n\nPinecone offers a production-ready vector database for high performance and reliable *semantic search* at scale. But did you know Pinecone's semantic search can be paired with the more traditional keyword search?\n\n[Semantic search](https://www.pinecone.io/learn/semantic-search/) is a compelling technology allowing us to search using abstract concepts and *meaning* rather than relying on specific words. However, sometimes a simple keyword search can be just as valuable — especially if we know the exact wording of what we're searching for.\n\n<video autoplay loop muted playsinline style=\"width:100%;\">\n  <source src=\"https://www.pinecone.io/images/basic-hybrid-search.mp4\" type=\"video/mp4\">\n</video>\n<small>Hybrid search allows us to use Pinecone’s <a href=\"https://www.pinecone.io/learn/vector-search-filtering/\">single-stage filtering</a> to restrict the search scope using specific keywords, then continue with a semantic search.</small>\n\nPinecone allows you to pair semantic search with a basic keyword filter. If you know that the document you're looking for contains a specific word or set of words, you simply tell Pinecone to restrict the search to *only include* documents with those keywords.\n\nWe even support functionality for keyword search using sets of words with *AND, OR, NOT* logic.\n\nIn this article, we will explore these features through a start-to-finish example of basic keyword search in Pinecone.\n\n<p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/iCkftKsnQgg\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>\n\n## Data Preparation and Upsert\n\nThe first thing we need to do is create some data. We will keep things simple with *10* sentences.\n\n```python\nall_sentences = [\n    \"purple is the best city in the forest\",\n    \"No way chimps go bananas for snacks!\",\n    \"it is not often you find soggy bananas on the street\",\n    \"green should have smelled more tranquil but somehow it just tasted rotten\",\n    \"joyce enjoyed eating pancakes with ketchup\",\n    \"throwing bananas on to the street is not art\",\n    \"as the asteroid hurtled toward earth becky was upset her dentist appointment had been canceled\",\n    \"I'm getting way too old. I don't even buy green bananas anymore.\",\n    \"to get your way you must not bombard the road with yellow fruit\",\n    \"Time flies like an arrow; fruit flies like a banana\"\n]\n```\n\nOn the *semantic* side of our search, we will introduce a new *query sentence* and search for the most semantically similar. To do this, we will need to create some sentence embeddings using our sentences. We will use a pretrained model from `sentence-transformers` for this.\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('flax-sentence-embeddings/all_datasets_v3_mpnet-base')\n\nall_embeddings = model.encode(all_sentences)\nall_embeddings.shape\n```\n\n```\n(10, 768)\n```\n\nWe now have *10* sentence embeddings, each with a dimensionality of *768*. If we just wanted semantic search, we could move onto *upserting* the data — but there is **one more step for keyword search**.\n\nKeyword search requires *keywords*, so we make a list of words (or *'tokens'*) for each sentence. To do this we can use a *word-level* tokenizer from Hugging Face’s `transformers`.\n\n```python\nfrom transformers import AutoTokenizer\n\n# transfo-xl tokenizer uses word-level encodings\ntokenizer = AutoTokenizer.from_pretrained('transfo-xl-wt103')\n\nall_tokens = [tokenizer.tokenize(sentence.lower()) for sentence in all_sentences]\nall_tokens[0]\n```\n\n```\n['purple', 'is', 'the', 'best', 'city', 'in', 'the', 'forest']\n```\n\nWe have all the data we need for our semantic and keyword search, so we can move on to initializing a connection to our Pinecone instance. All we need here is an [API key](https://app.pinecone.io), and then we can create a new index called `keyword-search` (you can name it anything you like). You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**.\n\n```python\nimport pinecone\n\n# connect to pinecone environment\npinecone.init(\n    api_key=\"YOUR_API_KEY\",\n    environment=\"YOUR_ENVIRONMENT\"\n)\n\npinecone.list_indexes()  # check if keyword-search index already exists\n\npinecone.create_index(name='keyword-search', dimension=all_embeddings.shape[1])\nindex = pinecone.Index('keyword-search')\n```\n\nAll we do now is `upsert` our data — which we reformat into a list of tuples where each tuple is structured as `(id, values, metadata)`.\n\n```python\nupserts = [(v['id'], v['values'], v['metadata']) for v in data]\n# then we upsert\nindex.upsert(vectors=upserts)\n```\n\n```\n{'upsertedCount': 10.0}\n```\n\nIt’s also possible to upsert data to Pinecone using cURL. For this we reformat our data and save it as a JSON file.\n\n```python\nimport json\n\n# reformat the data\nupserts = {'vectors': []}\nfor i, (embedding, tokens) in enumerate(zip(all_embeddings, all_tokens)):\n    vector = {'id':f'{i}',\n              'values': embedding.tolist(),\n              'metadata':{'tokens':tokens}}\n    upserts['vectors'].append(vector)\n\n# save to JSON\nwith open('./upsert.json', 'w') as f:\n    json.dump(upserts, f, indent=4)\n```\n\nHere we’ve build a JSON file containing a list of *10* records within the `vectors` key. Each record contains ID, embeddings, and metadata in the format:\n\n```json\n{\n    \"id\": \"i\",\n    \"values\": [0.001, 0.001, ...],\n    \"metadata\": {\n        \"tokens\": [\"purple\", \"is\", ...]\n    }\n}\n```\n\nTo upsert with curl, we first need the index URL — which can be found in your [Pinecone dashboard](https://app.pinecone.io), it should look something like:\n\n```\nhttps://keyword-search-1234.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/upsert\n```\n\nWith that, we upsert:\n\n```bash\n!curl -X POST \\\n    https://keyword-search-1234.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/upsert \\\n    -H ‘Content-Type: application/json’ \\\n    -H ‘Api-Key: <YOUR-API-KEY>’ \\\n    -d @./upsert.json\n```\n\nNow that we've upserted the data to our index, we can move on to semantic and keyword search.\n\n## Semantic and Keyword Search\n\nWe'll start with a semantic search *without* keywords. As we did with our indexed sentences, we need to `encode` a *query sentence*.\n\n```python\nquery_sentence = \"there is an art to getting your way and throwing bananas on to the street is not it\"\nxq = model.encode(query_sentence).tolist()\n```\n\nWe then find the most semantically similar sentences to this query vector `xq` with `query` — we will return all *ten* sentences by setting `top_k=10`.\n\n```python\nresult = index.query(xq, top_k=10, includeMetadata=True)\nresult\n```\n\n```\n{'matches': [{'id': '5',\n              'metadata': {'tokens': ['throwing',\n                                      'bananas',\n                                      'on',\n                                      'to',\n                                      'the',\n                                      'street',\n                                      'is',\n                                      'not',\n                                      'art']},\n              'score': 0.732851923,\n              'values': []},\n             {'id': '8',\n              'metadata': {'tokens': ['to',\n                                      'get',\n                                      'your',\n                                      'way',\n                                      'you',\n                                      'must',\n                                      'not',\n                                      'bombard',\n                                      'the',\n                                      'road',\n                                      'with',\n                                      'yellow',\n                                      'fruit']},\n              'score': 0.574427,\n              'values': []}],\n 'namespace': ''}\n```\n\n```python\n[x['id'] for x in result['matches']]\n```\n\n```\n['5', '8', '2', '1', '9', '7', '0', '3', '4', '6']\n```\n\nThe response shows both the most similar sentence IDs and their respective metadata field, which contains the list of *tokens* we created earlier.\n\nWe perform a keyword search by filtering records with the *tokens* metadata field. If we wanted to only return records that contain the token `'bananas'` we can like so:\n\n```python\nresult = index.query(xq, top_k=10, filter={'tokens': 'bananas'})\nids = [x['id'] for x in result['matches']]\nids\n```\n\n```\n['5', '2', '1', '7']\n```\n\nImmediately we can see that we return far fewer sentences. This is because there are only *four* records that contain the word *'bananas'*. We can use those `ids` to see which sentences we've returned.\n\n```python\nfor i in ids:\n    print(all_sentences[i])\n```\n\n```\nthrowing bananas on to the street is not art\nit is not often you find soggy bananas on the street\nNo way chimps go bananas for snacks!\nI'm getting way too old. I don't even buy green bananas anymore.\n```\n\nLooks great! We can extend the keyword search filter to include multiple words — specifying whether we'd like to return results that contain *all* words using `$and`, or *any* word using `$or`/`$in`.\n\nIf we wanted to return records that contain either *'bananas'* **or** *'way'* with [metadata filtering](https://www.pinecone.io/docs/metadata-filtering/): \n\n```{'$or': [{'tokens': 'bananas'}, {'tokens': 'way'}]}```\n\nThis filter will return *any* records that satisfy one or more of these conditions — where the *tokens* list contains *’bananas’* **or** the *tokens* list contains *’way’*.\n\n```python\nresult = index.query(xq, top_k=10, filter={'$or': [\n                         {'tokens': 'bananas'},\n                         {'tokens': 'way'}\n                     ]})\n\nids = [int(x['id']) for x in result['matches']]\nfor i in ids:\n    print(all_sentences[i])\n```\n\nAlternatively, we can write these multi-keyword `$or` queries using the `$in` condition. This modifier tells Pinecone to filter for records where the *tokens* list contains *any* word from the list we define.\n\n```python\nresult = index.query(xq, top_k=10, filter={\n    'tokens': {'$in': ['bananas', 'way']}\n})\n\nids = [int(x['id']) for x in result['matches']]\nfor i in ids:\n    print(all_sentences[i])\n```\n\n```\nthrowing bananas on to the street is not art\nto get your way you must not bombard the road with yellow fruit\nit is not often you find soggy bananas on the street\nNo way chimps go bananas for snacks!\nI'm getting way too old. I don't even buy green bananas anymore.\n```\n\nBoth `$or` and `$in` produce the same logic above. What if we wanted records that contain *both* *'bananas'* **and** *'way'*? All we do is swap `$or` for `$and`.\n\n```python\nresult = index.query(xq, top_k=10, filter={'$and': [\n                         {'tokens': 'bananas'},\n                         {'tokens': 'way'}\n                     ]})\n\nids = [int(x['id']) for x in result['matches']]\nfor i in ids:\n    print(all_sentences[i])\n```\n\n```\nNo way chimps go bananas for snacks!\nI'm getting way too old. I don't even buy green bananas anymore.\n```\n\nIf we have *a lot* of keywords, including *every single one* with the `$and` condition manually would not be fun, so we write something like this instead:\n\n```python\nkeywords = ['bananas', 'way', 'green']\nfilter_dict = [{'tokens': word} for word in keywords]\nfilter_dict\n```\n\n```\n[{'tokens': 'bananas'}, {'tokens': 'way'}, {'tokens': 'green'}]\n```\n\n```python\nresult = index.query(xq, top_k=10, filter={'$and': filter_dict})\n\nids = [int(x['id']) for x in result['matches']]\nfor i in ids:\n    print(all_sentences[i])\n```\n\n```\nI'm getting way too old. I don't even buy green bananas anymore.\n```\n\nAnd now we're restricting our semantic search to records that contain *any* word from *'bananas'*, *'way'*, or *'green'*.\n\nIf we like we can add negation to our logic too. For example we may want all sentences that *do not* contain *’bananas’* but *do* contain *’way’*. To do this we add **not equals** `$ne` to the *’bananas’* condition.\n\n```python\nresult = index.query(xq, top_k=10, filter={'$and': [\n                         {'tokens': {'$ne': 'bananas'}},\n                         {'tokens': 'way'}\n                     ]})\n\nids = [int(x['id']) for x in result['matches']]\nfor i in ids:\n    print(all_sentences[i])\n```\n\n```\nto get your way you must not bombard the road with yellow fruit\n```\n\nOr if we want to *not* return sentences that contain any of several words, we use the **not in** `$nin` modifier.\n\n```python\nresult = index.query(xq, top_k=10, filter={'tokens':\n    {'$nin': ['bananas', 'way']}\n})\n\nids = [int(x['id']) for x in result['matches']]\nfor i in ids:\n    print(all_sentences[i])\n```\n\n```\nTime flies like an arrow; fruit flies like a banana\npurple is the best city in the forest\ngreen should have smelled more tranquil but somehow it just tasted rotten\njoyce enjoyed eating pancakes with ketchup\nas the asteroid hurtled toward earth becky was upset her dentist appointment had been canceled\n```\n\nThat's it for this introduction to keyword search in Pinecone. We've set up and upserted our sentence embeddings for semantic search and a token list for keyword search. Then we explored how we restrict our search to records containing a specific keyword, or even *set of keywords* using the two `$and` / `$or` modifiers.\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccd8"
  },
  "filename": "sentiment-mining.md",
  "title": "Sentiment Analysis",
  "category": "630fc5235d91a70054705fb7",
  "content": "---\ntitle: Sentiment Analysis\ncategory: 630fc5235d91a70054705fb7\n---\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/analytics-and-ml/data-mining/sentiment-analysis/sentiment-analysis.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/analytics-and-ml/data-mining/sentiment-analysis/sentiment-analysis.ipynb) [![Open Github](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/github-shield.svg)](https://github.com/pinecone-io/examples/tree/master/analytics-and-ml/data-mining/sentiment-analysis/sentiment-analysis.ipynb)\n\nSentiment analysis, often known as opinion mining, is a technique used in natural language processing (NLP) to determine the emotional undertone of a text. This is a common method used by organizations to identify and group opinions about their product, service, and ideas. In this notebook, we will apply this technique to the hotel industry and understand customer perception and potential areas that need improvement. To do this, we will:\n\n1.\tGenerate Sentiment labels and scores based on customer reviews.\n2.\tStore them in a Pinecone index as metadata (alongside respective text vectors).\n3.\tQuery Pinecone index on selected areas and understand customer opinions.\n\nLet's get started.\n\n\n# Install Dependencies\n\n\n```python\n!pip install sentence_transformers pinecone-client datasets seaborn matplotlib\n```\n\n# Load and Prepare Dataset\n\nWe use a dataset containing ~90k hotel reviews provided by customers. This dataset can be loaded from the HuggingFace dataset hub as follows:\n\n\n```python\nfrom datasets import load_dataset\n\n# load the dataset and convert to pandas dataframe\ndf = load_dataset(\n    \"ashraq/hotel-reviews\",\n    split=\"train\"\n).to_pandas()\n```\n\n\n```python\n# keep only the first 800 characters from the reviews\ndf[\"review\"] = df[\"review\"].str[:800]\n# glimpse the dataset\ndf.head()\n```\n\n\n<div id=\"df-e185102e-db37-41cb-8524-1691d0ac13a3\">\n  <div class=\"colab-df-container\">\n    <div>\n      <style scoped>\n          .dataframe tbody tr th:only-of-type {\n              vertical-align: middle;\n          }\n          .dataframe tbody tr th {\n              vertical-align: top;\n          }\n          .dataframe thead th {\n              text-align: right;\n          }\n      </style>\n      <table border=\"1\" class=\"dataframe\">\n        <thead>\n          <tr style=\"text-align: right;\">\n            <th></th>\n            <th>review_date</th>\n            <th>hotel_name</th>\n            <th>review</th>\n          </tr>\n        </thead>\n        <tbody>\n          <tr>\n            <th>0</th>\n            <td>8/3/2017</td>\n            <td>Park Plaza County Hall London</td>\n            <td>Extra bed was the worst breakfast queue was r...</td>\n          </tr>\n          <tr>\n            <th>1</th>\n            <td>8/3/2017</td>\n            <td>Park Plaza County Hall London</td>\n            <td>Just the location and view</td>\n          </tr>\n          <tr>\n            <th>2</th>\n            <td>8/3/2017</td>\n            <td>Park Plaza County Hall London</td>\n            <td>Around the corner from the London eye and use...</td>\n          </tr>\n          <tr>\n            <th>3</th>\n            <td>8/2/2017</td>\n            <td>Park Plaza County Hall London</td>\n            <td>I wish you had wheat free snacks</td>\n          </tr>\n          <tr>\n            <th>4</th>\n            <td>8/2/2017</td>\n            <td>Park Plaza County Hall London</td>\n            <td>You re always my hotel of choice You re staff...</td>\n          </tr>\n        </tbody>\n      </table>\n    </div>\n    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e185102e-db37-41cb-8524-1691d0ac13a3')\" title=\"Convert this dataframe to an interactive table.\" style=\"display:none;\">\n      <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg>\n    </button>\n    <style>\n      .colab-df-container {\n        display:flex;\n        flex-wrap:wrap;\n        gap: 12px;\n      }\n      .colab-df-convert {\n        background-color: #E8F0FE;\n        border: none;\n        border-radius: 50%;\n        cursor: pointer;\n        display: none;\n        fill: #1967D2;\n        height: 32px;\n        padding: 0 0 0 0;\n        width: 32px;\n      }\n      .colab-df-convert:hover {\n        background-color: #E2EBFA;\n        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n        fill: #174EA6;\n      }\n      [theme=dark] .colab-df-convert {\n        background-color: #3B4455;\n        fill: #D2E3FC;\n      }\n      [theme=dark] .colab-df-convert:hover {\n        background-color: #434B5C;\n        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n        fill: #FFFFFF;\n      }\n    </style>\n    <script>\n      const buttonEl = document.querySelector('#df-e185102e-db37-41cb-8524-1691d0ac13a3 button.colab-df-convert');\n      buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none';\n      async function convertToInteractive(key) {\n        const element = document.querySelector('#df-e185102e-db37-41cb-8524-1691d0ac13a3');\n        const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {});\n        if (!dataTable) return;\n        const docLinkHtml = 'Like what you see? Visit the ' +\n          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n          + ' to learn more about interactive tables.';\n        element.innerHTML = '';\n        dataTable['output_type'] = 'display_data';\n        await google.colab.output.renderOutput(dataTable, element);\n        const docLink = document.createElement('div');\n        docLink.innerHTML = docLinkHtml;\n        element.appendChild(docLink);\n      }\n    </script>\n  </div>\n</div>\n\n\n\n\n# Initialize Sentiment Analysis Model\n\nWe will use a RoBERTa model fine-tuned for sentiment analysis to analyze the hotel reviews. The model can be loaded from the HuggingFace model hub as follows:\n\n\n```python\nimport torch\n\n# set device to GPU if available\ndevice = torch.cuda.current_device() if torch.cuda.is_available() else None\n```\n\n\n```python\nfrom transformers import (\n    pipeline,\n    AutoTokenizer,\n    AutoModelForSequenceClassification\n    )\n\nmodel_id = \"cardiffnlp/twitter-roberta-base-sentiment\"\n\n# load the model from huggingface\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_id,\n    num_labels=3\n)\n\n# load the tokenizer from huggingface\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# load the tokenizer and model into a sentiment analysis pipeline\nnlp = pipeline(\n    \"sentiment-analysis\",\n    model=model,\n    tokenizer=tokenizer,\n    device=device\n    )\n```\n\n\nThe sentiment analysis model returns `LABEL_0` for negative, `LABEL_1` for neutral and `LABEL_2` for positive labels. We can add them to a dictionary to easily access them when showing the results.\n\n\n```python\nlabels = {\n    \"LABEL_0\": \"negative\",\n    \"LABEL_1\": \"neutral\",\n    \"LABEL_2\": \"positive\"\n}\n```\n\n\n```python\n# view review number 241\ntest_review = df[\"review\"][241]\ntest_review\n```\n\n\n\n\n    ' Room was small for a superior room and poorly lit especially as it was an inside room and overlooked the inside wall of the hotel No view therefore needed better lighting within Restaurant tables were not well laid and had to go searching for cutlery at breakfast '\n\n\n\n\n```python\n# get the sentiment label and score for review number 241\nnlp(test_review)\n```\n\n\n\n\n    [{'label': 'LABEL_0', 'score': 0.7736575603485107}]\n\n\n\nOur pipeline is working as expected and accurately predicts the correct label for the review.\n\n# Initialize Retriever\n\nA retriever model is used to embed passages and queries, and it creates embeddings such that queries and passages with similar meanings are close in the vector space. We will use a sentence-transformer model as our retriever. The model can be loaded as follows:\n\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\n# load the model from huggingface\nretriever = SentenceTransformer(\n    'sentence-transformers/all-MiniLM-L6-v2',\n    device=device\n)\nretriever\n```\n\n# Initialize Pinecone Index\n\nNow we need to initialize our Pinecone index. The Pinecone index stores vector representations of our passages which we can retrieve using another vector (the query vector). We first need to initialize our connection to Pinecone. For this, we need a free [API key](https://app.pinecone.io/). You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**. We initialize the connection like so:\n\n\n```python\nimport pinecone\n\n# connect to pinecone environment\npinecone.init(\n    api_key=\"<<YOUR_API_KEY>>\",\n    environment=\"YOUR_ENVIRONMENT\"\n)\n```\n\nNow we can create our vector index. We will name it `sentiment-mining` (feel free to choose any name you prefer). We specify the metric type as `cosine` and dimension as `384` as these are the vector space and dimensionality of the vectors generated by the retriever model.\n\n\n```python\nindex_name = \"sentiment-mining\"\n\n# check if the sentiment-mining index exists\nif index_name not in pinecone.list_indexes():\n    # create the index if it does not exist\n    pinecone.create_index(\n        index_name,\n        dimension=384,\n        metric=\"cosine\"\n    )\n\n# connect to sentiment-mining index we created\nindex = pinecone.Index(index_name)\n```\n\n# Generate Embeddings and Upsert\n\nWe generate embeddings for all the reviews in the dataset. Alongside the embeddings, we also include the sentiment label and score in the Pinecone index as metadata. Later we will use this data to understand customer opinions.\n\nLet's first write a helper function to generate sentiment labels and scores for a batch of reviews.\n\n\n```python\ndef get_sentiment(reviews):\n    # pass the reviews through sentiment analysis pipeline\n    sentiments = nlp(reviews)\n    # extract only the label and score from the result\n    l = [labels[x[\"label\"]] for x in sentiments]\n    s = [x[\"score\"] for x in sentiments]\n    return l, s\n```\n\n\n```python\n# get sentiment labels for few reviews\nget_sentiment(df[\"review\"][:3].tolist())\n```\n\n\n\n\n    (['negative', 'neutral', 'positive'],\n     [0.906525194644928, 0.7716173529624939, 0.8975034952163696])\n\n\n\nWe need to convert the review dates to timestamps to filter query results for a given period. This is helpful if you want to understand customer sentiment over a specific period. Let's write another helper function to convert dates to timestamps.\n\n\n```python\nimport dateutil.parser\n\n# convert date to timestamp\ndef get_timestamp(dates):\n    timestamps = [dateutil.parser.parse(d).timestamp() for d in dates]\n    return timestamps\n```\n\n\n```python\nget_timestamp([df[\"review_date\"][0]])[0]\n```\n\n\n\n\n    1501718400.0\n\n\n\nNow we create the embeddings. We do this in batches of `64` to avoid overwhelming machine resources or API request limits.\n\n\n```python\nfrom tqdm.auto import tqdm\n\n# we will use batches of 64\nbatch_size = 64\n\nfor i in tqdm(range(0, len(df), batch_size)):\n    # find end of batch\n    i_end = min(i+batch_size, len(df))\n    # extract batch\n    batch = df.iloc[i:i_end]\n    # generate embeddings for batch\n    emb = retriever.encode(batch[\"review\"].tolist()).tolist()\n    # convert review_date to timestamp to enable period filters\n    timestamp = get_timestamp(batch[\"review_date\"].tolist())\n    batch[\"timestamp\"] = timestamp\n    # get sentiment label and score for reviews in the batch\n    label, score = get_sentiment(batch[\"review\"].tolist())\n    batch[\"label\"] = label\n    batch[\"score\"] = score\n    # get metadata\n    meta = batch.to_dict(orient=\"records\")\n    # create unique IDs\n    ids = [f\"{idx}\" for idx in range(i, i_end)]\n    # add all to upsert list\n    to_upsert = list(zip(ids, emb, meta))\n    # upsert/insert these records to pinecone\n    _ = index.upsert(vectors=to_upsert)\n \n# check that we have all vectors in index\nindex.describe_index_stats()\n```\n\nWe have successfully indexed all customer reviews and relevant metadata. We can move on to opinion mining.\n\n# Opinion Mining\n\nNow that we have all the customer reviews indexed, we will search for a few areas that customers usually consider when staying at a hotel and analyze the general opinion of the customers. Pinecone vector database makes it very flexible to do this as we can easily search for any topic and get customer reviews relevant to the search query along with sentiment labels as metadata.\n\nWe will start with a general question about the room sizes of hotels in London and return the top 500 reviews to analyze the overall customer sentiment.\n\n\n```python\nquery = \"are the customers satisfied with the room sizes in London hotels?\"\n# generate dense vector embeddings for the query\nxq = retriever.encode(query).tolist()\n# query pinecone\nresult = index.query(xq, top_k=500, include_metadata=True)\n```\n\nLet's take a look at a few reviews from the search results.\n\n\n```python\nresult[\"matches\"][:2]\n```\n\n\n\n\n    [{'id': '57268',\n      'metadata': {'hotel_name': 'Millennium Gloucester Hotel London',\n                   'label': 'neutral',\n                   'review': ' The size of the room compared to other london hotels '\n                             'And the location and friednly staff ',\n                   'review_date': datetime.date(2015, 10, 9),\n                   'score': 0.7984868884086609,\n                   'timestamp': 1444348800.0},\n      'score': 0.819122493,\n      'sparseValues': {},\n      'values': []}, {'id': '36931',\n      'metadata': {'hotel_name': 'DoubleTree by Hilton London Docklands Riverside',\n                   'label': 'positive',\n                   'review': ' Rooms great but on the small size but typical for a '\n                             'London hotel',\n                   'review_date': datetime.date(2015, 11, 8),\n                   'score': 0.8672299981117249,\n                   'timestamp': 1446940800.0},\n      'score': 0.816708684,\n      'sparseValues': {},\n      'values': []}]\n\n\n\n\n```python\nresult[\"matches\"][-2:]\n```\n\n\n\n\n    [{'id': '60964',\n      'metadata': {'hotel_name': 'St James Court A Taj Hotel London',\n                   'label': 'positive',\n                   'review': ' The location is perfect and I got a late checkout as '\n                             'I requested The layout of one of the rooms made it '\n                             'feel like smaller in comparison with the others but '\n                             'they were very helpful and changed it the next '\n                             'morning Perfect hotel from a frequent traveller to '\n                             'london this hotel will be my first choice from now on',\n                   'review_date': datetime.date(2016, 3, 6),\n                   'score': 0.9831811785697937,\n                   'timestamp': 1457222400.0},\n      'score': 0.643939376,\n      'sparseValues': {},\n      'values': []}, {'id': '46122',\n      'metadata': {'hotel_name': 'Park Grand London Kensington',\n                   'label': 'positive',\n                   'review': ' Bedroom was very comfortable and a great size for '\n                             'London as many rooms are rather small Had a super '\n                             'king bed which was fabulous Breakfast was very good '\n                             'and the staff both on reception and serving breakfast '\n                             'were on the whole very pleasant',\n                   'review_date': datetime.date(2015, 11, 10),\n                   'score': 0.985270082950592,\n                   'timestamp': 1447113600.0},\n      'score': 0.643873811,\n      'sparseValues': {},\n      'values': []}]\n\n\n\nWe have reviews relevant to room sizes from top to bottom of the search results. Now let's see the overall perception of the customers on London hotel room sizes. First, we need to extract the sentiment labels from the query results and count them. We will write a function to do this.\n\n\n```python\ndef count_sentiment(result):\n    # store count of sentiment labels\n    sentiments = {\n        \"negative\": 0,\n        \"neutral\": 0,\n        \"positive\": 0,\n    }\n    # iterate through search results\n    for r in result[\"matches\"]:\n        # extract the sentiment label and increase its count\n        sentiments[r[\"metadata\"][\"label\"]] += 1\n    return sentiments\n```\n\n\n```python\nsentiment = count_sentiment(result)\nsentiment\n```\n\n\n\n\n    {'negative': 54, 'neutral': 161, 'positive': 285}\n\n\n\nLet's plot the result for a better view.\n\n\n```python\nimport seaborn as sns\n\n# plot a barchart using seaborn\nsns.barplot(x=list(sentiment.keys()), y = list(sentiment.values()))\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x7f4a962d9890>\n\n\n\n\n    \n![png](https://raw.githubusercontent.com/pinecone-io/img/main/sentiment-mining-example-46_1.png)\n    \n\n\nThe customers are generally satisfied with the room sizes, although many are still neutral and negative.\n\nWe can be even more specific when searching for reviews with the help of Pinecone's metadata filtering. For instance, we can specify a period in our query to ensure that search results only contain customer reviews for that period. This is helpful if you want to understand the customer's opinion during a specific period.\n\nLet's do this for the same query as before. We will filter reviews from 25th December to 31st December 2015. Previously we added the `review_date` field as a timestamp to the metadata during indexing. We can convert the start and end date of the period to timestamp using the `get_timestamp` function and use a `$gte` (greater than or equal to) and a `$lte` (less than or equal to) filter to get reviews from only the selected period.\n\n\n```python\n# generate timestamps for start and end time of the period\nstart_time = get_timestamp([\"2015-12-25\"])[0]\nend_time = get_timestamp([\"2015-12-31\"])[0]\n```\n\n\n```python\nquery = \"are the customers satisified with the room sizes of hotels in London?\"\n# generate query embeddings\nxq = retriever.encode(query).tolist()\n# query pinecone with query embeddings and the period filter\nresult = index.query(\n    xq,\n    top_k=500,\n    include_metadata=True,\n    filter={\"timestamp\": {\"$gte\": start_time, \"$lte\":end_time}})\n# get an overall count of customer sentiment\nsentiment = count_sentiment(result)\n# plot a barchart using seaborn\nsns.barplot(x=list(sentiment.keys()), y = list(sentiment.values()))\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x7f4a95c2ee90>\n\n\n\n\n    \n![png](https://raw.githubusercontent.com/pinecone-io/img/main/sentiment-mining-example-50_1.png)\n    \n\n\nWe have a slightly different result now. Almost the same number of customers had either a neutral or negative view of the room size during the selected period.\n\n\n```python\nhotels =[\n    \"Strand Palace Hotel\",\n    \"Britannia International Hotel Canary Wharf\",\n    \"Grand Royale London Hyde Park\",\n    \"Intercontinental London The O2\",\n]\n```\n\nWe will look into five main areas:\n  1. Room Size\n  2. Cleanliness\n  3. Staff\n  4. Food\n  5. AC\n\nWe have a query for each of these areas below.\n\n\n```python\nqueries = {\n    \"Room Size\": \"are customers happy with the room sizes?\",\n    \"Cleanliness\": \"are customers satisfied with the cleanliness of the rooms?\",\n    \"Staff\": \"did the customers like how they were treated by the staff?\",\n    \"Food\": \"did the customers enjoy the food?\",\n    \"AC\": \"customer opinion on the AC\"\n}\n```\n\nWe need to iterate through all the hotels and run these queries for each hotel. This would give us customer reviews relevant to the selected hotel areas. After that, we count the sentiment labels and plot results for each hotel.\n\n\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nhotel_sentiments = []\n\n# iterate through the hotels\nfor hotel in hotels:\n    result = []\n    # iterate through the keys and values in the queries dict\n    for area, query in queries.items():\n        # generate query embeddings\n        xq = retriever.encode(query).tolist()\n        # query pinecone with query embeddings and the hotel filter\n        xc = index.query(xq, top_k=500, include_metadata=True, filter={\"hotel_name\": hotel})\n        # get an overall count of customer sentiment\n        sentiment = count_sentiment(xc)\n        # sort the sentiment to show area and each value side by side\n        for k, v in sentiment.items():\n            data = {\n                \"area\": area,\n                \"label\": k,\n                \"value\": v \n            }\n            # add the data to result list\n            result.append(data)\n    # convert the      \n    hotel_sentiments.append({\"hotel\": hotel, \"df\": pd.DataFrame(result)})\n\n```\n\nLet's see what our final data look like.\n\n\n```python\nhotel_sentiments[0][\"df\"]\n```\n\n\n\n\n\n<div id=\"df-401ac15f-793e-4bce-8a1a-10e2b983d865\">\n  <div class=\"colab-df-container\">\n    <div>\n      <style scoped>\n          .dataframe tbody tr th:only-of-type {\n              vertical-align: middle;\n          }\n          .dataframe tbody tr th {\n              vertical-align: top;\n          }\n          .dataframe thead th {\n              text-align: right;\n          }\n      </style>\n      <table border=\"1\" class=\"dataframe\">\n        <thead>\n          <tr style=\"text-align: right;\">\n            <th></th>\n            <th>area</th>\n            <th>label</th>\n            <th>value</th>\n          </tr>\n        </thead>\n        <tbody>\n          <tr>\n            <th>0</th>\n            <td>Room Size</td>\n            <td>negative</td>\n            <td>127</td>\n          </tr>\n          <tr>\n            <th>1</th>\n            <td>Room Size</td>\n            <td>neutral</td>\n            <td>187</td>\n          </tr>\n          <tr>\n            <th>2</th>\n            <td>Room Size</td>\n            <td>positive</td>\n            <td>186</td>\n          </tr>\n          <tr>\n            <th>3</th>\n            <td>Cleanliness</td>\n            <td>negative</td>\n            <td>90</td>\n          </tr>\n          <tr>\n            <th>4</th>\n            <td>Cleanliness</td>\n            <td>neutral</td>\n            <td>67</td>\n          </tr>\n          <tr>\n            <th>5</th>\n            <td>Cleanliness</td>\n            <td>positive</td>\n            <td>343</td>\n          </tr>\n          <tr>\n            <th>6</th>\n            <td>Staff</td>\n            <td>negative</td>\n            <td>68</td>\n          </tr>\n          <tr>\n            <th>7</th>\n            <td>Staff</td>\n            <td>neutral</td>\n            <td>36</td>\n          </tr>\n          <tr>\n            <th>8</th>\n            <td>Staff</td>\n            <td>positive</td>\n            <td>396</td>\n          </tr>\n          <tr>\n            <th>9</th>\n            <td>Food</td>\n            <td>negative</td>\n            <td>95</td>\n          </tr>\n          <tr>\n            <th>10</th>\n            <td>Food</td>\n            <td>neutral</td>\n            <td>54</td>\n          </tr>\n          <tr>\n            <th>11</th>\n            <td>Food</td>\n            <td>positive</td>\n            <td>351</td>\n          </tr>\n          <tr>\n            <th>12</th>\n            <td>AC</td>\n            <td>negative</td>\n            <td>175</td>\n          </tr>\n          <tr>\n            <th>13</th>\n            <td>AC</td>\n            <td>neutral</td>\n            <td>100</td>\n          </tr>\n          <tr>\n            <th>14</th>\n            <td>AC</td>\n            <td>positive</td>\n            <td>225</td>\n          </tr>\n        </tbody>\n      </table>\n    </div>\n    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-401ac15f-793e-4bce-8a1a-10e2b983d865')\" title=\"Convert this dataframe to an interactive table.\" style=\"display:none;\">\n    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg>\n    </button>\n    <style>\n      .colab-df-container {\n        display:flex;\n        flex-wrap:wrap;\n        gap: 12px;\n      }\n      .colab-df-convert {\n        background-color: #E8F0FE;\n        border: none;\n        border-radius: 50%;\n        cursor: pointer;\n        display: none;\n        fill: #1967D2;\n        height: 32px;\n        padding: 0 0 0 0;\n        width: 32px;\n      }\n      .colab-df-convert:hover {\n        background-color: #E2EBFA;\n        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n        fill: #174EA6;\n      }\n      [theme=dark] .colab-df-convert {\n        background-color: #3B4455;\n        fill: #D2E3FC;\n      }\n      [theme=dark] .colab-df-convert:hover {\n        background-color: #434B5C;\n        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n        fill: #FFFFFF;\n      }\n    </style>\n    <script>\n      const buttonEl = document.querySelector('#df-401ac15f-793e-4bce-8a1a-10e2b983d865 button.colab-df-convert');\n      buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none';\n      async function convertToInteractive(key) {\n        const element = document.querySelector('#df-401ac15f-793e-4bce-8a1a-10e2b983d865');\n        const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {});\n        if (!dataTable) return;\n        const docLinkHtml = 'Like what you see? Visit the ' +\n          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n          + ' to learn more about interactive tables.';\n        element.innerHTML = '';\n        dataTable['output_type'] = 'display_data';\n        await google.colab.output.renderOutput(dataTable, element);\n        const docLink = document.createElement('div');\n        docLink.innerHTML = docLinkHtml;\n        element.appendChild(docLink);\n      }\n    </script>\n  </div>\n</div>\n\n\n\n\nWe may now plot the final data to make inference.\n\n\n```python\n# create the figure and axes to plot barchart for all hotels\nfig, axs = plt.subplots(nrows=1, ncols=4, figsize=(25, 4.5))\nplt.subplots_adjust(hspace=0.25)\n\ncounter = 0\n# iterate through each hotel in the list and plot a barchart\nfor d, ax in zip(hotel_sentiments, axs.ravel()):\n    # plot barchart for each hotel\n    sns.barplot(x=\"label\", y=\"value\", hue=\"area\", data=d[\"df\"], ax=ax)\n    # display the hotel names\n    ax.set_title(d[\"hotel\"])\n    # remove x labels\n    ax.set_xlabel(\"\")\n    # remove legend from all charts except for the first one\n    counter += 1\n    if counter != 1: ax.get_legend().remove()\n# display the full figure\nplt.show()\n```\n\n\n    \n![png](https://raw.githubusercontent.com/pinecone-io/img/main/sentiment-mining-example-60_0.png)\n    \n\n\nThe following observations can be made for the hotels based on the sentiment analysis:\n1.  **Strand Palace Hotel:** most customers were pleased with the staff, food, and cleanliness of the rooms, while a considerable number of them were not very satisfied with the room sizes and the AC.\n2.  **Britannia International Hotel Canary Wharf:** customers were quite happy with the room size, but the majority were not satisfied with the AC.\n3.  **Grand Royale London Hyde Park**: the majority of the customers were not satisfied with the room size, while a good number of them were pretty satisfied with the food, staff, AC, and cleanliness of the rooms.\n4.  **Intercontinental London The O2**: the majority of the customers were really happy with the selected five areas, making this hotel the best among the selected hotels.\n\n\nAlthough we have experimented with a few selected areas, you can get creative with your queries and get the sentiment around your area of interest immediately. This approach can even be applied to live data as the Pinecone index refreshes in real-time and performs vector searches across billions of documents with millisecond latency.\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccd9"
  },
  "filename": "audio-search.md",
  "title": "none",
  "category": "none",
  "content": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/search/audio/audio-search/audio-search.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/search/audio/audio-search/audio-search.ipynb) [![Open Github](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/github-shield.svg)](https://github.com/pinecone-io/examples/tree/master/search/audio/audio-search/audio-search.ipynb)\n\n# Audio Similarity Search\n\nThis notebook shows how to use Pinecone as the vector DB within an audio search application. Audio search can be used to find songs and metadata within a catalog, finding similar sounds in an audio library, or detecting who's speaking in an audio file.\n\nWe will index a set of audio recordings as vector embeddings. These vector embeddings are rich, mathematical representations of the audio recordings, making it possible to determine how similar the recordings are to one another. We will then take some new (unseen) audio recording, search through the index to find the most similar matches, and play the returned audio in this notebook.\n\n# Install Dependencies\n\n\n```python\n!pip install -qU pinecone-client panns-inference datasets librosa\n```\n\n# Load Dataset\n\nIn this demo, we will use audio from the *ESC-50 dataset* — a labeled collection of 2000 environmental audio recordings, which are 5-second-long each. The dataset can be loaded from the HuggingFace model hub as follows:\n\n\n```python\nfrom datasets import load_dataset\n\n# load the dataset from huggingface model hub\ndata = load_dataset(\"ashraq/esc50\", split=\"train\")\ndata\n```\n\n    Dataset({\n        features: ['filename', 'fold', 'target', 'category', 'esc10', 'src_file', 'take', 'audio'],\n        num_rows: 2000\n    })\n\n\n\nThe audios in the dataset are sampled at 44100Hz and loaded into NumPy arrays. Let's take a look.\n\n\n```python\n# select the audio feature and display top three\naudios = data[\"audio\"]\naudios[:3]\n```\n\n\n    [{'path': None,\n      'array': array([0., 0., 0., ..., 0., 0., 0.]),\n      'sampling_rate': 44100},\n     {'path': None,\n      'array': array([-0.01184082, -0.10336304, -0.14141846, ...,  0.06985474,\n              0.04049683,  0.00274658]),\n      'sampling_rate': 44100},\n     {'path': None,\n      'array': array([-0.00695801, -0.01251221, -0.01126099, ...,  0.215271  ,\n             -0.00875854, -0.28903198]),\n      'sampling_rate': 44100}]\n\n\n\nWe only need the Numpy arrays as these contain all of the audio data. We will later input these Numpy arrays directly into our embedding model to generate audio embeddings.\n\n\n```python\nimport numpy as np\n\n# select only the audio data from the dataset and store in a numpy array\naudios = np.array([a[\"array\"] for a in data[\"audio\"]])\n```\n\n# Load Audio Embedding Model\n\nWe will use an audio tagging model trained from *PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition* paper to generate our audio embeddings. We use the *panns_inference* Python package, which provides an easy interface to load and use the model.\n\n\n```python\nfrom panns_inference import AudioTagging\n\n# load the default model into the gpu.\nmodel = AudioTagging(checkpoint_path=None, device='cuda') # change device to cpu if a gpu is not available\n```\n\n    Checkpoint path: /root/panns_data/Cnn14_mAP=0.431.pth\n    GPU number: 1\n\n\n# Initialize Pinecone Index\n\nThe Pinecone index stores the audio embeddings, which we can later retrieve using another vector embedding (a *query* audio vector). We first need to initialize our connection to Pinecone and create our vector index. For this, we need a [free API key](https://app.pinecone.io/) and then we initialize the connection like so:\n\n\n```python\nimport pinecone\n\n# connect to pinecone environment\npinecone.init(\n    api_key=\"YOUR_API_KEY\",\n    environment=\"YOUR_ENV\"  # find next to API key\n)\n```\n\nNow we create our index. We need to give it a name (you can choose anything, we use `\"audio-search-demo\"` here). The dimension is set to `2048` as the model we use to generate audio embeddings output 2048-dimension vectors. Finally, we use `cosine` as our similarity metric as the model is trained to embed audio into a cosine metric space.\n\n\n```python\nindex_name = \"audio-search-demo\"\n\n# check if the audio-search index exists\nif index_name not in pinecone.list_indexes():\n    # create the index if it does not exist\n    pinecone.create_index(\n        index_name,\n        dimension=2048,\n        metric=\"cosine\"\n    )\n\n# connect to audio-search index we created\nindex = pinecone.Index(index_name)\n```\n\n# Generate Embeddings and Upsert\n\nNow we generate the embeddings using the audio embedding model. We must do this in batches as processing all items at once will exhaust machine memory limits and API request limits.\n\n\n```python\nfrom tqdm.auto import tqdm\n\n# we will use batches of 64\nbatch_size = 64\n\nfor i in tqdm(range(0, len(audios), batch_size)):\n    # find end of batch\n    i_end = min(i+batch_size, len(audios))\n    # extract batch\n    batch = audios[i:i_end]\n    # generate embeddings for all the audios in the batch\n    _, emb = model.inference(batch)\n    # create unique IDs\n    ids = [f\"{idx}\" for idx in range(i, i_end)]\n    # add all to upsert list\n    to_upsert = list(zip(ids, emb.tolist()))\n    # upsert/insert these records to pinecone\n    _ = index.upsert(vectors=to_upsert)\n\n# check that we have all vectors in index\nindex.describe_index_stats()\n```\n\n\n    {'dimension': 2048,\n     'index_fullness': 0.0,\n     'namespaces': {'': {'vector_count': 2000}},\n     'total_vector_count': 2000}\n\n\n\nWe now have *2000* audio records indexed in Pinecone, we're ready to begin querying.\n\n# Querying\n\nLet's first listen to an audio from our dataset. We will generate embeddings for the audio and use it to find similar audios from the Pinecone index.\n\n\n```python\nfrom IPython.display import Audio, display\n\n# we set an audio number to select from the dataset\naudio_num = 400\n# get the audio data of the audio number\nquery_audio = data[audio_num][\"audio\"][\"array\"]\n# get the category of the audio number\ncategory = data[audio_num][\"category\"]\n# print the category and play the audio\nprint(\"Query Audio:\", category)\nAudio(query_audio, rate=44100)\n```\n\n    Query Audio: car_horn\n\n\nWe have got the sound of a car horn. Let's generate an embedding for this sound.\n\n\n```python\n# reshape query audio\nquery_audio = query_audio[None, :]\n# get the embeddings for the audio from the model\n_, xq = model.inference(query_audio)\nxq.shape\n```\n\n\n\n\n    (1, 2048)\n\n\n\nWe have now converted the audio into a 2048-dimension vector the same way we did for all the other audio we indexed. Let's use this to query our Pinecone index.\n\n\n```python\n# query pinecone index with the query audio embeddings\nresults = index.query(xq.tolist(), top_k=3)\nresults\n```\n\n\n\n\n    {'matches': [{'id': '400', 'score': 1.0, 'values': []},\n                 {'id': '1667', 'score': 0.842124522, 'values': []},\n                 {'id': '1666', 'score': 0.831768811, 'values': []}],\n     'namespace': ''}\n\n\n\nNotice that the top result is the audio number 400 from our dataset, which is our query audio (the most similar item should always be the query itself). Let's listen to the top three results. \n\n\n```python\n# play the top 3 similar audios\nfor r in results[\"matches\"]:\n    # select the audio data from the databse using the id as an index\n    a = data[int(r[\"id\"])][\"audio\"][\"array\"]\n    display(Audio(a, rate=44100))\n```\n\n\nWe have great results, everything aligns with what seems to be a busy city street with car horns.\n\nLet's write a helper function to run the queries using audio from our dataset easily. We do not need to embed these audio samples again as we have already, they are just stored in Pinecone. So, we specify the `id` of the query audio to search with and tell Pinecone to search with that.\n\n\n```python\ndef find_similar_audios(id):\n    print(\"Query Audio:\")\n    # select the audio data from the databse using the id as an index\n    query_audio = data[id][\"audio\"][\"array\"]\n    # play the query audio\n    display(Audio(query_audio, rate=44100))\n    # query pinecone index with the query audio id\n    result = index.query(id=str(id), top_k=5)\n    print(\"Result:\")\n    # play the top 5 similar audios\n    for r in result[\"matches\"]:\n        a = data[int(r[\"id\"])][\"audio\"][\"array\"]\n        display(Audio(a, rate=44100))\n```\n\n\n```python\nfind_similar_audios(1642)\n```\n\n\nHere we return a set of revving motors (they seem to either be vehicles or lawnmowers).\n\n\n```python\nfind_similar_audios(452)\n```\n\n\nAnd now a more relaxing set of birds chirping in nature.\n\nLet's use another audio sample from elsewhere (eg not this dataset) and see how the search performs with this.\n\n\n```python\n!wget https://storage.googleapis.com/audioset/miaow_16k.wav\n```\n\n    --2022-09-25 20:47:00--  https://storage.googleapis.com/audioset/miaow_16k.wav\n    Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.20.128, 108.177.98.128, 74.125.197.128, ...\n    Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.20.128|:443... connected.\n    HTTP request sent, awaiting response... 200 OK\n    Length: 215546 (210K) [audio/x-wav]\n    Saving to: ‘miaow_16k.wav.1’\n    \n    miaow_16k.wav.1     100%[===================>] 210.49K  --.-KB/s    in 0.004s  \n    \n    2022-09-25 20:47:00 (54.1 MB/s) - ‘miaow_16k.wav.1’ saved [215546/215546]\n    \n\n\nWe can load the audio into a Numpy array as follows:\n\n\n```python\nimport librosa\n\na, _ = librosa.load(\"miaow_16k.wav\", sr=44100)\nAudio(a, rate=44100)\n```\n\n\nNow we generate the embeddings for this audio and query the Pinecone index.\n\n\n```python\n# reshape query audio\nquery_audio = a[None, :]\n# get the embeddings for the audio from the model\n_, xq = model.inference(query_audio)\n\n# query pinecone index with the query audio embeddings\nresults = index.query(xq.tolist(), top_k=3)\n\n# play the top 3 similar audios\nfor r in results[\"matches\"]:\n    a = data[int(r[\"id\"])][\"audio\"][\"array\"]\n    display(Audio(a, rate=44100))\n```\n\n\nOur audio search application has identified a set of similar cat sounds, which is excellent.\n\n# Delete the Index\n\nDelete the index once you are sure that you do not want to use it anymore. Once the index is deleted, you cannot use it again.\n\n\n```python\npinecone.delete_index(index_name)\n```\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccda"
  },
  "filename": "it-threat-detection.md",
  "title": "Threat Detection",
  "category": "630fc5235d91a70054705fb7",
  "content": "---\ntitle: Threat Detection\ncategory: 630fc5235d91a70054705fb7\n---\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/security/it-threat-detection/it-threat-detection.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/security/it-threat-detection/it-threat-detection.ipynb) [![Open Github](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/github-shield.svg)](https://github.com/pinecone-io/examples/tree/master/security/it-threat-detection/it-threat-detection.ipynb)\n\nThis notebook shows how to use Pinecone's similarity search as a service to build an application for detecting rare events. Such application is common in cyber-security and fraud detection domains wherein only a tiny fraction of the events are malicious. \n\nHere we will build a network intrusion detector. Network intrusion detection systems monitor incoming and outgoing network traffic flow, raising alarms whenever a threat is detected. Here we use a deep-learning model and similarity search in detecting and classifying network intrusion traffic.\n\nWe will start by indexing a set of labeled traffic events in the form of vector embeddings. Each event is either benign or malicious. The vector embeddings are rich, mathematical representations of the network traffic events. It is making it possible to determine how similar the network events are to one another using similarity-search algorithms built into Pinecone. Here we will transform network traffic events into vectors using a deep learning model from recent academic work.\n\n\nWe will then take some new (unseen) network events and search through the index to find the most similar matches, along with their labels. In such a way, we will propagate the matched labels to classify the unseen events as benign or malicious. Mind that the intrusion detection task is a challenging classification task because malicious events are sporadic. The similarity search service helps us sift the most relevant historical labeled events. That way, we identify these rare events while keeping a low rate of false alarms. \n\n\n## Setting up Pinecone\n\nWe will first install and initialize Pinecone. You can get your [API Key here](https://www.pinecone.io/start). You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**.\n\n\n```python\n!pip install -qU pinecone-client\n```\n\n\n```python\nimport pinecone\nimport os\n# Load Pinecone API key\napi_key = os.getenv('PINECONE_API_KEY') or 'YOUR_API_KEY'\n# Set Pinecone environment. Find next to API key in console\nenv = os.getenv('PINECONE_ENVIRONMENT') or 'YOUR_ENVIRONMENT'\npinecone.init(api_key=api_key, environment=env)\n#List all present indexes associated with your key, should be empty on the first run\npinecone.list_indexes()\n```\n\n\n\n\n    []\n\n\n\n## Installing other dependencies\n\n\n```python\n!pip install -qU pip python-dateutil tensorflow==2.5 keras==2.4.0 scikit-learn matplotlib==3.1.0 seaborn\n```\n\n\n```python\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Model\nimport tensorflow.keras.backend as K\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix\n```\n\nWe will use some of the code from a recent [academic work](https://github.com/rambasnet/DeepLearning-IDS). Let's clone the repository that we will use to prepare data.\n\n\n```python\n!git clone -q https://github.com/rambasnet/DeepLearning-IDS.git \n```\n\n## Define a New Pinecone Index\n\n\n```python\n# Pick a name for the new service\nindex_name = 'it-threats'\n```\n\n\n```python\n# Make sure service with the same name does not exist\nif index_name in pinecone.list_indexes():\n    pinecone.delete_index(index_name)\n```\n\n**Create an index**\n\n\n```python\npinecone.create_index(name=index_name, dimension=128, metric='euclidean')\n```\n\n**Connect to the index**\n\nWe create an index object, a class instance of pinecone.Index , which will be used to interact with the created index.\n\n\n```python\nindex = pinecone.Index(index_name=index_name)\n```\n\n## Upload\nHere we transform network events into vector embeddings, then upload them into Pinecone's vector index. \n\n### Prepare Data\n\nThe datasets we use consist of benign (normal) network traffic and malicious traffic\ngenerated from several different network attacks. We will focus on web attacks only. \n\nThe web attack category consists of three common attacks: \n- Cross-site scripting (BruteForce-XSS), \n- SQL-Injection (SQL-Injection), \n- Brute force administrative and user passwords (BruteForce-Web)\n\nThe original data was recorded over two days.\n\n**Download data for 22-02-2018 and 23-02-2018**\n\n\n\n\nFiles should be downloaded to the current directory. We will be using one date for training and generating vectors, and another one for testing.\n\n\n```python\n!wget \"https://cse-cic-ids2018.s3.ca-central-1.amazonaws.com/Processed%20Traffic%20Data%20for%20ML%20Algorithms/Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv\" -q --show-progress\n!wget \"https://cse-cic-ids2018.s3.ca-central-1.amazonaws.com/Processed%20Traffic%20Data%20for%20ML%20Algorithms/Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\" -q --show-progress\n```\n\n    Thursday-22-02-2018 100%[===================>] 364.91M  3.07MB/s    in 2m 6s   \n    Friday-23-02-2018_T 100%[===================>] 365.10M  3.07MB/s    in 1m 53s  \n\n\nLet's look at the data events first.\n\n\n```python\ndata = pd.read_csv('Friday-23-02-2018_TrafficForML_CICFlowMeter.csv')\ndata.Label.value_counts()\n```\n\n\n\n\n    Benign              1048009\n    Brute Force -Web        362\n    Brute Force -XSS        151\n    SQL Injection            53\n    Name: Label, dtype: int64\n\n\n\n**Clean the data** using a python script from the cloned repository.\n\n\n```python\n!python DeepLearning-IDS/data_cleanup.py \"Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\" \"result23022018\"\n```\n\n    cleaning Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\n    total rows read = 1048576\n    all done writing 1042868 rows; dropped 5708 rows\n\n\nLoad the file that you got from the previous step.\n\n\n```python\ndata_23_cleaned = pd.read_csv('result23022018.csv')\ndata_23_cleaned.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dst Port</th>\n      <th>Protocol</th>\n      <th>Timestamp</th>\n      <th>Flow Duration</th>\n      <th>Tot Fwd Pkts</th>\n      <th>Tot Bwd Pkts</th>\n      <th>TotLen Fwd Pkts</th>\n      <th>TotLen Bwd Pkts</th>\n      <th>Fwd Pkt Len Max</th>\n      <th>Fwd Pkt Len Min</th>\n      <th>...</th>\n      <th>Fwd Seg Size Min</th>\n      <th>Active Mean</th>\n      <th>Active Std</th>\n      <th>Active Max</th>\n      <th>Active Min</th>\n      <th>Idle Mean</th>\n      <th>Idle Std</th>\n      <th>Idle Max</th>\n      <th>Idle Min</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>22</td>\n      <td>6</td>\n      <td>1.519374e+09</td>\n      <td>1532698</td>\n      <td>11</td>\n      <td>11</td>\n      <td>1179</td>\n      <td>1969</td>\n      <td>648</td>\n      <td>0</td>\n      <td>...</td>\n      <td>32</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>500</td>\n      <td>17</td>\n      <td>1.519374e+09</td>\n      <td>117573855</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1500</td>\n      <td>0</td>\n      <td>500</td>\n      <td>500</td>\n      <td>...</td>\n      <td>8</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>58786927.5</td>\n      <td>2.375324e+07</td>\n      <td>75583006</td>\n      <td>41990849</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>500</td>\n      <td>17</td>\n      <td>1.519374e+09</td>\n      <td>117573848</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1500</td>\n      <td>0</td>\n      <td>500</td>\n      <td>500</td>\n      <td>...</td>\n      <td>8</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>58786924.0</td>\n      <td>2.375325e+07</td>\n      <td>75583007</td>\n      <td>41990841</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>22</td>\n      <td>6</td>\n      <td>1.519374e+09</td>\n      <td>1745392</td>\n      <td>11</td>\n      <td>11</td>\n      <td>1179</td>\n      <td>1969</td>\n      <td>648</td>\n      <td>0</td>\n      <td>...</td>\n      <td>32</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>500</td>\n      <td>17</td>\n      <td>1.519374e+09</td>\n      <td>89483474</td>\n      <td>6</td>\n      <td>0</td>\n      <td>3000</td>\n      <td>0</td>\n      <td>500</td>\n      <td>500</td>\n      <td>...</td>\n      <td>8</td>\n      <td>4000364.0</td>\n      <td>0.0</td>\n      <td>4000364</td>\n      <td>4000364</td>\n      <td>21370777.5</td>\n      <td>1.528092e+07</td>\n      <td>41989576</td>\n      <td>7200485</td>\n      <td>Benign</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 80 columns</p>\n</div>\n\n\n\n\n```python\ndata_23_cleaned.Label.value_counts()\n```\n\n\n\n\n    Benign              1042301\n    Brute Force -Web        362\n    Brute Force -XSS        151\n    SQL Injection            53\n    Name: Label, dtype: int64\n\n\n\n### Load the Model\n\nHere we load the pretrained model. The model is trained using the data from the same date.\n\nWe have modified [the original model](https://github.com/rambasnet/DeepLearning-IDS/blob/master/keras_tensorflow_models/02-23-2018.csv_adam_10_10_multiclass_baseline_model_1561316601.model) slightly and changed the number of classes from four (Benign, BruteForce-Web, BruteForce-XSS, SQL-Injection) to two (Benign and Attack). In the step below we will download and unzip our modified model.\n\n\n```python\n!wget -q -O it_threat_model.model.zip \"https://drive.google.com/uc?export=download&id=1VYMHOk_XMAc-QFJ_8CAPvWFfHnLpS2J_\" \n!unzip -q it_threat_model.model.zip\n\n```\n\n\n```python\nmodel = keras.models.load_model('it_threat_model.model')\nmodel.summary()\n```\n\n    WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n    Model: \"sequential\"\n    _________________________________________________________________\n    Layer (type)                 Output Shape              Param #   \n    =================================================================\n    dense (Dense)                (None, 128)               10240     \n    _________________________________________________________________\n    dense_1 (Dense)              (None, 64)                8256      \n    _________________________________________________________________\n    dense_2 (Dense)              (None, 1)                 65        \n    =================================================================\n    Total params: 18,561\n    Trainable params: 18,561\n    Non-trainable params: 0\n    _________________________________________________________________\n\n\n\n```python\n# Select the first layer\nlayer_name = 'dense' \nintermediate_layer_model = Model(inputs=model.input,\n                                 outputs=model.get_layer(layer_name).output)\n```\n\n### Upload Data\n\n\nLet's define the item's ids in a way that will reflect the event's label.  Then, we index the events in Pinecone's vector index.\n\n\n```python\nfrom tqdm import tqdm\nitems_to_upload = []\n\nmodel_res = intermediate_layer_model.predict(K.constant(data_23_cleaned.iloc[:,:-1]))\n\nfor i, res in tqdm(zip(data_23_cleaned.iterrows(), model_res), total=len(model_res)):\n    benign_or_attack = i[1]['Label'][:3]\n    items_to_upload.append((benign_or_attack + '_' + str(i[0]), res.tolist()))\n```\n\n    100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1042867/1042867 [01:43<00:00, 10067.22it/s]\n\n\n\n```python\nimport itertools\n\ndef chunks(iterable, batch_size=100):\n    it = iter(iterable)\n    chunk = tuple(itertools.islice(it, batch_size))\n    while chunk:\n        yield chunk\n        chunk = tuple(itertools.islice(it, batch_size))\n```\n\nYou can lower the NUMBER_OF_ITEMS and, by doing so, limit the number of uploaded items. \n\n\n```python\nNUMBER_OF_ITEMS = len(items_to_upload)\n\nfor batch in chunks(items_to_upload[:NUMBER_OF_ITEMS], 50):\n    index.upsert(vectors=batch)\n```\n\n\n```python\nitems_to_upload.clear()\n```\n\nLet's verify all items were inserted. \n\n\n```python\nindex.describe_index_stats()\n```\n\n\n\n\n    {'dimension': 128, 'namespaces': {'': {'vector_count': 1042867}}}\n\n\n\n## Query\n\nFirst, we will randomly select a Benign/Attack event and query the vector index using the event embedding. Then, we will use data from different day, that contains same set of attacks to query on a bigger sample.\n\n\n### Evaluate the Rare Event Classification Model\n\nWe will use network intrusion dataset for 22-02-2018 for querying and testing the Pinecone.\n\nFirst, let's clean the data.\n\n\n```python\n!python DeepLearning-IDS/data_cleanup.py \"Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv\" \"result22022018\"\n```\n\n    cleaning Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv\n    total rows read = 1048576\n    all done writing 1042966 rows; dropped 5610 rows\n\n\n\n```python\ndata_22_cleaned = pd.read_csv('result22022018.csv')\ndata_22_cleaned.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dst Port</th>\n      <th>Protocol</th>\n      <th>Timestamp</th>\n      <th>Flow Duration</th>\n      <th>Tot Fwd Pkts</th>\n      <th>Tot Bwd Pkts</th>\n      <th>TotLen Fwd Pkts</th>\n      <th>TotLen Bwd Pkts</th>\n      <th>Fwd Pkt Len Max</th>\n      <th>Fwd Pkt Len Min</th>\n      <th>...</th>\n      <th>Fwd Seg Size Min</th>\n      <th>Active Mean</th>\n      <th>Active Std</th>\n      <th>Active Max</th>\n      <th>Active Min</th>\n      <th>Idle Mean</th>\n      <th>Idle Std</th>\n      <th>Idle Max</th>\n      <th>Idle Min</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>22</td>\n      <td>6</td>\n      <td>1.519288e+09</td>\n      <td>20553406</td>\n      <td>10</td>\n      <td>7</td>\n      <td>1063</td>\n      <td>1297</td>\n      <td>744</td>\n      <td>0</td>\n      <td>...</td>\n      <td>20</td>\n      <td>1027304.0</td>\n      <td>0.0</td>\n      <td>1027304</td>\n      <td>1027304</td>\n      <td>1.952608e+07</td>\n      <td>0.000000e+00</td>\n      <td>19526080</td>\n      <td>19526080</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>34989</td>\n      <td>6</td>\n      <td>1.519288e+09</td>\n      <td>790</td>\n      <td>2</td>\n      <td>0</td>\n      <td>848</td>\n      <td>0</td>\n      <td>848</td>\n      <td>0</td>\n      <td>...</td>\n      <td>20</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>500</td>\n      <td>17</td>\n      <td>1.519288e+09</td>\n      <td>99745913</td>\n      <td>5</td>\n      <td>0</td>\n      <td>2500</td>\n      <td>0</td>\n      <td>500</td>\n      <td>500</td>\n      <td>...</td>\n      <td>8</td>\n      <td>4000203.0</td>\n      <td>0.0</td>\n      <td>4000203</td>\n      <td>4000203</td>\n      <td>3.191524e+07</td>\n      <td>3.792787e+07</td>\n      <td>75584115</td>\n      <td>7200679</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>500</td>\n      <td>17</td>\n      <td>1.519288e+09</td>\n      <td>99745913</td>\n      <td>5</td>\n      <td>0</td>\n      <td>2500</td>\n      <td>0</td>\n      <td>500</td>\n      <td>500</td>\n      <td>...</td>\n      <td>8</td>\n      <td>4000189.0</td>\n      <td>0.0</td>\n      <td>4000189</td>\n      <td>4000189</td>\n      <td>3.191524e+07</td>\n      <td>3.792788e+07</td>\n      <td>75584130</td>\n      <td>7200693</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>500</td>\n      <td>17</td>\n      <td>1.519288e+09</td>\n      <td>89481361</td>\n      <td>6</td>\n      <td>0</td>\n      <td>3000</td>\n      <td>0</td>\n      <td>500</td>\n      <td>500</td>\n      <td>...</td>\n      <td>8</td>\n      <td>4000554.0</td>\n      <td>0.0</td>\n      <td>4000554</td>\n      <td>4000554</td>\n      <td>2.137020e+07</td>\n      <td>1.528109e+07</td>\n      <td>41990741</td>\n      <td>7200848</td>\n      <td>Benign</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 80 columns</p>\n</div>\n\n\n\n\n```python\ndata_22_cleaned.Label.value_counts()\n```\n\n\n\n\n    Benign              1042603\n    Brute Force -Web        249\n    Brute Force -XSS         79\n    SQL Injection            34\n    Name: Label, dtype: int64\n\n\n\nLet's define a sample that will include all different types of web attacks for this specific date.\n\n\n```python\ndata_sample = data_22_cleaned[-2000:]\ndata_sample.Label.value_counts()\n```\n\n\n\n\n    Benign              1638\n    Brute Force -Web     249\n    Brute Force -XSS      79\n    SQL Injection         34\n    Name: Label, dtype: int64\n\n\n\nNow, we will query the test dataset and save predicted and expected results to create a confusion matrix.\n\n\n```python\ny_true = []\ny_pred = []\n\nBATCH_SIZE = 100\n\nfor i in tqdm(range(0, len(data_sample), BATCH_SIZE)):\n    test_data = data_sample.iloc[i:i+BATCH_SIZE, :]\n    \n    # Create vector embedding using the model\n    test_vector = intermediate_layer_model.predict(K.constant(test_data.iloc[:, :-1]))\n    # Query using the vector embedding\n    query_results = []\n\n    for xq in test_vector.tolist():\n        query_res = index.query(xq, top_k=50)\n        query_results.append(query_res)\n    \n    ids = [res.id for result in query_results for res in result.matches]\n    \n    for label, res in zip(test_data.Label.values, query_results):\n        # Add to the true list\n        if label == 'Benign':\n            y_true.append(0)\n        else:\n            y_true.append(1)\n        \n        counter = Counter(match.id.split('_')[0] for match in res.matches)\n\n        # Add to the predicted list\n        if counter['Bru'] or counter['SQL']:\n            y_pred.append(1)\n        else:\n            y_pred.append(0)\n\n```\n\n    100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [10:48<00:00, 32.44s/it]\n\n\n\n```python\n# Create confusion matrix\nconf_matrix = confusion_matrix(y_true, y_pred)\n\n# Show confusion matrix\nax = plt.subplot()\nsns.heatmap(conf_matrix, annot=True, ax = ax, cmap='Blues', fmt='g', cbar=False)\n\n# Add labels, title and ticks\nax.set_xlabel('Predicted')\nax.set_ylabel('Acctual')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(['Benign', 'Attack'])\nax.yaxis.set_ticklabels(['Benign', 'Attack'])\n```\n\n\n\n\n    [Text(0, 0.5, 'Benign'), Text(0, 1.5, 'Attack')]\n\n\n\n\n    \n\n![Confusion matrix](https://raw.githubusercontent.com/pinecone-io/img/main/it-threat-detection-1.png)\n    \n\n\nNow we can calculate overall accuracy and per class accuracy.\n\n\n```python\n# Calculate accuracy\nacc = accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\n\nprint(f\"Accuracy: {acc:.3f}\")\nprint(f\"Precision: {precision:.3f}\")\nprint(f\"Recall: {recall:.3f}\")\n```\n\n    Accuracy: 0.923\n    Precision: 0.995\n    Recall: 0.577\n    \n\n\n```python\n# Calculate per class accuracy\ncmd = confusion_matrix(y_true, y_pred, normalize=\"true\").diagonal()\nper_class_accuracy_df = pd.DataFrame([(index, round(value,4)) for index, value in zip(['Benign', 'Attack'], cmd)], columns = ['type', 'accuracy'])\nper_class_accuracy_df = per_class_accuracy_df.round(2)\ndisplay(per_class_accuracy_df)\n```\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Benign</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Attack</td>\n      <td>0.58</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\nWe got great results using Pinecone! Let's see what happens if we skip the similarity search step and predict values from the model directly. In other words, let's use the model that created the embeddings as a classifier. It would be interesting to compare its and the similarity search approach accuracy. \n\n\n```python\nfrom keras.utils.np_utils import normalize\nimport numpy as np\n\ndata_sample = normalize(data_22_cleaned.iloc[:, :-1])[-2000:]\ny_pred_model = model.predict(normalize(data_sample)).flatten()\ny_pred_model = np.round(y_pred_model)\n```\n\n\n```python\n# Create confusion matrix\nconf_matrix = confusion_matrix(y_true, y_pred_model)\n\n# Show confusion matrix\nax = plt.subplot()\nsns.heatmap(conf_matrix, annot=True, ax = ax, cmap='Blues', fmt='g', cbar=False)\n\n# Add labels, title and ticks\nax.set_xlabel('Predicted')\nax.set_ylabel('Acctual')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(['Benign', 'Attack'])\nax.yaxis.set_ticklabels(['Benign', 'Attack'])\n```\n\n\n\n\n    [Text(0, 0.5, 'Benign'), Text(0, 1.5, 'Attack')]\n\n\n\n\n    \n![Confusion matix](https://raw.githubusercontent.com/pinecone-io/img/main/it-threat-detection-2.png)\n    \n\n\n\n```python\n# Calculate accuracy\nacc = accuracy_score(y_true, y_pred_model, normalize=True, sample_weight=None)\nprecision = precision_score(y_true, y_pred_model)\nrecall = recall_score(y_true, y_pred_model)\n\nprint(f\"Accuracy: {acc:.3f}\")\nprint(f\"Precision: {precision:.3f}\")\nprint(f\"Recall: {recall:.3f}\")\n```\n\n    Accuracy: 0.871\n    Precision: 1.000\n    Recall: 0.287\n    \n\n\n```python\n# Calculate per class accuracy\ncmd = confusion_matrix(y_true, y_pred_model, normalize=\"true\").diagonal()\nper_class_accuracy_df = pd.DataFrame([(index, round(value,4)) for index, value in zip(['Benign', 'Attack'], cmd)], columns = ['type', 'accuracy'])\nper_class_accuracy_df = per_class_accuracy_df.round(2)\ndisplay(per_class_accuracy_df)\n```\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n  .dataframe tbody tr th {\n        vertical-align: top;\n    }\n  .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Benign</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Attack</td>\n      <td>0.29</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\nAs we can see, the direct application of our model produced much worse results. Pinecone's similarity search over the same model's embeddings improved our threat detection (i.e., \"Attack\") accuracy by over 50%!\n\n### Result summary\n\nUsing standard vector embeddings with Pinecone's similarity search service, we detected 85% of the attacks while keeping a low 3% false-positive rate. We also showed that our similarity search approach outperforms the direct classification approach that utilizes the classifier's embedding model. Similarity search-based detection gained 50% higher accuracy compared to the direct detector.\n\n[Original published results](https://github.com/rambasnet/DeepLearning-IDS/blob/master/graphics/confusion_matrices/) for 02-22-2018 show that the model was able to correctly detect 208520 benign cases out of 208520 benign cases, and 24 (18+1+5) attacks out of 70 attacks in the test set making this model **34.3% accurate in predicting attacks**. For testing purposes, 20% of the data for 02-22-2018 was used. \n\n![02-22-2018--6-15%281%29.png](https://raw.githubusercontent.com/rambasnet/DeepLearning-IDS/master/graphics/confusion_matrices/02-22-2018--6-15(1).png)\n\nAs you can see, the model's performance for creating embeddings for Pinecone was much higher. \n\nThe model we have created follows the academic paper ([model for the same date](https://github.com/rambasnet/DeepLearning-IDS/blob/master/keras_tensorflow_models/) (02-23-2018)) and is slightly modified, but still a straightforward, sequential, shallow model. We have changed the number of classes from four (Benign, BruteForce-Web, BruteForce-XSS, SQL-Injection) to two (Benign and Attack), only interested in whether we are detecting an attack or not. We have also changed validation metrics to precision and recall. These changes improved our results. Yet, there is still room for further improvements, for example, by adding more data covering multiple days and different types of attacks.\n\n## Delete the Index\n\nDelete the index once you are sure that you do not want to use it anymore. Once it is deleted, you cannot reuse it.\n\n\n```python\npinecone.delete_index(index_name)\n```\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccdb"
  },
  "filename": "gif-search.md",
  "title": "GIF Search",
  "category": "630fc5235d91a70054705fb7",
  "content": "---\ntitle: GIF Search\ncategory: 630fc5235d91a70054705fb7\n---\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/search/semantic-search/gif-search/gif-search.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/search/semantic-search/gif-search/gif-search.ipynb) [![Open Github](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/github-shield.svg)](https://github.com/pinecone-io/examples/tree/master/search/semantic-search/gif-search/gif-search.ipynb)\n\nWe will use the [Tumblr GIF Description Dataset](http://raingo.github.io/TGIF-Release/), which contains over 100k animated GIFs and 120K sentences describing its visual content. Using this data with a *vector database* and *retriever* we are able to create an NLP-powered GIF search tool.\n\nThere are a few packages that must be installed for this notebook to run:\n\n\n```python\npip install -U pandas pinecone-client sentence-transformers tqdm\n```\n\nWe must also set the following notebook parameters to display the GIF images we will be working with.\n\n\n```python\nfrom IPython.display import HTML\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n```\n\n## Download and Extract Dataset\n\nFirst let's download and extract the dataset. The dataset is available [here](https://github.com/raingo/TGIF-Release) on GitHub. We can use the link below to download the dataset directly. We can also access the link from a browser to directly download the files.\n\n\n```python\n# Use wget to download the master.zip file which contains the dataset\n!wget https://github.com/raingo/TGIF-Release/archive/master.zip\n```\n\n\n```python\n# Use unzip to extract the master.zip file\n!unzip master.zip\n```\n\n## Explore the Dataset\n\nNow let's explore the downloaded files. The data we want is in *tgif-v1.0.tsv* file in the *data* folder. We can use *pandas* library to open the file. We need to set delimiter as `\\t` as the file contains tab separated values.\n\n\n```python\nimport pandas as pd\n```\n\n\n```python\n# Load dataset to a pandas dataframe\ndf = pd.read_csv(\n    \"./TGIF-Release-master/data/tgif-v1.0.tsv\",\n    delimiter=\"\\t\",\n    names=['url', 'description']\n)\ndf.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>url</th>\n      <th>description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://38.media.tumblr.com/9f6c25cc350f12aa74...</td>\n      <td>a man is glaring, and someone with sunglasses ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://38.media.tumblr.com/9ead028ef62004ef6a...</td>\n      <td>a cat tries to catch a mouse on a tablet</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://38.media.tumblr.com/9f43dc410be85b1159...</td>\n      <td>a man dressed in red is dancing.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://38.media.tumblr.com/9f659499c8754e40cf...</td>\n      <td>an animal comes close to another in the jungle</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://38.media.tumblr.com/9ed1c99afa7d714118...</td>\n      <td>a man in a hat adjusts his tie and makes a wei...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n*Note the dataset does not contain the actual GIF files. But it has URLs we can use to download/access the GIF files. This is great as we do not need to store/download all the GIF files. We can directly load the required GIF files using the URL when displaying the search results.*\n\nThere are some duplicate descriptions in the dataset.\n\n\n```python\nlen(df)\n```\n\n\n\n\n    125782\n\n\n\n\n```python\n# Number of *unique* GIFs in the dataset\nlen(df[\"url\"].unique())\n```\n\n\n\n\n    102068\n\n\n\n\n```python\ndupes = df['url'].value_counts().sort_values(ascending=False)\ndupes.head()\n```\n\n\n\n\n    https://38.media.tumblr.com/ddbfe51aff57fd8446f49546bc027bd7/tumblr_nowv0v6oWj1uwbrato1_500.gif    4\n    https://33.media.tumblr.com/46c873a60bb8bd97bdc253b826d1d7a1/tumblr_nh7vnlXEvL1u6fg3no1_500.gif    4\n    https://38.media.tumblr.com/b544f3c87cbf26462dc267740bb1c842/tumblr_n98uooxl0K1thiyb6o1_250.gif    4\n    https://33.media.tumblr.com/88235b43b48e9823eeb3e7890f3d46ef/tumblr_nkg5leY4e21sof15vo1_500.gif    4\n    https://31.media.tumblr.com/69bca8520e1f03b4148dde2ac78469ec/tumblr_npvi0kW4OD1urqm0mo1_400.gif    4\n    Name: url, dtype: int64\n\n\n\nLet's take a look at one of these duplicated URLs and it's descriptions.\n\n\n```python\ndupe_url = \"https://33.media.tumblr.com/88235b43b48e9823eeb3e7890f3d46ef/tumblr_nkg5leY4e21sof15vo1_500.gif\"\ndupe_df = df[df['url'] == dupe_url]\n\n# let's take a look at this GIF and it's duplicated descriptions\nfor _, gif in dupe_df.iterrows():\n    HTML(f\"<img src={gif['url']} style='width:120px; height:90px'>\")\n    print(gif[\"description\"])\n```\n\n\n\n\n<img src=https://33.media.tumblr.com/88235b43b48e9823eeb3e7890f3d46ef/tumblr_nkg5leY4e21sof15vo1_500.gif style='width:120px; height:90px'>\n\n\n\n    two girls are singing music pop in a concert\n\n\n\n\n\n<img src=https://33.media.tumblr.com/88235b43b48e9823eeb3e7890f3d46ef/tumblr_nkg5leY4e21sof15vo1_500.gif style='width:120px; height:90px'>\n\n\n\n    a woman sings sang girl on a stage singing\n\n\n\n\n\n<img src=https://33.media.tumblr.com/88235b43b48e9823eeb3e7890f3d46ef/tumblr_nkg5leY4e21sof15vo1_500.gif style='width:120px; height:90px'>\n\n\n\n    two girls on a stage sing into microphones.\n\n\n\n\n\n<img src=https://33.media.tumblr.com/88235b43b48e9823eeb3e7890f3d46ef/tumblr_nkg5leY4e21sof15vo1_500.gif style='width:120px; height:90px'>\n\n\n\n    two girls dressed in black are singing.\n\n\nThere is no reason for us to remove these duplicates, as shown here, every description is accurate. You can spot check a few of the other URLs but they all seem to be the same where we have several *accurate* descriptions for a single GIF.\n\nThat leaves us with 125,781 descriptions for 102,067 GIFs. We will use these descriptions to create *context* vectors that will be indexed in a vector database to create our GIF search tool. Let's take a look at a few more examples of GIFs and their descriptions.\n\n\n```python\nfor _, gif in df[:5].iterrows():\n  HTML(f\"<img src={gif['url']} style='width:120px; height:90px'>\")\n  print(gif[\"description\"])\n```\n\n\n\n\n<img src=https://38.media.tumblr.com/9f6c25cc350f12aa74a7dc386a5c4985/tumblr_mevmyaKtDf1rgvhr8o1_500.gif style='width:120px; height:90px'>\n\n\n\n    a man is glaring, and someone with sunglasses appears.\n\n\n\n\n\n<img src=https://38.media.tumblr.com/9ead028ef62004ef6ac2b92e52edd210/tumblr_nok4eeONTv1s2yegdo1_400.gif style='width:120px; height:90px'>\n\n\n\n    a cat tries to catch a mouse on a tablet\n\n\n\n\n\n<img src=https://38.media.tumblr.com/9f43dc410be85b1159d1f42663d811d7/tumblr_mllh01J96X1s9npefo1_250.gif style='width:120px; height:90px'>\n\n\n\n    a man dressed in red is dancing.\n\n\n\n\n\n<img src=https://38.media.tumblr.com/9f659499c8754e40cf3f7ac21d08dae6/tumblr_nqlr0rn8ox1r2r0koo1_400.gif style='width:120px; height:90px'>\n\n\n\n    an animal comes close to another in the jungle\n\n\n\n\n\n<img src=https://38.media.tumblr.com/9ed1c99afa7d71411884101cb054f35f/tumblr_mvtuwlhSkE1qbnleeo1_500.gif style='width:120px; height:90px'>\n\n\n\n    a man in a hat adjusts his tie and makes a weird face.\n\n\nWe can see that the description of the GIF accurately describes what is happening in the GIF, we can use these descriptions to search through our GIFs.\n\nUsing this data, we can build the GIF search tool with just *two* components:\n\n* a **retriever** to embed GIF descriptions\n* a **vector database** to store GIF description embeddings and retrieve relevant GIFs\n\n## Initialize Pinecone Index\n\nThe vector database stores vector representations of our GIF descriptions which we can retrieve using another vector (query vector). We will use the Pinecone vector database, a fully managed vector database that can store and search through billions of records in milliseconds. You could use any other vector database such as FAISS to build this tool. But you may need to manage the database yourself.\n\nTo initialize the database, we sign up for a [free Pinecone API key](https://app.pinecone.io/) and `pip install pinecone-client`. You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**. Once ready, we initialize our index with:\n\n\n```python\nimport pinecone\n\n# Connect to pinecone environment\npinecone.init(\n    api_key=\"<<YOUR_API_KEY>>\",\n    environment=\"YOUR_ENVIRONMENT\"\n)\n\nindex_name = 'gif-search'\n\n# check if the gif-search exists\nif index_name not in pinecone.list_indexes():\n    # create the index if it does not exist\n    pinecone.create_index(\n        index_name,\n        dimension=384,\n        metric=\"cosine\"\n    )\n\n# Connect to gif-search index we created\nindex = pinecone.Index(index_name)\n```\n\nHere we specify the name of the index where we will store our GIF descriptions and their URLs, the similarity metric, and the embedding dimension of the vectors. The similarity metric and embedding dimension can change depending on the embedding model used. However, most retrievers use \"cosine\" and 768.\n\n## Initialize Retriever\n\nNext, we need to initialize our retriever. The retriever will mainly do two things:\n\n1.\tGenerate embeddings for all the GIF descriptions (context vectors/embeddings)\n2.\tGenerate embeddings for the query (query vector/embedding)\n\nThe retriever will generate the embeddings in a way that the queries and GIF descriptions with similar meanings are in a similar vector space. Then we can use cosine similarity to calculate this similarity between the query and context embeddings and find the most relevant GIF to our query.\n\nWe will use a `SentenceTransformer` model trained based on Microsoft's MPNet as our retriever. This model performs well out-of-the-box when searching based on generic semantic similarity. \n\n\n```python\nfrom sentence_transformers import SentenceTransformer\n```\n\n\n```python\n# Initialize retriever with SentenceTransformer model \nretriever = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\nretriever\n```\n\n\n\n\n    SentenceTransformer(\n      (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n      (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n      (2): Normalize()\n    )\n\n\n\n## Generate Embeddings and Upsert\n\nNow our retriever and the pinecone index are initialized. Next, we need to generate embeddings for the GIF descriptions. We will do this in batches to help us more quickly generate embeddings. This means our retriever will generate embeddings for 64 GIF descriptions at once instead of generating them individually (much faster) and send a single API call for each batch of 64 (also much faster).\n\nWhen passing the documents to pinecone, we need an id (a unique value), embedding (embeddings for the GIF descriptions we have generated earlier), and metadata for each document representing GIFs in the dataset. The metadata is a dictionary containing data relevant to our embeddings. For the GIF search tool, we only need the URL and description.\n\n\n```python\nfrom tqdm.auto import tqdm\n\n# we will use batches of 64\nbatch_size = 64\n\nfor i in tqdm(range(0, len(df), batch_size)):\n    # find end of batch\n    i_end = min(i+batch_size, len(df))\n    # extract batch\n    batch = df.iloc[i:i_end]\n    # generate embeddings for batch\n    emb = retriever.encode(batch['description'].tolist()).tolist()\n    # get metadata\n    meta = batch.to_dict(orient='records')\n    # create IDs\n    ids = [f\"{idx}\" for idx in range(i, i_end)]\n    # add all to upsert list\n    to_upsert = list(zip(ids, emb, meta))\n    # upsert/insert these records to pinecone\n    _ = index.upsert(vectors=to_upsert)\n\n    \n# check that we have all vectors in index\nindex.describe_index_stats()\n```\n\n\n      0%|          | 0/1966 [00:00<?, ?it/s]\n\n\n\n\n\n    {'dimension': 384,\n     'index_fullness': 0.05,\n     'namespaces': {'': {'vector_count': 125782}}}\n\n\n\nWe can see all our documents are now in the pinecone index. Let's run some queries to test our GIF search tool.\n\n## Querying\n\nWe have two functions, `search_gif`, to handle our search query, and `display_gif`, to display the search results.\n\nThe `search_gif` function generates vector embedding for the search query using the retriever model and then runs the query on the pinecone index. `index.query` will compute the cosine similarity between the query embedding and the GIF description embeddings as we set the metric type as \"cosine\" when we initialize the pinecone index. The function will return the URL of the top 10 most relevant GIFs to our search query.\n\n\n```python\ndef search_gif(query):\n    # Generate embeddings for the query\n    xq = retriever.encode(query).tolist()\n    # Compute cosine similarity between query and embeddings vectors and return top 10 URls\n    xc = index.query(xq, top_k=10,\n                    include_metadata=True)\n    result = []\n    for context in xc['matches']:\n        url = context['metadata']['url']\n        result.append(url)\n    return result\n```\n\nThe `display_gif` can display multiple GIFs using its URLs in the jupyter notebook in a grid style. We use this function to display the top 10 GIFs returned by the `search_gif` function.\n\n\n```python\ndef display_gif(urls):\n    figures = []\n    for url in urls:\n        figures.append(f'''\n            <figure style=\"margin: 5px !important;\">\n              <img src=\"{url}\" style=\"width: 120px; height: 90px\" >\n            </figure>\n        ''')\n    return HTML(data=f'''\n        <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n        {''.join(figures)}\n        </div>\n    ''')\n```\n\nLet's begin testing some queries.\n\n\n```python\ngifs = search_gif(\"a dog being confused\")\ndisplay_gif(gifs)\n```\n\n\n\n\n\n<div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://38.media.tumblr.com/af53df8d946bbca23be97691db0ecd5e/tumblr_nq3l305zdF1s71nvbo1_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://33.media.tumblr.com/a574ab035e7edc7708db423ee67f3ac4/tumblr_nq1zodZJNx1uoke7ao1_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://38.media.tumblr.com/94703ea885174ffc97c44d57487d7ee9/tumblr_na6oo2PKSC1silsr6o1_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://38.media.tumblr.com/fa6a31e326066bb27776066150c8c810/tumblr_np38ipgJPd1tkkgpso1_250.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://38.media.tumblr.com/241d89939a5714c2db4566d9108245fe/tumblr_n9xv6aqQ5A1qmgppeo1_250.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://31.media.tumblr.com/a00ae69f826dbe89a5bdabad567ac88d/tumblr_n8x5e6ZcFW1sjpl9lo1_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://31.media.tumblr.com/28a9aac3c21941e1c61dd9ab4390c3f5/tumblr_nhdr3clKDa1sntw1mo1_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://31.media.tumblr.com/5cbd531e1d8cc7fefffdb8a68ec62b1d/tumblr_naysx8YTzn1tzl1owo1_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://38.media.tumblr.com/bc300fcbae8e4eb65c3901a246f46e4c/tumblr_niu5dzNP7G1u62tooo1_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://38.media.tumblr.com/1c3edb33951b52020b9271185942b2b2/tumblr_nflm4phy0P1u4txqeo1_250.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n</div>\n\n\n\n\n\n```python\ngifs = search_gif(\"animals being cute\")\ndisplay_gif(gifs)\n```\n\n\n\n\n\n<div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://33.media.tumblr.com/73841eb3b37ad5277b324359a83bb19e/tumblr_ngnz25VQpD1twctp1o1_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://38.media.tumblr.com/7b8ebff7051b8a7d0502294465559861/tumblr_na8n60gCmT1tiamw8o1_500.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://38.media.tumblr.com/49223a5564c8d7dfafe115063ba88c8a/tumblr_nnrps82EvG1sxvevjo1_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://31.media.tumblr.com/aa9c98f92f06cc3484ae395194db6d7f/tumblr_naeyc6yQWy1tahfdeo1_250.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://38.media.tumblr.com/e7a1d7ed5f2289db13e1812a91c0eedf/tumblr_nf8r2nWajt1s236zjo1_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://38.media.tumblr.com/7d8f9cac33b4fc76908a37bf28ab6fca/tumblr_noswtqDMKC1tyncywo1_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://38.media.tumblr.com/136d1d103edf3a82c2332bf8ef28d6d3/tumblr_nhm8rleyTk1u333yco2_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://33.media.tumblr.com/0f97de4f3cc8dca408ca4ab036460412/tumblr_njmp6tj53K1thqmhto1_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://31.media.tumblr.com/be2b34de9ff751da15cbde3144d25007/tumblr_nh4oj86LJO1slj978o1_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://38.media.tumblr.com/5f45e9a56121b070ddceca58b37e9ace/tumblr_njaggwVmdn1un7vpco1_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n</div>\n\n\n\n```python\ngifs = search_gif(\"an animal dancing\")\ndisplay_gif(gifs)\n```\n\n\n\n\n\n<div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://33.media.tumblr.com/7ada83ae354be1d83ea4407fea789ab8/tumblr_na0e6razjV1s71nvbo1_250.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://33.media.tumblr.com/3be31f4531ed041ff9b80465b56d810e/tumblr_nr0dycLuRO1useffdo1_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://33.media.tumblr.com/f0edc38b8dacce783bebcdf41db55a93/tumblr_npapb2c4Wz1uolkubo1_250.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://33.media.tumblr.com/2bf0f300d9ecfbcedf2dd3ba2b40b5e5/tumblr_ne5p2oTuCj1tdmffyo1_250.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://38.media.tumblr.com/3e1f37fea789bb1508d40e8c30f791ae/tumblr_na3xfcUdnK1tiamx1o1_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://38.media.tumblr.com/0b04187cb51a8889b0f41e5fbe390df2/tumblr_nbcltiDvcq1s7ri4yo1_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://38.media.tumblr.com/e52a1d77dc0a679840a715c02035e5da/tumblr_nfbeo6Qqr91tl8fnfo1_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://38.media.tumblr.com/a67f2f007b9881080aa3fe3584847bc5/tumblr_nc1wzyMaJP1tzj4j8o1_250.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://38.media.tumblr.com/61e9abf3681eeacea18dae288f084d62/tumblr_nbw9gwXM0e1tk2ngvo1_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://31.media.tumblr.com/78355496a2ed41f0aa9fe855f9460bc3/tumblr_nais3s7sWa1s3att3o1_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n</div>\n\n\n\n\nLet's describe the third GIF with the ginger dog dancing on his hind legs.\n\n\n```python\ngifs = search_gif(\"a fluffy dog being cute and dancing like a person\")\ndisplay_gif(gifs)\n```\n\n\n\n\n\n<div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://38.media.tumblr.com/a67f2f007b9881080aa3fe3584847bc5/tumblr_nc1wzyMaJP1tzj4j8o1_250.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://33.media.tumblr.com/2bf0f300d9ecfbcedf2dd3ba2b40b5e5/tumblr_ne5p2oTuCj1tdmffyo1_250.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://33.media.tumblr.com/ec768e8a6f881fbc0f329932c8591a88/tumblr_mpqwb14Fsq1rjcfxro1_250.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://33.media.tumblr.com/7ada83ae354be1d83ea4407fea789ab8/tumblr_na0e6razjV1s71nvbo1_250.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://38.media.tumblr.com/f8b6d3d79b59462019c2daf2ba8b4148/tumblr_np762bxBYV1t7jda2o1_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://38.media.tumblr.com/a5ae79c2d62c592d7565684a72af8f2c/tumblr_nageslBNqC1tstoffo1_500.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://38.media.tumblr.com/3e1f37fea789bb1508d40e8c30f791ae/tumblr_na3xfcUdnK1tiamx1o1_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://33.media.tumblr.com/aec9cbbdf826f98307e6d5f3d544a4c2/tumblr_mmlrbhGDAO1qaqutao1_500.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://38.media.tumblr.com/0b04187cb51a8889b0f41e5fbe390df2/tumblr_nbcltiDvcq1s7ri4yo1_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://33.media.tumblr.com/14f9b213a7355096c14b0af3a7768f5d/tumblr_npexfuFU2K1ti77bgo1_400.gif\" style=\"width: 120px; height: 90px\" >\n</figure>\n\n</div>\n\n\n\n\nThese look like pretty good, interesting results.\n\n## Example application\n\nTo try out an application like this one, see this [example\napplication](https://huggingface.co/spaces/pinecone/gif-search).\n\n\n---\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccdc"
  },
  "filename": "document-deduplication.md",
  "title": "Document Deduplication",
  "category": "630fc5235d91a70054705fb7",
  "content": "---\ntitle: Document Deduplication\ncategory: 630fc5235d91a70054705fb7\n---\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/search/semantic-search/deduplication/deduplication_scholarly_articles.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/search/semantic-search/deduplication/deduplication_scholarly_articles.ipynb) [![Open github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/pinecone-io/examples/blob/master/search/semantic-search/deduplication/deduplication_scholarly_articles.ipynb)\n\n\nThis notebook demonstrates how to use Pinecone's similarity search to create a simple application to identify duplicate documents. \n\nThe goal is to create a data deduplication application for eliminating near-duplicate copies of academic texts. In this example, we will perform the deduplication of a given text in two steps. First, we will sift a small set of candidate texts using a similarity-search service. Then, we will apply a near-duplication detector over these candidates. \n\nThe similarity search will use a vector representation of the texts. With this, semantic similarity is translated to proximity in a vector space. For detecting near-duplicates, we will employ a classification model that examines the raw text. \n\n## Install Dependencies\n\n\n```python\n!pip install -qU pinecone-client\n!pip install -qU datasketch mmh3 ipywidgets\n!pip install -qU gensim==4.0.1\n!pip install -qU sentence-transformers --no-cache-dir\n!pip install -qU datasets\n```\n\n## Download and Process Dataset\n\nThis tutorial will use the [Deduplication Dataset 2020](https://core.ac.uk/documentation/dataset/), which consists of 100,000 scholarly documents. We will use Hugging Face Datasets to download the dataset found at [*pinecone/core-2020-05-10-deduplication*](https://huggingface.co/datasets/pinecone/core-2020-05-10-deduplication).\n\n\n```python\nfrom datasets import load_dataset\n\ncore = load_dataset(\"pinecone/core-2020-05-10-deduplication\", split=\"train\")\ncore\n```\n\n\n    \n\n\n\n\n    Dataset({\n        features: ['core_id', 'doi', 'original_abstract', 'original_title', 'processed_title', 'processed_abstract', 'cat', 'labelled_duplicates'],\n        num_rows: 100000\n    })\n\n\n\nWe convert the dataset into Pandas dataframe format like so:\n\n\n```python\ndf = core.to_pandas()\ndf.head()\n```\n\n\n\n\n\n<div id=\"df-ac4b6c06-6b41-4df5-8555-e7ca549dcea5\">\n  <div class=\"colab-df-container\">\n    <style scoped>\n        .dataframe tbody tr th:only-of-type {\n            vertical-align: middle;\n        }\n        .dataframe tbody tr th {\n            vertical-align: top;\n        }\n        .dataframe thead th {\n            text-align: right;\n        }\n    </style>\n    <table class=\"dataframe\">\n      <thead>\n        <tr style=\"text-align: right;\">\n          <th></th>\n          <th>core_id</th>\n          <th>doi</th>\n          <th>original_abstract</th>\n          <th>original_title</th>\n          <th>processed_title</th>\n          <th>processed_abstract</th>\n          <th>cat</th>\n          <th>labelled_duplicates</th>\n        </tr>\n      </thead>\n      <tbody>\n        <tr>\n          <th>0</th>\n          <td>11251086</td>\n          <td>10.1016/j.ajhg.2007.12.013</td>\n          <td>Unobstructed vision requires a particular refr...</td>\n          <td>Mutation of solute carrier SLC16A12 associates...</td>\n          <td>mutation of solute carrier slc16a12 associates...</td>\n          <td>unobstructed vision refractive lens differenti...</td>\n          <td>exact_dup</td>\n          <td>[82332306]</td>\n        </tr>\n        <tr>\n          <th>1</th>\n          <td>11309751</td>\n          <td>10.1103/PhysRevLett.101.193002</td>\n          <td>Two-color multiphoton ionization of atomic hel...</td>\n          <td>Polarization control in two-color above-thresh...</td>\n          <td>polarization control in two-color above-thresh...</td>\n          <td>multiphoton ionization helium combining extrem...</td>\n          <td>exact_dup</td>\n          <td>[147599753]</td>\n        </tr>\n        <tr>\n          <th>2</th>\n          <td>11311385</td>\n          <td>10.1016/j.ab.2011.02.013</td>\n          <td>Lectin’s are proteins capable of recognising a...</td>\n          <td>Optimisation of the enzyme-linked lectin assay...</td>\n          <td>optimisation of the enzyme-linked lectin assay...</td>\n          <td>lectin’s capable recognising oligosaccharide t...</td>\n          <td>exact_dup</td>\n          <td>[147603441]</td>\n        </tr>\n        <tr>\n          <th>3</th>\n          <td>11992240</td>\n          <td>10.1016/j.jpcs.2007.07.063</td>\n          <td>In this work, we present a detailed transmissi...</td>\n          <td>Vertical composition fluctuations in (Ga,In)(N...</td>\n          <td>vertical composition fluctuations in (ga,in)(n...</td>\n          <td>microscopy interfacial uniformity wells grown ...</td>\n          <td>exact_dup</td>\n          <td>[148653623]</td>\n        </tr>\n        <tr>\n          <th>4</th>\n          <td>11994990</td>\n          <td>10.1016/S0169-5983(03)00013-3</td>\n          <td>Three-dimensional (3D) oscillatory boundary la...</td>\n          <td>Three-dimensional streaming flows driven by os...</td>\n          <td>three-dimensional streaming flows driven by os...</td>\n          <td>oscillatory attached deformable walls boundari...</td>\n          <td>exact_dup</td>\n          <td>[148656283]</td>\n        </tr>\n      </tbody>\n    </table>\n  </div>\n</div>\n\n\n\n\nWe will use the following columns from the dataset for our task.\n1. **core_id** - Unique indentifier for each article\n\n2. **processed_abstract** - This is obtained by applying preprocssing steps like [this](https://spacy.io/usage/processing-pipelines) to the original abstract of the article from the column **original abstract**.\n\n3. **processed_title** - Same as the abstract but for the title of the article.\n\n4. **cat** - Every article falls into one of the three possible categories: 'exact_dup', 'near_dup', 'non_dup'\n\n5. **labelled_duplicates** - A list of core_ids of articles that are duplicates of current article\n\nLet's calculate the frequency of duplicates per article. Observe that half of the articles have no duplicates, and only a small fraction of the articles have more than ten duplicates.\n\n\n```python\nlens = df.labelled_duplicates.apply(len)\nlens.value_counts()\n```\n\n\n\n\n    0     50000\n    1     36166\n    2      7620\n    3      3108\n    4      1370\n    5       756\n    6       441\n    7       216\n    8       108\n    10       66\n    9        60\n    11       48\n    13       28\n    12       13\n    Name: labelled_duplicates, dtype: int64\n\n\n\nReformat some of the columns to prevent later issues.\n\n\n```python\n# Make sure no processed abstracts are excessively long for upsert to Pinecone\ndf[\"processed_abstract\"] = df[\"processed_abstract\"].str[:8000]\n```\n\nWe will make use of the text data to create vectors for every article. We combine the **processed_abstract** and **processed_title** of the article to create a new **combined_text** column. \n\n\n```python\n# Define a new column for calculating embeddings\ndf[\"combined_text\"] = df[\"processed_title\"] + \" \" + df[\"processed_abstract\"]\n```\n\n## Initialize Pinecone Index\n\n\n```python\nimport pinecone\n\n# Connect to pinecone environment\npinecone.init(\n    api_key=\"YOUR_API_KEY\",\n    environment=\"YOUR_ENVIRONMENT\"\n)\n\n# Pick a name for the new index\nindex_name = \"deduplication\"\n\n# Check if the deduplication index exists\nif index_name not in pinecone.list_indexes():\n    # Create the index if it does not exist\n    pinecone.create_index(\n        index_name,\n        dimension=300,\n        metadata_config={\"indexed\": [\"processed_abstract\"]}\n    )\n\n# Connect to deduplication index we created\nindex = pinecone.Index(index_name)\n```\n[Get a free Pinecone API key](https://www.pinecone.io/start/) if you don’t have one already. You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**.\n\n\n## Initialize Embedding Model\n\nWe will use the [Average Word Embedding GloVe](https://nlp.stanford.edu/projects/glove/) model to transform text into vector embeddings. We then upload the embeddings into the Pinecone vector index.\n\n\n```python\nimport torch\nfrom sentence_transformers import SentenceTransformer\n\n# set device to GPU if available\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = SentenceTransformer(\"average_word_embeddings_glove.6B.300d\", device=device)\nmodel\n```\n\n\n\n\n    SentenceTransformer(\n      (0): WordEmbeddings(\n        (emb_layer): Embedding(400001, 300)\n      )\n      (1): Pooling({'word_embedding_dimension': 300, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n    )\n\n\n\n## Generate Embeddings and Upsert\n\n\n```python\nfrom tqdm.auto import tqdm\n\n# We will use batches of 256\nbatch_size = 256\nfor i in tqdm(range(0, len(df), batch_size)):\n    # Find end of batch\n    i_end = min(i+batch_size, len(df))\n    # Extract batch\n    batch = df.iloc[i:i_end]\n    # Generate embeddings for batch\n    emb = model.encode(batch[\"combined_text\"].to_list()).tolist()\n    # extract both indexed and not indexed metadata\n    meta = batch[[\"processed_abstract\"]].to_dict(orient=\"records\")\n    # create IDs\n    ids = batch.core_id.astype(str)\n    # add all to upsert list\n    to_upsert = list(zip(ids, emb, meta))\n    # upsert/insert these records to pinecone\n    _ = index.upsert(vectors=to_upsert)\n    \n# check that we have all vectors in index\nindex.describe_index_stats()\n```\n\n\n\n\n\n    100%|██████████| 391/391 [03:25<00:00, 2.47it/s]\n    \n\n    {'dimension': 300,\n     'index_fullness': 0.1,\n     'namespaces': {'': {'vector_count': 100000}}}\n\n\n\n## Searching for Candidates\n\nNow that we have created vectors for the articles and inserted them in the index, we will create a test set for querying. For each article in the test set we will query the index to get the most similar articles, they are the candidates on which we will performs the next classification step.\n\nBelow, we list statistics of the number of duplicates per article in the resulting test set.\n\n\n```python\nimport math\n\n# Create a sample from the dataset\nSAMPLE_FRACTION = 0.002\ntest_documents = (\n    df.groupby(df.labelled_duplicates.map(len))\n    .apply(lambda x: x.head(math.ceil(len(x) * SAMPLE_FRACTION)))\n    .reset_index(drop=True)\n)\n\nprint(\"Number of documents with specified number of duplicates:\")\nlens = test_documents.labelled_duplicates.apply(len)\nlens.value_counts()\n```\n\n    Number of documents with specified number of duplicates:\n    0     100\n    1      73\n    2      16\n    3       7\n    4       3\n    5       2\n    6       1\n    7       1\n    8       1\n    9       1\n    10      1\n    11      1\n    12      1\n    13      1\n    Name: labelled_duplicates, dtype: int64\n\n\n\n\n```python\n# Use the model to create embeddings for test articles, which will be the query vectors\nquery_vectors = model.encode(test_documents.combined_text.to_list()).tolist()\n```\n\n\n```python\n# Query the vector index\nquery_results = []\nfor xq in tqdm(query_vectors):\n    query_res = index.query(xq, top_k=100, include_metadata=True)\n    query_results.append(query_res)\n```\n\n\n    100%|██████████| 209/209 [01:01<00:00, 3.54it/s]\n\n\n\n```python\n# Save all retrieval recalls into a list\nrecalls = []\n\nfor id, res in tqdm(list(zip(test_documents.core_id.values, query_results))):\n    # Find document with id in labelled dataset\n    labeled_df = df[df.core_id.astype(str) == str(id)]\n    # Calculate the retrieval recall\n    top_k_list = set([match.id for match in res.matches])\n    labelled_duplicates = set(labeled_df.labelled_duplicates.values[0])\n    intersection = top_k_list.intersection(labelled_duplicates)\n    if len(labelled_duplicates) != 0:\n        recalls.append(len(intersection) / len(labelled_duplicates))\n```\n\n\n    100%|██████████| 0/209 [00:02<00:00, 104.50it/s]\n\n\n\n```python\nimport statistics\n\nprint(\"Mean for the retrieval recall is \" + str(statistics.mean(recalls)))\nprint(\"Standard Deviation is  \" + str(statistics.stdev(recalls)))\n```\n\n    Mean for the retrieval recall is 0.9702529886016125\n    Standard Deviation is  0.16219287104729735\n    \n\n### Running the Classifier \n\nWe mentioned earlier in the article that we will perform two steps for deduplication, searching to produce candidates and performing classifciation on them.\n\nWe will use Deduplication Classifier based on [LSH](https://en.wikipedia.org/wiki/Locality-sensitive_hashing) for detecting duplicates on the results from the previous step. We will run this on a sample of query results we got in the previous step. Feel free to try out the results on the entire set of query results.\n\n\n```python\nimport pandas as pd\nfrom gensim.utils import tokenize\nfrom datasketch.minhash import MinHash\nfrom datasketch.lsh import MinHashLSH\n```\n\n\n```python\n# Counters for correct/false predictions\nall_predictions = {\"Correct\": 0, \"False\": 0}\npredictions_per_category = {}\n\n# From the results in the previous step, we will take a subset to test our classifier\nquery_sample = query_results[::10]\nids_sample = test_documents.core_id.to_list()[::10]\n\nfor id, res in zip(ids_sample, query_sample):\n    \n    # Find document with id from the labelled dataset\n    labeled_df = df[df.core_id.astype(str) == str(id)]\n\n    \"\"\"\n    For every article in the result set, we store the scores and abstract of the articles most similar \n    to it, according to search in the previous step.\n    \"\"\"\n\n    df_result = pd.DataFrame(\n        {\n            \"id\": [match.id for match in res.matches],\n            \"document\": [match[\"metadata\"][\"processed_abstract\"] for match in res.matches],\n            \"score\": [match.score for match in res.matches],\n        }\n    )\n\n    print(df_result.head())\n\n    # We need content and labels for our classifier which we can get from the df_results\n    content = df_result.document.values\n    labels = list(df_result.id.values)\n    \n    # Create MinHash for each of the documents in result set\n    min_hashes = {}\n    for label, text in zip(labels, content):\n        m = MinHash(num_perm=128, seed=5)\n        tokens = set(tokenize(text))\n        for d in tokens:\n            m.update(d.encode('utf8'))\n        min_hashes[label] = m\n    \n    # Create LSH index\n    lsh = MinHashLSH(threshold=0.7, num_perm=128, )\n    for i, j in min_hashes.items():\n        lsh.insert(str(i), j)\n    \n    query_minhash = min_hashes[str(id)]\n    duplicates = lsh.query(query_minhash)\n    duplicates.remove(str(id))\n    \n    # Check whether prediction matches labeled duplicates. Here the groud truth is the set of duplicates from our original set\n    prediction = (\n        \"Correct\"\n        if set(labeled_df.labelled_duplicates.values[0]) == set(duplicates)\n        else \"False\"\n    )\n    \n    # Add to all predictions\n    all_predictions[prediction] += 1\n    \n    # Create and/or add to the specific category based on number of duplicates in original dataset\n    num_of_duplicates = len(labeled_df.labelled_duplicates.values[0])\n    if num_of_duplicates not in predictions_per_category:\n        predictions_per_category[num_of_duplicates] = [0, 0]\n\n    if prediction == \"Correct\":\n        predictions_per_category[num_of_duplicates][0] += 1\n    else:\n        predictions_per_category[num_of_duplicates][1] += 1\n\n    # Print the results for a document\n    print(\n        \"{}: expected: {}, predicted: {}, prediction: {}\".format(\n            id, labeled_df.labelled_duplicates.values[0], duplicates, prediction\n        )\n    )\n```\n\n             id                                           document     score\n    0  15080768  analyse centred methodology. discretisation so...  1.000000\n    1  52682462  audiencethe tissues pulses modelled compartmen...  0.787797\n    2  52900859  audiencethe tissues pulses modelled compartmen...  0.787797\n    3   2553555  multilayered illuminated acoustic electromagne...  0.781398\n    4  50544308  heterostructure schr dinger poisson numericall...  0.778778\n    15080768: expected: [], predicted: [], prediction: Correct\n              id                                           document     score\n    0   55110306  latrepirdine orally administered molecule init...  1.000000\n    1  188404434  cysteamine potentially numerous huntington dis...  0.903964\n    2   81634102  deutetrabenazine molecule deuterium attenuates...  0.880078\n    3   42021224  comorbidities. safe drugs available. efficacy ...  0.857741\n    4   78271101  promising prevent onset ultrahigh psychosis di...  0.849158\n    55110306: expected: [], predicted: [], prediction: Correct\n             id                                           document     score\n    0  10914205  read objectives schoolchildren sunscreen morni...  1.000000\n    1  77409456  overeating harmful alcohol tobacco aetiology c...  0.669037\n    2  10896024  sunlight cutaneous vitamin production. highlig...  0.633516\n    3  15070865  drink heavily nonstudent peers unaware drinkin...  0.633497\n    4  52131855  dette siste tekst versjon artikkelen inneholde...  0.627933\n    10914205: expected: [], predicted: [], prediction: Correct\n             id                                           document     score\n    0  43096919  publishedcomparative studymulticenter tcontext...  1.000000\n    1  77165332  cerebral amyloid aggregation pathological alzh...  0.871247\n    2  70343569  neurodegenerative heterogeneous disorders prog...  0.867806\n    3  18448676  beta amyloid beta deposition hallmarks alzheim...  0.855655\n    4  46964510  alzheimer unexplained. sought loci detect robu...  0.855137\n    43096919: expected: [], predicted: [], prediction: Correct\n             id                                           document     score\n    0  12203626  hypernatremia recipients homografts postoperat...  1.000000\n    1  82542813  abstractobjectivesto intravenous maintenance f...  0.800283\n    2  81206306  uromodulin tamm–horsfall abundant excreted uri...  0.794892\n    3  36026525  drinking sodium bicarbonated mineral cardiovas...  0.793452\n    4  83567081  drinking sodium bicarbonated mineral cardiovas...  0.793252\n    12203626: expected: [], predicted: [], prediction: Correct\n              id                                           document     score\n    0   15070865  drink heavily nonstudent peers unaware drinkin...  1.000000\n    1  154671698  updated alcohol suicidal level. searches retri...  0.889408\n    2   52132897  updated alcohol suicidal level. searches retri...  0.889408\n    3   43606482  fulltext .pdf publisher effectiveness drinking...  0.883402\n    4   82484980  abstractthe effectiveness drinking motive tail...  0.883145\n    15070865: expected: [], predicted: [], prediction: Correct\n              id                                           document     score\n    0   80341690  potentially inappropriate medicines pims older...  1.000000\n    1   39320843  elderly receive medications adverse effects. e...  0.807533\n    2   82162292  abstractbackgroundrisk assessments widely pred...  0.780006\n    3   77027179  assessments widely predict opioid disorder unc...  0.779406\n    4  153514317  yesbackground challenging person dementia. beh...  0.757255\n    80341690: expected: [], predicted: [], prediction: Correct\n             id                                           document     score\n    0   9066821  commotio retinae opacification retina blunt oc...  1.000000\n    1  78051578  neovascular macular degeneration anti–vascular...  0.731147\n    2  86422032  automated lesions challenging diagnostic lesio...  0.703925\n    3  48174418  audiencewe propose voxelwise images. relies ge...  0.699708\n    4  52434306  audiencewe propose voxelwise images. relies ge...  0.699708\n    9066821: expected: [], predicted: [], prediction: Correct\n              id                                           document     score\n    0   15052827  indirect schizophrenia australia incidence cos...  1.000000\n    1  154860392  illness schizophrenia bipolar disorder depress...  0.795662\n    2   51964867  audiencebackground cholesterol lowering jupite...  0.791904\n    3   75913230  thesis characterize burden cardiovascular deme...  0.775635\n    4  154672015  aims depression anxiety myocardial infarction ...  0.765936\n    15052827: expected: [], predicted: [], prediction: Correct\n             id                                           document     score\n    0  12203661  glomerulonephritis serious hemoptysis. antiglo...  1.000000\n    1  12204810  twenty alagille syndrome underwent transplanta...  0.811871\n    2  52198725  audiencepatients autoimmune polyendocrine synd...  0.810457\n    3  47112592  audiencepatients autoimmune polyendocrine synd...  0.810457\n    4  52460385  audiencepatients autoimmune polyendocrine synd...  0.810457\n    12203661: expected: [], predicted: [], prediction: Correct\n             id                                           document     score\n    0  11251086  unobstructed vision refractive lens differenti...  1.000000\n    1  82332306  unobstructed vision refractive lens differenti...  1.000000\n    2  61371524  aims osmotic oxidative progression advancement...  0.839048\n    3  59036307  aims osmotic oxidative progression advancement...  0.839048\n    4  11249430  dysfunction cilia nearly ubiquitously solitary...  0.796622\n    11251086: expected: ['82332306'], predicted: ['82332306'], prediction: Correct\n              id                                           document     score\n    0   12001088  presents vision successfully discriminates wee...  1.000000\n    1  148662402  presents vision successfully discriminates wee...  1.000000\n    2  148666025  proposes oriented crop maize weed pressure. vi...  0.904243\n    3   18424329  proposes oriented crop maize weed pressure. vi...  0.904243\n    4   18424394  proposes oriented identifying crop rows maize ...  0.861464\n    12001088: expected: ['148662402'], predicted: ['148662402'], prediction: Correct\n              id                                           document     score\n    0   11307919  reflectance exciton–polariton film polycrystal...  1.000000\n    1  147595688  reflectance exciton–polariton film polycrystal...  1.000000\n    2  147595695  photoluminescence reflectance oriented polycry...  0.816958\n    3   11307922  photoluminescence reflectance oriented polycry...  0.816958\n    4   33106913  macroscopic dielectric polycrystalline commonl...  0.804686\n    147595688: expected: ['11307919'], predicted: ['11307919'], prediction: Correct\n              id                                           document     score\n    0   12002296  thanks inherent probabilistic graphical prime ...  1.000000\n    1  148663921  thanks inherent probabilistic graphical prime ...  1.000000\n    2   52634130  audienceobject oriented brms platform automati...  0.869993\n    3   52294731  audienceobject oriented brms platform automati...  0.869993\n    4   34403460  acceptance artificial intelligence aims learn ...  0.865814\n    148663921: expected: ['12002296'], predicted: ['12002296'], prediction: Correct\n              id                                           document     score\n    0  151641478  stabilised soems unstable aircraft presented. ...  1.000000\n    1   11874260  stabilised soems unstable aircraft presented. ...  1.000000\n    2   29528077  projection snapshot balanced truncation unstab...  0.724496\n    3   77005252  projection snapshot balanced truncation unstab...  0.724496\n    4  148663435  ideas robust computationally amenable industri...  0.722027\n    151641478: expected: ['11874260'], predicted: ['11874260'], prediction: Correct\n              id                                           document     score\n    0  188365084  installed rapidly decade deployments deeper wa...  1.000000\n    1  158351487  installed rapidly decade deployments deeper wa...  1.000000\n    2  158370190  offshore turbine reliability biggest paper. un...  0.853790\n    3   83926778  offshore turbine reliability biggest paper. un...  0.853790\n    4   74226591  investigates overruns underruns occurring onsh...  0.834363\n    188365084: expected: ['158351487'], predicted: ['158351487'], prediction: Correct\n             id                                           document     score\n    0   2097371  propose vulnerability network. analogy balls l...  1.000000\n    1   9030380  propose vulnerability network. analogy balls l...  1.000000\n    2  49270269  audiencethis introduces validates sensor propa...  0.754055\n    3  43094896  peer reviewed brownjohn displacement sensor co...  0.745553\n    4  49271868  audiencea predictive giving displacement digit...  0.734554\n    2097371: expected: ['9030380'], predicted: ['9030380'], prediction: Correct\n              id                                           document     score\n    0  148674298  race segments swimmers. analysed finals sessio...  1.000000\n    1   33176265  race segments swimmers. analysed finals sessio...  1.000000\n    2  148674300  swimming race parameters. hundred fifty eight ...  0.886608\n    3   33176267  swimming race parameters. hundred fifty eight ...  0.886608\n    4  143900637  swimmers swimmers coaches trainers. video sens...  0.736030\n    33176265: expected: ['148674298'], predicted: ['148674298'], prediction: Correct\n             id                                           document     score\n    0  52844591  audiencehere geochemical lopevi volcano volcan...  1.000000\n    1  52308905  audiencehere geochemical lopevi volcano volcan...  1.000000\n    2  52722823  audiencehere geochemical lopevi volcano volcan...  1.000000\n    3  52717537  audiencethe volcanism cameroon volcanic mantle...  0.893717\n    4  52840980  audiencethe volcanism cameroon volcanic mantle...  0.893717\n    52308905: expected: ['52722823' '52844591'], predicted: ['52722823', '52844591'], prediction: Correct\n             id                                           document     score\n    0  44119402  lagrangian formalism supermembrane supergravit...  1.000000\n    1  35093363  lagrangian formalism supermembrane supergravit...  1.000000\n    2   2531039  lagrangian formalism supermembrane supergravit...  1.000000\n    3  35078501  lagrangian formalism supermembrane supergravit...  1.000000\n    4  35089833  supergravity correlators worldsheet analogous ...  0.847565\n    44119402: expected: ['2531039' '35078501' '35093363'], predicted: ['2531039', '35078501', '35093363'], prediction: Correct\n              id                                           document  score\n    0   52739626  microlensing surveys tens millions stars. unpr...    1.0\n    1   52456923  microlensing surveys tens millions stars. unpr...    1.0\n    2   47110549  microlensing surveys tens millions stars. unpr...    1.0\n    3   52695218  microlensing surveys tens millions stars. unpr...    1.0\n    4  152091185  microlensing surveys tens millions stars. unpr...    1.0\n    47110549: expected: ['46770666' '52456923' '152091185' '52695218' '52739626'], predicted: ['52456923', '52695218', '52739626', '152091185', '46770666'], prediction: Correct\n    \n\n\n```python\nall_predictions\n```\n\n\n\n\n    {'Correct': 21, 'False': 0}\n\n\n\n\n```python\n# Overall accuracy on a test\naccuracy = round(\n    all_predictions[\"Correct\"]\n    / (all_predictions[\"Correct\"] + all_predictions[\"False\"]),\n    4,\n)\naccuracy\n```\n\n\n\n\n    1.0\n\n\n\n\n```python\n# Print the prediction count for each class depending on the number of duplicates in labeled dataset\npd.DataFrame.from_dict(\n    predictions_per_category, orient=\"index\", columns=[\"Correct\", \"False\"]\n)\n```\n\n\n\n\n\n<div id=\"df-fc5ae990-df9a-48b2-8269-ff3efe85e71f\">\n  <div class=\"colab-df-container\">\n    <style scoped>\n        .dataframe tbody tr th:only-of-type {\n            vertical-align: middle;\n        }\n        .dataframe tbody tr th {\n            vertical-align: top;\n        }\n        .dataframe thead th {\n            text-align: right;\n        }\n    </style>\n    <table class=\"dataframe\">\n      <thead>\n        <tr style=\"text-align: right;\">\n          <th></th>\n          <th>Correct</th>\n          <th>False</th>\n        </tr>\n      </thead>\n      <tbody>\n        <tr>\n          <th>0</th>\n          <td>10</td>\n          <td>0</td>\n        </tr>\n        <tr>\n          <th>1</th>\n          <td>8</td>\n          <td>0</td>\n        </tr>\n        <tr>\n          <th>2</th>\n          <td>1</td>\n          <td>0</td>\n        </tr>\n        <tr>\n          <th>3</th>\n          <td>1</td>\n          <td>0</td>\n        </tr>\n        <tr>\n          <th>5</th>\n          <td>1</td>\n          <td>0</td>\n        </tr>\n      </tbody>\n    </table>\n  </div>\n</div>\n\n\n\n\n\n## Delete the Index\nDelete the index once you are sure that you do not want to use it anymore. Once the index is deleted, you cannot use it again.\n\n\n\n\n```python\n# Delete the index if it's not going to be used anymore\npinecone.delete_index(index_name)\n```\n\n## Summary\n\nIn this notebook we demonstrate how to perform a deduplication task of over 100,000 articles using Pinecone. With articles embedded as vectors, you can use Pinecone's vector index to find similar articles. For each query article, we then use an LSH classifier on the similar articles to identify duplicate articles. Overall, we show that it is ease to incorporate Pinecone wtih article embedding models and duplication classifiers to build a deduplication service.\n\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccdd"
  },
  "filename": "product-recommendation-engine.md",
  "title": "Product Recommender",
  "category": "630fc5235d91a70054705fb7",
  "content": "---\ntitle: Product Recommender\ncategory: 630fc5235d91a70054705fb7\n---\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/recommendation/product-recommender/product_recommender.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/recommendation/product-recommender/product_recommender.ipynb) [![Open github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/pinecone-io/examples/blob/master/recommendation/product-recommender/product_recommender.ipynb)\n\nLearn how to build a product recommendation engine using collaborative filtering and Pinecone.\n\nIn this example, we will generate product recommendations for ecommerce customers based on previous orders and trending items. This example covers preparing the vector embeddings, creating and deploying the Pinecone service, writing data to Pinecone, and finally querying Pinecone to receive a ranked list of recommended products.\n\n## Data Preparation\n\n**Import Python Libraries**\n\n\n```python\nimport os\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nimport scipy.sparse as sparse\nimport itertools\n```\n\n**Load the (Example) Instacart Data**\n\nWe are going to use the [Instacart Market Basket Analysis](https://www.kaggle.com/c/instacart-market-basket-analysis/data) dataset for this task.\n\nThe data used throughout this example is a set of files describing customers' orders over time. The main focus is on the *orders.csv* file, where each line represents a relation between a user and the order. In other words, each line has information on *userid* (user who made the order) and *orderid*. Note there is no information about products in this table. Product information related to specific orders is stored in the *order_product__*.csv* dataset.\n\n\n```python\norder_products_train = pd.read_csv('data/order_products__train.csv')\norder_products_prior = pd.read_csv('data/order_products__prior.csv')\nproducts = pd.read_csv('data/products.csv')\norders = pd.read_csv('data/orders.csv')\n\norder_products = order_products_train.append(order_products_prior)\n```\n\n**Preparing data for the model**\n\n\nThe Collaborative Filtering model used in this example requires only users’ historical preferences on a set of items. As there is no explicit rating in the data we are using, the purchase quantity can represent a “confidence” in terms of how strong the interaction was between the user and the products.\n\nThe dataframe data will store this data and will be the base for the model.\n\n\n```python\ncustomer_order_products = pd.merge(orders, order_products, how='inner',on='order_id')\n\n# creating a table with \"confidences\"\ndata = customer_order_products.groupby(['user_id', 'product_id'])[['order_id']].count().reset_index()\ndata.columns=[\"user_id\", \"product_id\", \"total_orders\"]\ndata.product_id = data.product_id.astype('int64')\n\n# Create a lookup frame so we can get the product names back in readable form later.\nproducts_lookup = products[['product_id', 'product_name']].drop_duplicates()\nproducts_lookup['product_id'] = products_lookup.product_id.astype('int64')\n```\n\nWe will create three prototype users here and add them to our data dataframe. Each user will be buying only a specific product:\n- The first user will be buying only **Mineral Water**\n- The second user will be buying baby products: **No More Tears Baby Shampoo** and **Baby Wash & Shampoo**\n\nThese users will be later used for querying and examination of the model results.\n\n\n```python\ndata_new = pd.DataFrame([[data.user_id.max() + 1, 22802, 97],\n                         [data.user_id.max() + 2, 26834, 89],\n                         [data.user_id.max() + 2, 12590, 77]\n                        ], columns=['user_id', 'product_id', 'total_orders'])\ndata_new\n```\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table  class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>product_id</th>\n      <th>total_orders</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>206210</td>\n      <td>22802</td>\n      <td>97</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>206211</td>\n      <td>26834</td>\n      <td>89</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>206211</td>\n      <td>12590</td>\n      <td>77</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\ndata = data.append(data_new).reset_index(drop = True)\ndata.tail()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table  class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>product_id</th>\n      <th>total_orders</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>13863744</th>\n      <td>206209</td>\n      <td>48697</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13863745</th>\n      <td>206209</td>\n      <td>48742</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>13863746</th>\n      <td>206210</td>\n      <td>22802</td>\n      <td>97</td>\n    </tr>\n    <tr>\n      <th>13863747</th>\n      <td>206211</td>\n      <td>26834</td>\n      <td>89</td>\n    </tr>\n    <tr>\n      <th>13863748</th>\n      <td>206211</td>\n      <td>12590</td>\n      <td>77</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\nIn the next step, we will first extract user and item unique ids, in order to create a CSR (Compressed Sparse Row) matrix. \n\n\n\n```python\nusers = list(np.sort(data.user_id.unique()))\nitems = list(np.sort(products.product_id.unique()))\npurchases = list(data.total_orders)\n\n# create zero-based index position <-> user/item ID mappings\nindex_to_user = pd.Series(users)\n\n# create reverse mappings from user/item ID to index positions\nuser_to_index = pd.Series(data=index_to_user.index + 1, index=index_to_user.values)\n\n# create zero-based index position <-> item/user ID mappings\nindex_to_item = pd.Series(items)\n\n# create reverse mapping from item/user ID to index positions\nitem_to_index = pd.Series(data=index_to_item.index, index=index_to_item.values)\n\n# Get the rows and columns for our new matrix\nproducts_rows = data.product_id.astype(int)\nusers_cols = data.user_id.astype(int)\n\n# Create a sparse matrix for our users and products containing number of purchases\nsparse_product_user = sparse.csr_matrix((purchases, (products_rows, users_cols)), shape=(len(items) + 1, len(users) + 1))\nsparse_product_user.data = np.nan_to_num(sparse_product_user.data, copy=False)\n\nsparse_user_product = sparse.csr_matrix((purchases, (users_cols, products_rows)), shape=(len(users) + 1, len(items) + 1))\nsparse_user_product.data = np.nan_to_num(sparse_user_product.data, copy=False)\n```\n\n## Implicit Model\n\nIn this section we will demonstrate creation and training of a recommender model using the **implicit** library. The recommendation model is based off the algorithms described in the paper [Collaborative Filtering for Implicit Feedback Datasets](https://www.researchgate.net/publication/220765111_Collaborative_Filtering_for_Implicit_Feedback_Datasets) with performance optimizations described in [Applications of the Conjugate Gradient Method for Implicit Feedback Collaborative Filtering](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.379.6473&rep=rep1&type=pdf).\n\n\n\n```python\n!pip install --quiet -U implicit\n```\n\n\n```python\nimport implicit\nfrom implicit import evaluation\n\n#split data into train and test sets\ntrain_set, test_set = evaluation.train_test_split(sparse_product_user, train_percentage=0.9)\n\n# initialize a model\nmodel = implicit.als.AlternatingLeastSquares(factors=100,\n                                             regularization=0.05,\n                                             iterations=50,\n                                             num_threads=1)\n\nalpha_val = 15\ntrain_set = (train_set * alpha_val).astype('double')\n\n# train the model on a sparse matrix of item/user/confidence weights\nmodel.fit(train_set, show_progress = True)\n```\n\n    WARNING:root:OpenBLAS detected. Its highly recommend to set the environment variable 'export OPENBLAS_NUM_THREADS=1' to disable its internal multithreading\n\n\n\n      0%|          | 0/50 [00:00<?, ?it/s]\n\n\nWe will evaluate the model using the inbuilt library function\n\n\n```python\ntest_set = (test_set * alpha_val).astype('double')\nevaluation.ranking_metrics_at_k(model, train_set.T, test_set.T, K=100,\n                         show_progress=True, num_threads=1)\n```\n\n\n      0%|          | 0/206212 [00:00<?, ?it/s]\n\n\n\n\n\n    {'precision': 0.27489359984895717,\n     'map': 0.04460861877969595,\n     'ndcg': 0.14436536385146576,\n     'auc': 0.6551648380086259}\n\n\n\nThis is what item and user factors look like. These vectors will be stored in our vector index later and used for recommendation.\n\n\n```python\nmodel.item_factors[1:3]\n```\n\n\n\n\n    array([[ 0.01009897,  0.00260342,  0.00165942,  0.01748168,  0.00649343,\n            -0.01647822,  0.01860397, -0.01009837,  0.01125452,  0.01987451,\n            -0.00579512,  0.00421128,  0.01707346, -0.00212536,  0.01915585,\n             0.03640049, -0.01142028,  0.01023709,  0.00446458, -0.00143529,\n            -0.00024208,  0.00909473, -0.01408565,  0.02619351,  0.00210135,\n            -0.00378899,  0.01231347,  0.00278133,  0.00071992,  0.00915809,\n             0.01640408,  0.00880539, -0.00648519, -0.01160682,  0.00664212,\n            -0.00406996,  0.01543106,  0.00690582,  0.00898032,  0.00277333,\n             0.00626428, -0.01610408,  0.01018737,  0.0008459 ,  0.02026955,\n            -0.01055363, -0.00107795,  0.01484767,  0.01800155, -0.00275021,\n            -0.0018283 , -0.00346971,  0.00077051, -0.01080908,  0.00037001,\n            -0.00290308,  0.00491365, -0.01362148, -0.00129594,  0.00192484,\n             0.00101756, -0.00051836,  0.00603317,  0.01611738,  0.00511096,\n            -0.0053055 ,  0.01907502,  0.01232757,  0.01042075,  0.01301588,\n             0.00567376,  0.0152219 ,  0.02414433,  0.01395251,  0.00916175,\n             0.01294622,  0.00187435,  0.01768819,  0.01806206,  0.01500281,\n             0.01065951,  0.02733074,  0.00765102,  0.00435439, -0.01976543,\n             0.01680202,  0.00840835,  0.00042277, -0.00216795,  0.00113048,\n            -0.00012699,  0.01142939,  0.01374972, -0.00985129,  0.00935802,\n             0.00541372,  0.01037668,  0.02024015, -0.00793628, -0.00261189],\n           [ 0.00088747,  0.00581244,  0.00074211,  0.00428396,  0.00124957,\n             0.00699728,  0.00304013,  0.00676518,  0.00414387,  0.00205417,\n             0.0029335 ,  0.00505301,  0.00522107,  0.00404108,  0.00236721,\n             0.00406507,  0.00101947,  0.00298186,  0.00049156,  0.00279067,\n             0.00343525,  0.00175488,  0.00907208,  0.00276436,  0.00414505,\n             0.00458229,  0.00363405,  0.00375954,  0.00198171,  0.00270804,\n             0.00479605,  0.00120687,  0.00249341,  0.00051512, -0.00110135,\n             0.00844493,  0.00641403,  0.00101385,  0.00484058,  0.00632413,\n             0.00334539,  0.00232208,  0.00288551,  0.00755766,  0.00279979,\n             0.00587453,  0.00742234,  0.00580525,  0.00412665,  0.00347631,\n             0.00433106,  0.00427196,  0.00670939,  0.00304596,  0.00385384,\n             0.00222394,  0.00511582,  0.00354225,  0.00200116,  0.00717725,\n             0.00186237,  0.00434178,  0.00102088,  0.00222063,  0.00230367,\n             0.00420666,  0.00698098,  0.00549557,  0.00345657,  0.00642341,\n             0.00036   ,  0.00464778,  0.00284442,  0.00530352,  0.00218676,\n             0.00493103,  0.00179086,  0.0041003 ,  0.00497837,  0.0068793 ,\n             0.00429972,  0.00396508,  0.00451153,  0.00486684,  0.00272128,\n             0.00467645,  0.00423267,  0.00388015,  0.00339444,  0.00115735,\n             0.00807636,  0.00298532,  0.00143811,  0.00293057,  0.00590145,\n             0.00418158,  0.00488713,  0.00097365, -0.00083799,  0.00363581]],\n          dtype=float32)\n\n\n\n\n```python\nmodel.user_factors[1:3]\n```\n\n\n\n\n    array([[ 7.24285245e-01,  5.59004486e-01,  4.96992081e-01,\n            -4.15437818e-01, -1.94785964e+00, -2.23764396e+00,\n            -1.76767483e-02, -2.21530461e+00, -6.52559578e-01,\n             2.78620571e-01,  6.03808701e-01,  1.27670407e-01,\n             3.06052566e-01, -9.93388355e-01, -5.34315288e-01,\n             1.20948291e+00, -2.11217976e+00,  1.67127061e+00,\n             1.03314137e+00,  8.54326487e-01,  1.85733151e+00,\n             5.69297194e-01, -8.93577933e-01,  1.76394248e+00,\n             1.28939009e+00,  3.32375497e-01, -2.60327369e-01,\n             4.21450347e-01, -1.72091925e+00,  1.10491872e+00,\n            -1.86411276e-01, -3.51959467e-02, -1.41517222e+00,\n            -9.19971287e-01,  4.63204056e-01, -4.07809407e-01,\n             1.23038590e+00, -8.25872004e-01, -1.50579488e+00,\n             8.65903348e-02, -7.29649186e-01, -5.21384776e-01,\n             1.59157085e+00, -8.51297379e-01,  2.81686401e+00,\n            -8.55669677e-01, -3.48052949e-01, -5.16085029e-01,\n             8.01080287e-01,  1.04207866e-01, -2.72860657e-02,\n            -5.18645883e-01, -1.77561533e+00, -1.22266948e+00,\n            -1.74415603e-01,  3.58568132e-01, -8.37117255e-01,\n            -1.45265543e+00,  2.43810445e-01,  5.80842435e-01,\n            -5.91480255e-01,  1.29645097e+00,  1.47483099e+00,\n            -6.84086800e-01, -7.20921755e-01, -1.11399984e+00,\n             2.38089368e-01,  2.19725475e-01,  3.29073220e-01,\n            -6.45937538e-03,  2.44079873e-01,  1.26761782e+00,\n             7.07967520e-01,  1.21964478e+00,  1.10735869e+00,\n             1.02583379e-01, -2.92189389e-01,  5.52688181e-01,\n             1.61700773e+00,  5.11932790e-01, -2.67194122e-01,\n             1.47362947e+00, -1.13380539e+00,  1.40330446e+00,\n             4.91484731e-01,  1.36100423e+00,  1.80482656e-01,\n             9.14917171e-01,  6.22740746e-01, -1.88607132e+00,\n            -1.34071469e+00, -2.27820247e-01,  1.15018475e+00,\n            -1.23491549e+00, -4.78476077e-01, -4.65549737e-01,\n             9.11170244e-01,  2.07606936e+00,  1.04314007e-01,\n             1.81862903e+00],\n           [ 8.30793440e-01,  3.86868089e-01, -1.63957000e-01,\n             6.93703368e-02,  1.53786719e+00, -5.87535620e-01,\n             3.72619987e+00,  1.22163899e-01, -8.54973614e-01,\n             1.11186251e-01, -1.42095876e+00, -8.75619590e-01,\n            -1.81247914e+00, -9.44502056e-01,  8.14570427e-01,\n            -5.43736219e-01, -6.02845371e-01,  2.01962996e+00,\n             1.60777140e+00,  2.20254612e+00,  2.08239055e+00,\n             8.16642225e-01, -4.42571700e-01,  6.22263908e-01,\n             6.29432023e-01, -1.16571808e+00,  2.32731175e+00,\n            -1.12640738e+00,  1.60938001e+00,  4.67458010e+00,\n            -1.46235943e+00,  1.46000063e+00,  1.11922979e-01,\n            -2.55218220e+00,  7.85077095e-01,  8.50843608e-01,\n            -1.10671151e+00, -6.06540870e-03,  2.76003122e-01,\n            -9.57318366e-01, -1.30121040e+00, -3.81188631e-01,\n             2.17489243e+00,  8.48001361e-01,  2.24089599e+00,\n            -1.32857335e+00,  9.44799244e-01,  2.29169533e-01,\n             1.10746622e+00, -3.48530680e-01, -2.12854624e+00,\n             4.96270150e-01, -1.30754066e+00,  1.41697776e+00,\n             2.73206377e+00,  1.48888981e+00, -1.58728147e+00,\n             1.58903934e-03,  1.66406441e+00, -1.75263867e-01,\n             2.02891684e+00, -1.95949566e+00,  1.52711666e+00,\n             8.71322572e-01,  1.82597125e+00,  1.37408182e-01,\n            -1.81464672e+00, -1.04905093e+00, -2.37590694e+00,\n             8.15740228e-01,  1.64217085e-01,  1.99734032e+00,\n            -1.54955173e+00, -5.57012379e-01,  1.32525837e+00,\n            -1.30014801e+00,  1.32985008e+00, -3.50400567e+00,\n             2.45490909e-01, -2.43037295e+00, -2.74685884e+00,\n            -2.12384558e+00, -1.42703640e+00, -6.69254959e-01,\n             1.30702591e+00, -2.15909433e+00,  1.44703603e+00,\n            -2.29611732e-02,  1.82583869e+00,  1.57409739e+00,\n            -3.97216320e-01, -6.94107652e-01,  2.89623165e+00,\n             2.33722359e-01, -5.27708590e-01,  1.04344904e+00,\n             8.51706207e-01, -4.50546294e-01,  1.38413882e+00,\n             2.07552814e+00]], dtype=float32)\n\n\n\n## Configure Pinecone\n\nInstall and setup Pinecone\n\n\n```python\n!pip install --quiet -U pinecone-client\n```\n\n\n```python\nimport pinecone\n```\n\n\n```python\n# Load Pinecone API key\napi_key = os.getenv('PINECONE_API_KEY') or 'YOUR_API_KEY'\n# Set Pinecone environment.\nenv = os.getenv('PINECONE_ENVIRONMENT') or 'YOUR_ENVIRONMENT'\n\npinecone.init(api_key=api_key, environment=env)\n```\n\n[Get a Pinecone API key](http://app.pinecone.io/) if you don't have one.\n\n\n```python\n#List all present indexes associated with your key, should be empty on the first run\npinecone.list_indexes()\n```\n\n\n\n\n    []\n\n\n\n**Create an Index**\n\n\n```python\n# Set a name for your index\nindex_name = 'shopping-cart-demo'\n```\n\n\n```python\n# Make sure service with the same name does not exist\nif index_name in pinecone.list_indexes():\n    pinecone.delete_index(index_name)\npinecone.create_index(name=index_name, dimension=100)\n```\n\n**Connect to the new index**\n\n\n```python\nindex = pinecone.Index(index_name=index_name)\n```\n\n## Load Data\n\nUploading all items (products that one can buy) and displaying some examples of products and their vector representations.\n\n\n\n```python\n# Get all of the items\nall_items = [title for title in products_lookup['product_name']]\n\n# Transform items into factors\nitems_factors = model.item_factors\n\n# Prepare item factors for upload\nitems_to_insert = list(zip(all_items, items_factors[1:].tolist()))\ndisplay(items_to_insert[:2])\n```\n\n\n    [('Chocolate Sandwich Cookies',\n      [0.010098974220454693,\n       0.0026034200564026833,\n       0.0016594183398410678,\n       0.017481675371527672,\n       0.006493427790701389,\n       -0.016478220000863075,\n       0.018603969365358353,\n       -0.010098369792103767,\n       0.01125451922416687,\n       0.019874505698680878,\n       -0.005795117933303118,\n       0.00421128049492836,\n       0.017073458060622215,\n       -0.0021253626327961683,\n       0.019155845046043396,\n       0.036400485783815384,\n       -0.01142028160393238,\n       0.010237086564302444,\n       0.004464581608772278,\n       -0.0014352924190461636,\n       -0.00024208369723055512,\n       0.009094727225601673,\n       -0.014085653237998486,\n       0.02619350701570511,\n       0.002101349411532283,\n       -0.0037889881059527397,\n       0.012313470244407654,\n       0.002781332703307271,\n       0.0007199185783974826,\n       0.009158086962997913,\n       0.016404075548052788,\n       0.008805392310023308,\n       -0.006485185585916042,\n       -0.01160681527107954,\n       0.006642122287303209,\n       -0.004069960676133633,\n       0.015431062318384647,\n       0.006905817426741123,\n       0.008980315178632736,\n       0.002773326588794589,\n       0.0062642814591526985,\n       -0.0161040760576725,\n       0.010187366977334023,\n       0.0008458984084427357,\n       0.02026955410838127,\n       -0.010553630068898201,\n       -0.0010779497679322958,\n       0.014847667887806892,\n       0.018001552671194077,\n       -0.0027502067387104034,\n       -0.0018282983219251037,\n       -0.0034697114024311304,\n       0.000770510989241302,\n       -0.010809078812599182,\n       0.0003700107627082616,\n       -0.002903081476688385,\n       0.004913648124784231,\n       -0.01362148392945528,\n       -0.001295942347496748,\n       0.0019248360767960548,\n       0.0010175565257668495,\n       -0.0005183601751923561,\n       0.006033174227923155,\n       0.016117379069328308,\n       0.005110959522426128,\n       -0.00530549930408597,\n       0.019075021147727966,\n       0.012327569536864758,\n       0.01042074803262949,\n       0.01301588024944067,\n       0.005673760548233986,\n       0.015221904963254929,\n       0.024144325405359268,\n       0.01395251415669918,\n       0.009161749854683876,\n       0.012946223840117455,\n       0.0018743481487035751,\n       0.017688188701868057,\n       0.018062060698866844,\n       0.015002812258899212,\n       0.010659514926373959,\n       0.02733074128627777,\n       0.0076510170474648476,\n       0.0043543861247599125,\n       -0.019765431061387062,\n       0.016802024096250534,\n       0.008408350870013237,\n       0.0004227694298606366,\n       -0.002167945960536599,\n       0.0011304811341688037,\n       -0.0001269889180548489,\n       0.01142938993871212,\n       0.013749724254012108,\n       -0.00985129363834858,\n       0.009358019568026066,\n       0.0054137222468853,\n       0.010376684367656708,\n       0.020240148529410362,\n       -0.007936276495456696,\n       -0.0026118927635252476]),\n     ('All-Seasons Salt',\n      [0.0008874664781615138,\n       0.0058124433271586895,\n       0.0007421106565743685,\n       0.00428396463394165,\n       0.001249574706889689,\n       0.006997276097536087,\n       0.0030401344411075115,\n       0.006765175145119429,\n       0.004143866710364819,\n       0.0020541702397167683,\n       0.002933498937636614,\n       0.005053007043898106,\n       0.00522107258439064,\n       0.004041083622723818,\n       0.002367211040109396,\n       0.004065068904310465,\n       0.0010194696951657534,\n       0.0029818632174283266,\n       0.0004915563040412962,\n       0.0027906731702387333,\n       0.0034352506045252085,\n       0.0017548849573358893,\n       0.009072077460587025,\n       0.002764355158433318,\n       0.004145053215324879,\n       0.004582288675010204,\n       0.003634049789980054,\n       0.0037595359608531,\n       0.00198170798830688,\n       0.002708042971789837,\n       0.004796050023287535,\n       0.0012068713549524546,\n       0.0024934052489697933,\n       0.0005151224322617054,\n       -0.001101348432712257,\n       0.00844493042677641,\n       0.006414031144231558,\n       0.001013854518532753,\n       0.0048405807465314865,\n       0.006324129644781351,\n       0.0033453928772360086,\n       0.0023220758885145187,\n       0.002885512774810195,\n       0.007557660341262817,\n       0.002799794776365161,\n       0.005874533671885729,\n       0.007422335911542177,\n       0.0058052497915923595,\n       0.004126648418605328,\n       0.0034763067960739136,\n       0.004331058822572231,\n       0.004271955695003271,\n       0.00670938566327095,\n       0.0030459642875939608,\n       0.0038538381922990084,\n       0.0022239401005208492,\n       0.005115816835314035,\n       0.003542253514751792,\n       0.002001164946705103,\n       0.007177253719419241,\n       0.0018623704090714455,\n       0.004341782070696354,\n       0.0010208759922534227,\n       0.0022206329740583897,\n       0.002303670858964324,\n       0.004206661134958267,\n       0.006980976089835167,\n       0.005495565943419933,\n       0.003456572536379099,\n       0.006423408165574074,\n       0.0003599990450311452,\n       0.004647782538086176,\n       0.0028444179333746433,\n       0.005303522571921349,\n       0.0021867596078664064,\n       0.004931030794978142,\n       0.0017908598529174924,\n       0.0041002980433404446,\n       0.004978368990123272,\n       0.006879299879074097,\n       0.004299724940210581,\n       0.0039650811813771725,\n       0.004511528182774782,\n       0.00486684450879693,\n       0.0027212793938815594,\n       0.004676445387303829,\n       0.0042326669208705425,\n       0.003880152478814125,\n       0.003394442144781351,\n       0.0011573455994948745,\n       0.008076360449194908,\n       0.0029853193555027246,\n       0.0014381115324795246,\n       0.0029305710922926664,\n       0.005901449825614691,\n       0.004181584343314171,\n       0.004887125454843044,\n       0.0009736462379805744,\n       -0.0008379911305382848,\n       0.0036358062643557787])]\n\n\n**Insert items into the index**\n\n\n```python\ndef chunks(iterable, batch_size=100):\n    it = iter(iterable)\n    chunk = tuple(itertools.islice(it, batch_size))\n    while chunk:\n        yield chunk\n        chunk = tuple(itertools.islice(it, batch_size))\n```\n\n\n```python\nprint('Index statistics before upsert:', index.describe_index_stats())\n\nfor e, batch in enumerate(chunks([(ii[:64],x) for ii,x in items_to_insert])):\n    index.upsert(vectors=batch)\n\nprint('Index statistics after upsert:', index.describe_index_stats())\n```\n\n    Index statistics before upsert: {'dimension': 0, 'namespaces': {}}\n    Index statistics after upsert: {'dimension': 100, 'namespaces': {'': {'vector_count': 49677}}}\n\n\nThis is a helper method for analysing recommendations later.\nThis method returns top N products that someone bought in the past (based on product quantity).\n\n\n```python\ndef products_bought_by_user_in_the_past(user_id: int, top: int = 10):\n\n    selected = data[data.user_id == user_id].sort_values(by=['total_orders'], ascending=False)\n\n    selected['product_name'] = selected['product_id'].map(products_lookup.set_index('product_id')['product_name'])\n    selected = selected[['product_id', 'product_name', 'total_orders']].reset_index(drop=True)\n    if selected.shape[0] < top:\n        return selected\n\n    return selected[:top]\n```\n\n\n```python\ndata.tail()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table  class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>product_id</th>\n      <th>total_orders</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>13863744</th>\n      <td>206209</td>\n      <td>48697</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13863745</th>\n      <td>206209</td>\n      <td>48742</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>13863746</th>\n      <td>206210</td>\n      <td>22802</td>\n      <td>97</td>\n    </tr>\n    <tr>\n      <th>13863747</th>\n      <td>206211</td>\n      <td>26834</td>\n      <td>89</td>\n    </tr>\n    <tr>\n      <th>13863748</th>\n      <td>206211</td>\n      <td>12590</td>\n      <td>77</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n## Query for Recommendations\n\nWe are now retrieving user factors for users that we have manually created before for testing purposes. Besides these users, we are adding a random existing user. We are also displaying these users so you can see what these factors look like.\n\n\n```python\nuser_ids = [206210, 206211, 103593]\nuser_factors = model.user_factors[user_to_index[user_ids]]\n\ndisplay(user_factors[1:])\n```\n\n\n    array([[-2.446773  , -0.62870413, -0.9166386 , -1.0933994 ,  0.9897131 ,\n            -2.166681  ,  0.09873585,  1.1049409 ,  1.6753025 ,  1.5794269 ,\n             1.8142459 ,  1.5048354 ,  0.7157051 , -0.7888281 ,  0.06156079,\n            -1.6539581 , -0.15790005,  0.5999737 , -1.4803663 , -0.03179923,\n             0.91451246,  0.14260213, -1.1541293 , -0.01566206, -1.3449577 ,\n            -2.232925  , -0.88052607,  0.19183849,  0.3109626 ,  1.32479   ,\n             0.16483077, -0.8045166 ,  1.36922   ,  0.81774026,  1.3368418 ,\n             2.8871357 ,  2.4540865 , -1.908394  ,  2.8208447 , -1.3499558 ,\n            -0.90089166,  1.0632626 ,  1.8107275 , -0.83986664,  1.1764408 ,\n            -1.6621239 , -1.4636188 , -2.3367987 , -1.2510904 ,  0.4349534 ,\n             0.08233324,  1.0688674 , -0.41190436,  1.6045849 , -2.3667567 ,\n            -1.8557758 , -0.1931467 ,  0.10383442,  1.3932719 ,  1.3465406 ,\n            -0.17274773,  0.41542327, -1.0992794 ,  1.7954347 , -0.9157203 ,\n            -0.3183454 ,  0.7724282 , -0.5658835 ,  1.0758705 , -1.7377888 ,\n             2.0294137 , -2.1382923 ,  1.0606468 ,  1.800927  , -1.3713943 ,\n             1.0659586 ,  0.31013912, -0.5963934 ,  0.69738954,  1.383554  ,\n             1.0078012 , -2.7117298 , -1.7087    ,  0.4050448 ,  3.548174  ,\n             0.27247337, -0.16570352, -0.92676795, -1.2243328 ,  0.63455725,\n            -1.5337977 , -2.8735108 ,  1.2812912 , -0.11600056,  1.2358317 ,\n             0.5591759 , -0.63913107,  1.2325013 ,  1.3712876 , -1.3370212 ],\n           [ 1.70396   , -1.5320156 ,  2.8847353 ,  0.32170388,  1.3340172 ,\n            -1.1947397 ,  1.9013127 , -0.4816413 , -2.0899863 , -1.2761233 ,\n            -1.8430734 , -0.6221577 ,  0.8063771 ,  1.2961249 ,  0.18268324,\n            -3.2958453 , -0.31202024,  3.8049164 ,  0.73393685,  1.7682556 ,\n             0.372242  ,  1.002703  ,  0.32070097,  0.2046866 ,  0.9008953 ,\n             1.3807229 ,  1.1176021 ,  0.1957425 , -1.3196671 ,  2.1180258 ,\n             0.48846507,  0.76666814, -0.30274457, -2.5167181 ,  0.3489467 ,\n             2.0131872 , -1.5119745 , -0.91736513,  1.3228838 , -1.5192536 ,\n            -1.1463904 , -1.0334512 ,  1.2355485 , -0.21977787,  2.3017268 ,\n            -1.4751832 , -0.6216355 ,  0.3089897 , -0.85497165, -0.31444585,\n            -3.100829  ,  2.390458  ,  0.07399248, -0.09938905, -1.0162137 ,\n             1.9475894 , -0.9248195 , -1.084834  ,  0.39212215,  0.6491842 ,\n             1.2028612 , -1.0323097 ,  2.6522071 , -0.8172474 ,  1.0873827 ,\n            -2.9416876 , -0.06957518, -0.7316911 , -0.7430743 ,  0.319504  ,\n            -0.9984044 ,  0.06710945, -3.003772  ,  0.6744962 ,  2.1210036 ,\n            -0.4559903 ,  0.6154137 , -1.7743443 ,  0.5672013 ,  1.004357  ,\n            -1.8588076 ,  0.05864619,  0.01209994,  2.0575655 , -1.1680491 ,\n             0.3783967 ,  1.6527759 ,  1.5397102 , -0.2965242 ,  2.5335467 ,\n            -0.40009058, -0.66989446, -1.6143844 ,  0.7761751 , -1.0538983 ,\n             0.48226374,  1.2432365 ,  2.1671696 ,  1.7070205 ,  0.2968687 ]],\n          dtype=float32)\n\n\n### Model recommendations\n\nWe will now retrieve recommendations from our model directly, just to have these results as a baseline.\n\n\n```python\nprint(\"Model recommendations\\n\")\n\nstart_time = time.process_time()\nrecommendations0 = model.recommend(userid=user_ids[0], user_items=sparse_user_product)\nrecommendations1 = model.recommend(userid=user_ids[1], user_items=sparse_user_product)\nrecommendations2 = model.recommend(userid=user_ids[2], user_items=sparse_user_product)\nprint(\"Time needed for retrieving recommended products: \" + str(time.process_time() - start_time) + ' seconds.\\n')\n\nprint('\\nRecommendations for person 0:')\nfor recommendation in recommendations0:\n    product_id = recommendation[0]\n    print(products_lookup[products_lookup.product_id == product_id]['product_name'].values)\n\nprint('\\nRecommendations for person 1:')\nfor recommendation in recommendations1:\n    product_id = recommendation[0]\n    print(products_lookup[products_lookup.product_id == product_id]['product_name'].values)\n\nprint('\\nRecommendations for person 2:')\nfor recommendation in recommendations2:\n    product_id = recommendation[0]\n    print(products_lookup[products_lookup.product_id == product_id]['product_name'].values)\n```\n\n    Model recommendations\n    \n    Time needed for retrieving recommended products: 0.0625 seconds.\n    \n    \n    Recommendations for person 0:\n    ['Sparkling Water']\n    ['Soda']\n    ['Smartwater']\n    ['Zero Calorie Cola']\n    ['Natural Artesian Water']\n    ['Natural Spring Water']\n    ['Distilled Water']\n    ['Sparkling Natural Mineral Water']\n    ['Spring Water']\n    ['Drinking Water']\n    \n    Recommendations for person 1:\n    ['Baby Wipes Sensitive']\n    ['YoKids Squeezers Organic Low-Fat Yogurt, Strawberry']\n    ['Organic Blackberries']\n    ['Organic Whole Milk']\n    ['Eggo Pancakes Minis']\n    ['Natural California Raisins Mini Snack Boxes']\n    ['100% Raw Coconut Water']\n    ['White Buttermints']\n    ['Danimals Strawberry Explosion Flavored Smoothie']\n    ['Strawberry Explosion/Banana Split Smoothie']\n    \n    Recommendations for person 2:\n    ['Organic Golden Delicious Apple']\n    ['Organic Red Delicious Apple']\n    ['Bartlett Pears']\n    ['Organic Blackberries']\n    ['Bag of Organic Bananas']\n    ['Black Seedless Grapes']\n    ['Organic Braeburn Apple']\n    ['Organic Blueberries']\n    [\"Organic D'Anjou Pears\"]\n    ['White Peach']\n\n\n### Query the index\n\nLet's now query the index to check how quickly we retrieve results. Please note that query speed depends in part on your internet connection.\n\n\n```python\n# Query by user factors\n\nstart_time = time.process_time()\nquery_results = index.query(queries=user_factors[:-1].tolist(), top_k=10)\nprint(\"Time needed for retrieving recommended products using Pinecone: \" + str(time.process_time() - start_time) + ' seconds.\\n')\n\nfor _id, res in zip(user_ids, query_results.results):\n    print(f'user_id={_id}')\n    df = pd.DataFrame(\n        {\n            'products': [match.id for match in res.matches],\n            'scores': [match.score for match in res.matches]\n        }\n    )\n    print(\"Recommendation: \")\n    display(df)\n    print(\"Top buys from the past: \")\n    display(products_bought_by_user_in_the_past(_id, top=15))\n```\n\n    Time needed for retrieving recommended products using Pinecone: 0.03125 seconds.\n    \n    user_id=206210\n    Recommendation: \n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table  class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>products</th>\n      <th>scores</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Mineral Water</td>\n      <td>0.919242</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Zero Calorie Cola</td>\n      <td>0.716640</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Orange &amp; Lemon Flavor Variety Pack Sparkling F...</td>\n      <td>0.631119</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Sparkling Water</td>\n      <td>0.603575</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Milk Chocolate Almonds</td>\n      <td>0.577868</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Extra Fancy Unsalted Mixed Nuts</td>\n      <td>0.577714</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Popcorn</td>\n      <td>0.565397</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Organic Coconut Water</td>\n      <td>0.547605</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Drinking Water</td>\n      <td>0.542832</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Tall Kitchen Bag With Febreze Odor Shield</td>\n      <td>0.538533</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n    Top buys from the past: \n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table  class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>product_id</th>\n      <th>product_name</th>\n      <th>total_orders</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>22802</td>\n      <td>Mineral Water</td>\n      <td>97</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n    user_id=206211\n    Recommendation: \n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table  class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>products</th>\n      <th>scores</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Baby Wash &amp; Shampoo</td>\n      <td>0.731054</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>No More Tears Baby Shampoo</td>\n      <td>0.695655</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Size 6 Baby Dry Diapers</td>\n      <td>0.526953</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Natural Applesauce Snack &amp; Go Pouches</td>\n      <td>0.478145</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>White Buttermints</td>\n      <td>0.475006</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Size 5 Cruisers Diapers Super Pack</td>\n      <td>0.474203</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Go-Gurt SpongeBob SquarePants Strawberry Ripti...</td>\n      <td>0.461982</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Baby Wipes Sensitive</td>\n      <td>0.461840</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Original Detergent</td>\n      <td>0.456813</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Stage 1 Newborn Hypoallergenic Liquid Detergent</td>\n      <td>0.456143</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n    Top buys from the past: \n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table  class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>product_id</th>\n      <th>product_name</th>\n      <th>total_orders</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>26834</td>\n      <td>No More Tears Baby Shampoo</td>\n      <td>89</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>12590</td>\n      <td>Baby Wash &amp; Shampoo</td>\n      <td>77</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n*Note* The inference using Pinecone is much faster compared to retrieving recommendations from a model directly. Please note that this result depends on your internet connection as well. \n\nAll that’s left to do is surface these recommendations on the shopping site, or feed them into other applications.\n\n## Clean up\n\nDelete the index once you are sure that you do not want to use it anymore. Once it is deleted, you cannot reuse it.\n\n\n```python\npinecone.delete_index(index_name)\n```\n\n## Summary\n\nIn this example we used [Pinecone](https://www.pinecone.io/) to build and deploy a product recommendation engine that uses collaborative filtering, relatively quickly.\n\nOnce deployed, the product recommendation engine can index new data, retrieve recommendations in milliseconds, and send results to production applications.\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccde"
  },
  "filename": "similarity-search-diversification.md",
  "title": "Search result diversification with post-processors",
  "category": "630fc5235d91a70054705fb7",
  "content": "---\ntitle: Search result diversification with post-processors\ncategory: 630fc5235d91a70054705fb7\n---\n\nThis notebook demonstrates how Pinecone's API lets you control the way your service handles requests. Here, we demonstrate Pinecone's powerful Post-processing API and define a post-processing function that will perform search result diversification.\n\nFor some search applications, you may want to exclude results that are near-duplicates to the query. For example, in a product search application, we often want to retrieve a diverse set of products instead of the same product with slight variations.\n\nThis demo notebook will walk you through building an image search application that achieves that. We will use [Pinecone](/) to tie everything together and expose the image search as a real-time service that will take any fashion article image and return a diverse set of similar fashion article images.\n\nWe will,\n1. implement a simple diversification filter as a Pinecone's post-processing function;\n2. upload the post-processing function to Pinecone's Model Hub;\n3. launch an image search service that includes a vector index backend and a diversification filter post-processor function;\n4. upload and index our image vectors;\n5. query our deployed service;\n6. and compare the service with a baseline service that does not contain a diversification functionality.\n\n## Install and Setup Pinecone Client\nFirst, let's install the [Pinecone client](https://docs.beta.pinecone.io/en/latest/intro/installation.html) and set up its API key. [Here](/start/) you can obtain an API key.\n\n\n```python\n!pip install --quiet -U numpy pinecone-client python-mnist matplotlib progressbar2 pandas ipywidgets\n```\n\n\n```python\nimport pinecone.graph\nimport pinecone.service\nimport pinecone.connector\nimport pinecone.hub\n\npinecone.init(api_key='FILL-YOUR-API-KEY')\n```\n\n## Define a Search Result Diversification Postprocessor\n\nThe following code computes a heterogeneous \"top-five\" subset out of a query result set. This is done simply by clustering the data into five clusters and choosing from each cluster a representative. We use the [k-means](https://en.wikipedia.org/wiki/K-means_clustering) clustering algorithm to minimize inner cluster distance variance while maximizing in-between clusters variance.\n\nRecall that our demo focus is on Pinecone's post-processing API. Therefore, we apply a simple diversification idea and assess it subjectively. For more rigorous search results diversification ideas, see for example this [work](https://www.microsoft.com/en-us/research/wp-content/uploads/2009/02/diversifying-wsdm09.pdf).\n\nOur diversification postprocessor is a python class that follows Pinecone Model Hub's [Postprocessor API](https://docs.beta.pinecone.io/en/latest/python_client/hub.html#pinecone.hub.postprocessor). In short, we should implement a `transform` function that receives the query results and manipulates them.\n\nNote that we save the code as a file because we will later package it as a [docker image](https://www.docker.com/), and upload it to [Pinecone's Model Hub](https://docs.beta.pinecone.io/en/latest/python_client/hub.html). This way, we will be able to define a search service with built-in search result diversification functionality.\n\n\n```python\n%%writefile diversity_postprocessor.py\n\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\nfrom pinecone.hub import postprocessor, QueryResult\n\n@postprocessor\nclass DiversityPostprocessor:\n    def __init__(self):\n        self._k = 5  # top k\n\n    def _diversity_filter(self, data):\n        kmeans = KMeans(n_clusters=self._k, random_state=0).fit(data)\n\n        inxs_per_cluster = [[i for i, value in enumerate(kmeans.labels_) if value == c] for c in range(self._k)]   # group cluster indices\n        results = set([inxs[ int(len(inxs)/2) ] for inxs in inxs_per_cluster]) # from each cluster take the \"median\" index\n\n        return results\n\n    def transform(self, queries, matches):\n        \"\"\"This is the postprocessor relevant function\"\"\"\n        output = []\n        for q, match in zip(queries, matches):\n            # Filter data\n            res = self._diversity_filter(match.data)\n\n            # Then rearrange results\n            new_scores = [s for i,s in enumerate(match.scores) if i in res]\n            new_ids = [id_ for i,id_ in enumerate(match.ids) if i in res]\n            new_data = np.array([x for i,x in enumerate(match.data) if i in res])\n            output.append(QueryResult(ids=new_ids, scores=new_scores, data=new_data))\n\n        return output\n```\n\n    Overwriting diversity_postprocessor.py\n\n\n### Create the Post-Processor Docker Image And Push It to Pinecone's Model Hub\n\n\n```python\ndiversity_filter_image_builder = pinecone.hub.ImageBuilder(\n    image=\"diversity_filter:v1\",  # The name of the docker image (you should also tag the image\n    build_path=\"./docker_build/diversity_filter/v1\",  # Path to which docker build artifacts are saved\n    model_path='./diversity_postprocessor.py', # Main model file\n    pip=['numpy', 'scikit-learn'],  # Additional pip packages needed\n    data_paths=[],  # Additional files or directories needed\n)\n\n# Log into Pinecone's Model Hub\nlogin_cmd = pinecone.hub.get_login_cmd()\n!{login_cmd}\n```\n\n    WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n    WARNING! Your password will be stored unencrypted in /home/jupyter/.docker/config.json.\n    Configure a credential helper to remove this warning. See\n    https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n    \n    Login Succeeded\n\n\n\n```python\ndiversity_filter_image_builder.package(exist_ok=True)\n!{diversity_filter_image_builder.get_build_cmd()}\n!{diversity_filter_image_builder.get_push_cmd()}\n```\n\n    ~/docker_build/diversity_filter/v1 ~\n    Sending build context to Docker daemon  4.096kB\n    Step 1/4 : FROM hub.beta.pinecone.io/pinecone/base:0.8.34\n     ---> e988c545396e\n    Step 2/4 : RUN pip3 install --quiet --upgrade pip\n     ---> Using cache\n     ---> 5ac13fbcd300\n    Step 3/4 : RUN pip3 install --quiet --no-cache-dir numpy scikit-learn\n     ---> Using cache\n     ---> f9b753cff6cd\n    Step 4/4 : COPY model.py ./model.py\n     ---> Using cache\n     ---> 1129e677325b\n    Successfully built 1129e677325b\n    Successfully tagged diversity_filter:v1\n    ~\n    /bin/bash: -c: line 0: syntax error near unexpected token `}'\n    /bin/bash: -c: line 0: `{diversity_filter_image_builder.get_push_cmd()}'\n\n\n## Set Up a Pinecone Service\n\n### Define How the Service Handles Requests\nHere we define how the service handles requests. We want to store and retrieve a diverse set of images. We store the vector embeddings in Pinecone's [vector index](https://www.pinecone.io/learn/what-is-a-vector-index/). We rank and retrieve them using the [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) measure. Finally, we apply our search results diversification function on the top matched vectors and retrieve the index's final results.\n\nLet's deploy these computation steps using Pinecone's [Index Graph](https://docs.beta.pinecone.io/en/latest/intro/concepts.html#graph). Observe that we initiate a Vector Index with Euclidean distance and attach our postprocessor function. The resulting graph defines how we set (i.e., `write`) or retrieve (i.e., `read`) data.\n\n\n```python\ngraph = pinecone.graph.IndexGraph(metric='euclidean')\n\n# Name of the hub images\ndiversity_filter_image_name = pinecone.hub.as_user_image(diversity_filter_image_builder.image)\n\n# Add to the graph function that will deduplicate results\ndiversity_filter_postprocessor = pinecone.hub.HubFunction(name='diversity-postprocessor', image=diversity_filter_image_name)\n\ngraph.add_postprocessor(fn=diversity_filter_postprocessor)\n\n# View the updated graph\ngraph.view()\n```\n\n\n\n\n\n\n\n### Deploy the Service and Set a Connection\n\n\n```python\nservice_name = \"diversity-postprocessor-demo\"\npinecone.service.deploy(service_name, graph, timeout=300)\nconn = pinecone.connector.connect(service_name)\nconn.info()\n```\n\n\n\n\n    InfoResult(index_size=0)\n\n\n\n## Upload Vectors to the Service\nLet's upload real image [vector embeddings](https://www.pinecone.io/learn/what-are-vectors-embeddings/) into the service!\n\nWe are using [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset that contains fashion item images. For the sake of simplicity, we will use the raw grayscale pixel values as our vector embedding. Note that this choice is not optimal. Therefore, we expect it to produce reasonable search results only. (Recall, the focus of the demo is on the pre-processing API.)\n\nFirst, let's download the dataset.\n\n\n```python\n!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n!gunzip -f train-images-idx3-ubyte.gz\n!gunzip -f train-labels-idx1-ubyte.gz\n```\n\n    --2021-03-21 11:28:42--  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n    Resolving fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)... 52.219.75.48\n    Connecting to fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)|52.219.75.48|:80... connected.\n    HTTP request sent, awaiting response... 200 OK\n    Length: 26421880 (25M) [binary/octet-stream]\n    Saving to: ‘train-images-idx3-ubyte.gz’\n    \n    train-images-idx3-u 100%[===================>]  25.20M  11.8MB/s    in 2.1s\n    \n    2021-03-21 11:28:45 (11.8 MB/s) - ‘train-images-idx3-ubyte.gz’ saved [26421880/26421880]\n    \n    --2021-03-21 11:28:45--  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n    Resolving fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)... 52.219.75.48\n    Connecting to fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)|52.219.75.48|:80... connected.\n    HTTP request sent, awaiting response... 200 OK\n    Length: 29515 (29K) [binary/octet-stream]\n    Saving to: ‘train-labels-idx1-ubyte.gz’\n    \n    train-labels-idx1-u 100%[===================>]  28.82K  --.-KB/s    in 0.1s\n    \n    2021-03-21 11:28:45 (199 KB/s) - ‘train-labels-idx1-ubyte.gz’ saved [29515/29515]\n\n\n\n\n```python\nfrom mnist import MNIST\nimport numpy as np\nimages, labels = MNIST('.').load_training()\nimages = np.array(images)\nnp.array(images).shape\n```\n\n\n\n\n    (60000, 784)\n\n\n\n#### Upload the Vectors\n\n\n```python\nimport progressbar\n\nupsert_acks = conn.upsert(items=( (f\"img-{i}\", img) for i,img in progressbar.progressbar(enumerate(images)))).collect()\n```\n\n    59999 Elapsed Time: 0:00:03\n\n\n#### Count the Index's New Size\n\n\n```python\nconn.info()\n```\n\n\n\n\n    InfoResult(index_size=60000)\n\n\n\n## Search Example\nLet's try our new service! We query the service with an arbitrary image. Observe that we set the desired number of matches to be 30. Our search results diversification filter will reduce these matches into five results only. Hence, our service will retrieve five final results.\n\n\n```python\nimport matplotlib.pyplot as plt\n```\n\n### Choose an Arbitrary Query Image\nHere we choose our query vector. This is just an arbitrary image from our dataset.\n\n\n```python\nquery_id = np.random.randint(images.shape[0])\n```\n\n\n```python\nquery = images[ query_id ]\n\nfig = plt.figure(figsize=(3, 3))\nimg = query.reshape([28, 28])\nplt.imshow(img, cmap='gray')\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n```\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-25_0.png)\n\n\n\n### Query the Service\nHere we query the service. Observe that we set the required (maximal) number of matches to be 30; we expect to retrieve five results. Also, note that we require that the results would contain the vectors data.  Recall that this is required by our post-processor function that clusters these vectors.\n\n\n```python\nres = conn.query(queries=[query], top_k=30, include_data=True).collect()[0]\n```\n\n### Retrieved Items\nLet's visualize the retrieved items.\n\n\n```python\ncolumns = 5\nrows = int(np.ceil(len(res.ids)/columns))\n\nfig = plt.figure(figsize=(8, 8))\n\nfor i in range(1, len(res.ids)+1):\n    data_idx = int(res.ids[i-1].split('-')[-1])\n    img = images[data_idx].reshape([28, 28])\n    lbl = labels[data_idx]\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(img, cmap='gray')\n    plt.axis('off')\nplt.tight_layout()\nplt.show()\n```\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-29_0.png)\n\n\n\n---\n## Compare With a Service Without Search Results Diversification Functionality\nDoes our search results diversification filter work alright? Let's compare it with a service that does not contain such a post-processing functionality.\n\nFirst, let's deploy a service with vector index only. This is the most basic Pinecone service functionality. Observe how simple and easy it is to deploy such a service.\n\n\n```python\ngraph = pinecone.graph.IndexGraph(metric='cosine')\ngraph.view()\n```\n\n\n\n\n\n![svg](/images/similarity-search-diversification-example-31_0.svg)\n\n\n\n\n#### Deploy the Baseline Service\n\nHere we deploy, fill in, and query the simple baseline service. We use the same query image as above. Then, we compare this baseline service vs. our search results diversification service.\n\n\n```python\nbaseline_service_name = \"diversity-postprocessor-demo-baseline\"\npinecone.service.deploy(baseline_service_name, graph, timeout=300)\nbaseline_conn = pinecone.connector.connect(baseline_service_name)\n\nupsert_acks = baseline_conn.upsert(items=( (f\"img-{i}\", img) for i,img in progressbar.progressbar(enumerate(images)))).collect()\n```\n\n    59999 Elapsed Time: 0:00:03\n\n\n\n```python\ndef show_results(res):\n    columns = 5\n    rows = int(np.ceil(len(res.ids)/columns))\n\n    fig = plt.figure(figsize=(5, 5))\n\n    for i in range(1, len(res.ids)+1):\n        data_idx = int(res.ids[i-1].split('-')[-1])\n        img = images[data_idx].reshape([28, 28])\n        lbl = labels[data_idx]\n        fig.add_subplot(rows, columns, i)\n        plt.imshow(img, cmap='gray')\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\ndef compare(query):\n    print(\"Query\")\n    fig = plt.figure(figsize=(2, 2))\n    img = query.reshape([28, 28])\n    plt.imshow(img, cmap='gray')\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n    print()\n\n    print(\"Baseline without a Diversirty Filter\")\n    res = baseline_conn.query(queries=[query], top_k=5, include_data=True).collect()[0]\n    show_results(res)\n    print(\"-\"*20)\n    print(\"Service with a Diversity Filter\")\n    res = conn.query(queries=[query], top_k=30, include_data=True).collect()[0]\n    show_results(res)\n    print(\"\\n\\n\")\n\n```\n\n### Cherry-Picked Examples\nLet's examine a few cherry-picked query examples.\n\n#### Diversity in Action\nObserve that all the baseline results (upper row) are near-duplicates, while our diversification-service results exhibit variance.\n\n\n```python\ncompare(images[27697])\n```\n\n    Query\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-36_1.png)\n\n\n\n\n    Baseline without a Diversirty Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-36_3.png)\n\n\n\n    --------------------\n    Service with a Diversity Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-36_5.png)\n\n\n\n\n\n\n\n\n#### Diversity Adds Noise\nAdding variance to the results comes with the risk of adding non-relevant results. In this example, the lower row's last match is less relevant. (Although it indeed eliminates a baseline duplicate match.)\n\n\n```python\ncompare(images[4647])\n```\n\n    Query\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-38_1.png)\n\n\n\n\n    Baseline without a Diversirty Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-38_3.png)\n\n\n\n    --------------------\n    Service with a Diversity Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-38_5.png)\n\n\n\n\n\n\n\n\n#### Baseline Results are Already Diverse\nSometimes it is hard to declare which of the options is best.\n\n\n```python\ncompare(images[6999])\n```\n\n    Query\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-40_1.png)\n\n\n\n\n    Baseline without a Diversirty Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-40_3.png)\n\n\n\n    --------------------\n    Service with a Diversity Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-40_5.png)\n\n\n\n\n\n\n\n\n#### Try It Yourself\nLet's run a quick subjective comparison. We pick ten query images at random (i.i.d.) and compare the diversification-filter vs. the baseline results.\n\n\n```python\nfor query_id in range(10):\n    query_id = np.random.randint(images.shape[0])\n    print(f\"qid {query_id}\")\n    compare(images[ query_id ])\n```\n\n    qid 34671\n    Query\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_1.png)\n\n\n\n\n    Baseline without a Diversirty Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_3.png)\n\n\n\n    --------------------\n    Service with a Diversity Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_5.png)\n\n\n\n\n\n\n    qid 42726\n    Query\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_7.png)\n\n\n\n\n    Baseline without a Diversirty Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_9.png)\n\n\n\n    --------------------\n    Service with a Diversity Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_11.png)\n\n\n\n\n\n\n    qid 54686\n    Query\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_13.png)\n\n\n\n\n    Baseline without a Diversirty Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_15.png)\n\n\n\n    --------------------\n    Service with a Diversity Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_17.png)\n\n\n\n\n\n\n    qid 17412\n    Query\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_19.png)\n\n\n\n\n    Baseline without a Diversirty Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_21.png)\n\n\n\n    --------------------\n    Service with a Diversity Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_23.png)\n\n\n\n\n\n\n    qid 49222\n    Query\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_25.png)\n\n\n\n\n    Baseline without a Diversirty Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_27.png)\n\n\n\n    --------------------\n    Service with a Diversity Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_29.png)\n\n\n\n\n\n\n    qid 36067\n    Query\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_31.png)\n\n\n\n\n    Baseline without a Diversirty Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_33.png)\n\n\n\n    --------------------\n    Service with a Diversity Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_35.png)\n\n\n\n\n\n\n    qid 22519\n    Query\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_37.png)\n\n\n\n\n    Baseline without a Diversirty Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_39.png)\n\n\n\n    --------------------\n    Service with a Diversity Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_41.png)\n\n\n\n\n\n\n    qid 56056\n    Query\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_43.png)\n\n\n\n\n    Baseline without a Diversirty Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_45.png)\n\n\n\n    --------------------\n    Service with a Diversity Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_47.png)\n\n\n\n\n\n\n    qid 21216\n    Query\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_49.png)\n\n\n\n\n    Baseline without a Diversirty Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_51.png)\n\n\n\n    --------------------\n    Service with a Diversity Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_53.png)\n\n\n\n\n\n\n    qid 54265\n    Query\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_55.png)\n\n\n\n\n    Baseline without a Diversirty Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_57.png)\n\n\n\n    --------------------\n    Service with a Diversity Filter\n\n\n\n\n![png](https://pinecone.io/images/similarity-search-diversification-example-42_59.png)\n\n\n\n\n\n\n\n\n### Conclusion\nAlthough we defined a simple search result diversification function and utilized a basically raw vector embedding technique, the cherry-picked examples demonstrate diversification's usefulness. Search results diversification is an active research area. If you seek more rigorous search results diversification ideas and evaluation methods, then [this book](https://www.nowpublishers.com/article/Details/INR-040) might be a good starting point.\n\nBesides that, the comparison demonstrates the ease-of-use and flexibility of Pinecone's Graph API. Observe how easily we could launch two different service flavors and compare them with live data. This gives a gist of what you could do with Pinecone. For example, rapid model development, live experiments (e.g., A/B tests), complex ETL and post-processing steps, and more.\n\n---\n## Shut down the Services\nWe will not use the services anymore. Let's shut them down.\n\nNote that this permanently shuts the service. You will not be able to restart the service, and all resources need to be recreated. We suggest that you only stop a service if no application is using it.\n\n\n```python\nfor srv in pinecone.service.ls():\n    pinecone.service.stop(srv)\n```\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccdf"
  },
  "filename": "image-similarity-search.md",
  "title": "Image Search",
  "category": "630fc5235d91a70054705fb7",
  "content": "---\ntitle: Image Search\ncategory: 630fc5235d91a70054705fb7\n---\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/search/image/image-search/image-search.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/search/image/image-search/image-search.ipynb) [![Open Github](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/github-shield.svg)](https://github.com/pinecone-io/examples/blob/master/search/image/image-search/image-search.ipynb)\n\n\n## Background\n\n### What is Image Search and how will we use it?\n\nOne may find themselves with an image, looking for similar images among a large image corpus. The difficult part of this requirement is instantly retrieving, at scale, similar images, especially when there are tens of millions or billions of images from which to choose.\n\nIn this example, we will walk you through the mechanics of how to solve this problem using an off-the-shelf, pretrained, neural network to generate data structures known as [vector embeddings](https://www.pinecone.io/learn/vector-embeddings/). We will use Pinecone's vector database offering to find images with similar vector embeddings to an _query image_.\n\n### Learning Goals and Estimated Reading Time\n\n_By the end of this 15 minute demo (on a recent MacBook Pro, or up to an hour on Google Colab), you will have:_\n 1. Learned about Pinecone's value for solving realtime image search requirements!\n 2. Stored and retrieved vectors from Pinecone your very-own Pinecone Vector Database.\n 3. Encoded images as vectors using a pretrained neural network (i.e. no model training necessary).\n 4. Queried Pinecone's Vector Database to find similar images to the query in question.\n \nOnce all data is encoded as vectors, and is in your Pinecone Index, results of Pinecone queries are returned, on average, in tens of milliseconds.\n\n## Setup: Prerequisites and Image Data\n\n### Python 3.7+\n\nThis code has been tested with Python 3.7. It is recommended to run this code in a virtual environment or Google Colab.\n\n### Acquiring your Pinecone API Key\n\nA Pinecone API key is required. You can obtain a complimentary key on our [our website](https://app.pinecone.io/). Either add `PINECONE_EXAMPLE_API_KEY` to your list of environmental variables, or manually enter it after running the below cell (a prompt will pop up requesting the API key, storing the result within this kernel (session)).\n\n### Installing and Importing Prerequisite Libraries:\nAll prerequisites are installed and listed in the next cell.\n\n#### Installing via `pip`\n\n\n```python\n!pip install -qU pinecone-client \\\n                 torchvision \\\n                 seaborn \\\n                 tqdm \\\n                 httpimport \\\n                 requests\n```\n\n#### Importing and Defining Constants\n\n\n```python\nimport os\nimport requests\n\nimport tqdm\nimport httpimport\nimport pinecone\nimport numpy as np\nfrom PIL import Image\n\nimport torch\nimport torchvision\n\nDATA_DIRECTORY = 'tmp'\nINDEX_NAME = 'image-search'\nINDEX_DIMENSION = 1000\nBATCH_SIZE=200\n```\n\n### Helper Module\n\nThis helper module will be imported and will enable this notebook to be self-contained.\n\n\n```python\n# There is a helper module required for this notebook to run.\n# When not present with this notebook, it will be streamed in from Pinecone's Example Repository.\n# You can find the module at https://github.com/pinecone-io/examples/tree/master/image_search\n\nif os.path.isfile('helper.py'):\n    import helper as h\nelse:\n    print('importing `helper.py` from https://github.com/pinecone-io')\n    with httpimport.github_repo(\n        username='startakovsky', \n        repo='pinecone-examples-fork',\n        module=['image_search'],\n        branch='may-2022-image-search-refresh'):\n        from image_search import helper as h\n```\n\n\nExtracting API Key from environmental variable `PINECONE_EXAMPLE_API_KEY`...\n\n\n\nPinecone API Key available at `h.pinecone_api_key`\n\n\n### Downloading Data\n\nTo demonstrate image search using Pinecone, we will download 100,000 small images using [built-in datasets](https://pytorch.org/vision/stable/datasets.html) available with the `torchvision` library.\n\n\n```python\ndatasets = {\n    'CIFAR10': torchvision.datasets.CIFAR10(DATA_DIRECTORY, transform=h.preprocess, download=True),\n    'CIFAR100': torchvision.datasets.CIFAR100(DATA_DIRECTORY, transform=h.preprocess, download=True)\n}\n```\n\n    Files already downloaded and verified\n    Files already downloaded and verified\n    \n\n### Inspecting Images\nThese are some of the images from what was just downloaded. If interested, read about the CIFAR image dataset [here](https://www.cs.toronto.edu/~kriz/cifar.html).\n\n\n```python\nh.show_random_images_from_full_dataset(datasets['CIFAR100'])\n```\n\n\n    \n![png](https://raw.githubusercontent.com/pinecone-io/img/main/image-search-1.png)\n    \n\n\n## Generating Embeddings and Sending them to Pinecone\n\n### Loading a Pretrained Computer Vision Model\n\nWe will use a pretrained model that, like the dataset above, is shipped with PyTorch. This model will create a 1000-dimensional sequence of floats for each input image. We will use this output as an embedding associated with an image.\n\n\n```python\nmodel = torchvision.models.squeezenet1_1(pretrained=True).eval()\n```\n\n### Why SqueezeNet?\n\nWe chose the [SqueezeNet](https://en.wikipedia.org/wiki/SqueezeNet) model because it is a very small model and basic model that has been trained on [millions of images](https://en.wikipedia.org/wiki/ImageNet) across 1000 classes. It is easy to instantiate with one line of code and generates embeddings quite a bit faster than deeper models.\n\n### On Comparing Embeddings\n\nTwo embeddings might look like something like this:\n\n- \\[-0.02, 0.06, 0.0, 0.01, 0.08, -0.03, 0.01, 0.02, 0.01, 0.02, -0.07, -0.11, -0.01, 0.08, -0.04\\]\n- \\[-0.04, -0.09, 0.04, -0.1, -0.05, -0.01, -0.06, -0.04, -0.02, -0.04, -0.04, 0.07, 0.03, 0.02, 0.03\\]\n\nIn order to determine how similar they are, we use a [simple](https://towardsdatascience.com/importance-of-distance-metrics-in-machine-learning-modelling-e51395ffe60d) formula that takes a very short time to compute. Similarity scores are, in general, an excellent proxy for image similarity.\n\n### Creating Our Pinecone Index\n\nThe process for creating a Pinecone Index requires your Pinecone API key, the name of your index, and the number of dimensions of each vector (1000).\n\nIn this example, to compare embeddings, we will use the [cosine similarity score](https://en.wikipedia.org/wiki/Cosine_similarity) because this model generates un-normalized probability vectors. While this calculation is trivial when comparing two vectors, it will take quite a long time when needing to compare a query vector against millions or billions of vectors and determine those most similar with the query vector.\n\n### What is Pinecone for?\n\nThere is often a technical requirement to compare one vector to tens or hundreds of millions or more vectors, to do so with low latency (less than 50ms) and a high throughput. Pinecone solves this problem with its managed vector database service, and we will demonstrate this below.\n\n\n```python\n# authenticate with Pinecone API, keys and environment available at your project at https://app.pinecone.io\npinecone.init(h.pinecone_api_key, environment='YOUR_ENVIRONMENT')\n# if the index does not already exist, we create it\nif INDEX_NAME not in pinecone.list_indexes():\n    pinecone.create_index(name=INDEX_NAME, dimension=INDEX_DIMENSION)\n# instantiate connection to your Pinecone index\nindex = pinecone.Index(INDEX_NAME)\n```\n\n### Preparing Vector Embeddings\n\nWe will encode the downloaded images for upload to Pinecone, and store the associated class of each image as metadata.\n\n#### Creating Vector IDs\nEach vector ID will have a prefix corresponding to _CIFAR10_ or _CIFAR100_.\n\n\n```python\ndef get_vector_ids(batch_number, batch_size, prefix):\n    \"\"\"Return vector ids.\"\"\"\n    start_index = batch_number * batch_size\n    end_index = start_index + batch_size\n    ids = np.arange(start_index, end_index)\n    # create id based on prefix \n    # eg. if id == 5, prefix == 'CIFAR10', then create 'CIFAR10.5' as vector id.\n    ids_with_prefix = map(lambda x: f'{prefix}.{str(x)}', ids)\n    return ids_with_prefix\n```\n\n#### Creating metadata for each vector containing class label\n\n\n```python\ndef get_vector_metadata(label_indices, class_list):\n    \"\"\"Return list of {'label': <class name>}.\"\"\"\n    get_class_name = lambda index: {'label': class_list[index]}\n    return map(get_class_name, label_indices)\n```\n\n#### Constructing Vector Embeddings\n\nIn a Pinecone Vector Database, there are three components to every Pinecone vector embedding:\n\n - a vector ID\n - a sequence of floats of a user-defined, fixed dimension\n - vector metadata (a key-value mapping, used for filtering at runtime)\n\n\n```python\ndef get_vectors_from_batch(preprocessed_data, label_indices, batch_number, dataset):\n    \"\"\"Return list of tuples like (vector_id, vector_values, vector_metadata).\"\"\"\n    num_records = len(preprocessed_data)\n    prefix = dataset.__class__.__name__\n    with torch.no_grad():\n        # generate image embeddings with PyTorch model\n        vector_values = model(preprocessed_data).tolist()\n    # return respective IDs/metadata for each image embedding\n    vector_metadata = get_vector_metadata(label_indices, dataset.classes)\n    vector_ids = get_vector_ids(batch_number, num_records, prefix)\n    return list(zip(vector_ids, vector_values, vector_metadata))\n```\n\n#### Example Vector Embedding\nThe below code is an example of a vector embedding, showing just the first 3 components of the associated vector.\n\n\n```python\ndataset = datasets['CIFAR100']\nlist_of_preprocessed_tensors, label_indices = list(zip(*[dataset[i] for i in range(BATCH_SIZE)]))\npreprocessed_data = torch.stack(list_of_preprocessed_tensors)\nvectors = get_vectors_from_batch(preprocessed_data, label_indices, 0, dataset)\nid_, embedding, metadata = vectors[123]\nprint(id_, embedding[:3], metadata, sep=', ')\n```\n\n    CIFAR100.123, [4.237038612365723, 11.179943084716797, 1.3662679195404053], {'label': 'orange'}\n    \n\n#### Upsert Vectors to Pinecone\nThis function iterates through a dataset in batches, generates a list of vector embeddings (as in the the above example) and upserts in batches to Pinecone.\n\n\n```python\ndef upsert_image_embeddings(dataset, pinecone_index, batch_size=BATCH_SIZE, num_rows=None):\n    \"\"\"Iterate through dataset, generate embeddings and upsert in batches to Pinecone index.\n    \n    Args:\n     - dataset: a PyTorch Dataset\n     - pinecone_index: your Pinecone index\n     - batch_size: batch size\n     - num_rows: Number of initial rows to use of dataset, use all rows if None. \n    \"\"\"\n    if num_rows > len(dataset):\n        raise ValueError(f'`num_rows` should not exceed length of dataset: {len(dataset)}')\n    if num_rows:\n        sampler = range(num_rows)\n    else:\n        sampler = None\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=sampler)\n    tqdm_kwargs = h.get_tqdm_kwargs(dataloader)\n    for batch_number, (data, label_indices) in tqdm.notebook.tqdm(enumerate(dataloader), **tqdm_kwargs):\n        vectors = get_vectors_from_batch(\n            data, \n            label_indices, \n            batch_number, \n            dataloader.dataset)\n        pinecone_index.upsert(vectors)\n```\n\n### Begin Upsert for all 100,000 Images\nOne progress bar is generated per dataset. Truncate number of rows in each dataset by modifying `num_rows` parameter value in the cell below. Each of the CIFAR datasets have 50,000 rows.\n\n\n```python\nfor dataset in datasets.values():\n    upsert_image_embeddings(dataset, index, num_rows=50_000)\n```\n\n\n      0%|          | 0/250 [00:00<?, ?chunk of 200 CIFAR10 vectors/s]\n\n\n\n      0%|          | 0/250 [00:00<?, ?chunk of 200 CIFAR100 vectors/s]\n\n\n### View Progress On The [Pinecone Console](https://app.pinecone.io) (sample screenshot below)\n\n![pinecone-console](https://raw.githubusercontent.com/pinecone-io/img/main/image-search-5.png)\n\n## Querying Pinecone\n\nNow that all the embeddings of the images are on Pinecone's database, it's time to demonstrate Pinecone's lightning fast query capabilities.\n\n###  Pinecone Example Usage\n\nIn the below example we query Pinecone's API with an embedding of a query image to return the vector embeddings that have the highest similarity score. Pinecone effeciently estimates which of the uploaded vector embeddings have the highest similarity when paired with the query term's embedding, and the database will scale to billions of embeddings maintaining low-latency and high throughput. In this example we have upserted 100,000 embeddings. Our starter plan supports up to one million.\n\n#### Example: Pinecone API Request and Response\n\nLet's find images similar to the `query_image` variable, shown below.\n\n#### Example Query Image\n\n\n```python\nurl = 'https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog4.png'\nr = requests.get(url, stream=True)\nquery_image = Image.open(r.raw)\nh.printmd(\"#### A sample image\")\nquery_image.resize((125,125))\n```\n\n\n#### A sample image\n\n\n\n\n\n    \n![png](https://raw.githubusercontent.com/pinecone-io/img/main/image-search-2.png)\n    \n\n\n\n\n```python\nquery_embedding = model(h.preprocess(query_image).unsqueeze(0)).tolist()\nresponse = index.query(query_embedding, top_k=4, include_metadata=True)\nh.printmd(f\"#### A sample response from Pinecone \\n ==============\\n \\n\")\nh.printmd(f\"```python\\n{response}\\n```\")\n```\n\n\n#### A sample response from Pinecone \n ==============\n \n\n\n\n\n```python\n{'matches': [{'id': 'CIFAR10.11519',\n              'metadata': {'label': 'dog'},\n              'score': 1.00000012,\n              'values': []},\n             {'id': 'CIFAR10.21059',\n              'metadata': {'label': 'dog'},\n              'score': 0.982942224,\n              'values': []},\n             {'id': 'CIFAR10.48510',\n              'metadata': {'label': 'dog'},\n              'score': 0.982879698,\n              'values': []},\n             {'id': 'CIFAR100.32560',\n              'metadata': {'label': 'seal'},\n              'score': 0.982618093,\n              'values': []}],\n 'namespace': ''}\n```\n\n\n#### Enriched Response\nIn the next few lines, we look up the actual images associated to the vector embeddings.\n\n\n```python\nh.show_response_as_grid(response, datasets, 1, 4, figsize=(10, 10))\n```\n\n\n    \n![png](https://raw.githubusercontent.com/pinecone-io/img/main/image-search-3.png)\n    \n\n\n#### Results\n\nWe invite the reader to explore various queries to see how they come up. In the one above, we chose one of the CIFAR-10 images as the query image. Note that the query image embedding need not exist in your Pinecone index in order to find similar images. Additionally, the search results are only as good as the embeddings, which are based on the quality and quantity of the images as well as how expressive the model used is. There are plenty of other out of the box, pretrained models in PyTorch and elsewhere!\n\n### Pinecone Example Usage with Metadata\n\nExtensive predicate logic can be applied to metadata filtering, just like the [WHERE clause](https://www.pinecone.io/learn/vector-search-filtering/) in SQL! Pinecone's [metadata feature](https://www.pinecone.io/docs/metadata-filtering/) provides easy-to-implement filtering.\n\n#### Example using Metadata\n\nFor demonstration, let's use metadata to find all images classified as a _seal_ that look like the `query_image` variable shown above.\n\n\n```python\nresponse = index.query(\n    query_embedding, \n    top_k=25, \n    filter={\"label\": {\"$eq\": \"seal\"}},\n    include_metadata=True\n)\nh.show_response_as_grid(response, datasets, 5, 5, figsize=(10, 10))\n```\n\n\n    \n![png](https://raw.githubusercontent.com/pinecone-io/img/main/image-search-4.png)\n    \n\n\n#### Results\n\nAll of the results returned are indeed seals, and many of them do look like the query image! Note how the cosine similarity scores are returned in descending order.\n\n#### Additional Note On Querying Pinecone\n\nIn this example, you queried your Pinecone index with an embedding that was already in the index, however that is not necessary at all. For this index, _any 1000-dimensional embedding_ can be used to query Pinecone.\n\n## Conclusion\n\nIn this example, we demonstrated how Pinecone makes it possible to do realtime image similarity search using a pre-trained computer vision model! We also demonstrated the use of metadata filtering with querying Pinecone's vector database.\n\n### Like what you see? Explore our [community](https://www.pinecone.io/community/) \n\nLearn more about semantic search and the rich, performant, and production-level feature set of Pinecone's Vector Database by visiting https://pinecone.io, connecting with us [here](https://www.pinecone.io/contact/) and following us on [LinkedIn](https://www.linkedin.com/company/pinecone-io). If interested in some of the algorithms that allow for effecient estimation of similar vectors, visit our Algorithms and Libraries section of our [Learning Center](https://www.pinecone.io/learn/).\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1cce0"
  },
  "filename": "haystack-old.md",
  "title": "Haystack integration",
  "category": "630fc5235d91a70054705fb7",
  "content": "---\ntitle: Haystack integration\ncategory: 630fc5235d91a70054705fb7\n---\n\n\n> ⚠️  Warning\n>\n> This guide is intended for older versions of Haystack (`<1.3`). For newer versions refer to the [updated page](/integrations/haystack).\n\nIn this guide we will see how to integrate Pinecone and the popular [Haystack library](https://github.com/deepset-ai/haystack) for *Question-Answering*.\n\n## Installing Haystack\n\nWe start by installing the latest version of Haystack with all dependencies required for the `PineconeDocumentStore`.\n\n```bash\n!pip install -U 'farm-haystack[pinecone]'\n```\n\n## Initializing the PineconeDocumentStore\n\nWe initialize a `PineconeDocumentStore` by providing an API key and environment name. ([Create an account](https://app.pinecone.io) to get your API key. You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**.)\n\n```python\nfrom haystack.document_stores import PineconeDocumentStore\n\ndocument_store = PineconeDocumentStore(\n    api_key='<YOUR_API_KEY>',\n    environment='YOUR_ENVIRONMENT'\n)\n```\n\n> ⚠️  Warning\n>\n> If you see a **ModuleNotFoundError** or **ImportError**, try installing the\n> Pinecone client manually using `pip install -U pinecone-client`.\n\n## Data Preparation\n\nBefore adding data to the document store, we must download and convert data into the Document format that Haystack uses.\n\nWe download pages from the Game of Thrones wiki.\n\n```python\nfrom haystack.utils import clean_wiki_text, convert_files_to_dicts, fetch_archive_from_http, print_answers\n\ndoc_dir = \"data/article_txt_got\"\ns3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt.zip\"\nfetch_archive_from_http(url=s3_url, output_dir=doc_dir)\n```\n\nThen convert these files into the Document format.\n\n```python\ndicts = convert_files_to_dicts(\n    dir_path=doc_dir,\n    clean_func=clean_wiki_text,\n    split_paragraphs=True\n)\n```\n\nThis Document format contains two fields; *'content'* for the text content or paragraphs, and *'meta'* where we can place any additional information that can later be used to apply metadata filtering in our search. Here is an example of the Document format:\n\n```json\n{'content': \"'''David Benioff''' (; né '''Friedman''' ; September 25, 1970) is \"\n            'an American screenwriter and television producer, writer, and '\n            'director. Along with his collaborator D. B. Weiss, he is best '\n            \"known as co-creator, showrunner, and writer of ''Game of \"\n            \"Thrones'' (2011–2019), the HBO adaptation of George R. R. \"\n            \"Martin's series of books ''A Song of Ice and Fire''. He is also \"\n            \"known for writing ''Troy'' (2004) and co-writing ''X-Men Origins: \"\n            \"Wolverine'' (2009).\",\n 'meta': {'name': '33_David_Benioff.txt'}}\n```\n\n## Indexing Documents\n\nTo index the documents we use the `PineconeDocumentStore.write_documents` method.\n\n```python\ndocument_store.write_documents(dicts)\n```\n\n## Creating and Upserting Embeddings\n\nTo create embeddings for our documents we must initialize a `DensePassageRetriever` model.\n\n```python\nfrom haystack.nodes import DensePassageRetriever\nretriever = DensePassageRetriever(\n    document_store=document_store,\n    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n    max_seq_len_query=64,\n    max_seq_len_passage=256,\n    batch_size=2,\n    use_gpu=True,\n    embed_title=True,\n    use_fast_tokenizers=True\n)\n```\n\nThen we run the `PineconeDocumentStore.update_embeddings` method with the `retriever` provided as an argument. GPU acceleration can greatly reduce the time required for this step.\n\n```python\ndocument_store.update_embeddings(\n    retriever,\n    batch_size=16\n)\n```\n\n## Inspect Documents and Embeddings\n\nWe can get documents by their ID with the `PineconeDocumentStore.get_documents_by_id` method.\n\n```python\nd = document_store.get_documents_by_id(ids=['49091c797d2236e73fab510b1e9c7f6b'], return_embedding=True)[0]\n```\n\nFrom here we return can view document content with `d.content` and the document embedding with `d.embedding`.\n\n## Initializing an Extractive QA Pipeline\n\nAn `ExtractiveQAPipeline` contains three key components by default:\n\n* a document store (`PineconeDocumentStore`)\n* a retriever model\n* a reader model\n\nWe can initialize a reader model from the HuggingFace Model Hub named `deepset/roberta-base-squad2`.\n\n```python\nfrom haystack.nodes import FARMReader\n\nreader = FARMReader(\n    model_name_or_path=\"deepset/roberta-base-squad2\", \n    use_gpu=True\n)\n```\n\nWe are now ready to initialize the `ExtractiveQAPipeline`.\n\n```python\nfrom haystack.pipelines import ExtractiveQAPipeline\n\npipe = ExtractiveQAPipeline(reader, retriever)\n```\n\n## Asking Questions\n\nUsing our QA pipeline we can begin querying with `pipe.run`.\n\n```python\nprediction = pipe.run(\n    query=\"Who created the Dothraki vocabulary?\",\n    params={\n        \"Retriever\": {\"top_k\": 10},\n        \"Reader\": {\"top_k\": 5}\n    }\n)\n```\n\nWe are also passing two `top_k` values, the retriever `top_k` defines how many records to retrieve from Pinecone. These records are then passed to the reader model which identifies a specific answer from each content paragraph and reranks the returned records. The reader `top_k` defines how many of these reranked records to return.\n\nTo view the answers we use `haystack.utils.print_answers`.\n\n```python\nfrom haystack.utils import print_answers\nprint_answers(prediction, details=\"minimum\")\n```\n\nThis will return:\n\n```json\nQuery: Who created the Dothraki vocabulary?\nAnswers:\n[   {   'answer': 'David J. Peterson',\n        'context': 'orld. The language was developed for the TV series by the '\n                   'linguist David J. Peterson, working off the Dothraki words '\n                   \"and phrases in Martin's novels.\\n\"\n                   ','},\n    {   'answer': 'David J. Peterson',\n        'context': '\\n'\n                   '===Valyrian===\\n'\n                   'David J. Peterson, who created the Dothraki language for '\n                   'the first season of the show, was entrusted by the '\n                   'producers to design a new '},\n    {   'answer': 'David J. Peterson',\n        'context': \"age for ''Game of Thrones''\\n\"\n                   'The Dothraki vocabulary was created by David J. Peterson '\n                   'well in advance of the adaptation. HBO hired the Language '\n                   'Creatio'},\n    {   'answer': 'D. B. Weiss and David Benioff',\n        'context': '\\n'\n                   '===Conception and development===\\n'\n                   'Showrunners D. B. Weiss and David Benioff created the '\n                   'series, wrote most of its episodes and directed several.\\n'\n                   'In Ja'},\n    {   'answer': 'books',\n        'context': 'ints.  First, the language had to match the uses already '\n                   'put down in the books. Secondly, it had to be easily '\n                   'pronounceable or learnable by the actors'}]\n```\n\nWe can view more details including the score of each answer by specifying `details=\"all\"`.\n\n```python\nprint_answers(prediction, details=\"all\")\n```\n\nWhich returns:\n\n```python\nQuery: Who created the Dothraki vocabulary?\nAnswers:\n[   <Answer {'answer': 'David J. Peterson', 'type': 'extractive', 'score': 0.9532108306884766, 'context': \"orld. The language was developed for the TV series by the linguist David J. Peterson, working off the Dothraki words and phrases in Martin's novels.\\n,\", 'offsets_in_document': [{'start': 329, 'end': 346}], 'offsets_in_context': [{'start': 67, 'end': 84}], 'document_id': '308dca876f94e5e839187f1463693015', 'meta': {'name': '214_Dothraki_language.txt'}}>,\n    <Answer {'answer': 'David J. Peterson', 'type': 'extractive', 'score': 0.8807850480079651, 'context': '\\n===Valyrian===\\nDavid J. Peterson, who created the Dothraki language for the first season of the show, was entrusted by the producers to design a new ', 'offsets_in_document': [{'start': 16, 'end': 33}], 'offsets_in_context': [{'start': 16, 'end': 33}], 'document_id': 'b368200c210d555625bd409b0dc27be1', 'meta': {'name': '87_Valar_Dohaeris.txt'}}>,\n    <Answer {'answer': 'David J. Peterson', 'type': 'extractive', 'score': 0.8687494099140167, 'context': \"age for ''Game of Thrones''\\nThe Dothraki vocabulary was created by David J. Peterson well in advance of the adaptation. HBO hired the Language Creatio\", 'offsets_in_document': [{'start': 139, 'end': 156}], 'offsets_in_context': [{'start': 67, 'end': 84}], 'document_id': '27baa56e5aab6b04d38f19e97e078bc6', 'meta': {'name': '214_Dothraki_language.txt'}}>,\n    <Answer {'answer': 'D. B. Weiss and David Benioff', 'type': 'extractive', 'score': 0.10197015851736069, 'context': '\\n===Conception and development===\\nShowrunners D. B. Weiss and David Benioff created the series, wrote most of its episodes and directed several.\\nIn Ja', 'offsets_in_document': [{'start': 46, 'end': 75}], 'offsets_in_context': [{'start': 46, 'end': 75}], 'document_id': 'd8b7f165cc64c549532b74249cc692dd', 'meta': {'name': '229_Game_of_Thrones.txt'}}>,\n    <Answer {'answer': 'books', 'type': 'extractive', 'score': 0.0460672490298748, 'context': 'ints.  First, the language had to match the uses already put down in the books. Secondly, it had to be easily pronounceable or learnable by the actors', 'offsets_in_document': [{'start': 166, 'end': 171}], 'offsets_in_context': [{'start': 73, 'end': 78}], 'document_id': '8767e85c7a9bcec61f95e13bb61f3e98', 'meta': {'name': '214_Dothraki_language.txt'}}>]\n```\n\n## Metadata Filtering\n\nThe `PineconeDocumentStore` gives us access to Pinecone's powerful metadata filtering functionality. When performing filtering with Haystack we use a slightly different filter syntax to that used by Pinecone.\n\nUsing the Game of Thrones dataset we can filter by filename.\n\n```python\nprediction = pipe.run(\n    query=\"Who created the Dothraki vocabulary?\",\n    params={\"Retriever\": {\n        \"top_k\": 10,\n        \"filters\": {\n            \"name\": {\"$eq\": \"368_Jaime_Lannister.txt\"}\n        }\n    }, \"Reader\": {\"top_k\": 5}}\n)\n```\n\n### Haystack Filter Examples\n\nHere are a few more examples of Haystack filtering syntax.\n\n```python\nfilters = {\n    \"$and\": {\n        \"type\": {\"$eq\": \"article\"},\n        \"date\": {\"$gte\": \"2015-01-01\", \"$lt\": \"2021-01-01\"},\n        \"rating\": {\"$gte\": 3},\n        \"$or\": {\n            \"genre\": {\"$in\": [\"economy\", \"politics\"]},\n            \"publisher\": {\"$eq\": \"nytimes\"}\n        }\n    }\n}\n# or simpler using default operators\nfilters = {\n    \"type\": \"article\",\n    \"date\": {\"$gte\": \"2015-01-01\", \"$lt\": \"2021-01-01\"},\n    \"rating\": {\"$gte\": 3},\n    \"$or\": {\n        \"genre\": [\"economy\", \"politics\"],\n        \"publisher\": \"nytimes\"\n    }\n}\n```\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1cce1"
  },
  "filename": "facial-similarity-search.md",
  "title": "Facial Similarity Search",
  "category": "630fc5235d91a70054705fb7",
  "content": "---\ntitle: Facial Similarity Search\ncategory: 630fc5235d91a70054705fb7\n---\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/search/image/facial-similarity-search/facial-similarity-search.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/search/image/facial-similarity-search/facial-similarity-search.ipynb) [![Open github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/pinecone-io/examples/blob/master/search/image/facial-similarity-search/facial-similarity-search.ipynb)\n\n\nIn this notebook, we will demonstrate how to use Pinecone to build an image-based vector search application to discover people with similar facial features. We will:\n\n1. Extract faces from a celebrity image dataset\n2. Convert the faces to embeddings and store them in a Pinecone index (alongside metadata related to the celebrities)\n3. Query the Pinecone index with an image of a person and find the most similar celebrities\n\n\n# Install Dependencies\n\n\n```python\n!pip install datasets pinecone-client[grpc] facenet-pytorch requests Pillow\n```\n\n# Load Dataset\n\nWe will use a dataset containing photos of ~115K most popular people on The Movie Database (TMDB). This dataset can be loaded from Huggingface as follows:\n\n\n```python\nfrom datasets import load_dataset\n\n# load the dataset\nceleb_faces = load_dataset(\"ashraq/tmdb-people-image\", split=\"train\")\nceleb_faces\n```\n\n    Dataset({\n        features: ['adult', 'also_known_as', 'biography', 'birthday', 'deathday', 'gender', 'homepage', 'id', 'imdb_id', 'known_for_department', 'name', 'place_of_birth', 'popularity', 'profile_path', 'image'],\n        num_rows: 116404\n    })\n\n\n\nWe have got few metadata about the people and their image in the dataset. Let's take a look:\n\n\n```python\nceleb = celeb_faces[10]\nceleb\n```\n\n\n\n\n    {'adult': False,\n     'also_known_as': \"['Thomas Stanley Holland', 'Том Холланд', 'トム・ホランド', '톰 홀랜드', 'توم هولاند', 'ทอม ฮอลแลนด์', '汤姆·赫兰德', 'Τομ Χόλαντ', 'Том Голланд', '湯姆·霍蘭德', 'טום הולנד', 'תומאס סטנלי הולנד', 'Nhện Đệ Tam', 'ტომ ჰოლანდი']\",\n     'biography': 'Thomas \"Tom\" Stanley Holland is an English actor and dancer. He is best known for playing Peter Parker / Spider-Man in the Marvel Cinematic Universe and has appeared as the character in six films: Captain America: Civil War (2016), Spider-Man: Homecoming (2017), Avengers: Infinity War (2018), Avengers: Endgame (2019), Spider-Man: Far From Home (2019), and Spider-Man: No Way Home (2021). He is also known for playing the title role in Billy Elliot the Musical at the Victoria Palace Theatre, London, as well as for starring in the 2012 film The Impossible.',\n     'birthday': '1996-06-01',\n     'deathday': None,\n     'gender': 2,\n     'homepage': None,\n     'id': 1136406,\n     'imdb_id': 'nm4043618',\n     'known_for_department': 'Acting',\n     'name': 'Tom Holland',\n     'place_of_birth': 'Surrey, England, UK',\n     'popularity': 104.302,\n     'profile_path': 'bBRlrpJm9XkNSg0YT5LCaxqoFMX.jpg',\n     'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=421x632 at 0x7FA4FC04CB50>}\n\n\n\n\n```python\nceleb[\"image\"].resize((200, 300))\n```\n\n\n\n\n    \n![png](https://raw.githubusercontent.com/pinecone-io/img/main/facial-similarity-search_9_0.png)\n    \n\n\n\nWe do not need all these metadata fields. So we will remove the ones we do not need and convert the rest into a pandas dataframe.\n\n\n```python\n# remove metadata fields not needed, convert into a pandas dataframe\nmetadata = celeb_faces.remove_columns(['adult', 'also_known_as', 'biography', 'deathday', 'gender', 'homepage', 'id', 'imdb_id', 'known_for_department', 'image']).to_pandas()\n# replace any empty fields with None\nmetadata = metadata.fillna(\"None\")\n```\n\n# Embedding Model\n\nWe will use two models: one for extracting faces and another for generating vector embeddings of the face. We're focusing on faces only because using full images would introduce too much noise and result in poor results.\n\nFor face extraction, we will use MTCNN, which is a popular choice due to its ability to accurately detect and align faces in images despite variations in pose and appearance. We can use a Pytorch implementation of MTCNN with the ``facenet-pytorch`` package. Since the images in our dataset are already in PIL format, we can directly test the MTCNN model, which expects PIL image objects as inputs, as shown below:\n\n\n```python\nfrom facenet_pytorch import MTCNN\n\n# initialize the MTCNN model\nmtcnn = MTCNN()\n# create a copy of the face\nimg = celeb[\"image\"].copy()\n# detect face and get coordinates of the face with probability\nboxes, prob = mtcnn.detect(img)\nboxes, prob\n```\n\n\n\n\n    (array([[ 91.4824  , 112.335335, 316.80338 , 409.37723 ]], dtype=float32),\n     array([0.9999924], dtype=float32))\n\n\n\nThe detect method in MTCNN gives us the coordinates of the face and how confident it was in detecting the face, in this case, with 99% accuracy. Let's draw a rectangle on the image using these coordinates to see if it correctly detected the face.\n\n\n```python\nfrom PIL import Image, ImageDraw\n\n# draw a rectangle on the image using coordinates returned by the MTCNN model\ndraw = ImageDraw.Draw(img)\ndraw.rectangle(boxes.reshape((2,2)), width=3)\n# resize the image to display a smaller size\nimg.resize((200, 290))\n```\n\n\n\n\n    \n![png](https://raw.githubusercontent.com/pinecone-io/img/main/facial-similarity-search_16_0.png)\n    \n\n\n\nAs we can see, the model has successfully identified the face. To extract the face, we can crop the image to only include the area within the rectangle, using either ``opencv`` or another package. Alternatively, the ``facenet-pytorch`` package has a function that does this for us and returns the result as Pytorch tensors that can be used as input for the embedding model directly. This can be done as follows:\n\n\n```python\n# pass the image or batch of images directly through mtcnn model\nface = mtcnn(img)\nface.shape\n```\n\n\n\n\n    torch.Size([3, 160, 160])\n\n\n\nTo generate embeddings, we will use VGGFace2, which is a deep learning model for facial recognition that was trained on the VGGFace2 dataset, which includes more than 3 million images of over 9000 people. The model can be loaded and used as follows:\n\n\n```python\nfrom facenet_pytorch import InceptionResnetV1\nimport torch\n\n# initialize VGGFace2 model\nresnet = InceptionResnetV1(pretrained=\"vggface2\").eval()\n# generate embedding for the face extracted using mtcnn above\nembedding = resnet(torch.stack([face]))\nembedding.shape\n```\n\n\n    torch.Size([1, 512])\n\n\n\nWe can now generated vector embedding for the face. Let's write a pipeline to easy do all of this in batches.\n\n\n```python\nimport numpy as np\n\n\nclass FacenetEmbedder:\n    def __init__(self):\n        # set device to use GPU if available\n        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n        # initialize MTCNN model\n        self.mtcnn = MTCNN(device=self.device)\n        # initialize VGGFace2 model\n        self.resnet = InceptionResnetV1(pretrained='vggface2', device=self.device).eval()\n\n    def detect_face(self, batch):\n        # get coordinates of the face\n        faces = self.mtcnn.detect(batch)\n        return faces\n\n    def encode(self, batch):\n        # pass the batch of images directly through mtcnn model\n        face_batch = self.mtcnn(batch)\n        # remove any images that does not contain a face\n        face_batch = [i for i in face_batch if i is not None]\n        # concatenate face batch to form a single tensor\n        aligned = torch.stack(face_batch)\n        # if using gpu move the input batch to gpu\n        if self.device.type == \"cuda\": \n            aligned = aligned.to(self.device)\n        # generate embedding\n        embeddings = self.resnet(aligned).detach().cpu()\n        return embeddings.tolist()\n```\n\n\n```python\n# initialize the embedding pipeline\nfacenet = FacenetEmbedder()\n```\n\n\n```python\n# test the pipeline using a small image batch\nbatch = celeb_faces[10:20][\"image\"]\nlen(facenet.encode(batch))\n```\n\n\n\n\n    10\n\n\n\nWe can now simply call the ``encode`` method in the ``FacenetEmbedder`` with a batch of PIL images and it would extract the faces and generate embedding for us. Keep in mind that batch encoding only works if all the images in the batch have the same shape. We can use the following function to reshape a batch of PIL images to ensure it always works.\n\n\n```python\ndef reshape(batch):\n    batch = [image.convert(\"RGB\").resize((421, 632)) for image in batch]\n    return batch\n```\n\n# Initialize Pinecone Index\n\nNow we need to set up the Pinecone index, which stores vector representations of our images that can be retrieved using the embedding of another image (called the query vector). Before we can do this, we have to establish a connection to Pinecone using an API key. You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**. This connection is initialized as follows:\n\n\n```python\nimport pinecone\n\n# connect to pinecone environment\npinecone.init(\n    api_key=\"YOUR_API_KEY\",\n    environment=\"YOUR_ENVIRONMENT\"\n)\n```\n\nNow, we can create our vector index and name it \"tmdb-people\" (although you can choose any name you like). We specify the metric type as cosine and the dimension as 512, as these are the vector space and dimensionality of the vectors produced by the embedding model we use.\n\n\n```python\nindex_name = \"tmdb-people\"\n\n# check if the tmdb-people index exists\nif index_name not in pinecone.list_indexes():\n    # create the index if it does not exist\n    pinecone.create_index(\n        index_name,\n        dimension=512,\n        metric=\"cosine\"\n    )\n\n# connect to tmdb-people index we created\nindex = pinecone.GRPCIndex(index_name)\n```\n\n# Generate Embeddings and Upsert\n\nNext, we need to generate embeddings for the celebrity faces and upload them into the Pinecone index. To do this efficiently, we will process them in batches and upload the resulting embeddings to the Pinecone index. For each celebrity in the dataset, we need to provide Pinecone with a unique id, the corresponding embedding, and metadata. The metadata is a collection of information related to the celebrities, including their name, profile image url, date of birth, etc.\n\n\n```python\nfrom tqdm.auto import tqdm\n\n# we will use batches of 64\nbatch_size = 64\n\nfor i in tqdm(range(0, len(celeb_faces), batch_size)):\n    # find end of batch\n    i_end = min(i+batch_size, len(celeb_faces))\n    # extract batch\n    batch = celeb_faces[i:i_end][\"image\"]\n    # reshape the images to ensure they all have same shape\n    batch = reshape(batch)\n    # generate embeddings for batch\n    emb = facenet.encode(batch)\n    # create unique IDs\n    ids = [f\"{idx}\" for idx in range(i, i_end)]\n    # add metadata\n    meta = metadata[i:i_end].to_dict(orient=\"records\")\n    # add all to upsert list\n    to_upsert = list(zip(ids, emb, meta))\n    # upsert/insert these records to pinecone\n    _ = index.upsert(vectors=to_upsert)\n\n# # check that we have all vectors in index\nindex.describe_index_stats()\n```\n\nWe have successfully added everything we need to the Pinecone index.\n\n# Find Similar Celebrities\n\nNow we can query the Pinecone index with an embedding of a face and instantly get the celebrities that are most similar. First, let's write a helper functions to query pinecone and display the results.\n\n\n```python\nfrom IPython.core.display import HTML\n\n\ndef display_result(metadata):\n    figures = []\n    for m in metadata:\n        figures.append(f'''\n            <figure style=\"margin: 5px !important;\">\n                <img src=\"https://image.tmdb.org/t/p/h632/{m[\"profile_path\"]}\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n                <figcaption>{m[\"name\"]}</figcaption>\n            </figure>\n        ''')\n    return HTML(data=f'''\n        <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n        {''.join(figures)}\n        </div>\n    ''')\n```\n\n\n```python\ndef find_similar_faces(face, top_k=10):\n    # pass the image through the embedding pipeline\n    emb = facenet.encode([face])\n    # query pinecone with the face embedding\n    result = index.query(emb[0], top_k=6, include_metadata=True)\n    # extract metadata from the search results and display results \n    r = [x[\"metadata\"] for x in result[\"matches\"]]\n    return display_result(r)\n```\n\nLet's run some test queries using celebrity images from the dataset to find other celebrities who look alike.\n\n\n```python\nceleb = celeb_faces[40][\"image\"]\nceleb.resize((190,240))\n```\n\n\n\n\n    \n![png](https://raw.githubusercontent.com/pinecone-io/img/main/facial-similarity-search_41_0.png)\n    \n\n\n\n\n```python\nfind_similar_faces(celeb)\n```\n\n\n\n\n\n<div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/dSK2BBupETZYcsO0DfP2OD1AMnT.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Claudia Harrison</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/bHGtm29qlx6OYsgwOq84xd6MRpF.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Sonya Walger</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/6TSe26ctGPvmZRKDgul1Gos6old.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Bettina Mittendorfer</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/2RJ30pPSQQxteuoMjhN1FWTwxlZ.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Julianne Nicholson</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/8Jju93UvSPszRzZ9Glvjp9z2anF.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Aglaia Szyszkowitz</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/duMdWlldESIqFYDpaJNAcLFzTTq.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Hannah Barefoot</figcaption>\n    </figure>\n</div>\n\n\n\n\nThe search result looks good as we can definately see some celebrities with similar facial features. Let's run more queries.\n\n\n```python\nceleb = celeb_faces[35][\"image\"]\nfind_similar_faces(celeb)\n```\n\n\n\n\n\n<div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/jpurJ9jAcLCYjgHHfYF32m3zJYm.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Chris Hemsworth</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/As5ujeSSJRQN4TE3odg3kDk0b5o.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Liam Hemsworth</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/jcDCpX5WiH0AkLDpR6ELreIn2Ta.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Ed Hendrik</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/yvo81AU4zUKbcNtWEQjaSTvLTPS.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Chris Reid</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/lLsbZLO0JQPIiaRZTcVYbVxfv4m.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Kenny Doughty</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/6lqQsR2Ot7nYthCGxfKBzmPzyV3.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Christopher Russell</figcaption>\n    </figure>\n</div>\n\n\n\n\n\n```python\nceleb = celeb_faces[1][\"image\"]\nfind_similar_faces(celeb)\n```\n\n\n\n\n\n<div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/8dCFK8FDFQbYFZvzAE9IIeaDMKo.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Seung Ha</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/bsSM5aG1T5oXwpcL3glOf0ViNy4.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Shara Lin</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/cVA40E5Pus66fLzq2mWLJDNNtNh.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Yura</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/3JTUzCh3PrbamSBf5M5yxMqbxGY.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Napasorn Weerayuttwilai</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/ksilFsztYt9Ojp2cbMk75nxumOS.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Go Joon-hee</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/kpwv7ozzwUmBaCCgx412CmHqofb.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>An Danwei</figcaption>\n    </figure>\n</div>\n\n\n\n\n\n```python\nceleb = celeb_faces[12][\"image\"]\nfind_similar_faces(celeb)\n```\n\n\n\n\n\n<div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/whNwkEQYWLFJA8ij0WyOOAD5xhQ.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Jason Statham</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/2v1MXtsjWJHqx0U0ATZ5VrBh2dD.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Vladimir Raiman</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/nJXxgwcGdAL5P1KfgcGrjDaCbsY.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Brendan Kelly</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/tnDgEU5q47anoWZJDzzudC678m7.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Scott C. Brown</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/pAkKjbzxP6Z6Y5zAGpwmWQo1iji.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Michael Chiklis</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/8XBU3bsvmPSDTf6d0edNb2eSDpb.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Huw Garmon</figcaption>\n    </figure>\n</div>\n\n\n\n\n\n```python\nceleb = celeb_faces[17][\"image\"]\nfind_similar_faces(celeb)\n```\n\n\n\n\n\n<div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/kL0lWLJA6lbvmPM3YL0ISE6rVr6.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Joey King</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/gdTcDujkkPIoKtDZrGcbQ2voFI3.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Elva Trill</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/iF9rvJzqepJCxzqQC07OG4bTxEf.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Rosalind Halstead</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/AeYvjBU8swVC9zKS933fX8YsnKt.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Svetlana Svetlichnaya</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/caXilGBPQtpF4nZEPUlDb0UfVnL.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Megan Parkinson</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/IgGOiwSg4qq3OEzXG1LROXTsel.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Clara Ponsot</figcaption>\n    </figure>\n</div>\n\n\n\n\n\n```python\nceleb = celeb_faces[29][\"image\"]\nfind_similar_faces(celeb)\n```\n\n\n\n\n\n<div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/52KsHFCu0LToakebnxqC4VeRixl.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Luke Grimes</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/bkAEduLHkQg1AZmempKPLpoqibA.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Philip Ettinger</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/aE8WEoNAs2PtgUzF6AcGodwLQvy.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Nick Thune</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/fHSGEllSO8O7p5StwFbWr1du7Bw.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Vicente Alves do Ó</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/iR0ElrLlagBaGSsOZVphTLbU8TE.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Thomas McDonell</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/yxOUD7RXqwp1EokJqslrJgzI38t.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Irhad Mutic</figcaption>\n    </figure>\n</div>\n\n\n\n\n\n```python\nceleb = celeb_faces[64][\"image\"]\nfind_similar_faces(celeb)\n```\n\n\n\n\n\n<div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/m8bdrmh6ExDCGQ64E83mHg002YV.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Jeffrey Dean Morgan</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/5By3t8kAp5Iiu9aULc3qryyTOWO.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Pompeu José</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/btBOmL6PvUMzBjmALO9x0R44ncf.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Özcan Varaylı</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/j7AFVjuiU0h0K0Bke7FJnZQpcTz.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Selahattin Taşdöğen</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/9Shbe0mLDItn2Is7QloNIu3G6x1.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Vagelis Rokos</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/nyLa6LmV9iGv4h9Zqbh1xMp7T26.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Darrell D'Silva</figcaption>\n    </figure>\n</div>\n\n\n\n\nThe search results look excellent. To further test our system, let's try using images that are not in the dataset. The following function, ``get_image``, can be utilized to load an image as a PIL object using a URL:\n\n\n```python\nfrom PIL import Image\nimport requests\n\ndef get_image(url):\n  img = Image.open(requests.get(url, stream=True).raw)\n  return img\n  \n```\n\n\n```python\nurl = \"https://live.staticflickr.com/7442/9509564504_21d2dc42e1_z.jpg\"\n# load the image as PIL object from url\nceleb = get_image(url)\nceleb.resize((190,240))\n```\n\n\n\n\n    \n![png](https://raw.githubusercontent.com/pinecone-io/img/main/facial-similarity-search_52_0.png)\n    \n\n\n\n\n```python\nfind_similar_faces(celeb)\n```\n\n\n\n\n\n<div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/f7cyXplSWuYFX1e7JxcNMiRfbaH.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Jennifer Lawrence</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/qpSUiChjYsgsFhaBNtPRNATkqx3.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Aislyn Watson</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/8ZYFDTg7qVripncGatvG7ufxeyx.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Carrie Underwood</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/1midcRjbkFrMX3km1290Jlwf6xi.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Carissa Capobianco</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/djzcylhUn3FzpKFrwHQbhbZqMpn.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Kimberley Klaver</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/l3wEP7cqUYQs9qzbWkx05zTpMnN.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Esti Ginzburg</figcaption>\n    </figure>\n</div>\n\n\n\n\n\n```python\nurl = \"https://live.staticflickr.com/3563/3304692615_bc67db2606_z.jpg\"\n# load the image as PIL object from url\nceleb = get_image(url)\nceleb.resize((190,240))\n```\n\n\n\n\n    \n![png](https://raw.githubusercontent.com/pinecone-io/img/main/facial-similarity-search_54_0.png)\n    \n\n\n\n\n```python\nfind_similar_faces(celeb)\n```\n\n\n\n\n\n<div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/ajNaPmXVVMJFg9GWmu6MJzTaXdV.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Brad Pitt</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/boqqxkt8GcjCUmUsGRFzr3rSTDh.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Peter M. Lenkov</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/wWRbCMgZnG6ItMBcMHsJ495FrU8.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Michał Lewandowski</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/b19aJzhUAfyQXd0mtmDvLAyOth8.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Luke Arnold</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/hiuSdXFjI7EIMv8Y83zh2KtGbbk.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>David Berry</figcaption>\n    </figure>\n    <figure style=\"margin: 5px !important;\">\n        <img src=\"https://image.tmdb.org/t/p/h632/w2oRqYtVa8B3ALOrBqajwp72YQf.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >\n        <figcaption>Marco Quaglia</figcaption>\n    </figure>\n</div>\n\n\n\n\nAs we can see, the search result correctly identifies the celebrity in the picture as the top match and also finds other celebrities with similar facial features.\n\n# Example Application\n\nAre you curious if you share a resemblance with a famous celebrity? Try this [demo app](https://huggingface.co/spaces/pinecone/find-your-celebrity-match), which has been built based on this notebook, to find out.\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1cce2"
  },
  "filename": "gen-qa-openai.md",
  "title": "Generative QA with OpenAI",
  "category": "630fc5235d91a70054705fb7",
  "content": "---\ntitle: Generative QA with OpenAI\ncategory: 630fc5235d91a70054705fb7\n---\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/generation/generative-qa/openai/gen-qa-openai/gen-qa-openai.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/generation/generative-qa/openai/gen-qa-openai/gen-qa-openai.ipynb) [![Open Github](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/github-shield.svg)](https://github.com/pinecone-io/examples/blob/master/generation/generative-qa/openai/gen-qa-openai/gen-qa-openai.ipynb)\n\n\nIn this notebook we will learn how to build a **retrieval enhanced generative question-answering system** with Pinecone and OpenAI. This will allow us to retrieve relevant contexts to our queries from Pinecone, and pass these to a generative OpenAI model to generate an answer backed by real data sources. Required installs for this notebook are:\n\n\n```python\n!pip install -qU openai pinecone-client datasets\n```\n\nInitialize OpenAI connection with:\n\n```python\nimport openai\n\n# get API key from top-right dropdown on OpenAI website\nopenai.api_key = \"OPENAI_API_KEY\"\n```\n\nFor many questions *state-of-the-art (SOTA)* LLMs are more than capable of answering correctly.\n\n\n```python\nquery = \"who was the 12th person on the moon and when did they land?\"\n\n# now query text-davinci-003 WITHOUT context\nres = openai.Completion.create(\n    engine='text-davinci-003',\n    prompt=query,\n    temperature=0,\n    max_tokens=400,\n    top_p=1,\n    frequency_penalty=0,\n    presence_penalty=0,\n    stop=None\n)\n\nres['choices'][0]['text'].strip()\n```\n\n\n\n\n    'The 12th person on the moon was Harrison Schmitt, and he landed on December 11, 1972.'\n\n\n\nHowever, that isn't always the case. Let's first rewrite the above into a simple function so we're not rewriting this every time.\n\n\n```python\ndef complete(prompt):\n    # query text-davinci-003\n    res = openai.Completion.create(\n        engine='text-davinci-003',\n        prompt=prompt,\n        temperature=0,\n        max_tokens=400,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0,\n        stop=None\n    )\n    return res['choices'][0]['text'].strip()\n```\n\nNow let's ask a more specific question about training a specific type of transformer model called a *sentence-transformer*. The ideal answer we'd be looking for is _\"Multiple Negatives Ranking (MNR) loss\"_.\n\nDon't worry if this is a new term to you, it isn't required to understand what we're doing or demoing here.\n\n\n```python\nquery = (\n    \"Which training method should I use for sentence transformers when \" +\n    \"I only have pairs of related sentences?\"\n)\n\ncomplete(query)\n```\n\n\n\n\n    'If you only have pairs of related sentences, then the best training method to use for sentence transformers is the supervised learning approach. This approach involves providing the model with labeled data, such as pairs of related sentences, and then training the model to learn the relationships between the sentences. This approach is often used for tasks such as natural language inference, semantic similarity, and paraphrase identification.'\n\n\n\nAnother common answer produced by the LLM is:\n\n```\nThe best training method to use for fine-tuning a pre-trained model with sentence transformers is the Masked Language Model (MLM) training. MLM training involves randomly masking some of the words in a sentence and then training the model to predict the masked words. This helps the model to learn the context of the sentence and better understand the relationships between words.\n```\n\nBoth answers seem convincing. Yet, they're both wrong. For the former about `supervised learning approach` being the most suitable. This is completely true, but it's not specific and doesn't answer the question.\n\nFor the latter, MLM is typically used in the pretraining step of a transformer model but *cannot* be used to fine-tune a sentence-transformer, and has nothing to do with having _\"pairs of related sentences\"_.\n\nWe have two options for enabling our LLM in understanding and correctly answering this question:\n\n1. We fine-tune the LLM on text data covering the topic mentioned, likely on articles and papers talking about sentence transformers, semantic search training methods, etc.\n\n2. We use **R**etrieval **A**ugmented **G**eneration (RAG), a technique that implements an information retrieval component to the generation process. Allowing us to retrieve relevant information and feed this information into the generation model as a *secondary* source of information.\n\nWe will demonstrate option **2**.\n\n---\n\n## Building a Knowledge Base\n\nWith open **2** the retrieval of relevant information requires an external _\"Knowledge Base\"_, a place where we can store and use to efficiently retrieve information. We can think of this as the external _long-term memory_ of our LLM.\n\nWe will need to retrieve information that is semantically related to our queries, to do this we need to use _\"dense vector embeddings\"_. These can be thought of as numerical representations of the *meaning* behind our sentences.\n\nThere are many options for creating these dense vectors, like open source [sentence transformers](https://pinecone.io/learn/nlp/) or OpenAI's [ada-002 model](https://youtu.be/ocxq84ocYi0). We will use OpenAI's offering in this example.\n\nWe have already authenticated our OpenAI connection, to create an embedding we just do:\n\n\n```python\nembed_model = \"text-embedding-ada-002\"\n\nres = openai.Embedding.create(\n    input=[\n        \"Sample document text goes here\",\n        \"there will be several phrases in each batch\"\n    ], engine=embed_model\n)\n```\n\nIn the response `res` we will find a JSON-like object containing our new embeddings within the `'data'` field.\n\n\n```python\nres.keys()\n```\n\n\n\n\n    dict_keys(['object', 'data', 'model', 'usage'])\n\n\n\nInside `'data'` we will find two records, one for each of the two sentences we just embedded. Each vector embedding contains `1536` dimensions (the output dimensionality of the `text-embedding-ada-002` model.\n\n\n```python\nlen(res['data'])\n```\n\n\n\n\n    2\n\n\n\n\n```python\nlen(res['data'][0]['embedding']), len(res['data'][1]['embedding'])\n```\n\n\n\n\n    (1536, 1536)\n\n\n\nWe will apply this same embedding logic to a dataset containing information relevant to our query (and many other queries on the topics of ML and AI).\n\n### Data Preparation\n\nThe dataset we will be using is the `jamescalam/youtube-transcriptions` from Hugging Face _Datasets_. It contains transcribed audio from several ML and tech YouTube channels. We download it with:\n\n\n```python\nfrom datasets import load_dataset\n\ndata = load_dataset('jamescalam/youtube-transcriptions', split='train')\ndata\n```\n\n    Using custom data configuration jamescalam--youtube-transcriptions-6a482f3df0aedcdb\n    Reusing dataset json (/Users/jamesbriggs/.cache/huggingface/datasets/jamescalam___json/jamescalam--youtube-transcriptions-6a482f3df0aedcdb/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n\n\n\n\n\n    Dataset({\n        features: ['title', 'published', 'url', 'video_id', 'channel_id', 'id', 'text', 'start', 'end'],\n        num_rows: 208619\n    })\n\n\n\n\n```python\ndata[0]\n```\n\n\n\n\n    {'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4',\n     'published': '2021-07-06 13:00:03 UTC',\n     'url': 'https://youtu.be/35Pdoyi6ZoQ',\n     'video_id': '35Pdoyi6ZoQ',\n     'channel_id': 'UCv83tO5cePwHMt1952IVVHw',\n     'id': '35Pdoyi6ZoQ-t0.0',\n     'text': 'Hi, welcome to the video.',\n     'start': 0.0,\n     'end': 9.36}\n\n\n\nThe dataset contains many small snippets of text data. We will need to merge many snippets from each video to create more substantial chunks of text that contain more information.\n\n\n```python\nfrom tqdm.auto import tqdm\n\nnew_data = []\n\nwindow = 20  # number of sentences to combine\nstride = 4  # number of sentences to 'stride' over, used to create overlap\n\nfor i in tqdm(range(0, len(data), stride)):\n    i_end = min(len(data)-1, i+window)\n    if data[i]['title'] != data[i_end]['title']:\n        # in this case we skip this entry as we have start/end of two videos\n        continue\n    text = ' '.join(data[i:i_end]['text'])\n    # create the new merged dataset\n    new_data.append({\n        'start': data[i]['start'],\n        'end': data[i_end]['end'],\n        'title': data[i]['title'],\n        'text': text,\n        'id': data[i]['id'],\n        'url': data[i]['url'],\n        'published': data[i]['published'],\n        'channel_id': data[i]['channel_id']\n    })\n```\n\n\n      0%|          | 0/52155 [00:00<?, ?it/s]\n\n\n\n```python\nnew_data[0]\n```\n\n\n\n\n    {'start': 0.0,\n     'end': 74.12,\n     'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4',\n     'text': \"Hi, welcome to the video. So this is the fourth video in a Transformers from Scratch mini series. So if you haven't been following along, we've essentially covered what you can see on the screen. So we got some data. We built a tokenizer with it. And then we've set up our input pipeline ready to begin actually training our model, which is what we're going to cover in this video. So let's move over to the code. And we see here that we have essentially everything we've done so far. So we've built our input data, our input pipeline. And we're now at a point where we have a data loader, PyTorch data loader, ready. And we can begin training a model with it. So there are a few things to be aware of. So I mean, first, let's just have a quick look at the structure of our data.\",\n     'id': '35Pdoyi6ZoQ-t0.0',\n     'url': 'https://youtu.be/35Pdoyi6ZoQ',\n     'published': '2021-07-06 13:00:03 UTC',\n     'channel_id': 'UCv83tO5cePwHMt1952IVVHw'}\n\n### Indexing Data in Vector DB\n\nNow we need a place to store these embeddings and enable a efficient _vector search_ through them all. To do that we use Pinecone, we can get a [free API key](https://app.pinecone.io) and enter it below where we will initialize our connection to Pinecone and create a new index. You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**.\n\n\n```python\nimport pinecone\n\nindex_name = 'openai-youtube-transcriptions'\n\n# initialize connection to pinecone (get API key at app.pinecone.io)\npinecone.init(\n    api_key=\"PINECONE_API_KEY\",\n    environment=\"YOUR_ENVIRONMENT\"\n)\n\n# check if index already exists (it shouldn't if this is first time)\nif index_name not in pinecone.list_indexes():\n    # if does not exist, create index\n    pinecone.create_index(\n        index_name,\n        dimension=len(res['data'][0]['embedding']),\n        metric='cosine',\n        metadata_config={'indexed': ['channel_id', 'published']}\n    )\n# connect to index\nindex = pinecone.Index(index_name)\n# view index stats\nindex.describe_index_stats()\n```\n\n\n\n\n    {'dimension': 1536,\n     'index_fullness': 0.0,\n     'namespaces': {},\n     'total_vector_count': 0}\n\n\n\nWe can see the index is currently empty with a `total_vector_count` of `0`. We can begin populating it with OpenAI `text-embedding-ada-002` built embeddings like so:\n\n\n```python\nfrom tqdm.auto import tqdm\nimport datetime\nfrom time import sleep\n\nbatch_size = 100  # how many embeddings we create and insert at once\n\nfor i in tqdm(range(0, len(new_data), batch_size)):\n    # find end of batch\n    i_end = min(len(new_data), i+batch_size)\n    meta_batch = new_data[i:i_end]\n    # get ids\n    ids_batch = [x['id'] for x in meta_batch]\n    # get texts to encode\n    texts = [x['text'] for x in meta_batch]\n    # create embeddings (try-except added to avoid RateLimitError)\n    try:\n        res = openai.Embedding.create(input=texts, engine=embed_model)\n    except:\n        done = False\n        while not done:\n            sleep(5)\n            try:\n                res = openai.Embedding.create(input=texts, engine=embed_model)\n                done = True\n            except:\n                pass\n    embeds = [record['embedding'] for record in res['data']]\n    # cleanup metadata\n    meta_batch = [{\n        'start': x['start'],\n        'end': x['end'],\n        'title': x['title'],\n        'text': x['text'],\n        'url': x['url'],\n        'published': x['published'],\n        'channel_id': x['channel_id']\n    } for x in meta_batch]\n    to_upsert = list(zip(ids_batch, embeds, meta_batch))\n    # upsert to Pinecone\n    index.upsert(vectors=to_upsert)\n```\n\n\n      0%|          | 0/487 [00:00<?, ?it/s]\n\n### Making Queries\n\nNow we search, for this we need to create a _query vector_ `xq`:\n\n\n```python\nres = openai.Embedding.create(\n    input=[query],\n    engine=embed_model\n)\n\n# retrieve from Pinecone\nxq = res['data'][0]['embedding']\n\n# get relevant contexts (including the questions)\nres = index.query(xq, top_k=2, include_metadata=True)\n```\n\n\n```python\nres\n```\n\n\n\n\n    {'matches': [{'id': 'pNvujJ1XyeQ-t418.88',\n                  'metadata': {'channel_id': 'UCv83tO5cePwHMt1952IVVHw',\n                               'end': 568.4,\n                               'published': datetime.date(2021, 11, 24),\n                               'start': 418.88,\n                               'text': 'pairs of related sentences you can go '\n                                       'ahead and actually try training or '\n                                       'fine-tuning using NLI with multiple '\n                                       \"negative ranking loss. If you don't have \"\n                                       'that fine. Another option is that you have '\n                                       'a semantic textual similarity data set or '\n                                       'STS and what this is is you have so you '\n                                       'have sentence A here, sentence B here and '\n                                       'then you have a score from from 0 to 1 '\n                                       'that tells you the similarity between '\n                                       'those two scores and you would train this '\n                                       'using something like cosine similarity '\n                                       \"loss. Now if that's not an option and your \"\n                                       'focus or use case is on building a '\n                                       'sentence transformer for another language '\n                                       'where there is no current sentence '\n                                       'transformer you can use multilingual '\n                                       'parallel data. So what I mean by that is '\n                                       'so parallel data just means translation '\n                                       'pairs so if you have for example a English '\n                                       'sentence and then you have another '\n                                       'language here so it can it can be anything '\n                                       \"I'm just going to put XX and that XX is \"\n                                       'your target language you can fine-tune a '\n                                       'model using something called multilingual '\n                                       'knowledge distillation and what that does '\n                                       'is takes a monolingual model for example '\n                                       'in English and using those translation '\n                                       'pairs it distills the knowledge the '\n                                       'semantic similarity knowledge from that '\n                                       'monolingual English model into a '\n                                       'multilingual model which can handle both '\n                                       'English and your target language. So '\n                                       \"they're three options quite popular very \"\n                                       'common that you can go for and as a '\n                                       'supervised methods the chances are that '\n                                       'probably going to outperform anything you '\n                                       'do with unsupervised training at least for '\n                                       'now. So if none of those sound like '\n                                       'something',\n                               'title': 'Today Unsupervised Sentence Transformers, '\n                                        'Tomorrow Skynet (how TSDAE works)',\n                               'url': 'https://youtu.be/pNvujJ1XyeQ'},\n                  'score': 0.865277052,\n                  'sparseValues': {},\n                  'values': []},\n                 {'id': 'WS1uVMGhlWQ-t737.28',\n                  'metadata': {'channel_id': 'UCv83tO5cePwHMt1952IVVHw',\n                               'end': 900.72,\n                               'published': datetime.date(2021, 10, 20),\n                               'start': 737.28,\n                               'text': \"were actually more accurate. So we can't \"\n                                       \"really do that. We can't use this what is \"\n                                       'called a mean pooling approach. Or we '\n                                       \"can't use it in its current form. Now the \"\n                                       'solution to this problem was introduced by '\n                                       'two people in 2019 Nils Reimers and Irenia '\n                                       'Gurevich. They introduced what is the '\n                                       'first sentence transformer or sentence '\n                                       'BERT. And it was found that sentence BERT '\n                                       'or S BERT outformed all of the previous '\n                                       'Save the Art models on pretty much all '\n                                       'benchmarks. Not all of them but most of '\n                                       'them. And it did it in a very quick time. '\n                                       'So if we compare it to BERT, if we wanted '\n                                       'to find the most similar sentence pair '\n                                       'from 10,000 sentences in that 2019 paper '\n                                       'they found that with BERT that took 65 '\n                                       'hours. With S BERT embeddings they could '\n                                       'create all the embeddings in just around '\n                                       'five seconds. And then they could compare '\n                                       'all those with cosine similarity in 0.01 '\n                                       \"seconds. So it's a lot faster. We go from \"\n                                       '65 hours to just over five seconds which '\n                                       'is I think pretty incredible. Now I think '\n                                       \"that's pretty much all the context we need \"\n                                       'behind sentence transformers. And what we '\n                                       'do now is dive into a little bit of how '\n                                       'they actually work. Now we said before we '\n                                       'have the core transform models and what S '\n                                       'BERT does is fine tunes on sentence pairs '\n                                       'using what is called a Siamese '\n                                       'architecture or Siamese network. What we '\n                                       'mean by a Siamese network is that we have '\n                                       'what we can see, what can view as two BERT '\n                                       'models that are identical and the weights '\n                                       'between those two models are tied. Now in '\n                                       'reality when implementing this we just use '\n                                       'a single BERT model. And what we do is we '\n                                       'process one sentence, a sentence A through '\n                                       'the model and then we process another '\n                                       'sentence, sentence B through the model. '\n                                       \"And that's the sentence pair. So with our \"\n                                       'cross-linked we were processing the '\n                                       'sentence pair together. We were putting '\n                                       'them both together, processing them all at '\n                                       'once. This time we process them '\n                                       'separately. And during training what '\n                                       'happens is the weights',\n                               'title': 'Intro to Sentence Embeddings with '\n                                        'Transformers',\n                               'url': 'https://youtu.be/WS1uVMGhlWQ'},\n                  'score': 0.85855335,\n                  'sparseValues': {},\n                  'values': []}],\n     'namespace': ''}\n\nTo make this and the next step of building an expanded query simpler, we pack everything into a function named `retrieve`.\n\n\n```python\nlimit = 3750\n\ndef retrieve(query):\n    res = openai.Embedding.create(\n        input=[query],\n        engine=embed_model\n    )\n\n    # retrieve from Pinecone\n    xq = res['data'][0]['embedding']\n\n    # get relevant contexts\n    res = index.query(xq, top_k=3, include_metadata=True)\n    contexts = [\n        x['metadata']['text'] for x in res['matches']\n    ]\n\n    # build our prompt with the retrieved contexts included\n    prompt_start = (\n        \"Answer the question based on the context below.\\n\\n\"+\n        \"Context:\\n\"\n    )\n    prompt_end = (\n        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n    )\n    # append contexts until hitting limit\n    for i in range(1, len(contexts)):\n        if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) >= limit:\n            prompt = (\n                prompt_start +\n                \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +\n                prompt_end\n            )\n            break\n        elif i == len(contexts)-1:\n            prompt = (\n                prompt_start +\n                \"\\n\\n---\\n\\n\".join(contexts) +\n                prompt_end\n            )\n    return prompt\n```\n\nUsing `retrieve` we return the expanded query:\n\n\n```python\n# first we retrieve relevant items from Pinecone\nquery_with_contexts = retrieve(query)\nquery_with_contexts\n```\n\n\n\n\n    \"Answer the question based on the context below.\\n\\nContext:\\npairs of related sentences you can go ahead and actually try training or fine-tuning using NLI with multiple negative ranking loss. If you don't have that fine. Another option is that you have a semantic textual similarity data set or STS and what this is is you have so you have sentence A here, sentence B here and then you have a score from from 0 to 1 that tells you the similarity between those two scores and you would train this using something like cosine similarity loss. Now if that's not an option and your focus or use case is on building a sentence transformer for another language where there is no current sentence transformer you can use multilingual parallel data. So what I mean by that is so parallel data just means translation pairs so if you have for example a English sentence and then you have another language here so it can it can be anything I'm just going to put XX and that XX is your target language you can fine-tune a model using something called multilingual knowledge distillation and what that does is takes a monolingual model for example in English and using those translation pairs it distills the knowledge the semantic similarity knowledge from that monolingual English model into a multilingual model which can handle both English and your target language. So they're three options quite popular very common that you can go for and as a supervised methods the chances are that probably going to outperform anything you do with unsupervised training at least for now. So if none of those sound like something\\n\\n---\\n\\nwere actually more accurate. So we can't really do that. We can't use this what is called a mean pooling approach. Or we can't use it in its current form. Now the solution to this problem was introduced by two people in 2019 Nils Reimers and Irenia Gurevich. They introduced what is the first sentence transformer or sentence BERT. And it was found that sentence BERT or S BERT outformed all of the previous Save the Art models on pretty much all benchmarks. Not all of them but most of them. And it did it in a very quick time. So if we compare it to BERT, if we wanted to find the most similar sentence pair from 10,000 sentences in that 2019 paper they found that with BERT that took 65 hours. With S BERT embeddings they could create all the embeddings in just around five seconds. And then they could compare all those with cosine similarity in 0.01 seconds. So it's a lot faster. We go from 65 hours to just over five seconds which is I think pretty incredible. Now I think that's pretty much all the context we need behind sentence transformers. And what we do now is dive into a little bit of how they actually work. Now we said before we have the core transform models and what S BERT does is fine tunes on sentence pairs using what is called a Siamese architecture or Siamese network. What we mean by a Siamese network is that we have what we can see, what can view as two BERT models that are identical and the weights between those two models are tied. Now in reality when implementing this we just use a single BERT model. And what we do is we process one sentence, a sentence A through the model and then we process another sentence, sentence B through the model. And that's the sentence pair. So with our cross-linked we were processing the sentence pair together. We were putting them both together, processing them all at once. This time we process them separately. And during training what happens is the weights\\n\\n---\\n\\nTransformer-based Sequential Denoising Autoencoder. So what we'll do is jump straight into it and take a look at where we might want to use this training approach and and how we can actually implement it. So the first question we need to ask is do we really need to resort to unsupervised training? Now what we're going to do here is just have a look at a few of the most popular training approaches and what sort of data we need for that. So the first one we're looking at here is Natural Language Inference or NLI and NLI requires that we have pairs of sentences that are labeled as either contradictory, neutral which means they're not necessarily related or as entailing or as inferring each other. So you have pairs that entail each other so they are both very similar pairs that are neutral and also pairs that are contradictory. And this is the traditional NLI data. Now using another version of fine-tuning with NLI called a multiple negatives ranking loss you can get by with only entailment pairs so pairs that are related to each other or positive pairs and it can also use contradictory pairs to improve the performance of training as well but you don't need it. So if you have positive pairs of related sentences you can go ahead and actually try training or fine-tuning using NLI with multiple negative ranking loss. If you don't have that fine. Another option is that you have a semantic textual similarity data set or STS and what this is is you have so you have sentence A here, sentence B\\n\\nQuestion: Which training method should I use for sentence transformers when I only have pairs of related sentences?\\nAnswer:\"\n\n\nNow we pass our expanded query to the LLM:\n\n```python\n# then we complete the context-infused query\ncomplete(query_with_contexts)\n```\n\n\n\n\n    'You should use Natural Language Inference (NLI) with multiple negative ranking loss.'\n\n\n\nAnd we get a pretty great answer straight away, specifying to use _multiple-rankings loss_ (also called _multiple negatives ranking loss_).\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1cce3"
  },
  "filename": "abstractive-question-answering.md",
  "title": "Abstractive Question Answering",
  "category": "630fc5235d91a70054705fb7",
  "content": "---\ntitle: Abstractive Question Answering\ncategory: 630fc5235d91a70054705fb7\n---\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/search/question-answering/abstractive-question-answering.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/search/question-answering/abstractive-question-answering.ipynb) [![Open Github](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/github-shield.svg)](https://github.com/pinecone-io/examples/tree/master/search/question-answering/abstractive-question-answering.ipynb)\n\nAbstractive question-answering focuses on the generation of multi-sentence answers to open-ended questions. It usually works by searching massive document stores for relevant information and then using this information to synthetically generate answers. This notebook demonstrates how Pinecone helps you build an abstractive question-answering system. We need three main components:\n\n- A vector index to store and run semantic search\n- A retriever model for embedding context passages\n- A generator model to generate answers\n\n# Install Dependencies\n\n\n```python\n!pip install -qU datasets pinecone-client sentence-transformers torch\n```\n    \n\n# Load and Prepare Dataset\n\nOur source data will be taken from the Wiki Snippets dataset, which contains over 17 million passages from Wikipedia. But, since indexing the entire dataset may take some time, we will only utilize 50,000 passages in this demo that include \"History\" in the \"section title\" column. If you want, you may utilize the complete dataset. Pinecone vector database can effortlessly manage millions of documents for you.\n\n\n```python\nfrom datasets import load_dataset\n\n# load the dataset from huggingface in streaming mode and shuffle it\nwiki_data = load_dataset(\n    'vblagoje/wikipedia_snippets_streamed',\n    split='train',\n    streaming=True\n).shuffle(seed=960)\n```\n\n\nWe are loading the dataset in the streaming mode so that we don't have to wait for the whole dataset to download (which is over 9GB). Instead, we iteratively download records one at a time.\n\n\n```python\n# show the contents of a single document in the dataset\nnext(iter(wiki_data))\n```\n\n\n\n\n    {'wiki_id': 'Q7649565',\n     'start_paragraph': 20,\n     'start_character': 272,\n     'end_paragraph': 24,\n     'end_character': 380,\n     'article_title': 'Sustainable Agriculture Research and Education',\n     'section_title': \"2000s & Evaluation of the program's effectiveness\",\n     'passage_text': \"preserving the surrounding prairies. It ran until March 31, 2001.\\nIn 2008, SARE celebrated its 20th anniversary. To that date, the program had funded 3,700 projects and was operating with an annual budget of approximately $19 million. Evaluation of the program's effectiveness As of 2008, 64% of farmers who had received SARE grants stated that they had been able to earn increased profits as a result of the funding they received and utilization of sustainable agriculture methods. Additionally, 79% of grantees said that they had experienced a significant improvement in soil quality though the environmentally friendly, sustainable methods that they were\"}\n\n\n\n\n```python\n# filter only documents with History as section_title\nhistory = wiki_data.filter(\n    lambda d: d['section_title'].startswith('History')\n)\n```\n\nLet's iterate through the dataset and apply our filter to select the 50,000 historical passages. We will extract `article_title`, `section_title` and `passage_text` from each document.\n\n\n```python\nfrom tqdm.auto import tqdm  # progress bar\n\ntotal_doc_count = 50000\n\ncounter = 0\ndocs = []\n# iterate through the dataset and apply our filter\nfor d in tqdm(history, total=total_doc_count):\n    # extract the fields we need\n    doc = {\n        \"article_title\": d[\"article_title\"],\n        \"section_title\": d[\"section_title\"],\n        \"passage_text\": d[\"passage_text\"]\n    }\n    # add the dict containing fields we need to docs list\n    docs.append(doc)\n\n    # stop iteration once we reach 50k\n    if counter == total_doc_count:\n        break\n\n    # increase the counter on every iteration\n    counter += 1\n```\n\n\n      100%|██████████| 50000/50000 [05:18<00:00, 145.03it/s]\n\n\n\n```python\nimport pandas as pd\n\n# create a pandas dataframe with the documents we extracted\ndf = pd.DataFrame(docs)\ndf.head()\n```\n\n\n\n\n\n<div id=\"df-7daab3f7-5df6-4a4c-bc59-831df588cd12\">\n  <div class=\"colab-df-container\">\n    <div>\n      <style scoped>\n          .dataframe tbody tr th:only-of-type {\n              vertical-align: middle;\n          }\n          .dataframe tbody tr th {\n              vertical-align: top;\n          }\n          .dataframe thead th {\n              text-align: right;\n          }\n      </style>\n      <table border=\"1\" class=\"dataframe\">\n        <thead>\n          <tr style=\"text-align: right;\">\n            <th></th>\n            <th>article_title</th>\n            <th>section_title</th>\n            <th>passage_text</th>\n          </tr>\n        </thead>\n        <tbody>\n          <tr>\n            <th>0</th>\n            <td>Taupo District</td>\n            <td>History</td>\n            <td>was not until the 1950s that the region starte...</td>\n          </tr>\n          <tr>\n            <th>1</th>\n            <td>Sutarfeni</td>\n            <td>History &amp; Western asian analogues</td>\n            <td>Sutarfeni History strand-like pheni were Phena...</td>\n          </tr>\n          <tr>\n            <th>2</th>\n            <td>The Bishop Wand Church of England School</td>\n            <td>History</td>\n            <td>The Bishop Wand Church of England School Histo...</td>\n          </tr>\n          <tr>\n            <th>3</th>\n            <td>Teufelsmoor</td>\n            <td>History &amp; Situation today</td>\n            <td>made to preserve the original landscape, altho...</td>\n          </tr>\n          <tr>\n            <th>4</th>\n            <td>Surface Hill Uniting Church</td>\n            <td>History</td>\n            <td>in perpetual reminder that work and worship go...</td>\n          </tr>\n        </tbody>\n      </table>\n    </div>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n  <script>\n        const buttonEl =\n          document.querySelector('#df-7daab3f7-5df6-4a4c-bc59-831df588cd12 button.colab-df-convert');\n        buttonEl.style.display =\n          google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n        async function convertToInteractive(key) {\n          const element = document.querySelector('#df-7daab3f7-5df6-4a4c-bc59-831df588cd12');\n          const dataTable =\n            await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                    [key], {});\n          if (!dataTable) return;\n\n          const docLinkHtml = 'Like what you see? Visit the ' +\n            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n            + ' to learn more about interactive tables.';\n          element.innerHTML = '';\n          dataTable['output_type'] = 'display_data';\n          await google.colab.output.renderOutput(dataTable, element);\n          const docLink = document.createElement('div');\n          docLink.innerHTML = docLinkHtml;\n          element.appendChild(docLink);\n        }\n  </script>\n  </div>  \n</div>\n\n\n\n\n# Initialize Pinecone Index\n\nThe Pinecone index stores vector representations of our historical passages which we can retrieve later using another vector (query vector). To build our vector index, we must first establish a connection with Pinecone. For this, we need an API from Pinecone. You can get one for free from [here](https://app.pinecone.io/). You also need to know the environment for your index; for new accounts, the default environment is `us-east1-gcp`. \n\nWe initialize the connection as follows:\n\n\n```python\nimport pinecone\n\n# connect to pinecone environment\npinecone.init(\n    api_key=\"<<YOUR_API_KEY>>\",\n    environment=\"<<YOUR_ENVIRONMENT>>\"\n)\n```\n\nNow we create a new index. We will name it \"abstractive-question-answering\" — you can name it anything we want. We specify the metric type as \"cosine\" and dimension as 768 because the retriever we use to generate context embeddings is optimized for cosine similarity and outputs 768-dimension vectors.\n\n\n```python\nindex_name = \"abstractive-question-answering\"\n\n# check if the abstractive-question-answering index exists\nif index_name not in pinecone.list_indexes():\n    # create the index if it does not exist\n    pinecone.create_index(\n        index_name,\n        dimension=768,\n        metric=\"cosine\"\n    )\n\n# connect to abstractive-question-answering index we created\nindex = pinecone.Index(index_name)\n```\n\n# Initialize Retriever\n\nNext, we need to initialize our retriever. The retriever will mainly do two things:\n\n- Generate embeddings for all historical passages (context vectors/embeddings)\n- Generate embeddings for our questions (query vector/embedding)\n\nThe retriever will create embeddings such that the questions and passages that hold the answers to our queries are close to one another in the vector space. We will use a SentenceTransformer model based on Microsoft's MPNet as our retriever. This model performs quite well for comparing the similarity between queries and documents. We can use Cosine Similarity to compute the similarity between query and context vectors generated by this model (Pinecone automatically does this for us).\n\n\n```python\nimport torch\nfrom sentence_transformers import SentenceTransformer\n\n# set device to GPU if available\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n# load the retriever model from huggingface model hub\nretriever = SentenceTransformer(\"flax-sentence-embeddings/all_datasets_v3_mpnet-base\", device=device)\nretriever\n```\n\n\n    SentenceTransformer(\n      (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: MPNetModel \n      (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n      (2): Normalize()\n    )\n\n\n\n# Generate Embeddings and Upsert\n\nNext, we need to generate embeddings for the context passages. We will do this in batches to help us more quickly generate embeddings and upload them to the Pinecone index. When passing the documents to Pinecone, we need an id (a unique value), context embedding, and metadata for each document representing context passages in the dataset. The metadata is a dictionary containing data relevant to our embeddings, such as the article title, section title, passage text, etc.\n\n\n```python\n# we will use batches of 64\nbatch_size = 64\n\nfor i in tqdm(range(0, len(df), batch_size)):\n    # find end of batch\n    i_end = min(i+batch_size, len(df))\n    # extract batch\n    batch = df.iloc[i:i_end]\n    # generate embeddings for batch\n    emb = retriever.encode(batch[\"passage_text\"].tolist()).tolist()\n    # get metadata\n    meta = batch.to_dict(orient=\"records\")\n    # create unique IDs\n    ids = [f\"{idx}\" for idx in range(i, i_end)]\n    # add all to upsert list\n    to_upsert = list(zip(ids, emb, meta))\n    # upsert/insert these records to pinecone\n    _ = index.upsert(vectors=to_upsert)\n\n# check that we have all vectors in index\nindex.describe_index_stats()\n```\n\n\n      100%|██████████| 782/782 [13:56<00:00, 1.13it/s]\n\n    {'dimension': 768,\n     'index_fullness': 0.1,\n     'namespaces': {'': {'vector_count': 50001}},\n     'total_vector_count': 50001}\n\n\n\n# Initialize Generator\n\nWe will use ELI5 BART for the generator which is a Sequence-To-Sequence model trained using the ‘Explain Like I’m 5’ (ELI5) dataset. Sequence-To-Sequence models can take a text sequence as input and produce a different text sequence as output.\n\nThe input to the ELI5 BART model is a single string which is a concatenation of the query and the relevant documents providing the context for the answer. The documents are separated by a special token &lt;P>, so the input string will look as follows:\n\n>question: What is a sonic boom? context: &lt;P> A sonic boom is a sound associated with shock waves created when an object travels through the air faster than the speed of sound. &lt;P> Sonic booms generate enormous amounts of sound energy, sounding similar to an explosion or a thunderclap to the human ear. &lt;P> Sonic booms due to large supersonic aircraft can be particularly loud and startling, tend to awaken people, and may cause minor damage to some structures. This led to prohibition of routine supersonic flight overland.\n\nMore detail on how the ELI5 dataset was built is available [here](https://arxiv.org/abs/1907.09190) and how ELI5 BART model was trained is available [here](https://yjernite.github.io/lfqa.html).\n\nLet's initialize the BART model using transformers.\n\n\n```python\nfrom transformers import BartTokenizer, BartForConditionalGeneration\n\n# load bart tokenizer and model from huggingface\ntokenizer = BartTokenizer.from_pretrained('vblagoje/bart_lfqa')\ngenerator = BartForConditionalGeneration.from_pretrained('vblagoje/bart_lfqa').to(device)\n```\n\n\nAll the components of our abstract QA system are complete and ready to be queried. But first, let's write some helper functions to retrieve context passages from Pinecone index and to format the query in the way the generator expects the input.\n\n\n```python\ndef query_pinecone(query, top_k):\n    # generate embeddings for the query\n    xq = retriever.encode([query]).tolist()\n    # search pinecone index for context passage with the answer\n    xc = index.query(xq, top_k=top_k, include_metadata=True)\n    return xc\n```\n\n\n```python\ndef format_query(query, context):\n    # extract passage_text from Pinecone search result and add the <P> tag\n    context = [f\"<P> {m['metadata']['passage_text']}\" for m in context]\n    # concatinate all context passages\n    context = \" \".join(context)\n    # contcatinate the query and context passages\n    query = f\"question: {query} context: {context}\"\n    return query\n```\n\nLet's test the helper functions. We will query the Pinecone index function we created earlier with the `query_pinecone` to get context passages and pass them to the `format_query` function.\n\n\n```python\nquery = \"when was the first electric power system built?\"\nresult = query_pinecone(query, top_k=1)\nresult\n```\n\n\n\n\n    {'matches': [{'id': '3593',\n                  'metadata': {'article_title': 'Electric power system',\n                               'passage_text': 'Electric power system History In '\n                                               '1881, two electricians built the '\n                                               \"world's first power system at \"\n                                               'Godalming in England. It was '\n                                               'powered by two waterwheels and '\n                                               'produced an alternating current '\n                                               'that in turn supplied seven '\n                                               'Siemens arc lamps at 250 volts and '\n                                               '34 incandescent lamps at 40 volts. '\n                                               'However, supply to the lamps was '\n                                               'intermittent and in 1882 Thomas '\n                                               'Edison and his company, The Edison '\n                                               'Electric Light Company, developed '\n                                               'the first steam-powered electric '\n                                               'power station on Pearl Street in '\n                                               'New York City. The Pearl Street '\n                                               'Station initially powered around '\n                                               '3,000 lamps for 59 customers. The '\n                                               'power station generated direct '\n                                               'current and',\n                               'section_title': 'History'},\n                  'score': 0.69118017,\n                  'values': []}],\n     'namespace': ''}\n\n\n\n\n```python\nfrom pprint import pprint\n```\n\n\n```python\n# format the query in the form generator expects the input\nquery = format_query(query, result[\"matches\"])\npprint(query)\n```\n\n    ('question: when was the first electric power system built? context: <P> '\n     \"Electric power system History In 1881, two electricians built the world's \"\n     'first power system at Godalming in England. It was powered by two '\n     'waterwheels and produced an alternating current that in turn supplied seven '\n     'Siemens arc lamps at 250 volts and 34 incandescent lamps at 40 volts. '\n     'However, supply to the lamps was intermittent and in 1882 Thomas Edison and '\n     'his company, The Edison Electric Light Company, developed the first '\n     'steam-powered electric power station on Pearl Street in New York City. The '\n     'Pearl Street Station initially powered around 3,000 lamps for 59 customers. '\n     'The power station generated direct current and')\n    \n\nThe output looks great. Now let's write a function to generate answers.\n\n\n```python\ndef generate_answer(query):\n    # tokenize the query to get input_ids\n    inputs = tokenizer([query], max_length=1024, return_tensors=\"pt\")\n    # use generator to predict output ids\n    ids = generator.generate(inputs[\"input_ids\"], num_beams=2, min_length=20, max_length=40)\n    # use tokenizer to decode the output ids\n    answer = tokenizer.batch_decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n    return pprint(answer)\n```\n\n\n```python\ngenerate_answer(query)\n```\n\n    ('The first electric power system was built in 1881 at Godalming in England. '\n     'It was powered by two waterwheels and produced alternating current that in '\n     'turn supplied seven Siemens arc lamps')\n    \n\nAs we can see, the generator used the provided context to answer our question. Let's run some more queries.\n\n\n```python\nquery = \"How was the first wireless message sent?\"\ncontext = query_pinecone(query, top_k=5)\nquery = format_query(query, context[\"matches\"])\ngenerate_answer(query)\n```\n\n    ('The first wireless message was sent in 1866 by Mahlon Loomis, who had a kite '\n     'on a mountaintop 14 miles apart. The kite was connected to a cable')\n    \n\nTo confirm that this answer is correct, we can check the contexts used to generate the answer.\n\n\n```python\nfor doc in context[\"matches\"]:\n    print(doc[\"metadata\"][\"passage_text\"], end='\\n---\\n')\n```\n\n    by electrostatic induction or electromagnetic induction, which had too short a range to be practical. In 1866 Mahlon Loomis claimed to have transmitted an electrical signal through the atmosphere between two 600 foot wires held aloft by kites on mountaintops 14 miles apart. Thomas Edison had come close to discovering radio in 1875; he had generated and detected radio waves which he called \"etheric currents\" experimenting with high-voltage spark circuits, but due to lack of time did not pursue the matter. David Edward Hughes in 1879 had also stumbled on radio wave transmission which he received with his carbon microphone\n    ---\n    the east coast of India, then on to Penang, Malacca, Singapore, Batavia (current Jakarta), to finally reach Darwin, Australia. It was the first direct link between Australia and Great Britain. The company that laid the first part of the cable took the name of Falmouth, Gibraltar and Malta Telegraph Company and had been founded in 1869. This company later operated as the Eastern Telegraph Company from Mount Pleasant in Gibraltar and eventually became Cable & Wireless.\n    The first telephones were introduced to Gibraltar in 1886 by a private company which was later taken over by the colonial authorities. The first wireless\n    ---\n    audio distance records, and were heard as far west as Hawaii. They were also received in Paris, France, which marked the first transmission of speech across the Atlantic.\n    With the entrance of the United States into World War I in April 1917 the federal government took over full control of the radio industry, and it became illegal for civilians to possess an operational radio receiver. However NAA continued to operate during the conflict. In addition to time signals and weather reports, it also broadcast news summaries received by troops on land and aboard ships in the Atlantic. Effective April 15, 1919\n    ---\n    Message from space (science fiction) For other uses, see Message from Space (disambiguation).\n    \"Message from space\" is a type of \"first contact\" theme in science fiction . Stories of this type involve receiving an interstellar message which reveals the existence of other intelligent life in the universe. History An early short story, A Message from Space (Joseph Schlossel, Weird Tales, March 1926) tells of an amateur who builds a ham TV set and suddenly sees an alien, The latter one realises it is being watched and tells its soap opera story. The verdict of Everett Franklin Bleiler: \"original ideas, but clumsy\n    ---\n    radio operators, interested in a practical benefit from their hobby, and jewelers, who previously had been reliant on time services transmitted over telegraph wires, which had a reputation for being both expensive and of questionable reliability, especially compared to the free and very accurate NAA transmissions.\n    NAA's original transmitters were only capable of producing the dots-and-dashes of Morse code. The later development of vacuum tube transmitters made audio transmissions practical, and in 1915 the American Telephone and Telegraph Company (AT&T) received permission from the Navy to conduct a series of tests at the NAA facility. These experimental transmissions set impressive new\n    ---\n    \n\nIn this case, the answer looks correct. If we ask a question and no relevant contexts are retrieved, the generator will typically return nonsensical or false answers, like with this question about COVID-19:\n\n\n```python\nquery = \"where did COVID-19 originate?\"\ncontext = query_pinecone(query, top_k=3)\nquery = format_query(query, context[\"matches\"])\ngenerate_answer(query)\n```\n\n    ('COVID-19 is a zoonotic disease, which means that it is a virus that is '\n     'transmitted from one animal to another. It is not a virus that can be '\n     'transmitted from person')\n    \n\n\n```python\nfor doc in context[\"matches\"]:\n    print(doc[\"metadata\"][\"passage_text\"], end='\\n---\\n')\n```\n\n    to establish with certainty which diseases jumped from other animals to humans, but there is increasing evidence from DNA and RNA sequencing, that measles, smallpox, influenza, HIV, and diphtheria came to humans this way. Various forms of the common cold and tuberculosis also are adaptations of strains originating in other species.\n    Zoonoses are of interest because they are often previously unrecognized diseases or have increased virulence in populations lacking immunity. The West Nile virus appeared in the United States in 1999 in the New York City area, and moved through the country in the summer of 2002, causing much distress. Bubonic\n    ---\n    plague is a zoonotic disease, as are salmonellosis, Rocky Mountain spotted fever, and Lyme disease.\n    A major factor contributing to the appearance of new zoonotic pathogens in human populations is increased contact between humans and wildlife. This can be caused either by encroachment of human activity into wilderness areas or by movement of wild animals into areas of human activity. An example of this is the outbreak of Nipah virus in peninsular Malaysia in 1999, when intensive pig farming began on the habitat of infected fruit bats. Unidentified infection of the pigs amplified the force of infection, eventually transmitting the virus\n    ---\n    man killed and twenty-nine died of disease.\n    ---\n    \n\nLet’s finish with a final few questions.\n\n\n```python\nquery = \"what was the war of currents?\"\ncontext = query_pinecone(query, top_k=5)\nquery = format_query(query, context[\"matches\"])\ngenerate_answer(query)\n```\n\n    ('The War of Currents was a series of events in the early 1900s between Edison '\n     'and Westinghouse. The two companies were competing for the market share of '\n     'electric power in the United States')\n    \n\n\n```python\nquery = \"who was the first person on the moon?\"\ncontext = query_pinecone(query, top_k=10)\nquery = format_query(query, context[\"matches\"])\ngenerate_answer(query)\n```\n\n    ('The first person to walk on the moon was Neil Armstrong in 1969. He walked '\n     'on the moon in 1969. He was the first person to walk on the moon.')\n    \n\n\n```python\nquery = \"what was NASAs most expensive project?\"\ncontext = query_pinecone(query, top_k=3)\nquery = format_query(query, context[\"matches\"])\ngenerate_answer(query)\n```\n\n    ('The Space Shuttle was the most expensive project in the history of NASA. It '\n     'cost about $10 billion to build.')\n    \n\nAs we can see, the model can generate some great answers.\n\n# Example Application\n\nTo try out an example application of abstractive QA, see this [demo app](https://huggingface.co/spaces/pinecone/abstractive-question-answering).\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1cce4"
  },
  "filename": "feast-feature-store.md",
  "title": "Operationalize vector search with Pinecone and Feast Feature Store",
  "category": "630fc5235d91a70054705fb7",
  "content": "---\ntitle: Operationalize vector search with Pinecone and Feast Feature Store\ncategory: 630fc5235d91a70054705fb7\n---\n\nVector embeddings are the key ingredient that makes [similarity search](https://www.pinecone.io/learn/what-is-similarity-search/) possible. Raw data goes from a data store or data stream, through an embedding model to be converted into a [vector embedding](https://www.pinecone.io/learn/vector-embeddings/), and finally into the [vector search index](https://www.pinecone.io/learn/vector-database/).\n\n![Vector search with Pinecone](https://raw.githubusercontent.com/pinecone-io/img/main/pinecone-feast-vector-search.png)\n\nIf you have multiple data sources, frequent data updates, and are constantly experimenting with different models, then it becomes harder to maintain an accurate and up-to-date search index. That could lead to subpar results in your recommender systems, search applications, or wherever you are using vector search.\n\nHow you store and manage the assets — vector embeddings — is crucial to the accuracy and freshness of your vector search results. This is where “feature stores” come in. [Features stores](https://www.tecton.ai/blog/what-is-a-feature-store/) provide a centralized place for managing vector embeddings within organizations with sprawling data sources and frequently updated models. They enable efficient feature engineering and management, feature reuse, and consistency between online and batch embedding models.\n\nCombining a feature store with a similarity search service leads to more accurate and reliable retrieval within your AI/ML applications. In this article, we will build a [question-answering application](question-answering.md) to demonstrate how the [Feast feature store](https://feast.dev/) can be used alongside the Pinecone vector search solution.\n\n![Vector search with Feast feature store and Pinecone](https://raw.githubusercontent.com/pinecone-io/img/main/pinecone-feast-question-answering.png)\n\n\nThe steps are:\n\n1. Create a catalog of questions with known answers by loading the raw text and their vector embeddings into Feast.\n1. Index vector embeddings of those questions in Pinecone so we can search through them by [semantic similarity](https://www.pinecone.io/learn/semantic-search/).\n1. Transform new, incoming questions into vector embeddings and catalog them in Feast, then query Pinecone for the IDs of the most similar known questions, and finally fetch the text of those questions from Feast and display results to the user.\n\nLet’s begin! You can also [view the source code on GitHub](https://github.com/pinecone-io/examples/blob/master/feature-store/feast_feature_store_notebook.ipynb).\n\n## Setup\n\nLet's install and load necessary Python packages in your preferred cloud environment, like Google Colab.\n\n```python\n!pip install -qU feast\n!pip install -qU sentence-transformers --no-cache-dir\n!pip install -qU pinecone-client\n```\n\n> If you are using Google Colab, restart the runtime after the installation.\n>\n\n```python\nimport os\nimport pandas as pd\nimport numpy as np\n```\n\n## Dataset and Model\n\nWe use the [Quora Question Pairs Dataset](https://www.kaggle.com/c/quora-question-pairs) that enables a question-answering application. We index a set of questions that can be associated with answers. The application utilizes a new question's vector embedding to retrieve the top relevant stored question and its associated answer. \n\nThe embeddings stored in the feature store are created using the [Average Word Embeddings Models](https://www.sbert.net/docs/pretrained_models.html#average-word-embeddings-models). Since we want to query new questions and find the most similar match among questions in the feature store, we need to create a comparable vector. This means that once we define a new question, we will transform it into a vector embedding using the same model.\n\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('average_word_embeddings_komninos')\n```\n\n\n## Feast Feature Store\n\nIt's time to set up our Feast feature store. We will follow the [tutorial for creating a Feast feature store](https://docs.feast.dev/quickstart). We intend to use Feast for storing questions and their vector embeddings.  \n\nFor each question, we will store the following information:\n* A question identifier number. We will store these ids along with the corresponding embeddings in the similarity search index. \n* The question's text. \n* The question's vector embeddings. Here, the features being learned using a deep neural network and don't have an intuitive meaning. Thus, we denote them with their index number `e0 ... e300`.\n\nNote that if our data contained the answers, we should have stored them along with this information. \n\n```python\n# Initialize feast feature store\n!feast init feature_repo\nos.chdir('feature_repo')\n```\n\nChoose one of these two options to include the necessary file.\n\n*   You can find the file *questions.parquet* as part of the example. This file contains pre-computed embeddings for each question from the past. We will load this data into our feature store. Please add the *questions.parquet* file to the /feature_repo/data path.\n\n*   You can run the code from the section **Create parquet file yourself (Optional)**. Using this code, you can control the number of questions you include in the example.\n\nOnce we created the feature store and placed the parquet file where necessary, we have to overwrite the default *example.py* file. This file defines the file source, the entity definition, and the feature view to serve once online. \n\nWe will define another file - *test_example.py*, which will contain the feature view definition for the test questions. Test questions will be defined later, saved into a new parquet file, and loaded into a feature store.\n\n> *Note: We added a one-day expiration to the feature view (notice the TTL field).\n\n\n```python\n%%writefile ./example.py\n\n\nfrom feast import Entity, Feature, FeatureView, ValueType\nfrom feast.data_source import FileSource\nimport os\nimport platform\n\npath = os.getcwd() + \"/data/questions.parquet\"\nsource = FileSource(\n    path= path if platform.system() != 'Windows' else path.replace('/', '\\\\'),\n    event_timestamp_column=\"datetime\",\n)\n\nquestion = Entity(name=\"qid1\", value_type=ValueType.INT64)\n\nquestion_feature = Feature(\n    name=\"question1\",\n    dtype=ValueType.STRING\n)\n\nembedding_features = [\n        Feature(name=f\"e_{i}\", dtype=ValueType.FLOAT)\n        for i in range(300)\n      ]\n\nquestions_view = FeatureView(\n    name=\"questions\",\n    entities=[\"qid1\"],\n    ttl=timedelta(days=1),\n    features= [question_feature, *embedding_features],\n    \n    input=source,\n    \n)\n\n```\n\n    Overwriting ./example.py\n\n\n\n```python\n%%writefile ./test_example.py\n\nfrom google.protobuf.duration_pb2 import Duration\n\nfrom feast import Entity, Feature, FeatureView, ValueType\nfrom feast.data_source import FileSource\nimport os\nimport platform\n\npath = os.getcwd() + \"/data/test_questions.parquet\"\nsource = FileSource(\n    path= path if platform.system() != 'Windows' else path.replace('/', '\\\\'),\n    event_timestamp_column=\"datetime\",\n    created_timestamp_column=\"created\",\n)\n\ntest_question = Entity(name=\"qid1\", value_type=ValueType.INT64, description=\"question id\",)\n\nquestion_feature = Feature(\n    name=\"question1\",\n    dtype=ValueType.STRING\n)\n\nembedding_features = [\n        Feature(name=f\"e_{i}\", dtype=ValueType.FLOAT)\n        for i in range(300)\n      ]\n\ntest_questions_view = FeatureView(\n    name=\"test_questions\",\n    entities=[\"qid1\"],\n    ttl=Duration(seconds=86400 * 1),\n    features= [question_feature, *embedding_features],\n    online=True,\n    input=source,\n    tags={},\n)\n```\n\n    Overwriting ./test_example.py\n\n\nTo deploy our infrastructure, we need to run the following command.\n\n\n```python\n# Register the features\n!feast apply\n```\n\nFinally, we need to populate the online store with the most recent features from the offline store. We can do that with the following command.\n\n\n```python\n!feast materialize 2021-06-02T00:00:00 2021-07-10T00:00:00 --views questions\n```\n\n> Note: Don't forget to change the end date if you created the parquet file yourself!\n\n ## Uploading Vectors into Pinecone \n\nAfter setting up our feature store, we are ready to index our question vectors within Pinecone's similarity search service. Let's start by defining a Pinecone index, and then uploading the stored vectors into Pinecone. \n\n### Pinecone Setup\n\n```python\nimport pinecone\n```\n\nUse your API key to connect to Pinecone. In case you don't have one, [get your API key here](https://www.pinecone.io/start/).\n\n\n```python\n# Load Pinecone API key\napi_key = os.getenv(\"PINECONE_API_KEY\") or '<YOUR API KEY>'\npinecone.init(api_key=api_key)\npinecone.list_indexes()\n```\n\nCreate a new vector index.\n\n```python\n# Pick a name for the new index\nindex_name = 'feast-questions'\n```\n\n\n```python\nif index_name in pinecone.list_indexes():\n    pinecone.delete_index(index_name)\n```\n\n\n```python\n# Create a new vector index\npinecone.create_index(name=index_name, metric='cosine', shards=1)\n```\n### Upload from Feature Store\n\nWe fetch the questions' vectors from the feature store in batches and upload them into Pinecone's vector index.\n\n\n```python\n# Get question ids from the file\nquestion_ids = pd.read_parquet('./data/questions.parquet', columns=['qid1'])\n```\n\n\n```python\n# Define a batch size to read from Feast\nBATCH_SIZE = 1000\n```\n\n\n```python\n# Connect to the created index\nindex = pinecone.Index(name = index_name, response_timeout=300)\n```\n\n\n```python\n# Print info\nindex.info()\n```\n\n    InfoResult(index_size=0)\n\n```python\nfrom feast import FeatureStore\n\nstore = FeatureStore(repo_path=\".\")\n\nfor i in range(0, len(question_ids), BATCH_SIZE):\n    batch = question_ids[i: i+BATCH_SIZE]\n\n    feature_vectors = store.get_online_features(\n        feature_refs=[f'questions:e_{i}'\n                      for i in range(300)\n                     ],\n        entity_rows=[{\"qid1\":_id} for _id in batch.qid1.to_list()]\n    ).to_dict()\n\n    # Prepare list of items to upload into Pinecone's index\n    items_to_insert = []\n\n    for e in range(len(feature_vectors['qid1'])):\n        l = [feature_vectors[f'questions__e_{i}'][e] for i in range(300)]\n        items_to_insert.append((feature_vectors['qid1'][e], np.array(l)))\n    \n    # Upsert batch data\n    index.upsert(items=items_to_insert)  \n```\n\n\n```python\n# Print index info\nindex.info()\n```\n\n    InfoResult(index_size=10000)\n\n\n## Query\n\nWe are now all set to start querying our similarity search index. Our queries are questions in text format. We will transform the question into a vector embedding, serve this query vector into Pinecone's service, and retrieve a set of top-matched stored question IDs. Since Feast acts as the centralized source of truth for feature vectors, we will store the transformed question vectors in Feast and materialize query vectors before forwarding them to Pinecone. \n\nThis section describes how to:\n\n* Define new questions and create their embeddings\n* Manage these embeddings in Feast:\n  * Load these embeddings into Feast\n  * Fetch test question embeddings from Feast\n* Query Pinecone with the fetched vector embeddings\n\n\n### Define New questions and Create their embeddings\n\nLet's define new questions first.\n\n```python\ndf_new_questions = pd.DataFrame([[1000001, 'How can I make money using Youtube?'], \n                                 [1000002, 'What is the best book for learning Python?']], columns=['qid1', 'question1'])\ndf_new_questions\n```\n\n\n<div>\n<table class=\"table table-responsive\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>qid1</th>\n      <th>question1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000001</td>\n      <td>How can I make money using Youtube?</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000002</td>\n      <td>What is the best book for learning Python?</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\nThen, we create embeddings for these questions and save them in a new parquet file.\n\n```python\n# Create embedding for each question\ndf_new_questions['question_vector'] = df_new_questions.question1.apply(lambda x: model.encode(str(x), show_progress_bar=False))\n\n# Create timestamps \ndf_new_questions['created'] = datetime.datetime.utcnow()\ndf_new_questions['datetime'] = df_new_questions['created'].dt.floor('h')\n\n# Generate columns for vector elements\ndf_new_questions2 = df_new_questions.question_vector.apply(pd.Series)\ndf_new_questions2.columns = [f'e_{i}' for i in range(300)]\nresult = pd.concat([df_new_questions, df_new_questions2], axis=1)\n\n# Exclude some columns\nresult = result.drop(['question_vector'], axis=1)\n\n# Change directory if needed\nif os.getcwd().split('/')[-1] != 'feature_repo':\n    os.chdir('feature_repo')\n\n# Save to parquet file\nresult.to_parquet('./data/test_questions.parquet')\n```\n\n### Manage the Embeddings in Feast\n\nRecall that we created and deployed a feature view called **test_questions** earlier that loads the file we have just created.\n\nWe will make these questions accessible when querying the feature store online. \n\n```python\n!feast materialize 2021-06-02T00:00:00 2021-07-10T00:00:00 --views test_questions\n```\n\nNow that we have their embeddings in the feature store, we will show how you can fetch the questions using the ids. \n\n```python\n# Fetch the feature store and get feature vectors for the query questions\nstore = FeatureStore(repo_path=\".\")\n\nfeature_vectors = store.get_online_features(\n    feature_refs=[f'test_questions:question1',\n                  *[f'test_questions:e_{i}'\n                    for i in range(300)\n                  ]],\n    entity_rows=[{\"qid1\":_id} for _id in df_new_questions.qid1.tolist()]\n).to_dict()\n\n# Prepare list of vectors to query Pinecone\nquery_vectors = []\n\nfor e in range(len(feature_vectors['qid1'])):\n    l = [feature_vectors[f'test_questions__e_{i}'][e] for i in range(300)]\n    query_vectors.append(np.array(l))\n```\n\n### Query Pinecone\n\nNext, we query Pinecone and show the most similar questions (from the sample dataset).\n\n```python\n# Query Pinecone's index\nquery_results = index.query(queries=query_vectors, top_k=5)\n\n# Show results\nfor e, res in enumerate(query_results):\n    print(e)\n    print('\\n\\n\\n Original question: ' + feature_vectors['test_questions__question1'][e])\n    print('\\n Most similar questions based on Pinecone vector search: \\n')\n\n    # Fetch from Feast to get question text\n    result_feature_vectors = store.get_online_features(\n        feature_refs=[f'questions:question1'],\n        entity_rows=[{\"qid1\":int(_id)} for _id in res.ids]\n    ).to_dict()\n\n    # Prepare and display table\n    df_result = pd.DataFrame({'id':res.ids,\n                              'question': result_feature_vectors['questions__question1'],\n                              'score':res.scores})\n    display(df_result)\n```\n\n     Original question: How can I make money using Youtube?\n    \n     Most similar questions based on Pinecone vector search: \n\n<div>\n<table class=\"table table-responsive\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>id</th>\n      <th>question</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1292</td>\n      <td>How do I make money with YouTube?</td>\n      <td>0.944259</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>14375</td>\n      <td>How do I make money using Instagram?</td>\n      <td>0.936641</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1126</td>\n      <td>How can I earn money from YouTube?</td>\n      <td>0.866271</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3759</td>\n      <td>How do you make money giving through a app?</td>\n      <td>0.864226</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>157</td>\n      <td>How can I make money through the Internet?</td>\n      <td>0.858337</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n     Original question: What is the best book for learning Python?\n    \n     Most similar questions based on Pinecone vector search: \n\n\n\n\n<div>\n<table class=\"table table-responsive\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>id</th>\n      <th>question</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10033</td>\n      <td>What is the best Python learning book for beginners?</td>\n      <td>0.945661</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>16072</td>\n      <td>Which is the best book for learning Python 3 for absolute beginners?</td>\n      <td>0.872750</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13142</td>\n      <td>What's the best way to learn python on my own?</td>\n      <td>0.847575</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8939</td>\n      <td>Which is the best book for learning android programming from sratch?</td>\n      <td>0.845041</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7023</td>\n      <td>What is the best beginner friendly book on python?</td>\n      <td>0.829327</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n## Turn off the Pinecone Service\n\nTurn off the service once you are sure that you do not want to use it anymore. Once the service is stopped, you cannot use it again.\n\n```python\npinecone.delete_index(index_name)\n```\n\n## Summary\n\nWe demonstrated the integration between two emerging core ML/AI infrastructure technologies, feature stores and vector similarity search engines.\n\nThese technologies deal with feature vectors, the core information unit of any AI/ML application. Feature stores are responsible for all operational aspects of feature vectors, while similarity search engines enable numerous applications relying on semantic retrieval of those vectors. \n\n\n---\n## Optional: Create Your Parquet File\n\nThis section presents the code for creating a *questions.parquet* file for the feature store. We used a sample of 10,000 questions in the default parquet file that we showed. Using the following code, you can create a *questions.parquet* file with a different number of questions. That way, you can try out what happens once you have fewer/more questions.\n\n\n```python\n# Download dataset\nimport requests, os, zipfile\n\nDATA_DIR = \"tmp\"\nQA_DIR = f\"{DATA_DIR}/quora_duplicate_questions\"\nQA_FILE = f\"{DATA_DIR}/quora_duplicate_questions.tsv\"\nQA_URL = \"https://qim.fs.quoracdn.net/quora_duplicate_questions.tsv\"\n\n\ndef download_data():\n    os.makedirs(DATA_DIR, exist_ok=True)\n\n    if not os.path.exists(QA_DIR):\n        if not os.path.exists(QA_FILE):\n            r = requests.get(QA_URL) \n            with open(QA_FILE, \"wb\") as f:\n                f.write(r.content)\n\ndownload_data()\n```\n\n\n```python\npd.set_option('display.max_colwidth', 500)\ndf = pd.read_csv(QA_FILE, sep='\\t',  usecols=[\"qid1\", \"question1\"], index_col=False)\ndf = df.reset_index(drop=True)\ndf.drop_duplicates(inplace=True)\ndf.head()\n```\n<div>\n<table class=\"table table-responsive\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>qid1</th>\n      <th>question1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>What is the step by step guide to invest in share market in india?</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>What is the story of Kohinoor (Koh-i-Noor) Diamond?</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>How can I increase the speed of my internet connection while using a VPN?</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7</td>\n      <td>Why am I mentally very lonely? How can I solve it?</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9</td>\n      <td>Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n```python\n# Set any value for number of questions\nNUM_OF_QUESTIONS = 10000\n# Or select the complete dataset\n#NUM_OF_QUESTIONS = len(df)\n```\n\n\n```python\nimport datetime\n\n# Use only defined number of rows\ndf = df[:NUM_OF_QUESTIONS]\n\n# Create embedding for each question\ndf['question_vector'] = df.question1.apply(lambda x: model.encode(str(x)))\n\n# Create timestamps \ndf['created'] = datetime.datetime.utcnow()\ndf['datetime'] = df['created'].dt.floor('h')\n\n# Generate columns for vector elements\ndf2 = df.question_vector.apply(pd.Series)\ndf2.columns = [f'e_{i}' for i in range(300)]\nresult = pd.concat([df, df2], axis=1)\n\n# Exclude some columns\nresult = result.drop(['question_vector'], axis=1)\n\n# Change directory if needed\nif os.getcwd().split('/')[-1] != 'feature_repo':\n    os.chdir('feature_repo')\n    \n# Save to parquet file\nresult[:NUM_OF_QUESTIONS].to_parquet('./data/questions.parquet')\n```\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1cce5"
  },
  "filename": "table-qa.md",
  "title": "Table Question Answering",
  "category": "630fc5235d91a70054705fb7",
  "content": "---\ntitle: Table Question Answering\ncategory: 630fc5235d91a70054705fb7\n---\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/search/question-answering/table-qa.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/search/question-answering/table-qa.ipynb) [![Open Github](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/github-shield.svg)](https://github.com/pinecone-io/examples/tree/master/search/question-answering/table-qa.ipynb)\n\nTable Question Answering (Table QA) refers to providing precise answers from tables to answer a user's question. With recent works on Table QA, is it now possible to answer natural language queries from tabular data. This notebook demonstrates how you can build a Table QA system that can answer your natural language queries using the Pinecone vector database. \n\nWe need three main components to build the Table QA system:\n\n- A vector index to store table embeddings\n- A retriever model for embedding queries and tables\n- A reader model to read the tables and extract answers\n\n# Install Dependencies\n\n\n```python\n# torch-scatter may take few minutes to install\n!pip install datasets pinecone-client sentence_transformers torch-scatter\n```\n\n# Load the Dataset\n\nWe will work with a subset of the Open Table-and-Text Question Answering ([OTT-QA](https://github.com/wenhuchen/OTT-QA)) dataset, consisting of texts and tables from Wikipedia. The subset contains 20,000 tables, and it can be loaded from the Huggigface Datasets hub as follows:\n\n\n```python\nfrom datasets import load_dataset\n\n# load the dataset from huggingface datasets hub\ndata = load_dataset(\"ashraq/ott-qa-20k\", split=\"train\")\ndata\n```\n\n    Dataset({\n        features: ['url', 'title', 'header', 'data', 'section_title', 'section_text', 'uid', 'intro'],\n        num_rows: 20000\n    })\n\n\n\n\n```python\ndata[2]\n```\n\n\n\n\n    {'url': 'https://en.wikipedia.org/wiki/1976_New_York_Mets_season',\n     'title': '1976 New York Mets season',\n     'header': ['Level', 'Team', 'League', 'Manager'],\n     'data': [['AAA', 'Tidewater Tides', 'International League', 'Tom Burgess'],\n      ['AA', 'Jackson Mets', 'Texas League', 'John Antonelli'],\n      ['A', 'Lynchburg Mets', 'Carolina League', 'Jack Aker'],\n      ['A', 'Wausau Mets', 'Midwest League', 'Bill Monbouquette'],\n      ['Rookie', 'Marion Mets', 'Appalachian League', 'Al Jackson']],\n     'section_title': 'Farm system',\n     'section_text': 'See also : Minor League Baseball',\n     'uid': '1976_New_York_Mets_season_7',\n     'intro': 'The New York Mets season was the 15th regular season for the Mets, who played home games at Shea Stadium. Led by manager Joe Frazier, the team had an 86-76 record and finished in third place in the National League East.'}\n\n\n\nAs we can see, the dataset includes both textual and tabular data that are related to one another. Let's extract and transform the dataset's tables into pandas dataframes as we will only be using the tables in this example.\n\n\n```python\nimport pandas as pd\n\n# store all tables in the tables list\ntables = []\n# loop through the dataset and convert tabular data to pandas dataframes\nfor doc in data:\n    table = pd.DataFrame(doc[\"data\"], columns=doc[\"header\"])\n    tables.append(table)\n```\n\n\n```python\ntables[2]\n```\n\n\n\n\n\n<div id=\"df-5746ac5e-ff5f-42a9-9fb9-48b8dd8bebd9\">\n  <div class=\"colab-df-container\">\n    <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Level</th>\n      <th>Team</th>\n      <th>League</th>\n      <th>Manager</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AAA</td>\n      <td>Tidewater Tides</td>\n      <td>International League</td>\n      <td>Tom Burgess</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AA</td>\n      <td>Jackson Mets</td>\n      <td>Texas League</td>\n      <td>John Antonelli</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A</td>\n      <td>Lynchburg Mets</td>\n      <td>Carolina League</td>\n      <td>Jack Aker</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A</td>\n      <td>Wausau Mets</td>\n      <td>Midwest League</td>\n      <td>Bill Monbouquette</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Rookie</td>\n      <td>Marion Mets</td>\n      <td>Appalachian League</td>\n      <td>Al Jackson</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5746ac5e-ff5f-42a9-9fb9-48b8dd8bebd9')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n       width=\"24px\">\n    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n  </svg>\n      </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n  <script>\n    const buttonEl =\n      document.querySelector('#df-5746ac5e-ff5f-42a9-9fb9-48b8dd8bebd9 button.colab-df-convert');\n    buttonEl.style.display =\n      google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n    async function convertToInteractive(key) {\n      const element = document.querySelector('#df-5746ac5e-ff5f-42a9-9fb9-48b8dd8bebd9');\n      const dataTable =\n        await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                  [key], {});\n      if (!dataTable) return;\n\n      const docLinkHtml = 'Like what you see? Visit the ' +\n        '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n        + ' to learn more about interactive tables.';\n      element.innerHTML = '';\n      dataTable['output_type'] = 'display_data';\n      await google.colab.output.renderOutput(dataTable, element);\n      const docLink = document.createElement('div');\n      docLink.innerHTML = docLinkHtml;\n      element.appendChild(docLink);\n    }\n  </script>\n</div>\n  </div>\n\n\n\n\n# Initialize Retriever\n\nThe retriever transforms natural language queries and tabular data into embeddings/vectors. It will generate embeddings in a way that the natural language questions and tables containing answers to our questions are nearby in the vector space.\n\nWe will use a SentenceTransformer model trained specifically for embedding tabular data for retrieval tasks. The model can be loaded from the Huggingface Models hub as follows:\n\n\n```python\nimport torch\nfrom sentence_transformers import SentenceTransformer\n\n# set device to GPU if available\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n# load the table embedding model from huggingface models hub\nretriever = SentenceTransformer(\"deepset/all-mpnet-base-v2-table\", device=device)\nretriever\n```\n\n    SentenceTransformer(\n      (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n      (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n      (2): Normalize()\n    )\n\n\n\nThe retriever expects tables to be in a particular format. Let's write a function to convert the tables to this format.\n\n\n```python\ndef _preprocess_tables(tables: list):\n    processed = []\n    # loop through all tables\n    for table in tables:\n        # convert the table to csv and \n        processed_table = \"\\n\".join([table.to_csv(index=False)])\n        # add the processed table to processed list\n        processed.append(processed_table)\n    return processed\n\n```\n\nNotice that we are only using tables here. However, if you want the retriever to take the metadata into account while retrieving the tables, you can join any metadata strings, such as title, section_title, etc., separated by new line characters at the beginning of the processed table.\n\nLet's take a look at the formatted tables.\n\n\n```python\n# format all the dataframes in the tables list\nprocessed_tables = _preprocess_tables(tables)\n# display the formatted table\nprocessed_tables[2]\n```\n\n\n\n\n    'Level,Team,League,Manager\\nAAA,Tidewater Tides,International League,Tom Burgess\\nAA,Jackson Mets,Texas League,John Antonelli\\nA,Lynchburg Mets,Carolina League,Jack Aker\\nA,Wausau Mets,Midwest League,Bill Monbouquette\\nRookie,Marion Mets,Appalachian League,Al Jackson\\n'\n\n\n\nThe formatted table may not make sense to us, but the embedding model is trained to understand it and generate accurate embeddings.\n\n# Initialize Pinecone Index\n\nWe will use the Pinecone vector database as our vector index. The Pinecone index stores vector representations of our tables which we can retrieve using a natural language query (query vector). Pinecone does this by computing the similarity between the query vector and the embedded tables stored in the vector index. \n\nTo use Pinecone, we first need to initialize a connection to Pinecone. For this, we need a [free API key](https://app.pinecone.io/). You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**. We initialize the connection like so:\n\n\n```python\nimport pinecone\n\n# connect to pinecone environment\npinecone.init(\n    api_key=\"YOUR API KEY\",\n    environment=\"YOUR_ENVIRONMENT\"\n)\n```\n\nNow we create a new index. We specify the metric type as \"cosine\" and dimension as 768 because the retriever we use to generate context embeddings outputs 768-dimension vectors. Pinecone will use cosine similarity to compute the similarity between the query and table embeddings.\n\n\n```python\n# you can choose any name for the index\nindex_name = \"table-qa\"\n\n# check if the table-qa index exists\nif index_name not in pinecone.list_indexes():\n    # create the index if it does not exist\n    pinecone.create_index(\n        index_name,\n        dimension=768,\n        metric=\"cosine\"\n    )\n\n# connect to table-qa index we created\nindex = pinecone.Index(index_name)\n```\n\n# Generate Embeddings and Upsert\n\n\nNext we need to generate the table embeddings and upload it to the Pinecone index. We can easily do that as follows:\n\n\n```python\nfrom tqdm.auto import tqdm\n\n# we will use batches of 64\nbatch_size = 64\n\nfor i in tqdm(range(0, len(processed_tables), batch_size)):\n    # find end of batch\n    i_end = min(i+batch_size, len(processed_tables))\n    # extract batch\n    batch = processed_tables[i:i_end]\n    # generate embeddings for batch\n    emb = retriever.encode(batch).tolist()\n    # create unique IDs ranging from zero to the total number of tables in the dataset\n    ids = [f\"{idx}\" for idx in range(i, i_end)]\n    # add all to upsert list\n    to_upsert = list(zip(ids, emb))\n    # upsert/insert these records to pinecone\n    _ = index.upsert(vectors=to_upsert)\n\n# check that we have all vectors in index\nindex.describe_index_stats()\n```\n\n\n      100%|██████████| 313/313 [09:12<00:00, 1.49s/it]\n\n    {'dimension': 768,\n     'index_fullness': 0.0,\n     'namespaces': {'': {'vector_count': 20000}},\n     'total_vector_count': 20000}\n\n\n\nNow the Pinecone index is ready for querying. Let's test to see if it returns tables relevant to our queries.\n\n\n```python\nquery = \"which country has the highest GDP in 2020?\"\n# generate embedding for the query\nxq = retriever.encode([query]).tolist()\n# query pinecone index to find the table containing answer to the query\nresult = index.query(xq, top_k=1)\nresult\n\n```\n\n\n\n\n    {'matches': [{'id': '19931', 'score': 0.822087, 'values': []}], 'namespace': ''}\n\n\n\nThe Pinecone index has returned the ```id``` of a table that would contain the answer to our query with 82.2% confidence. Let's see if this table actually contains the answer. We can use the returned ```id``` as an index to get the relevant pandas dataframe from the ```tables``` list.\n\n\n```python\nid = int(result[\"matches\"][0][\"id\"])\ntables[id].head()\n```\n\n\n\n\n\n  <div id=\"df-74ed8f27-1770-4db1-9983-2633ef78b0c5\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Rank</th>\n      <th>Country</th>\n      <th>GDP ( PPP , Peak Year ) millions of USD</th>\n      <th>Peak Year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>China</td>\n      <td>27,804,953</td>\n      <td>2020</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>India</td>\n      <td>11,321,280</td>\n      <td>2020</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Russia</td>\n      <td>4,389,960</td>\n      <td>2019</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Indonesia</td>\n      <td>3,778,134</td>\n      <td>2020</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Brazil</td>\n      <td>3,596,841</td>\n      <td>2020</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-74ed8f27-1770-4db1-9983-2633ef78b0c5')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n       width=\"24px\">\n    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n  </svg>\n      </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n  <script>\n    const buttonEl =\n      document.querySelector('#df-74ed8f27-1770-4db1-9983-2633ef78b0c5 button.colab-df-convert');\n    buttonEl.style.display =\n      google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n    async function convertToInteractive(key) {\n      const element = document.querySelector('#df-74ed8f27-1770-4db1-9983-2633ef78b0c5');\n      const dataTable =\n        await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                  [key], {});\n      if (!dataTable) return;\n\n      const docLinkHtml = 'Like what you see? Visit the ' +\n        '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n        + ' to learn more about interactive tables.';\n      element.innerHTML = '';\n      dataTable['output_type'] = 'display_data';\n      await google.colab.output.renderOutput(dataTable, element);\n      const docLink = document.createElement('div');\n      docLink.innerHTML = docLinkHtml;\n      element.appendChild(docLink);\n    }\n  </script>\n</div>\n  </div>\n\n\n\n\nThe table returned by the Pinecone index indeed has the answer to our query. Now we need a model that can read this table and extract the precise answer.\n\n# Initialize Table Reader\n\nAs the reader, we will use a TAPAS model fine-tuned for the Table QA task. TAPAS is a BERT-like Transformer model pretrained in a self-supervised manner on a large corpus of English language data from Wikipedia. We load the model and tokenizer from the Huggingface model hub into a question-answering pipeline.\n\n\n```python\nfrom transformers import pipeline, TapasTokenizer, TapasForQuestionAnswering\n\nmodel_name = \"google/tapas-base-finetuned-wtq\"\n# load the tokenizer and the model from huggingface model hub\ntokenizer = TapasTokenizer.from_pretrained(model_name)\nmodel = TapasForQuestionAnswering.from_pretrained(model_name, local_files_only=False)\n# load the model and tokenizer into a question-answering pipeline\npipe = pipeline(\"table-question-answering\",  model=model, tokenizer=tokenizer, device=device)\n```\n\n\nLet's run the table returned by the Pinecone index and the query we used before into the question-answering pipeline to extract the answer.\n\n\n```python\npipe(table=tables[id], query=query)\n```\n\n\n\n\n    {'answer': 'China',\n     'coordinates': [(0, 1)],\n     'cells': ['China'],\n     'aggregator': 'NONE'}\n\n\n\nThe model has precisely answered our query. Let's run some more queries.\n\n# Querying\n\nFirst, we will define two function to handle our queries and extract answers from tables.\n\n\n```python\ndef query_pinecone(query):\n    # generate embedding for the query\n    xq = retriever.encode([query]).tolist()\n    # query pinecone index to find the table containing answer to the query\n    result = index.query(xq, top_k=1)\n    # return the relevant table from the tables list\n    return tables[int(result[\"matches\"][0][\"id\"])]\n```\n\n\n```python\ndef get_answer_from_table(table, query):\n    # run the table and query through the question-answering pipeline\n    answers = pipe(table=table, query=query)\n    return answers\n```\n\n\n```python\nquery = \"which car manufacturers produce cars with a top speed of above 180 kph?\"\ntable = query_pinecone(query)\ntable\n```\n\n\n\n\n\n  <div id=\"df-dc62f5db-6acf-4ae5-80e4-3e6233b55474\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Manufacturer</th>\n      <th>Model</th>\n      <th>Engine</th>\n      <th>Power Output</th>\n      <th>Max . Speed ( kph )</th>\n      <th>Dry Weight ( kg )</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Fiat</td>\n      <td>805-405</td>\n      <td>FIAT 1979cc S6 supercharged</td>\n      <td>130 bhp</td>\n      <td>220</td>\n      <td>680</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Alfa Romeo</td>\n      <td>GPR ( P1 )</td>\n      <td>Alfa Romeo 1990cc S6</td>\n      <td>95 bhp</td>\n      <td>180</td>\n      <td>850</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Diatto</td>\n      <td>Tipo 20 S</td>\n      <td>Diatto 1997cc S4</td>\n      <td>75 bhp</td>\n      <td>155</td>\n      <td>700</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Bugatti</td>\n      <td>Type 32</td>\n      <td>Bugatti 1991cc S8</td>\n      <td>100 bhp</td>\n      <td>190</td>\n      <td>660</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Voisin</td>\n      <td>C6 Laboratoire</td>\n      <td>Voisin 1978cc S6</td>\n      <td>90 bhp</td>\n      <td>175</td>\n      <td>710</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Sunbeam</td>\n      <td></td>\n      <td>Sunbeam 1988cc S6</td>\n      <td>108 bhp</td>\n      <td>180</td>\n      <td>675</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Mercedes</td>\n      <td>M7294</td>\n      <td>Mercedes 1990cc S4 supercharged</td>\n      <td>120 bhp</td>\n      <td>180</td>\n      <td>750</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Benz</td>\n      <td>RH Tropfenwagen</td>\n      <td>Benz 1998cc S6</td>\n      <td>95 bhp</td>\n      <td>185</td>\n      <td>745</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Miller</td>\n      <td>122</td>\n      <td>Miller 1978cc S8</td>\n      <td>120 bhp</td>\n      <td>186</td>\n      <td>850</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc62f5db-6acf-4ae5-80e4-3e6233b55474')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n       width=\"24px\">\n    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n  </svg>\n      </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n  <script>\n    const buttonEl =\n      document.querySelector('#df-dc62f5db-6acf-4ae5-80e4-3e6233b55474 button.colab-df-convert');\n    buttonEl.style.display =\n      google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n    async function convertToInteractive(key) {\n      const element = document.querySelector('#df-dc62f5db-6acf-4ae5-80e4-3e6233b55474');\n      const dataTable =\n        await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                  [key], {});\n      if (!dataTable) return;\n\n      const docLinkHtml = 'Like what you see? Visit the ' +\n        '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n        + ' to learn more about interactive tables.';\n      element.innerHTML = '';\n      dataTable['output_type'] = 'display_data';\n      await google.colab.output.renderOutput(dataTable, element);\n      const docLink = document.createElement('div');\n      docLink.innerHTML = docLinkHtml;\n      element.appendChild(docLink);\n    }\n  </script>\n</div>\n  </div>\n\n\n\n\n\n```python\nget_answer_from_table(table, query)\n```\n\n\n\n\n    {'answer': 'Fiat, Bugatti, Benz, Miller',\n     'coordinates': [(0, 0), (3, 0), (7, 0), (8, 0)],\n     'cells': ['Fiat', 'Bugatti', 'Benz', 'Miller'],\n     'aggregator': 'NONE'}\n\n\n\n\n```python\nquery = \"which scientist is known for improving the steam engine?\"\ntable = query_pinecone(query)\ntable.head()\n```\n\n\n\n\n\n  <div id=\"df-985a94cf-fc9c-4e3a-969f-e0c6498f57bd\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Year</th>\n      <th>Name</th>\n      <th>Location</th>\n      <th>Rationale</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1839</td>\n      <td>Robert Hare</td>\n      <td>Philadelphia , Pennsylvania</td>\n      <td>Inventor of the oxy-hydrogen blowpipe</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1862</td>\n      <td>John Ericsson</td>\n      <td>New York , New York</td>\n      <td>His work improved the field of heat management...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1865</td>\n      <td>Daniel Treadwell</td>\n      <td>Cambridge , Massachusetts</td>\n      <td>Heat management . He was awarded especially fo...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1866</td>\n      <td>Alvan Clark</td>\n      <td>Cambridge , Massachusetts</td>\n      <td>Improved refracting telescopes</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1869</td>\n      <td>George Henry Corliss</td>\n      <td>Providence , Rhode Island</td>\n      <td>For improving the steam engine</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-985a94cf-fc9c-4e3a-969f-e0c6498f57bd')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n       width=\"24px\">\n    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n  </svg>\n      </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n  <script>\n    const buttonEl =\n      document.querySelector('#df-985a94cf-fc9c-4e3a-969f-e0c6498f57bd button.colab-df-convert');\n    buttonEl.style.display =\n      google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n    async function convertToInteractive(key) {\n      const element = document.querySelector('#df-985a94cf-fc9c-4e3a-969f-e0c6498f57bd');\n      const dataTable =\n        await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                  [key], {});\n      if (!dataTable) return;\n\n      const docLinkHtml = 'Like what you see? Visit the ' +\n        '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n        + ' to learn more about interactive tables.';\n      element.innerHTML = '';\n      dataTable['output_type'] = 'display_data';\n      await google.colab.output.renderOutput(dataTable, element);\n      const docLink = document.createElement('div');\n      docLink.innerHTML = docLinkHtml;\n      element.appendChild(docLink);\n    }\n  </script>\n</div>\n  </div>\n\n\n\n\n\n```python\nget_answer_from_table(table, query)\n```\n\n\n\n\n    {'answer': 'George Henry Corliss',\n     'coordinates': [(4, 1)],\n     'cells': ['George Henry Corliss'],\n     'aggregator': 'NONE'}\n\n\n\n\n```python\nquery = \"What is the Maldivian island name for Oblu Select at Sangeli\tresort?\"\ntable = query_pinecone(query)\ntable.head()\n```\n\n\n\n\n\n  <div id=\"df-a83c0554-e109-4f76-907a-43a909cb8156\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Resort Name</th>\n      <th>Geographic Atoll</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Asdhoo</td>\n      <td>Asdu Sun Island Resort</td>\n      <td>North Male Atoll</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Akirifushi</td>\n      <td>Oblu Select at Sangeli</td>\n      <td>North Male Atoll</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Baros</td>\n      <td>Baros Island Resort</td>\n      <td>North Male Atoll</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Biyaadhoo</td>\n      <td>Biyadhoo Island Resort</td>\n      <td>South Male Atoll</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Bodubandos</td>\n      <td>Bandos Maldives Resort</td>\n      <td>North Male Atoll</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a83c0554-e109-4f76-907a-43a909cb8156')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n       width=\"24px\">\n    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n  </svg>\n      </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n  <script>\n    const buttonEl =\n      document.querySelector('#df-a83c0554-e109-4f76-907a-43a909cb8156 button.colab-df-convert');\n    buttonEl.style.display =\n      google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n    async function convertToInteractive(key) {\n      const element = document.querySelector('#df-a83c0554-e109-4f76-907a-43a909cb8156');\n      const dataTable =\n        await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                  [key], {});\n      if (!dataTable) return;\n\n      const docLinkHtml = 'Like what you see? Visit the ' +\n        '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n        + ' to learn more about interactive tables.';\n      element.innerHTML = '';\n      dataTable['output_type'] = 'display_data';\n      await google.colab.output.renderOutput(dataTable, element);\n      const docLink = document.createElement('div');\n      docLink.innerHTML = docLinkHtml;\n      element.appendChild(docLink);\n    }\n  </script>\n</div>\n  </div>\n\n\n\n\n\n```python\nget_answer_from_table(table, query)\n```\n\n\n\n\n    {'answer': 'Akirifushi',\n     'coordinates': [(1, 0)],\n     'cells': ['Akirifushi'],\n     'aggregator': 'NONE'}\n\n\n\nAs we can see, our Table QA system can retrieve the correct table from the Pinecone index and extract precise answers from the table. The TAPAS model we use supports more advanced queries. It has an aggregation head which indicates whether we need to count, sum, or average cells to answer the questions. Let's run some advanced queries that require aggregation to answer.\n\n\n```python\nquery = \"what was the total GDP of China and Indonesia in 2020?\"\ntable = query_pinecone(query)\ntable.head()\n```\n\n\n\n\n\n  <div id=\"df-a20fc2ac-c471-4bc5-8526-26ab08545922\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Rank</th>\n      <th>Country</th>\n      <th>GDP ( PPP , Peak Year ) millions of USD</th>\n      <th>Peak Year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>China</td>\n      <td>27,804,953</td>\n      <td>2020</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>India</td>\n      <td>11,321,280</td>\n      <td>2020</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Russia</td>\n      <td>4,389,960</td>\n      <td>2019</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Indonesia</td>\n      <td>3,778,134</td>\n      <td>2020</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Brazil</td>\n      <td>3,596,841</td>\n      <td>2020</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a20fc2ac-c471-4bc5-8526-26ab08545922')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n       width=\"24px\">\n    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n  </svg>\n      </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n  <script>\n    const buttonEl =\n      document.querySelector('#df-a20fc2ac-c471-4bc5-8526-26ab08545922 button.colab-df-convert');\n    buttonEl.style.display =\n      google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n    async function convertToInteractive(key) {\n      const element = document.querySelector('#df-a20fc2ac-c471-4bc5-8526-26ab08545922');\n      const dataTable =\n        await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                  [key], {});\n      if (!dataTable) return;\n\n      const docLinkHtml = 'Like what you see? Visit the ' +\n        '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n        + ' to learn more about interactive tables.';\n      element.innerHTML = '';\n      dataTable['output_type'] = 'display_data';\n      await google.colab.output.renderOutput(dataTable, element);\n      const docLink = document.createElement('div');\n      docLink.innerHTML = docLinkHtml;\n      element.appendChild(docLink);\n    }\n  </script>\n</div>\n  </div>\n\n\n\n\n\n```python\nget_answer_from_table(table, query)\n```\n\n\n\n\n    {'answer': 'SUM > 27,804,953, 3,778,134',\n     'coordinates': [(0, 2), (3, 2)],\n     'cells': ['27,804,953', '3,778,134'],\n     'aggregator': 'SUM'}\n\n\n\nHere the QA system suggests the correct cells to add in order to get the total GDP of China and Indonesia in 2020.\n\n\n```python\nquery = \"what is the average carbon emission of power stations in australia, canada and germany?\"\ntable = query_pinecone(query)\ntable.head()\n```\n\n\n\n\n\n  <div id=\"df-85612087-ccea-4ebf-9fdb-f783d2e968d9\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CO 2 intensity ( kg/kWh )</th>\n      <th>Power station</th>\n      <th>Country</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.58</td>\n      <td>Hazelwood Power Station , Victoria closed 31 M...</td>\n      <td>Australia</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.56</td>\n      <td>Edwardsport IGCC , Edwardsport , Indiana , clo...</td>\n      <td>United States</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.27</td>\n      <td>Frimmersdorf power plant , Grevenbroich</td>\n      <td>Germany</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.25</td>\n      <td>HR Milner Generating Station , Grande Cache , ...</td>\n      <td>Canada</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.18</td>\n      <td>C. TG . Portes Gil , Río Bravo</td>\n      <td>Mexico</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85612087-ccea-4ebf-9fdb-f783d2e968d9')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n       width=\"24px\">\n    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n  </svg>\n      </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n  <script>\n    const buttonEl =\n      document.querySelector('#df-85612087-ccea-4ebf-9fdb-f783d2e968d9 button.colab-df-convert');\n    buttonEl.style.display =\n      google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n    async function convertToInteractive(key) {\n      const element = document.querySelector('#df-85612087-ccea-4ebf-9fdb-f783d2e968d9');\n      const dataTable =\n        await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                  [key], {});\n      if (!dataTable) return;\n\n      const docLinkHtml = 'Like what you see? Visit the ' +\n        '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n        + ' to learn more about interactive tables.';\n      element.innerHTML = '';\n      dataTable['output_type'] = 'display_data';\n      await google.colab.output.renderOutput(dataTable, element);\n      const docLink = document.createElement('div');\n      docLink.innerHTML = docLinkHtml;\n      element.appendChild(docLink);\n    }\n  </script>\n</div>\n  </div>\n\n\n\n\n\n```python\nget_answer_from_table(table, query)\n```\n\n\n\n\n    {'answer': 'AVERAGE > 1.58, 1.27, 1.25',\n     'coordinates': [(0, 0), (2, 0), (3, 0)],\n     'cells': ['1.58', '1.27', '1.25'],\n     'aggregator': 'AVERAGE'}\n\n\n\nAs we can see, the QA system correctly identified which cells to average to answer our question.\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1cce6"
  },
  "filename": "personalized-content-recommendations.md",
  "title": "Article Recommender",
  "category": "630fc5235d91a70054705fb7",
  "content": "---\ntitle: Article Recommender\ncategory: 630fc5235d91a70054705fb7\n---\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/recommendation/article-recommender/article_recommendations.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/recommendation/article-recommender/article_recommendations.ipynb) [![Open github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/pinecone-io/examples/blob/master/recommendation/article-recommender/article_recommendations.ipynb)\n\nThis notebook demonstrates how to use Pinecone's similarity search to create a simple personalized article or content recommender.\n\nThe goal is to create a recommendation engine that retrieves the best article recommendations for each user. When making recommendations with content-based filtering, we evaluate the user’s past behavior and the content items themselves. So in this example, users will be recommended articles that are similar to those they've already read.\n\n## Install and Import Python Packages\n\n```python\n!pip install --quiet wordcloud pandas\n!pip install --quiet sentence-transformers --no-cache-dir\n```\n\n\n```python\nimport os\nimport pandas as pd\nimport numpy as np\nimport time\nimport re\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\nfrom statistics import mean\n%matplotlib inline\n```\n\nIn the following sections, we will use Pinecone to easily build an article recommendation engine. Pinecone will be responsible for storing embeddings for articles, maintaining a live index of those vectors, and returning recommended articles on-demand.\n\n## Pinecone Setup\n\n```python\n!pip install --quiet -U pinecone-client\n```\n\n\n```python\nimport pinecone\n```\n\n\n```python\n# Load Pinecone API key\napi_key = os.getenv('PINECONE_API_KEY') or 'YOUR_API_KEY'\n# Set Pinecone environment. Default environment is YOUR_ENVIRONMENT\nenv = os.getenv('PINECONE_ENVIRONMENT') or 'YOUR_ENVIRONMENT'\npinecone.init(api_key=api_key, environment=env)\n```\n\n[Get a Pinecone API key](https://www.pinecone.io/start/) if you don’t have one already. You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**.\n\n```python\nindex_name = 'articles-recommendation'\n```\n\n\n```python\n# If index of the same name exists, then delete it\nif index_name in pinecone.list_indexes():\n    pinecone.delete_index(index_name)\n```\n\nCreate an index.\n\n```python\npinecone.create_index(index_name, dimension=300)\n```\n\nConnect to the new index.\n\n```python\nindex = pinecone.Index(index_name)\nindex.describe_index_stats()\n```\n\n    {'dimension': 300, 'namespaces': {}}\n\n## Upload Articles\n\nNext, we will prepare data for the Pinecone vector index, and insert it in batches.\n\nThe [dataset](https://components.one/datasets/all-the-news-2-news-articles-dataset/) used throughout this example contains 2.7 million news articles and essays from 27 American publications.\n\nLet's download the dataset.\n\n```python\n!rm all-the-news-2-1.zip\n!rm all-the-news-2-1.csv\n!wget https://www.dropbox.com/s/cn2utnr5ipathhh/all-the-news-2-1.zip -q --show-progress\n!unzip -q all-the-news-2-1.zip\n```\n\n    rm: cannot remove 'all-the-news-2-1.zip': No such file or directory\n    rm: cannot remove 'all-the-news-2-1.csv': No such file or directory\n    all-the-news-2-1.zi     [             <=>    ]   3.13G  82.7MB/s    in 39s\n\n### Create Vector Embeddings\n\nThe model used in this example is the [Average Word Embeddings Models](https://www.sbert.net/docs/pretrained_models.html#average-word-embeddings-models). This model allows us to create vector embeddings for each article, using the content and title of each.\n\n```python\nimport torch\nfrom sentence_transformers import SentenceTransformer\n\n# set device to GPU if available\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = SentenceTransformer('average_word_embeddings_komninos', device=device)\n```\n\nUsing the complete dataset may require more time for the model to generate vector embeddings. We will use only a sample, but if you want to try uploading the whole dataset, set the **NROWS** flag to **None**.\n\n```python\nNROWS = 200000      # number of rows to be loaded from the csv, set to None for loading all rows, reduce if you have a low amount of RAM or want a faster execution\nBATCH_SIZE = 500    # batch size for upserting\n```\n\nLet's prepare data for upload.\n\nUploading the data may take a while, and depends on the network you use.\n\n```python\n#%%time     \n\ndef prepare_data(data) -> pd.DataFrame:\n    'Preprocesses data and prepares it for upsert.'\n    \n    # add an id column\n    print(\"Preparing data...\")\n    data[\"id\"] = range(len(data))\n\n    # extract only first few sentences of each article for quicker vector calculations\n    data['article'] = data['article'].fillna('')\n    data['article'] = data.article.apply(lambda x: ' '.join(re.split(r'(?<=[.:;])\\s', x)[:4]))\n    data['title_article'] = data['title'] + data['article']\n    \n    # create a vector embedding based on title and article columns\n    print('Encoding articles...')\n    encoded_articles = model.encode(data['title_article'])\n    data['article_vector'] = pd.Series(encoded_articles.tolist())\n    \n    return data\n\n\ndef upload_items(data):\n    'Uploads data in batches.'\n    print(\"Uploading items...\")\n    \n    # create a list of items for upload\n    items_to_upload = [(str(row.id), row.article_vector) for i,row in data.iterrows()]\n    \n    # upsert\n    for i in range(0, len(items_to_upload), BATCH_SIZE):\n        index.upsert(vectors=items_to_upload[i:i+BATCH_SIZE])\n\n    \ndef process_file(filename: str) -> pd.DataFrame:\n    'Reads csv files in chunks, prepares and uploads data.'\n    \n    data = pd.read_csv(filename, nrows=NROWS)\n    data = prepare_data(data)\n    upload_items(data)\n    return data\n            \nuploaded_data = process_file(filename='all-the-news-2-1.csv')\n```\n\n\n\n    Preparing data...\n    Encoding articles...\n    Uploading items...\n\n```python\n# Print index statistics\nindex.describe_index_stats()\n```\n\n    {'dimension': 300, 'namespaces': {'': {'vector_count': 200000}}}\n\n## Query the Pinecone Index\n\nWe will query the index for the specific users. The users are defined as a set of the articles that they previously read. More specifically, we will define 10 articles for each user, and based on the article embeddings, we will define a unique embedding for the user.\n\nWe will create three users and query Pinecone for each of them:\n\n- User who likes to read Sport News\n- User who likes to read Entertainment News\n- User who likes to read Business News\n\nLet's define mappings for titles, sections, and publications for each article.\n\n```python\ntitles_mapped = dict(zip(uploaded_data.id, uploaded_data.title))\nsections_mapped = dict(zip(uploaded_data.id, uploaded_data.section))\npublications_mapped = dict(zip(uploaded_data.id, uploaded_data.publication))\n```\n\nAlso, we will define a function that uses _wordcloud_ to visualize results.\n\n```python\ndef get_wordcloud_for_user(recommendations):\n\n    stopwords = set(STOPWORDS).union([np.nan, 'NaN', 'S'])\n\n    wordcloud = WordCloud(\n                   max_words=50000, \n                   min_font_size =12, \n                   max_font_size=50, \n                   relative_scaling = 0.9, \n                   stopwords=set(STOPWORDS),\n                   normalize_plurals= True\n    )\n\n    clean_titles = [word for word in recommendations.title.values if word not in stopwords]\n    title_wordcloud = wordcloud.generate(' '.join(clean_titles))\n\n    plt.imshow(title_wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()\n```\n\nLet's query the Pinecone index using three users.\n\n### Query Sports User\n\n```python\n# first create a user who likes to read sport news about tennis\nsport_user = uploaded_data.loc[((uploaded_data['section'] == 'Sports News' ) | \n                                (uploaded_data['section'] == 'Sports')) &\n                                (uploaded_data['article'].str.contains('Tennis'))][:10]\n\nprint('\\nHere is the example of previously read articles by this user:\\n')\ndisplay(sport_user[['title', 'article', 'section', 'publication']])\n\n# then create a vector for this user\na = sport_user['article_vector']\nsport_user_vector = [*map(mean, zip(*a))]\n\n# query the pinecone\nres = index.query(sport_user_vector, top_k=10)\n\n# print results\nids = [match.id for match in res.matches]\nscores = [match.score for match in res.matches]\ndf = pd.DataFrame({'id': ids, \n                   'score': scores,\n                   'title': [titles_mapped[int(_id)] for _id in ids],\n                   'section': [sections_mapped[int(_id)] for _id in ids],\n                   'publication': [publications_mapped[int(_id)] for _id in ids]\n                    })\n\nprint(\"\\nThis table contains recommended articles for the user:\\n\")\ndisplay(df)\nprint(\"\\nA word-cloud representing the results:\\n\")\nget_wordcloud_for_user(df)\n```\n\n    Here is the example of previously read articles by this user:\n\n  <div id=\"df-f22bbca3-ad57-487f-8fff-30873403b02f\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n\n</style>\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>article</th>\n      <th>section</th>\n      <th>publication</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2261</th>\n      <td>Son of Borg makes quiet debut on London grassc...</td>\n      <td>LONDON (Reuters) - A blonde-haired, blue-eyed ...</td>\n      <td>Sports News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>12373</th>\n      <td>Cilic offers Nadal a Wimbledon reality check</td>\n      <td>LONDON (Reuters) - Spaniard Rafael Nadal got a...</td>\n      <td>Sports News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>17124</th>\n      <td>Perth confirmed as host for Fed Cup final</td>\n      <td>(Reuters) - Perth has been named host city for...</td>\n      <td>Sports News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>18411</th>\n      <td>Fed Cup gets revamp with 12-nation Finals in B...</td>\n      <td>LONDON (Reuters) - The Fed Cup’s existing form...</td>\n      <td>Sports News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>26574</th>\n      <td>Nadal to prepare for Wimbledon at Hurlingham e...</td>\n      <td>(Reuters) - World number two Rafa Nadal has en...</td>\n      <td>Sports News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>34957</th>\n      <td>Tennis Legend Margaret Court Went Off the Rail...</td>\n      <td>Margaret Court, the most decorated tennis play...</td>\n      <td>Sports</td>\n      <td>Vice</td>\n    </tr>\n    <tr>\n      <th>35508</th>\n      <td>Puck City: The Enduring Success of Ice Hockey ...</td>\n      <td>This article originally appeared on VICE Sport...</td>\n      <td>Sports</td>\n      <td>Vice</td>\n    </tr>\n    <tr>\n      <th>38393</th>\n      <td>As if by royal command, seven Britons make it ...</td>\n      <td>LONDON (Reuters) - Tennis fan the Duchess of C...</td>\n      <td>Sports News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>62445</th>\n      <td>Williams fined $17,000 for U.S. Open code viol...</td>\n      <td>NEW YORK (Reuters) - Serena Williams has been ...</td>\n      <td>Sports News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>84122</th>\n      <td>Kyrgios still wrestling with his tennis soul a...</td>\n      <td>LONDON (Reuters) - Timothy Gallwey’s million-s...</td>\n      <td>Sports News</td>\n      <td>Reuters</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f22bbca3-ad57-487f-8fff-30873403b02f')\"\n              title=\"Convert this dataframe to an interactive table.\" style=\"display:none;\"\n              >\n\n<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 0 24 24\"\n       width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg>\n</button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n<script>\nconst buttonEl =\n    document.querySelector('#df-f22bbca3-ad57-487f-8fff-30873403b02f button.colab-df-convert');\nbuttonEl.style.display =\n    google.colab.kernel.accessAllowed ? 'block' : 'none';\n\nasync function convertToInteractive(key) {\n    const element = document.querySelector('#df-f22bbca3-ad57-487f-8fff-30873403b02f');\n    const dataTable =\n    await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                [key], {});\n    if (!dataTable) return;\n\n    const docLinkHtml = 'Like what you see? Visit the ' +\n    '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n    + ' to learn more about interactive tables.';\n    element.innerHTML = '';\n    dataTable['output_type'] = 'display_data';\n    await google.colab.output.renderOutput(dataTable, element);\n    const docLink = document.createElement('div');\n    docLink.innerHTML = docLinkHtml;\n    element.appendChild(docLink);\n}\n</script>\n</div>\n  </div>\n\n    This table contains recommended articles for the user:\n\n  <div id=\"df-2e1273af-dda4-4c1c-91a7-5c6c54a03777\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n\n</style>\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n      <th>title</th>\n      <th>section</th>\n      <th>publication</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>138865</td>\n      <td>0.966407</td>\n      <td>Federer survives first-set wobble to down Wimb...</td>\n      <td>Sports News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>26574</td>\n      <td>0.965867</td>\n      <td>Nadal to prepare for Wimbledon at Hurlingham e...</td>\n      <td>Sports News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>12373</td>\n      <td>0.965307</td>\n      <td>Cilic offers Nadal a Wimbledon reality check</td>\n      <td>Sports News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>155913</td>\n      <td>0.963684</td>\n      <td>U.S. men likely to wander Wimbledon wilderness...</td>\n      <td>Sports News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>60613</td>\n      <td>0.962414</td>\n      <td>Auger-Aliassime powers past Tsitsipas into Que...</td>\n      <td>Sports News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>22764</td>\n      <td>0.962373</td>\n      <td>Serena headed to Wimbledon seeking return to form</td>\n      <td>Sports News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>71768</td>\n      <td>0.962168</td>\n      <td>Venus, Serena, and the Power of Believing</td>\n      <td>Sports</td>\n      <td>Vice</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2261</td>\n      <td>0.961590</td>\n      <td>Son of Borg makes quiet debut on London grassc...</td>\n      <td>Sports News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>45469</td>\n      <td>0.961451</td>\n      <td>Tennis: Barty a win away from world number one</td>\n      <td>Sports News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>55061</td>\n      <td>0.960677</td>\n      <td>Warrior on court, diplomat off it, classy Bart...</td>\n      <td>Sports News</td>\n      <td>Reuters</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e1273af-dda4-4c1c-91a7-5c6c54a03777')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 0 24 24\"\n       width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg>\n</button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n<script>\nconst buttonEl =\n    document.querySelector('#df-2e1273af-dda4-4c1c-91a7-5c6c54a03777 button.colab-df-convert');\nbuttonEl.style.display =\n    google.colab.kernel.accessAllowed ? 'block' : 'none';\n\nasync function convertToInteractive(key) {\n    const element = document.querySelector('#df-2e1273af-dda4-4c1c-91a7-5c6c54a03777');\n    const dataTable =\n    await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                [key], {});\n    if (!dataTable) return;\n\n    const docLinkHtml = 'Like what you see? Visit the ' +\n    '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n    + ' to learn more about interactive tables.';\n    element.innerHTML = '';\n    dataTable['output_type'] = 'display_data';\n    await google.colab.output.renderOutput(dataTable, element);\n    const docLink = document.createElement('div');\n    docLink.innerHTML = docLinkHtml;\n    element.appendChild(docLink);\n}\n</script>\n</div>\n\n  </div>\n\n    A word-cloud representing the results:\n\n![Wordcloud of recommended sports articles](https://raw.githubusercontent.com/pinecone-io/img/main/personalized-content-recommendations-example-1.png)\n\n### Query Entertainment User\n\n```python\n# first create a user who likes to read news about Xbox\nentertainment_user = uploaded_data.loc[((uploaded_data['section'] == 'Entertainment') |\n                                        (uploaded_data['section'] == 'Games') |\n                                        (uploaded_data['section'] == 'Tech by VICE')) &\n                                        (uploaded_data['article'].str.contains('Xbox'))][:10]\n\nprint('\\nHere is the example of previously read articles by this user:\\n')\ndisplay(entertainment_user[['title', 'article', 'section', 'publication']])\n\n# then create a vector for this user\na = entertainment_user['article_vector']\nentertainment_user_vector = [*map(mean, zip(*a))]\n\n# query the pinecone\nres = index.query(entertainment_user_vector, top_k=10)\n\n# print results\nids = [match.id for match in res.matches]\nscores = [match.score for match in res.matches]\ndf = pd.DataFrame({'id': ids, \n                   'score': scores,\n                   'title': [titles_mapped[int(_id)] for _id in ids],\n                   'section': [sections_mapped[int(_id)] for _id in ids],\n                   'publication': [publications_mapped[int(_id)] for _id in ids]\n                    })\n\nprint(\"\\nThis table contains recommended articles for the user:\\n\")\ndisplay(df)\nprint(\"\\nA word-cloud representing the results:\\n\")\nget_wordcloud_for_user(df)\n```\n\n    Here is the example of previously read articles by this user:\n\n  <div id=\"df-40de5ea3-0e1d-48b5-b827-718415441735\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n\n</style>\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>article</th>\n      <th>section</th>\n      <th>publication</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4977</th>\n      <td>A Canadian Man Is Pissed That His Son Ran Up a...</td>\n      <td>A Pembroke, Ontario, gun shop owner is \"mad as...</td>\n      <td>Games</td>\n      <td>Vice</td>\n    </tr>\n    <tr>\n      <th>12016</th>\n      <td>'I Expect You to Die' is One of Virtual Realit...</td>\n      <td>The reason I bought a Vive over and Oculus ear...</td>\n      <td>Games</td>\n      <td>Vice</td>\n    </tr>\n    <tr>\n      <th>16078</th>\n      <td>Windows 10's Killer App? Xbox One Games</td>\n      <td>Microsoft's crusade to get the world to instal...</td>\n      <td>Tech by VICE</td>\n      <td>Vice</td>\n    </tr>\n    <tr>\n      <th>20318</th>\n      <td>Black Friday Not Your Thing? Play These Free G...</td>\n      <td>It's Black Friday, the oh-so-American shopping...</td>\n      <td>Games</td>\n      <td>Vice</td>\n    </tr>\n    <tr>\n      <th>25785</th>\n      <td>Nintendo’s Win at E3 Shows That It's a Console...</td>\n      <td>​ E3 has come and gone for 2016, the LA expo o...</td>\n      <td>Games</td>\n      <td>Vice</td>\n    </tr>\n    <tr>\n      <th>29653</th>\n      <td>You Can Smell Like a Gamer With Lynx’s New Xbo...</td>\n      <td>Gamers in Australia and New Zealand will soon ...</td>\n      <td>Games</td>\n      <td>Vice</td>\n    </tr>\n    <tr>\n      <th>33234</th>\n      <td>It’s Old and It’s Clunky, But You Really Must ...</td>\n      <td>When Dragon's Dogma first popped up in 2012, t...</td>\n      <td>Games</td>\n      <td>Vice</td>\n    </tr>\n    <tr>\n      <th>34617</th>\n      <td>Nintendo’s Win at E3 Shows That It's a Console...</td>\n      <td>E3 has come and gone for 2016, the LA expo of ...</td>\n      <td>Games</td>\n      <td>Vice</td>\n    </tr>\n    <tr>\n      <th>38608</th>\n      <td>PC Gaming Is Still Way Too Hard</td>\n      <td>Here's Motherboard's super simple guide to bui...</td>\n      <td>Tech by VICE</td>\n      <td>Vice</td>\n    </tr>\n    <tr>\n      <th>41444</th>\n      <td>Here’s Everything That Happened at the Xbox E3...</td>\n      <td>That's Xbox's Big Show for E3 2016 over and do...</td>\n      <td>Games</td>\n      <td>Vice</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-40de5ea3-0e1d-48b5-b827-718415441735')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 0 24 24\"\n       width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg>\n</button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n<script>\nconst buttonEl =\n    document.querySelector('#df-40de5ea3-0e1d-48b5-b827-718415441735 button.colab-df-convert');\nbuttonEl.style.display =\n    google.colab.kernel.accessAllowed ? 'block' : 'none';\n\nasync function convertToInteractive(key) {\n    const element = document.querySelector('#df-40de5ea3-0e1d-48b5-b827-718415441735');\n    const dataTable =\n    await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                [key], {});\n    if (!dataTable) return;\n\n    const docLinkHtml = 'Like what you see? Visit the ' +\n    '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n    + ' to learn more about interactive tables.';\n    element.innerHTML = '';\n    dataTable['output_type'] = 'display_data';\n    await google.colab.output.renderOutput(dataTable, element);\n    const docLink = document.createElement('div');\n    docLink.innerHTML = docLinkHtml;\n    element.appendChild(docLink);\n}\n</script>\n</div>\n\n  </div>\n\n    This table contains recommended articles for the user:\n\n  <div id=\"df-ad60df1b-7a3e-439a-9304-5cdca611d5f4\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n\n</style>\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n      <th>title</th>\n      <th>section</th>\n      <th>publication</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>34617</td>\n      <td>0.966389</td>\n      <td>Nintendo’s Win at E3 Shows That It's a Console...</td>\n      <td>Games</td>\n      <td>Vice</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>63293</td>\n      <td>0.965053</td>\n      <td>A Title Card vs Six Teraflops: How Metroid Sto...</td>\n      <td>Games</td>\n      <td>Vice</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>25785</td>\n      <td>0.964193</td>\n      <td>Nintendo’s Win at E3 Shows That It's a Console...</td>\n      <td>Games</td>\n      <td>Vice</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>16771</td>\n      <td>0.963487</td>\n      <td>The Lo-Fi Flaws That Define Our Favorite Old G...</td>\n      <td>Games</td>\n      <td>Vice</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>38608</td>\n      <td>0.960349</td>\n      <td>PC Gaming Is Still Way Too Hard</td>\n      <td>Tech by VICE</td>\n      <td>Vice</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>121140</td>\n      <td>0.960174</td>\n      <td>Microsoft’s New Direction All Started With the...</td>\n      <td>Tech by VICE</td>\n      <td>Vice</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>160409</td>\n      <td>0.959802</td>\n      <td>Sometimes a David Bowie Song Gets Your Favorit...</td>\n      <td>Tech by VICE</td>\n      <td>Vice</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>29653</td>\n      <td>0.959628</td>\n      <td>You Can Smell Like a Gamer With Lynx’s New Xbo...</td>\n      <td>Games</td>\n      <td>Vice</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>156585</td>\n      <td>0.959380</td>\n      <td>Google Takes Aim at PlayStation, Xbox With Gam...</td>\n      <td>Games</td>\n      <td>Vice</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>185864</td>\n      <td>0.958856</td>\n      <td>The Switch Succeeds on Nintendo's Historic \"To...</td>\n      <td>Games</td>\n      <td>Vice</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad60df1b-7a3e-439a-9304-5cdca611d5f4')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 0 24 24\"\n       width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg>\n</button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n<script>\nconst buttonEl =\n    document.querySelector('#df-ad60df1b-7a3e-439a-9304-5cdca611d5f4 button.colab-df-convert');\nbuttonEl.style.display =\n    google.colab.kernel.accessAllowed ? 'block' : 'none';\n\nasync function convertToInteractive(key) {\n    const element = document.querySelector('#df-ad60df1b-7a3e-439a-9304-5cdca611d5f4');\n    const dataTable =\n    await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                [key], {});\n    if (!dataTable) return;\n\n    const docLinkHtml = 'Like what you see? Visit the ' +\n    '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n    + ' to learn more about interactive tables.';\n    element.innerHTML = '';\n    dataTable['output_type'] = 'display_data';\n    await google.colab.output.renderOutput(dataTable, element);\n    const docLink = document.createElement('div');\n    docLink.innerHTML = docLinkHtml;\n    element.appendChild(docLink);\n}\n</script>\n</div>\n\n  </div>\n\n    A word-cloud representing the results:\n\n![Wordcloud of recommended entertainment articles](https://raw.githubusercontent.com/pinecone-io/img/main/personalized-content-recommendations-example-2.png)\n\n### Query Business User\n\n```python\n# first create a user who likes to read about Wall Street business news\nbusiness_user = uploaded_data.loc[((uploaded_data['section'] == 'Business News')|\n                                   (uploaded_data['section'] == 'business')) &\n                                   (uploaded_data['article'].str.contains('Wall Street'))][:10]\n\nprint('\\nHere is the example of previously read articles by this user:\\n')\ndisplay(business_user[['title', 'article', 'section', 'publication']])\n\n# then create a vector for this user\na = business_user['article_vector']\nbusiness_user_vector = [*map(mean, zip(*a))]\n\n# query the pinecone\nres = index.query(business_user_vector, top_k=10)\n\n# print results\nids = [match.id for match in res.matches]\nscores = [match.score for match in res.matches]\ndf = pd.DataFrame({'id': ids, \n                   'score': scores,\n                   'title': [titles_mapped[int(_id)] for _id in ids],\n                   'section': [sections_mapped[int(_id)] for _id in ids],\n                   'publication': [publications_mapped[int(_id)] for _id in ids]\n                    })\n\nprint(\"\\nThis table contains recommended articles for the user:\\n\")\ndisplay(df)\nprint(\"\\nA word-cloud representing the results:\\n\")\nget_wordcloud_for_user(df)\n```\n\n    Here is the example of previously read articles by this user:\n\n  <div id=\"df-e8d6ebdc-61ac-4489-87a5-ac67cd1e8dfc\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n\n</style>\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>article</th>\n      <th>section</th>\n      <th>publication</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>370</th>\n      <td>Wall St. falls as investors eye a united hawki...</td>\n      <td>NEW YORK (Reuters) - Wall Street’s major index...</td>\n      <td>Business News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>809</th>\n      <td>Oil surges on tanker attacks; stocks rise on F...</td>\n      <td>NEW YORK (Reuters) - Oil futures rose on Thurs...</td>\n      <td>Business News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>885</th>\n      <td>A look at Tesla's nine-member board</td>\n      <td>(Reuters) - Tesla Inc’s board has named a spec...</td>\n      <td>Business News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>1049</th>\n      <td>Home Depot posts rare sales miss as delayed sp...</td>\n      <td>(Reuters) - Home Depot Inc (HD.N) on Tuesday m...</td>\n      <td>Business News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>1555</th>\n      <td>PepsiCo's mini-sized sodas boost quarterly res...</td>\n      <td>(Reuters) - PepsiCo Inc’s (PEP.O) quarterly re...</td>\n      <td>Business News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>1638</th>\n      <td>Wall Street extends rally on U.S.-China trade ...</td>\n      <td>NEW YORK (Reuters) - U.S. stocks rallied on Fr...</td>\n      <td>Business News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>1900</th>\n      <td>U.S. plans limits on Chinese investment in U.S...</td>\n      <td>WASHINGTON (Reuters) - The U.S. Treasury Depar...</td>\n      <td>Business News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>2109</th>\n      <td>Exxon Mobil, Chevron dogged by refining, chemi...</td>\n      <td>HOUSTON (Reuters) - Exxon Mobil Corp and Chevr...</td>\n      <td>Business News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>2286</th>\n      <td>Wall Street soars on U.S. rate cut hopes</td>\n      <td>NEW YORK (Reuters) - Wall Street’s three major...</td>\n      <td>Business News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>2563</th>\n      <td>Apple shares drop on iPhone suppliers' warnings</td>\n      <td>(Reuters) - Apple Inc (AAPL.O) shares fell to ...</td>\n      <td>Business News</td>\n      <td>Reuters</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n  <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e8d6ebdc-61ac-4489-87a5-ac67cd1e8dfc')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n   <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 0 24 24\"\n       width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg>\n  </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n<script>\nconst buttonEl =\n    document.querySelector('#df-e8d6ebdc-61ac-4489-87a5-ac67cd1e8dfc button.colab-df-convert');\nbuttonEl.style.display =\n    google.colab.kernel.accessAllowed ? 'block' : 'none';\n\nasync function convertToInteractive(key) {\n    const element = document.querySelector('#df-e8d6ebdc-61ac-4489-87a5-ac67cd1e8dfc');\n    const dataTable =\n    await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                [key], {});\n    if (!dataTable) return;\n\n    const docLinkHtml = 'Like what you see? Visit the ' +\n    '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n    + ' to learn more about interactive tables.';\n    element.innerHTML = '';\n    dataTable['output_type'] = 'display_data';\n    await google.colab.output.renderOutput(dataTable, element);\n    const docLink = document.createElement('div');\n    docLink.innerHTML = docLinkHtml;\n    element.appendChild(docLink);\n}\n</script>\n</div>\n\n  </div>\n\n    This table contains recommended articles for the user:\n\n  <div id=\"df-533066f8-08a5-4e0e-88fe-a60ad0d5ad3a\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n\n</style>\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n      <th>title</th>\n      <th>section</th>\n      <th>publication</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>131603</td>\n      <td>0.970929</td>\n      <td>US STOCKS-Wall Street muted as rate cut bets t...</td>\n      <td>Market News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>93287</td>\n      <td>0.970408</td>\n      <td>MONEY MARKETS-U.S. rate-cut bets in June slip ...</td>\n      <td>Bonds News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>159587</td>\n      <td>0.970357</td>\n      <td>Wall Street ekes out gain, Apple cuts revenue ...</td>\n      <td>Business News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>53602</td>\n      <td>0.969963</td>\n      <td>US STOCKS-Wall St drops on trade worries, Fed ...</td>\n      <td>Market News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>45533</td>\n      <td>0.969199</td>\n      <td>Wall Street wavers as tech gives ground and in...</td>\n      <td>Business News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>147320</td>\n      <td>0.968576</td>\n      <td>Dented Fed rate cut hopes drag on stocks; doll...</td>\n      <td>Davos</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>152313</td>\n      <td>0.968503</td>\n      <td>MIDEAST - Factors to watch - July 9</td>\n      <td>Earnings Season</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>34583</td>\n      <td>0.968178</td>\n      <td>Global stocks rally after speech by Fed's Powe...</td>\n      <td>Business News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>89976</td>\n      <td>0.968087</td>\n      <td>Stocks, yields rise after deal announced to en...</td>\n      <td>Business News</td>\n      <td>Reuters</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>96107</td>\n      <td>0.968018</td>\n      <td>Wall Street surges on higher oil after U.S. qu...</td>\n      <td>Business News</td>\n      <td>Reuters</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-533066f8-08a5-4e0e-88fe-a60ad0d5ad3a')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 0 24 24\"\n       width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg>\n</button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n<script>\nconst buttonEl =\n    document.querySelector('#df-533066f8-08a5-4e0e-88fe-a60ad0d5ad3a button.colab-df-convert');\nbuttonEl.style.display =\n    google.colab.kernel.accessAllowed ? 'block' : 'none';\n\nasync function convertToInteractive(key) {\n    const element = document.querySelector('#df-533066f8-08a5-4e0e-88fe-a60ad0d5ad3a');\n    const dataTable =\n    await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                [key], {});\n    if (!dataTable) return;\n\n    const docLinkHtml = 'Like what you see? Visit the ' +\n    '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n    + ' to learn more about interactive tables.';\n    element.innerHTML = '';\n    dataTable['output_type'] = 'display_data';\n    await google.colab.output.renderOutput(dataTable, element);\n    const docLink = document.createElement('div');\n    docLink.innerHTML = docLinkHtml;\n    element.appendChild(docLink);\n}\n</script>\n</div>\n\n  </div>\n\n    A word-cloud representing the results:\n\n![Wordcloud of recommended business articles](https://raw.githubusercontent.com/pinecone-io/img/main/personalized-content-recommendations-example-3.png)\n\n### Query Results\n\nWe can see that each user's recommendations have a high similarity to what the user actually reads. A user who likes Tennis news has plenty of Tennis news recommendations. A user who likes to read about Xbox has that kind of news. And a business user has plenty of Wall Street news that he/she enjoys.\n\nFrom the word-cloud, you can see the most frequent words that appear in the recommended articles' titles.\n\nSince we used only the title and the content of the article to define the embeddings, and we did not take publications and sections into account, a user may get recommendations from a publication/section that he does not regularly read. You may try adding this information when creating embeddings as well and check your query results then!\n\nAlso, you may notice that some articles appear in the recommendations, although the user has already read them. These articles could be removed as part of postprocessing the query results, in case you prefer not to see them in the recommendations.\n\n## Delete the index\n\nDelete the index once you are sure that you do not want to use it anymore. Once it is deleted, you cannot use it again.\n\n```python\npinecone.delete_index(index_name)\n```\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1cce7"
  },
  "filename": "movie-recommender.md",
  "title": "Movie Recommender",
  "category": "630fc5235d91a70054705fb7",
  "content": "---\ntitle: Movie Recommender\ncategory: 630fc5235d91a70054705fb7\n---\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/recommendation/movie-recommender/00_movie_recommender.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/recommendation/movie-recommender/00_movie_recommender.ipynb) [![Open github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/pinecone-io/examples/blob/master/recommendation/movie-recommender/00_movie_recommender.ipynb)\n\nThis notebook demonstrates how Pinecone helps you build a simple Movie Recommender System. There are three parts to this recommender system:\n\n- A dataset containing movie ratings\n- Two neural network models for embedding movies and users\n- A vector index to perform similarity search on those embeddings\n\nThe architecture of our recommender system is shown below. We have two models, a user model and a movie model, which generate embedding for users and movies. The two models are trained such that the proximity between a user and a movie in the multi-dimensional vector space depends on the rating given by the user for that movie. This means if a user gives a high rating to a movie, the movie will be closer to the user in the multi-dimensional vector space and vice versa. The result is that users with similar movie preferences and the movies they rated highly become closer in the vector space. A similarity search in this vector space for a user would give new recommendations based on the shared movie preference with other users.\n\n![Network Architecture Diagram](https://raw.githubusercontent.com/pinecone-io/img/main/movie-recommender.png)\n\n## Install Dependencies\n\n\n```python\n!pip install datasets transformers pinecone-client tensorflow\n```\n\n## Load the Dataset\n\nWe will use a subset of the [MovieLens 25M Dataset](https://grouplens.org/datasets/movielens/25m/) in this project. This dataset contains ~1M user ratings provided by over 30k unique users for the most recent ~10k movies from the [MovieLens 25M Dataset](https://grouplens.org/datasets/movielens/25m/). The subset is available [here](https://huggingface.co/datasets/pinecone/movielens-recent-ratings) on HuggingFace datasets.\n\n\n```python\nfrom datasets import load_dataset\n\n# load the dataset into a pandas datafame\nmovies = load_dataset(\"pinecone/movielens-recent-ratings\", split=\"train\").to_pandas()\n```\n\n\n```python\n# drop duplicates to return only unique movies\nunique_movies = movies.drop_duplicates(subset=\"imdb_id\")\nunique_movies.head()\n```\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>imdb_id</th>\n      <th>movie_id</th>\n      <th>user_id</th>\n      <th>rating</th>\n      <th>title</th>\n      <th>poster</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>tt5027774</td>\n      <td>6705</td>\n      <td>4556</td>\n      <td>4.0</td>\n      <td>Three Billboards Outside Ebbing, Missouri (2017)</td>\n      <td>https://m.media-amazon.com/images/M/MV5BMjI0OD...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>tt5463162</td>\n      <td>7966</td>\n      <td>20798</td>\n      <td>3.5</td>\n      <td>Deadpool 2 (2018)</td>\n      <td>https://m.media-amazon.com/images/M/MV5BMDkzNm...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>tt4007502</td>\n      <td>1614</td>\n      <td>26543</td>\n      <td>4.5</td>\n      <td>Frozen Fever (2015)</td>\n      <td>https://m.media-amazon.com/images/M/MV5BMjY3YT...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>tt4209788</td>\n      <td>7022</td>\n      <td>4106</td>\n      <td>4.0</td>\n      <td>Molly's Game (2017)</td>\n      <td>https://m.media-amazon.com/images/M/MV5BNTkzMz...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>tt2948356</td>\n      <td>3571</td>\n      <td>15259</td>\n      <td>4.0</td>\n      <td>Zootopia (2016)</td>\n      <td>https://m.media-amazon.com/images/M/MV5BOTMyMj...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n## Initialize Embedding Models\n\nThe `user_model` and `movie_model` are trained using Tensorflow Keras. The `user_model` transforms a given `user_id` into a 32-dimensional embedding in the same vector space as the movies, representing the user’s movie preference. The movie recommendations are then fetched based on proximity to the user’s location in the multi-dimensional space.\n\nSimilarly, the `movie_model` transforms a given `movie_id` into a 32-dimensional embedding in the same vector space as other similar movies — making it possible to find movies similar to a given movie.\n\n\n```python\nfrom huggingface_hub import from_pretrained_keras\n\n# load the user model and movie model from huggingface\nuser_model = from_pretrained_keras(\"pinecone/movie-recommender-user-model\")\nmovie_model = from_pretrained_keras(\"pinecone/movie-recommender-movie-model\")\n```\n\n\n## Create Pinecone Index\n\nTo create our vector index, we first need to initialize our connection to Pinecone. For this we need a [free API key](https://app.pinecone.io/). You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**. Once we have those, we initialize the connection like so:\n\n\n```python\nimport pinecone\n\n# connect to pinecone environment\npinecone.init(\n    api_key=\"<<YOUR_API_KEY>>\",\n    environment=\"YOUR_ENVIRONMENT\"\n)\n```\n\nNow we create a new index called `\"movie-emb\"`. What we name this isn't important.\n\n\n```python\nindex_name = 'movie-emb'\n\n# check if the movie-emb index exists\nif index_name not in pinecone.list_indexes():\n    # create the index if it does not exist\n    pinecone.create_index(\n        index_name,\n        dimension=32,\n        metric=\"cosine\"\n    )\n\n# connect to movie-emb index we created\nindex = pinecone.Index(index_name)\n```\n\n## Create Movie Embeddings\n\nWe will be creating movie embeddings using the pretrained `movie_model`. All of the movie embeddings will be upserted to the new `\"movie-emb\"` index in Pinecone.\n\n\n```python\nfrom tqdm.auto import tqdm\n\n# we will use batches of 64\nbatch_size = 64\n\nfor i in tqdm(range(0, len(unique_movies), batch_size)):\n    # find end of batch\n    i_end = min(i+batch_size, len(unique_movies))\n    # extract batch\n    batch = unique_movies.iloc[i:i_end]\n    # generate embeddings for batch\n    emb = movie_model.predict(batch['movie_id']).tolist()\n    # get metadata\n    meta = batch.to_dict(orient='records')\n    # create IDs\n    ids = batch[\"imdb_id\"].values.tolist()\n    # add all to upsert list\n    to_upsert = list(zip(ids, emb, meta))\n    # upsert/insert these records to pinecone\n    _ = index.upsert(vectors=to_upsert)\n\n# check that we have all vectors in index\nindex.describe_index_stats()\n```\n\n    {'dimension': 32,\n     'index_fullness': 0.0,\n     'namespaces': {'': {'vector_count': 10269}},\n     'total_vector_count': 10269}\n\n\n\n## Get Recommendations\n\nWe now have movie embeddings stored in Pinecone. To get recommendations, we can do one of two things:\n\n+ Get a user embedding via a user embedding model and our `user_id`s, and retrieve movie embeddings that are most similar from Pinecone.\n+ Use an existing movie embedding to retrieve other similar movies.\n\nBoth of these options use the same approach; the only difference is the source of data (user vs. movie) and the embedding model (user vs. movie).\n\nWe will start with the strategy of getting recommendations for a user embedding.\n\n\n### Get recommendations for a user\n```python\n# we do this to display movie posters in a jupyter notebook\nfrom IPython.core.display import HTML\n```\n\nWe will start by looking at a user's top rated movies. We can find this information inside the `movies` dataframe by filtering for movie ratings by a specific user (as per their `user_id`) and ordering these by the rating score.\n\n\n```python\ndef top_movies_user_rated(user):\n    # get list of movies that the user has rated\n    user_movies = movies[movies[\"user_id\"] == user]\n    # order by their top rated movies\n    top_rated = user_movies.sort_values(by=['rating'], ascending=False)\n    # return the top 14 movies\n    return top_rated['poster'].tolist()[:14], top_rated['rating'].tolist()[:14]\n```\n\nAfter this, we can define a function called `display_posters` that will take a list of movie posters (like those returned by `top_movies_user_rated`) and display them in the notebook.\n\n\n```python\ndef display_posters(posters):\n    figures = []\n    for poster in posters:\n        figures.append(f'''\n            <figure style=\"margin: 5px !important;\">\n              <img src=\"{poster}\" style=\"width: 120px; height: 150px\" >\n            </figure>\n        ''')\n    return HTML(data=f'''\n        <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n        {''.join(figures)}\n        </div>\n    ''')\n```\n\nLet's take a look at user `3`'s top rated movies:\n\n\n```python\nuser = 3\ntop_rated, scores = top_movies_user_rated(user)\ndisplay_posters(top_rated)\n```\n\n\n<div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMDliOTIzNmUtOTllOC00NDU3LWFiNjYtMGM0NDc1YTMxNjYxXkEyXkFqcGdeQXVyNTM3NzExMDQ@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMjQ0MTgyNjAxMV5BMl5BanBnXkFtZTgwNjUzMDkyODE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTM4OGJmNWMtOTM4Ni00NTE3LTg3MDItZmQxYjc4N2JhNmUxXkEyXkFqcGdeQXVyNTgzMDMzMTg@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTExMzU0ODcxNDheQTJeQWpwZ15BbWU4MDE1OTI4MzAy._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTc2MTQ3MDA1Nl5BMl5BanBnXkFtZTgwODA3OTI4NjE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n</div>\n\n\n```python\nprint(scores)\n```\n\n    [4.5, 4.0, 4.0, 2.5, 2.5]\n\n\nUser `3` has rated these five movies, with *Big Hero 6*, *Civil War*, and *Avengers* being given good scores. They seem less enthusiastic about more sci-fi films like *Arrival* and *The Martian*.\n\nNow let's see how to make some movie recommendations for this user.\n\nStart by defining the `get_recommendations` function. Given a specific `user_id`, this uses the `user_model` to create a user embedding (`xq`). It then retrieves the most similar movie vectors from Pinecone (`xc`), and extracts the relevant movie posters so we can display them later.\n\n\n```python\ndef get_recommendations(user):\n    # generate embeddings for the user\n    xq = user_model([user]).numpy().tolist()\n    # compute cosine similarity between user and movie vectors and return top k movies\n    xc = index.query(xq, top_k=14,\n                    include_metadata=True)\n    result = []\n    # iterate through results and extract movie posters\n    for match in xc['matches']:\n        poster = match['metadata']['poster']\n        result.append(poster)\n    return result\n```\n\nNow we can retrieve recommendations for the user.\n\n\n```python\nurls = get_recommendations(user)\ndisplay_posters(urls)\n```\n\n<div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMDliOTIzNmUtOTllOC00NDU3LWFiNjYtMGM0NDc1YTMxNjYxXkEyXkFqcGdeQXVyNTM3NzExMDQ@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMjQ0MTgyNjAxMV5BMl5BanBnXkFtZTgwNjUzMDkyODE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTM4OGJmNWMtOTM4Ni00NTE3LTg3MDItZmQxYjc4N2JhNmUxXkEyXkFqcGdeQXVyNTgzMDMzMTg@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMGQzN2Y0NDYtOGNlOS00OTVjLTkzMGUtZjYzNzdlMjQxMzgzXkEyXkFqcGdeQXVyNTY4NTYzMDM@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMjMxNjY2MDU1OV5BMl5BanBnXkFtZTgwNzY1MTUwNTM@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BNTk4ODQ1MzgzNl5BMl5BanBnXkFtZTgwMTMyMzM4MTI@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMjE0ODYxMzI2M15BMl5BanBnXkFtZTgwMDczODA2MDE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BNjM0NTc0NzItM2FlYS00YzEwLWE0YmUtNTA2ZWIzODc2OTgxXkEyXkFqcGdeQXVyNTgwNzIyNzg@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTU0NDE0Nzk1NF5BMl5BanBnXkFtZTgwMTY1NTAxMzE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMGZlNTY1ZWUtYTMzNC00ZjUyLWE0MjQtMTMxN2E3ODYxMWVmXkEyXkFqcGdeQXVyMDM2NDM2MQ@@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTAwMjU5OTgxNjZeQTJeQWpwZ15BbWU4MDUxNDYxODEx._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMjMyNDkzMzI1OF5BMl5BanBnXkFtZTgwODcxODg5MjI@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTc5MDE2ODcwNV5BMl5BanBnXkFtZTgwMzI2NzQ2NzM@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BYjhlNDljNTgtZjc4My00NmZmLTk2YzAtYWE5MDYwYjM4MTkzXkEyXkFqcGdeQXVyODE5NzE3OTE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n</div>\n\n\nThat looks good: the top results actually match the user's three favorite results. Following this, we see a lot of Marvel superhero films, which user `3` is probably going to enjoy, judging from their current ratings.\n\nLet's see another user. This time, we choose user `128`.\n\n\n```python\nuser = 128\ntop_rated, scores = top_movies_user_rated(user)\ndisplay_posters(top_rated)\n```\n\n\n<div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTAwMjU5OTgxNjZeQTJeQWpwZ15BbWU4MDUxNDYxODEx._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BOTgwMzFiMWYtZDhlNS00ODNkLWJiODAtZDVhNzgyNzJhYjQ4L2ltYWdlXkEyXkFqcGdeQXVyNzEzOTYxNTQ@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMjQ0MTgyNjAxMV5BMl5BanBnXkFtZTgwNjUzMDkyODE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTk0MDQ3MzAzOV5BMl5BanBnXkFtZTgwNzU1NzE3MjE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BYmI5ZGIxOGMtMjcwMS00Yzk3LWE0YWUtMzc5YTFhNGQ4OWZmXkEyXkFqcGdeQXVyNTIzOTk5ODM@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BOTc2OTA1MDM4M15BMl5BanBnXkFtZTgwNjczMDk5MjE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTQ1MjQwMTE5OF5BMl5BanBnXkFtZTgwNjk3MTcyMDE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BOTgxMDQwMDk0OF5BMl5BanBnXkFtZTgwNjU5OTg2NDE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BOTAzODEzNDAzMl5BMl5BanBnXkFtZTgwMDU1MTgzNzE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTUxMjQ2NjI4OV5BMl5BanBnXkFtZTgwODc2NjUwNDE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BOTMyMjEyNzIzMV5BMl5BanBnXkFtZTgwNzIyNjU0NzE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTM4OGJmNWMtOTM4Ni00NTE3LTg3MDItZmQxYjc4N2JhNmUxXkEyXkFqcGdeQXVyNTgzMDMzMTg@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BNzQ1MjQzMzM3OF5BMl5BanBnXkFtZTcwMzg3NzQ3OQ@@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTc2MTQ3MDA1Nl5BMl5BanBnXkFtZTgwODA3OTI4NjE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n</div>\n\n```python\nprint(scores)\n```\n\n    [4.5, 4.5, 4.5, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0]\n\n\nBecause this user seems to like everything, they also get recommended a mix of different things:\n\n\n```python\nurls = get_recommendations(user)\ndisplay_posters(urls)\n```\n\n\n<div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BYjFhOWY0OTgtNDkzMC00YWJkLTk1NGEtYWUxNjhmMmQ5ZjYyXkEyXkFqcGdeQXVyMjMxOTE0ODA@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BNjIwYjg1ZTEtOWRjYy00MTY3LWIyYTktMTI1Zjk2YzZkNDhiL2ltYWdlL2ltYWdlXkEyXkFqcGdeQXVyMjExNjgyMTc@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BNDEwZjU4ZmYtNjk0Ny00ZjVjLWE4OGUtNWE5NzFhNDI0MjgyXkEyXkFqcGdeQXVyNjU2NTIyOTE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTUzMjAxMzg5M15BMl5BanBnXkFtZTgwNjIxNjk5NzE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTQ4NzI2OTg5NV5BMl5BanBnXkFtZTgwNjQ3MDgyMjE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BNDQ4YTAyNDktMDhhYi00MzgyLWI0ZTktMjNiMGQ4MGU0NDQyXkEyXkFqcGdeQXVyNDY5MTUyNjU@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMjk4NGZiMzAtODU1NS00MmQ4LWJiNmQtNWU5ZWU4Y2VmNWI0XkEyXkFqcGdeQXVyODE5NzE3OTE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BYWE1NWFhZWEtOTYxZS00NTZmLWE5OWItMGQ2MTYzODNiNjQxXkEyXkFqcGdeQXVyNDM1ODc2NzE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BZWVlZmQ5N2EtZTQ2My00ZDUzLThkMmQtMDgyYTgwZWZlMjA0XkEyXkFqcGdeQXVyMjQ5NjMxNDA@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BYjkzZWIyZTctN2U3Ny00MDZlLTkzZTYtMTI2MWI5YTFiZWZkXkEyXkFqcGdeQXVyNTM2NTg3Nzg@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BYmJhZmJlYTItZmZlNy00MGY0LTg0ZGMtNWFkYWU5NTA1YTNhXkEyXkFqcGdeQXVyODE5NzE3OTE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMDNjZjkyNjQtNWMyMC00ODA5LTgyODctOGRiOWUwYTAzOWVjXkEyXkFqcGdeQXVyODE5NzE3OTE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTQzMjE5NDQwMl5BMl5BanBnXkFtZTgwMjI2NzA2MDE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BY2ViYjZiYTktZmJmMS00MGU5LTkxYjgtZWNkYzIyMGFjNWU4XkEyXkFqcGdeQXVyNTMzOTU3NzA@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n</div>\n\n\n```python\nuser = 20000\ntop_rated, scores = top_movies_user_rated(user)\ndisplay_posters(top_rated)\n```\n\n<div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTU2NjA1ODgzMF5BMl5BanBnXkFtZTgwMTM2MTI4MjE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BN2U1YzdhYWMtZWUzMi00OWI1LWFkM2ItNWVjM2YxMGQ2MmNhXkEyXkFqcGdeQXVyNjU0OTQ0OTY@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMzM5NjUxOTEyMl5BMl5BanBnXkFtZTgwNjEyMDM0MDE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTc2MTQ3MDA1Nl5BMl5BanBnXkFtZTgwODA3OTI4NjE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMDliOTIzNmUtOTllOC00NDU3LWFiNjYtMGM0NDc1YTMxNjYxXkEyXkFqcGdeQXVyNTM3NzExMDQ@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BOTgwMzFiMWYtZDhlNS00ODNkLWJiODAtZDVhNzgyNzJhYjQ4L2ltYWdlXkEyXkFqcGdeQXVyNzEzOTYxNTQ@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BOTMyMjEyNzIzMV5BMl5BanBnXkFtZTgwNzIyNjU0NzE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n</div>\n\n\n```python\nprint(scores)\n```\n\n    [5.0, 4.0, 3.5, 3.5, 3.5, 3.0, 1.0]\n\n\nWe can see more of a trend towards action films with this user, so we can expect to see similar action-focused recommendations.\n\n\n```python\nurls = get_recommendations(user)\ndisplay_posters(urls)\n```\n\n\n<div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMjE2NDkxNTY2M15BMl5BanBnXkFtZTgwMDc2NzE0MTI@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTU2NjA1ODgzMF5BMl5BanBnXkFtZTgwMTM2MTI4MjE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTUyODU0ODU1Ml5BMl5BanBnXkFtZTgwNzM1MjIyMDE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMjIzMzA5NDk0NF5BMl5BanBnXkFtZTgwMDY2OTE2OTE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTg5MTE2NjA4OV5BMl5BanBnXkFtZTgwMTUyMjczMTE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BOGQyYTZjMDktMWQwNS00Y2NiLTg5MDctMjE3NjU5MjhmZDdiXkEyXkFqcGdeQXVyNTE0MDY4Mjk@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BNDA0MTlkYWQtNzNiMS00ZWE3LTg2ODUtZTQwNmVkN2E3M2NhXkEyXkFqcGdeQXVyNjUzNjY0NTE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTk1MjUzNzM0OF5BMl5BanBnXkFtZTgwMTg2MzIxMTI@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BYzc5MTU4N2EtYTkyMi00NjdhLTg3NWEtMTY4OTEyMzJhZTAzXkEyXkFqcGdeQXVyNjc1NTYyMjg@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMjMyNDkzMzI1OF5BMl5BanBnXkFtZTgwODcxODg5MjI@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BODg2OTVhZGQtYTU3Yi00NDg3LTljNzQtMjZhNDBhZjNlOGEyXkEyXkFqcGdeQXVyNjU1OTg4OTM@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BOTQyODc5MTAwM15BMl5BanBnXkFtZTgwNjMwMjA1MjE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BYmY0NGNiNzQtYmQ2Yi00OWEyLThmMWMtZjUzM2UwNDg1YjUxXkEyXkFqcGdeQXVyNTg4MTExMTg@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BM2Y3ZGM1OGItMTNjZS00MzI3LThkOGEtMDA2MmFlOTVlMTVhXkEyXkFqcGdeQXVyMTMxODk2OTU@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n</div>\n\n### Find Similar Movies\n\nNow let's see how to find some similar movies.\n\nStart by defining the `get_similar_movies` function. Given a specific `imdb_id`, we query directly using the pre-existing embedding for that ID stored in Pinecone.\n\n\n```python\n# search for similar movies in pinecone index\ndef get_similar_movies(imdb_id):\n    # compute cosine similarity between movie and embedding vectors and return top k movies\n    xc = index.query(id=imdb_id, top_k=14, include_metadata=True)\n    result = []\n    # iterate through results and extract movie posters\n    for match in xc['matches']:\n        poster = match['metadata']['poster']\n        result.append(poster)\n    return result\n```\n\n\n```python\n# imdbid of Avengers Infinity War\nimdb_id = \"tt4154756\"\n# filter the imdbid from the unique_movies\nmovie = unique_movies[unique_movies[\"imdb_id\"] == imdb_id]\nmovie\n```\n\n\n  <div id=\"df-ab0e361d-f5c4-46bb-a2b9-b79d18186f20\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>imdb_id</th>\n      <th>movie_id</th>\n      <th>user_id</th>\n      <th>rating</th>\n      <th>title</th>\n      <th>poster</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>11</th>\n      <td>tt4154756</td>\n      <td>1263</td>\n      <td>153</td>\n      <td>4.0</td>\n      <td>Avengers: Infinity War - Part I (2018)</td>\n      <td>https://m.media-amazon.com/images/M/MV5BMjMxNj...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab0e361d-f5c4-46bb-a2b9-b79d18186f20')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n       width=\"24px\">\n    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n  </svg>\n      </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n  <script>\n    const buttonEl =\n      document.querySelector('#df-ab0e361d-f5c4-46bb-a2b9-b79d18186f20 button.colab-df-convert');\n    buttonEl.style.display =\n      google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n    async function convertToInteractive(key) {\n      const element = document.querySelector('#df-ab0e361d-f5c4-46bb-a2b9-b79d18186f20');\n      const dataTable =\n        await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                  [key], {});\n      if (!dataTable) return;\n\n      const docLinkHtml = 'Like what you see? Visit the ' +\n        '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n        + ' to learn more about interactive tables.';\n      element.innerHTML = '';\n      dataTable['output_type'] = 'display_data';\n      await google.colab.output.renderOutput(dataTable, element);\n      const docLink = document.createElement('div');\n      docLink.innerHTML = docLinkHtml;\n      element.appendChild(docLink);\n    }\n  </script>\n</div>\n  </div>\n\n\n```python\n# display the poster of the movie\ndisplay_posters(movie[\"poster\"])\n```\n\n\n<div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMjMxNjY2MDU1OV5BMl5BanBnXkFtZTgwNzY1MTUwNTM@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n</div>\n\nNow we have *Avengers: Infinity War*. Let's find movies that are similar to this movie.\n\n\n```python\nsimilar_movies = get_similar_movies(imdb_id)\ndisplay_posters(similar_movies)\n```\n\n\n<div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMjMxNjY2MDU1OV5BMl5BanBnXkFtZTgwNzY1MTUwNTM@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTc5MDE2ODcwNV5BMl5BanBnXkFtZTgwMzI2NzQ2NzM@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMjMyNDkzMzI1OF5BMl5BanBnXkFtZTgwODcxODg5MjI@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMjQ0MTgyNjAxMV5BMl5BanBnXkFtZTgwNjUzMDkyODE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTAwMjU5OTgxNjZeQTJeQWpwZ15BbWU4MDUxNDYxODEx._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BNjM0NTc0NzItM2FlYS00YzEwLWE0YmUtNTA2ZWIzODc2OTgxXkEyXkFqcGdeQXVyNTgwNzIyNzg@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTM4OGJmNWMtOTM4Ni00NTE3LTg3MDItZmQxYjc4N2JhNmUxXkEyXkFqcGdeQXVyNTgzMDMzMTg@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BNTk4ODQ1MzgzNl5BMl5BanBnXkFtZTgwMTMyMzM4MTI@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BYzc5MTU4N2EtYTkyMi00NjdhLTg3NWEtMTY4OTEyMzJhZTAzXkEyXkFqcGdeQXVyNjc1NTYyMjg@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BYzFhMGM5ZTMtOGEyMC00ZTY5LWE1ZDUtNjQzM2NjZDdiMjg3XkEyXkFqcGdeQXVyNzIzMzE0NDY@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BNmUyMzU3YjgtZTliNS00NWM2LWI5ODgtYWE3ZjAzODgyNjNhXkEyXkFqcGdeQXVyNjY1MTg4Mzc@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMjMwNDkxMTgzOF5BMl5BanBnXkFtZTgwNTkwNTQ3NjM@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMDkzNmRhNTMtZDI4NC00Zjg1LTgxM2QtMjYxZDQ3OWJlMDRlXkEyXkFqcGdeQXVyNTU5MjkzMTU@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTc2MTQ3MDA1Nl5BMl5BanBnXkFtZTgwODA3OTI4NjE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n</div>\n\nThe top results closely match *Avengers: Infinity War*, the most similar movie being that movie itself. Following this, we see a lot of other Marvel superhero films.\n\n\nLet's try another movie, this time a cartoon.\n\n\n```python\n# imdbid of Moana\nimdb_id = \"tt3521164\"\n# filter the imdbid from the unique_movies\nmovie = unique_movies[unique_movies[\"imdb_id\"] == imdb_id]\nmovie\n```\n\n\n  <div id=\"df-8d5ad3ee-589c-4ab9-8835-80dd6049362e\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>imdb_id</th>\n      <th>movie_id</th>\n      <th>user_id</th>\n      <th>rating</th>\n      <th>title</th>\n      <th>poster</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>97</th>\n      <td>tt3521164</td>\n      <td>5138</td>\n      <td>24875</td>\n      <td>5.0</td>\n      <td>Moana (2016)</td>\n      <td>https://m.media-amazon.com/images/M/MV5BMjI4Mz...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d5ad3ee-589c-4ab9-8835-80dd6049362e')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n       width=\"24px\">\n    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n  </svg>\n      </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n  <script>\n    const buttonEl =\n      document.querySelector('#df-8d5ad3ee-589c-4ab9-8835-80dd6049362e button.colab-df-convert');\n    buttonEl.style.display =\n      google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n    async function convertToInteractive(key) {\n      const element = document.querySelector('#df-8d5ad3ee-589c-4ab9-8835-80dd6049362e');\n      const dataTable =\n        await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                  [key], {});\n      if (!dataTable) return;\n\n      const docLinkHtml = 'Like what you see? Visit the ' +\n        '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n        + ' to learn more about interactive tables.';\n      element.innerHTML = '';\n      dataTable['output_type'] = 'display_data';\n      await google.colab.output.renderOutput(dataTable, element);\n      const docLink = document.createElement('div');\n      docLink.innerHTML = docLinkHtml;\n      element.appendChild(docLink);\n    }\n  </script>\n</div>\n</div>\n\n\n\n\n\n```python\n# display the poster of the movie\ndisplay_posters(movie[\"poster\"])\n```\n\n\n\n\n\n<div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMjI4MzU5NTExNF5BMl5BanBnXkFtZTgwNzY1MTEwMDI@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n</div>\n\n\n\n\n\n```python\nsimilar_movies = get_similar_movies(imdb_id)\ndisplay_posters(similar_movies)\n```\n\n\n\n\n\n<div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMjI4MzU5NTExNF5BMl5BanBnXkFtZTgwNzY1MTEwMDI@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMzJmNGFmYmMtMmZhOC00MGM2LTk5NWItYzMzZmM1MzgzMTgxXkEyXkFqcGdeQXVyNTM3MDMyMDQ@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMTM4ODg0MzM0MV5BMl5BanBnXkFtZTcwNDY2MTc3Nw@@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BYjQ5NjM0Y2YtNjZkNC00ZDhkLWJjMWItN2QyNzFkMDE3ZjAxXkEyXkFqcGdeQXVyODIxMzk5NjA@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BZGUxZGMzYTYtNjJlMS00OGQ5LTg5YjItN2JjM2Y2NjQzMzdkL2ltYWdlXkEyXkFqcGdeQXVyNTAyODkwOQ@@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BNDMyZDc3YzktNWI3Yy00ZDM1LWJjYTMtY2I2YzRmZGQ5MTU4XkEyXkFqcGdeQXVyMTY5Nzc4MDY@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMjA2Mzg2NDMzNl5BMl5BanBnXkFtZTgwMjcwODUzOTE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMjAyODEwOTE4OV5BMl5BanBnXkFtZTgwNDIzMDc3ODE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BNDk3MzYyMjU5NF5BMl5BanBnXkFtZTgwNzQ5MDkzMzE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BZDUyNzhjMTAtNGI5OC00MjYzLWFlNDUtMTQzYTdhZjliMDk0XkEyXkFqcGdeQXVyNTc5OTMwOTQ@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BOGY5ZDA4MDEtNWIzNi00YjkxLWE3Y2EtNmJiNzBhOWEyMWVjXkEyXkFqcGdeQXVyNTE1NjY5Mg@@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BYzMzZWM0NjYtMmNjMi00MzUzLThlNTAtZWQ1NjQzM2QyNzIwXkEyXkFqcGdeQXVyNDQ5MDYzMTk@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BMzg2Mzg4YmUtNDdkNy00NWY1LWE3NmEtZWMwNGNlMzE5YzU3XkEyXkFqcGdeQXVyMjA5MTIzMjQ@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n<figure style=\"margin: 5px !important;\">\n  <img src=\"https://m.media-amazon.com/images/M/MV5BNDc3YTEzZDItNjE2Yy00Nzg2LTgxMDAtNWMxOTJiMWQxZmNiXkEyXkFqcGdeQXVyMjExNjgyMTc@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" >\n</figure>\n\n</div>\n\n\nThis result quality is good again. The top results include plenty of cartoons.\n\nWith that, we have built a recommendation system able to recommend movies based both on user movie ratings *and* similar movies.\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1cce8"
  },
  "filename": "video-search.md",
  "title": "Video Transcript Search",
  "category": "630fc5235d91a70054705fb7",
  "content": "---\ntitle: Video Transcript Search\ncategory: 630fc5235d91a70054705fb7\n---\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/search/semantic-search/yt-search/00-data-build.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/search/semantic-search/yt-search/00-data-build.ipynb) [![Open Github](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/github-shield.svg)](https://github.com/pinecone-io/examples/tree/master/search/semantic-search/yt-search)\n\nWe will work through an example of indexing and querying YouTube video transcriptions data. The prerequisite packages can be installed with:\n\n```python\n!pip install -U datasets sentence-transformers pinecone-client tqdm\n```\n\nWe start by loading the dataset.\n\n\n```python\nfrom datasets import load_dataset\n\nytt = load_dataset(\n    \"pinecone/yt-transcriptions\",\n    split=\"train\",\n    revision=\"926a45\"\n)\nytt\n```\n\n    Dataset({\n        features: ['video_id', 'text', 'start_second', 'end_second', 'url', 'title', 'thumbnail'],\n        num_rows: 11298\n    })\n\n\n\nEach sample includes video-level information (ID, title, url and thumbnail) and snippet-level information (text, start_second, end_second).\n\n\n```python\nfor x in ytt:\n    print(x)\n    break\n```\n\n    {'video_id': 'ZPewmEu7644', 'text': \" hi this is Jeff Dean welcome to applications of deep neural networks of Washington University in this video we're going to look at how we can use ganz to generate additional training data for the latest on my a I course and projects click subscribe in the bell next to it to be notified of every new video Dan's have a wide array of uses beyond just the face generation that you\", 'start_second': 0, 'end_second': 20, 'url': 'https://www.youtube.com/watch?v=ZPewmEu7644&t=0s', 'title': 'GANS for Semi-Supervised Learning in Keras (7.4)', 'thumbnail': 'https://i.ytimg.com/vi/ZPewmEu7644/maxresdefault.jpg'}\n\n\n## Inserting Documents to Pinecone Index\n\nThe next step is indexing this dataset in Pinecone. For this, we need a sentence transformer model to encode the text into embeddings and a Pinecone index.\n\nWe will initialize the sentence transformer first.\n\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nretriever = SentenceTransformer('flax-sentence-embeddings/all_datasets_v3_mpnet-base')\nretriever\n```\n\n\n    SentenceTransformer(\n      (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: MPNetModel \n      (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n      (2): Normalize()\n    )\n\n\n\nWe can see the embedding dimension of `768` above. We will need this when creating our Pinecone index.\n\n\n```python\nembed_dim = retriever.get_sentence_embedding_dimension()\nembed_dim\n```\n\n\n    768\n\n\n\nNow we can initialize our index.\n\n\n```python\nimport pinecone\n\n# get api key from app.pinecone.io\npinecone.init(\n    api_key=\"<<YOUR_API_KEY>>\",\n    environment=\"YOUR_ENVIRONMENT\"\n)\n\n# create index\npinecone.create_index(\n    \"youtube-search\",\n    dimension=embed_dim,\n    metric=\"cosine\"\n)\n\n# connect to new index\nindex = pinecone.Index(\"youtube-search\")\n```\n\nYou can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**.\n\nWe will index our data in batches of `64`. The data we insert into our index will contain records (here, *documents*) containing a unique document/snippet ID, embedding, and metadata in the following format:\n\n```json\n{\n    'doc-id',\n    [0.0, 0.3, 0.1, ...],\n    {'title': '???', 'start_seconds': 12, ...}\n}\n```\n\nTo create these *documents* and insert them to Pinecone, we run the following loop:\n\n```python\nfrom tqdm.auto import tqdm\n\ndocs = []  # this will store IDs, embeddings, and metadata\n\nbatch_size = 64\n\nfor i in tqdm(range(0, len(ytt), batch_size)):\n    i_end = min(i+batch_size, len(ytt))\n    # extract batch from YT transactions data\n    batch = ytt[i:i_end]\n    # encode batch of text\n    embeds = retriever.encode(batch['text']).tolist()\n    # each snippet needs a unique ID\n    # we will merge video ID and start_seconds for this\n    ids = [f\"{x[0]}-{x[1]}\" for x in zip(batch['video_id'], batch['start_second'])]\n    # create metadata records\n    meta = [{\n        'video_id': x[0],\n        'title': x[1],\n        'text': x[2],\n        'start_second': x[3],\n        'end_second': x[4],\n        'url': x[5],\n        'thumbnail': x[6]\n    } for x in zip(\n        batch['video_id'],\n        batch['title'],\n        batch['text'],\n        batch['start_second'],\n        batch['end_second'],\n        batch['url'],\n        batch['thumbnail']\n    )]\n    # create list of (IDs, vectors, metadata) to upsert\n    to_upsert = list(zip(ids, embeds, meta))\n    # add to pinecone\n    index.upsert(vectors=to_upsert)\nindex.describe_index_stats()\n```\n\n\n    {'dimension': 768,\n     'index_fullness': 0.01,\n     'namespaces': {'': {'vector_count': 11298}}}\n\nUsing `index.describe_index_stats()` we can see that the index now contains *11'298* vectors, the full `pinecone/yt-transcriptions` dataset.\n\n\n## Querying\n\nWhen query we encode our text with the same retriever model and pass it to the Pinecone `query` endpoint.\n\n\n```python\nquery = \"What is deep learning?\"\n\nxq = retriever.encode(query).tolist()\n```\n\n\n```python\nxc = index.query(xq, top_k=5,\n                 include_metadata=True)\nfor context in xc['matches']:\n    print(context['metadata']['text'], end=\"\\n---\\n\")\n```\n\n     terms of optimization but what's the algorithm for updating the parameters or updating whatever the state of the network is and then the the last part is the the data set like how do you actually represent the world as it comes into your machine learning system so I think of deep learning as telling us something about what does the model look like and basically to qualify as deep I\n    ---\n     any theoretical components any theoretical things that you need to understand about deep learning can be sick later for that link again just watched the word doc file again in that I mentioned the link also the second channel is my channel because deep learning might be complete deep learning playlist that I have created is completely in order okay to the other\n    ---\n     under a rock for the last few years you have heard of the deep networks and how they have revolutionised computer vision and kind of the standard classic way of doing this is it's basically a classic supervised learning problem you are giving a network which you can think of as a big black box a pairs of input images and output labels XY pairs okay and this big black box essentially you\n    ---\n     do the task at hand. Now deep learning is just a subset of machine learning which takes this idea even a step further and says how can we automatically extract the useful pieces of information needed to inform those future predictions or make a decision And that's what this class is all about teaching algorithms how to learn a task directly from raw data. We want to\n    ---\n     algorithm and yelled at everybody in a good way that nobody was answering it correctly everybody knew what the alkyl it was graduate course everybody knew what an algorithm was but they weren't able to answer it well let me ask you in that same spirit what is deep learning I would say deep learning is any kind of machine learning that involves learning parameters of more than one consecutive\n    ---\n\n## Example application\n\nTo try out an application like this one, see this [example\napplication](https://huggingface.co/spaces/pinecone/yt-search).\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1cce9"
  },
  "filename": "semantic-text-search.md",
  "title": "Semantic Search",
  "category": "630fc5235d91a70054705fb7",
  "content": "---\ntitle: Semantic Search\ncategory: 630fc5235d91a70054705fb7\n---\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/semantic_text_search/semantic_text_search.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/semantic_text_search/semantic_text_search.ipynb) [![Open github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/pinecone-io/examples/blob/master/semantic_text_search/semantic_text_search.ipynb)\n\n\nThis notebook demonstrates how to create a simple semantic text search using Pinecone’s similarity search service.\n\nThe goal is to create a search application that retrieves news articles based on short description queries (e.g., article titles). To achieve that, we will store vector representations of the articles in Pinecone's index. These vectors and their proximity capture semantic relations. Nearby vectors indicate similar content, and contents from faraway vectors are dissimilar.\n\nSemantic textual search is a technique used for solving other text-based applications. For example, our deduplication, question-answering and personalized article recommendation [demos](https://www.pinecone.io/docs/examples/) were solved using semantic textual search.\n\n## Pinecone Setup\n\n\n```python\n!pip install -qU pinecone-client ipywidgets\n```\n\n\n```python\nimport pinecone\n```\n\n\n```python\n# Load Pinecone API key\nimport os\napi_key = os.getenv(\"PINECONE_API_KEY\") or \"YOUR-API-KEY\"\npinecone.init(api_key=api_key, environment='YOUR_ENVIRONMENT')\n\n# List all indexes currently present for your key\npinecone.list_indexes()\n```\n\n    []\n\n\n[Get a Pinecone API key](https://www.pinecone.io/start/) if you don’t have one already. You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**.\n\n## Install and Import Python Packages\n\n\n```python\n!pip install -qU wordcloud pandas-profiling\n!pip install -qU sentence-transformers --no-cache-dir\n```\n\n\n```python\nimport pandas as pd\nimport numpy as np\nimport time\nimport re\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\nimport sqlite3\n\npd.set_option('display.max_colwidth', 200)\n```\n\n## Create a New Service\n\n\n```python\n# Pick a name for the new index\nindex_name = 'semantic-text-search'\n```\n\n\n```python\n# Check whether the index with the same name already exists\nif index_name in pinecone.list_indexes():\n    pinecone.delete_index(index_name)\n```\n\n\n```python\npinecone.create_index(name=index_name, dimension=300)\n```\n\n\n```python\nindex = pinecone.Index(index_name=index_name)\n```\n\n## Upload\n\nWe will define two separate sub-indexes using Pinecone's namespace feature. One for indexing articles by **content**, and the other by **title**. At query time, we will return an aggregation of the results from the content and title indexes.\n\n\n\nFirst, we will load data and the model, and then create embeddings and upsert them into the namespaces.\n\n\n###  Load data\n\nThe [dataset](https://components.one/datasets/all-the-news-articles-dataset) used throughout this example contains 204,135 articles from 18 American publications.\n\nLet's download the dataset and load data.\n\n\n```python\nimport requests, os\nDATA_DIR = 'tmp'\nURL = \"https://www.dropbox.com/s/b2cyb85ib17s7zo/all-the-news.db?dl=1\"\nFILE = f\"{DATA_DIR}/all-the-news.db\"\n\ndef download_data():\n    os.makedirs(DATA_DIR, exist_ok=True)\n\n    if not os.path.exists(FILE):\n        r = requests.get(URL)  # create HTTP response object\n        with open(FILE, \"wb\") as f:            \n            f.write(r.content)\n\ndownload_data()\n```\n\n\n```python\ncnx = sqlite3.connect(FILE)\ndata = pd.read_sql_query(\"SELECT * FROM longform\", cnx)\ndata.set_index('id', inplace=True)\ndata.head()\n```\n\n\n\n\n\n  <div id=\"df-0ad3a18a-9598-42a1-83a0-f51c54cd3e5e\">\n    <div class=\"colab-df-container\">\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>author</th>\n      <th>date</th>\n      <th>content</th>\n      <th>year</th>\n      <th>month</th>\n      <th>publication</th>\n      <th>category</th>\n      <th>digital</th>\n      <th>section</th>\n      <th>url</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Agent Cooper in Twin Peaks is the audience: once delighted, now disintegrating</td>\n      <td>\\nTasha Robinson\\n</td>\n      <td>2017-05-31</td>\n      <td>And never more so than in Showtime’s new series revival Some spoilers ahead through episode 4 of season 3 of Twin Peaks. On May 21st, Showtime brought back David Lynch’s groundbreaking TV se...</td>\n      <td>2017</td>\n      <td>5</td>\n      <td>Verge</td>\n      <td>Longform</td>\n      <td>1.0</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AI, the humanity!</td>\n      <td>\\nSam Byford\\n</td>\n      <td>2017-05-30</td>\n      <td>AlphaGo’s victory isn’t a defeat for humans — it’s an opportunity A loss for humanity! Man succumbs to machine! If you heard about AlphaGo’s latest exploits last week — crushing the world’s ...</td>\n      <td>2017</td>\n      <td>5</td>\n      <td>Verge</td>\n      <td>Longform</td>\n      <td>1.0</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The Viral Machine</td>\n      <td>\\nKaitlyn Tiffany\\n</td>\n      <td>2017-05-25</td>\n      <td>Super Deluxe built a weird internet empire. Can it succeed on TV? When Wolfgang Hammer talks about the future of entertainment, people listen. Hammer is the mastermind behind the American re...</td>\n      <td>2017</td>\n      <td>5</td>\n      <td>Verge</td>\n      <td>Longform</td>\n      <td>1.0</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>How Anker is beating Apple and Samsung at their own accessory game</td>\n      <td>\\nNick Statt\\n</td>\n      <td>2017-05-22</td>\n      <td>Steven Yang quit his job at Google in the summer of 2011 to build the products he felt the world needed: a line of reasonably priced accessories that would be better than the ones you could ...</td>\n      <td>2017</td>\n      <td>5</td>\n      <td>Verge</td>\n      <td>Longform</td>\n      <td>1.0</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Tour Black Panther’s reimagined homeland with Ta-Nehisi Coates</td>\n      <td>\\nKwame Opam\\n</td>\n      <td>2017-05-15</td>\n      <td>Ahead of Black Panther’s 2018 theatrical release, Marvel turned to Ta-Nehisi Coates to breathe new life into the nation of Wakanda. “I made most of my career analyzing the forces of racism a...</td>\n      <td>2017</td>\n      <td>5</td>\n      <td>Verge</td>\n      <td>Longform</td>\n      <td>1.0</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n```python\n# Define number of test articles\nNUM_OF_TEST_ARTICLES = 2\n\n# Remove test articles from data and keep them in separate dataframe\ntest_articles = data[['title','content']][97::81][:NUM_OF_TEST_ARTICLES]\ndata.drop(list(test_articles.index), inplace=True)\n```\n\n### Use Ready Made Vector Embedding Model\n\nWe will use an [Average Word Embeddings Model](https://www.sbert.net/docs/pretrained_models.html#average-word-embeddings-models) to create both title and content embeddings. Pinecone allows you to create paritions in the index that we call namespaces. This will allow us to maintain separate embeddings for the data that can be used for different tasks.\n\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('average_word_embeddings_komninos')\n```\n\n### Upload Vectors of Titles\n\nHere we index articles by title only. You can notice we create a *title* namespace for this purpose.\n\n\n```python\nfrom typing import Iterator\n\nclass BatchGenerator:\n    \"\"\" Models a simple batch generator that make chunks out of an input DataFrame. \"\"\"\n    \n    def __init__(self, batch_size: int = 10) -> None:\n        self.batch_size = batch_size\n    \n    def to_batches(self, df: pd.DataFrame) -> Iterator[pd.DataFrame]:\n        \"\"\" Makes chunks out of an input DataFrame. \"\"\"\n        splits = self.splits_num(df.shape[0])\n        if splits <= 1:\n            yield df\n        else:\n            for chunk in np.array_split(df, splits):\n                yield chunk\n    \n    def splits_num(self, elements: int) -> int:\n        \"\"\" Determines how many chunks DataFrame contians. \"\"\"\n        return round(elements / self.batch_size)\n    \n    __call__ = to_batches\n\ndf_batcher = BatchGenerator(300)\n```\n\n\n```python\n# Fill missing and remove redundant data\ndata['title'] = data['title'].fillna('')\n\n# Create vector embeddings based on the title column\nprint('Encoding titles...')\nencoded_titles = model.encode(data['title'].tolist(), show_progress_bar=True)\ndata['title_vector'] = encoded_titles.tolist()\n```\n\n    Encoding titles...\n\n\n\n    Batches:   0%|          | 0/6380 [00:00<?, ?it/s]\n\n\n\n```python\ndata['vector_id'] = data.index\ndata['vector_id'] = data['vector_id'].apply(str)\n```\n\n\n```python\n# Upsert title vectors in title namespace\nprint(\"Uploading vectors to title namespace..\")\nfor batch_df in df_batcher(data):\n    index.upsert(vectors=zip(batch_df.vector_id, batch_df.title_vector), namespace='title')\n```\n\n    Uploading vectors to title namespace..\n\n\n### Upload Vectors of Content\n\nNow we index articles by their content. We want to separately maintain embeddings for both title and content hence we use a separate namespace in the same index.\n\n\n```python\n# Fill missing data\ndata['content'] = data['content'].fillna('')\n\n# Extract only first few sentences of each article for quicker vector calculations\ndata['content'] = data.content.apply(lambda x: ' '.join(re.split(r'(?<=[.:;])\\s', x)[:10]))\n\n# Create vector embeddings based on the content column\nprint('Encoding content...')\nencoded_content = model.encode(data['content'].tolist(), show_progress_bar=True)\ndata['content_vector'] = encoded_content.tolist()\n```\n\n    Encoding content...\n\n\n\n    Batches:   0%|          | 0/6380 [00:00<?, ?it/s]\n\n\n\n```python\n# Upsert title vectors in content namespace\nprint(\"Uploading vectors to content namespace..\")\nfor batch_df in df_batcher(data):\n    index.upsert(vectors=zip(batch_df.vector_id, batch_df.content_vector), namespace='content')\n```\n\n    Uploading vectors to content namespace..\n\n\nNow that we have upserted data, we can check the size of each namespace.\n\n\n\n\n```python\n# Check index size for each namespace\nindex.describe_index_stats()\n```\n\n\n\n\n    {'dimension': 300,\n     'index_fullness': 0.06,\n     'namespaces': {'content': {'vector_count': 204133},\n                    'title': {'vector_count': 204133}}}\n\n\n\n## Query\n\nLet's see what our test articles look like first.\n\n\n```python\n# Print test articles\ndisplay(test_articles)\n```\n\n\n\n  <div id=\"df-746d8ca7-7405-4dda-84b2-c12520410b09\">\n    <div class=\"colab-df-container\">\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>content</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>111</th>\n      <td>The Rise and Fall and Rise of Virtual Reality</td>\n      <td>In the wake of Facebook's purchase of Oculus VR, can this revolutionary technology triumph anew?</td>\n    </tr>\n    <tr>\n      <th>6467</th>\n      <td>Who should go to Mars?</td>\n      <td>Elon Musk laid out his plan to colonize Mars at a conference on Tuesday, but it was during the Q&amp;ampampA session that a woman asked one of the key questions: who will be chosen to embark on ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\nThe following utility functions help us process and present the results.\n\n\n```python\ntitles_mapped = dict(zip(data.index, data.title))\ncontent_mapped = dict(zip(data.index, data.content))\n```\n\n\n```python\ndef get_wordcloud_for_article(recommendations, namespace):\n    'Generates word cloud for the recommendations (titles or content).'\n\n    stopwords = set(STOPWORDS).union([np.nan, 'NaN', 'S'])\n    wordcloud = WordCloud(\n                   max_words=50000, \n                   min_font_size =12, \n                   max_font_size=50, \n                   relative_scaling = 0.9, \n                   stopwords=set(STOPWORDS),\n                   normalize_plurals= True\n                  )\n    \n    if namespace == 'title':\n        clean_titles = [word for word in recommendations.title.values if word not in stopwords]\n        wordcloud = wordcloud.generate(' '.join(clean_titles))\n    else:\n        clean_content = [word for word in recommendations.content.values if word not in stopwords]\n        wordcloud = wordcloud.generate(' '.join(clean_content))\n\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()\n\n\ndef print_query_results(query_result, query, namespace, show_options={'wordcloud':True, 'tabular':True}):\n    'Prints query result with wordcloud.'\n      \n    print(f'\\nMost similar results querying {query} in \"{namespace}\" namespace:\\n')\n    if not query_result.matches:\n        print('no query result')\n    \n    matches = query_result.matches\n    ids = [res.id for res in matches]\n    scores = [res.score for res in matches]\n    df = pd.DataFrame({'id':ids, \n                       'score':scores,\n                       'title': [titles_mapped[int(_id)] if int(_id) in titles_mapped else ' '  for _id in ids],\n                       'content': [content_mapped[int(_id)] if int(_id) in content_mapped else ' '  for _id in ids],\n                       })\n    if show_options['tabular']:\n        display(df.head(5))\n    if show_options['wordcloud']:\n        get_wordcloud_for_article(df, namespace)\n    print('\\n')\n```\n\nThe following two functions we use to query the test article's title or content in either of the namespaces we created. This means we can query the title in the \"title\" namespace or the \"content\" namespace. The same is with the article content.\n\n\n```python\ndef query_article_title(test_article, namespace, top_k=5, show_options={'wordcloud':True, 'tabular':True}):\n    '''Queries an article using its title in the specified\n     namespace and prints results.'''\n\n    # Create vector embeddings based on the title column\n    encoded_titles = model.encode(test_article['title'], \n                                  show_progress_bar=False)\n    test_article['title_vector'] = encoded_titles.tolist()\n\n    # Query namespace passed as parameter using title vector\n    query_result_titles = index.query(test_article.title_vector, \n                                      namespace=namespace, \n                                      top_k=top_k)\n\n    # Print query results \n    if show_options['wordcloud'] or show_options['tabular']:\n        print_query_results(query_result_titles, query='title', \n                            namespace=namespace, \n                            show_options=show_options)\n\n    return query_result_titles\n```\n\nWhen querying content, we will first create the article content vector and search for the most similar vectors in the \"title\" or the \"content\" namespace.\n\n\n```python\ndef query_article_content(test_article, namespace, top_k=5, show_options={'wordcloud':True, 'tabular':True}):\n    '''Queries an article using its content in the specified \n    namespace and prints results.'''\n\n    # Create vector embeddings based on the content column\n    encoded_content = model.encode(test_article['content'], \n                                   show_progress_bar=False)\n    test_article['content_vector'] = encoded_content.tolist()\n\n    # Query content namespace using content vector\n    query_result_content = index.query(test_article.content_vector, \n                                       namespace=namespace, \n                                       top_k=top_k)\n\n    # Print query results \n    if show_options['wordcloud'] or show_options['tabular']:\n        print_query_results(query_result_content, \n                            query='content', \n                            namespace=namespace, \n                            show_options=show_options)\n\n    return query_result_content\n```\n\nNow it's time to do the cross namespace querying and aggregate the results.\nThe following functions query for four combinations (title/content only and title-content, content-title). Then aggregates the results of the four queries to calculate the total occurrence of the articles and their average scores. They are ranked accordingly and the most similar articles are returned.\n\n\n```python\ndef aggregate_results(article):\n    '''Aggregates results after querying both namespaces\n       for both the article's title and content.'''\n\n    results = []\n    \n    results.append(query_article_title(article, namespace='title', top_k=30, show_options={'wordcloud':False, 'tabular':False}))\n    results.append(query_article_title(article, namespace='content', top_k=30, show_options={'wordcloud':False, 'tabular':False}))\n    results.append(query_article_content(article, namespace='title', top_k=30, show_options={'wordcloud':False, 'tabular':False}))\n    results.append(query_article_content(article, namespace='content', top_k=30, show_options={'wordcloud':False, 'tabular':False}))\n\n    articles_scores = {}\n    articles_count = {}\n\n    for res in results:\n        ids = [r.id for r in res.matches]\n        scores = [r.score for r in res.matches]\n        for id, score in zip(ids, scores):\n            if id not in articles_scores:\n                articles_scores[id] = score\n                articles_count[id] = 1\n            else:\n                articles_scores[id] += score\n                articles_count[id] += 1\n    \n    return articles_scores , articles_count\n\ndef show_aggregated_results(results_dict, counts, show_options={'wordcloud':True, 'tabular':True}):\n    '''Shows results after aggregation. Values are sorted based\n    on the number of queries they appear (1-4) and based on their\n    average score.'''\n    \n    df = pd.DataFrame({'id':results_dict.keys(), \n                       'count': counts.values(),\n                       'average_score':[round(r/c, 3) for r, c in zip(results_dict.values(),counts.values())],\n                       'title': [titles_mapped[int(_id)] if int(_id) in titles_mapped else ' '  for _id in results_dict.keys()],\n                       'content': [content_mapped[int(_id)] if int(_id) in content_mapped else ' '  for _id in results_dict.keys()],\n                       })\n    df.sort_values(by=['count', 'average_score'], ascending=False, inplace=True)\n    \n    if show_options['tabular']:\n        print('\\nMost similar results after aggregation:\\n')\n        display(df.head(5))\n    if show_options['wordcloud']:\n        print('\\nWordcloud for titles and content after aggregation:')\n        print('-Titles:')\n        get_wordcloud_for_article(df[:10], 'title')\n        print('-Content:')\n        get_wordcloud_for_article(df[:10], 'content')\n    print('\\n')\n```\n\n### Query by Aggregation\nWe are ready to query our service! We will use all the above auxiliary functions to query the test articles. We will be using our cross-namespace approach that combines four query results into one.\n\nNote that you can add the tabular data results for each query by changing the ```show_options``` flags below.\n\n\n```python\n# Query index using simple and cross namespace approach\nfor e, (_, test_article) in enumerate(test_articles.iterrows()):\n    print(f'\\nArticle {e+1}')\n    print(f'\\n Title: {test_article.title}')\n    print(f' Content: {test_article.content[:200].strip()}' + ('...' if len(test_article.content) > 200 else ''))\n    \n    # Uncomment to query the titles in title namespace\n    # query_article_title(test_article, 'title',  show_options={'wordcloud':True, 'tabular':False})\n\n    # Uncomment to query the content in content namespace\n    # query_article_content(test_article, namespace='content', show_options={'wordcloud':True, 'tabular':False})\n\n    # Cross namespace query\n    aggregated_results, counts = aggregate_results(test_article)\n    show_aggregated_results(aggregated_results, counts, show_options={'wordcloud':False, 'tabular':True})\n```\n\n    \n    Article 1\n    \n     Title: The Rise and Fall and Rise of Virtual Reality\n     Content: In the wake of Facebook's purchase of Oculus VR, can this revolutionary technology triumph anew?\n    \n    Most similar results after aggregation:\n    \n\n\n\n\n  <div id=\"df-95f8bb0e-16a0-4668-b9e8-2cefcd42975d\">\n    <div class=\"colab-df-container\">\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>count</th>\n      <th>average_score</th>\n      <th>title</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>59</th>\n      <td>19720</td>\n      <td>2</td>\n      <td>0.809</td>\n      <td>Oculus Founder, at Center of Legal Battle Over VR, Departs Facebook - The New York Times</td>\n      <td>SAN FRANCISCO — Palmer Luckey, a founder of the virtual-reality technology company Oculus, has left Facebook three years after the social network acquired his company for close to $3 billion. Mr. ...</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>7201</td>\n      <td>2</td>\n      <td>0.808</td>\n      <td>Flush with cash, Oculus plans ambitious new VR headset</td>\n      <td>According to Oculus Rift inventor Palmer Luckey, virtual reality is near and dear to Marc Andreessen&amp;amprsquos heart. Twenty years ago&amp;ampnbsp&amp;ampmdash&amp;ampnbspbefore he created the Mosaic we...</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>34611</td>\n      <td>2</td>\n      <td>0.806</td>\n      <td>Microsoft Introducing VR Headsets at Half the Price of Oculus Rift - Breitbart</td>\n      <td>On October 26, Microsoft doubled down on virtual reality by announcing their own VR headsets at the Windows 10 event.[Unless you’ve got $599 for the Oculus Rift, or $799 for Valve’s HTC Vive, your...</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>13199</td>\n      <td>2</td>\n      <td>0.800</td>\n      <td>Oculus VR founder Palmer Luckey talks GoPro, 'Minecraft' and eSports - LA Times</td>\n      <td>Oculus VR founder Palmer Luckey answers questions at the Loews Hollywood Hotel on Sept. 24. ', 'A few years ago, journalism major Palmer Luckey dropped out of Cal State Long Beach to work on a dev...</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>9533</td>\n      <td>2</td>\n      <td>0.800</td>\n      <td>Virtual reality visionary Palmer Luckey leaves Facebook 3 years after $2-billion Oculus deal - LA Times</td>\n      <td>Palmer Luckey, the Long Beach entrepreneur whose zeal for virtual reality kickstarted mass investment in the technology, has left Facebook three years after selling his start-up Oculus VR to the s...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n    \n    Article 2\n    \n     Title: Who should go to Mars?\n     Content: Elon Musk laid out his plan to colonize Mars at a conference on Tuesday, but it was during the Q&ampampA session that a woman asked one of the key questions: who will be chosen to embark on a ri...\n    \n    Most similar results after aggregation:\n    \n\n\n\n\n  <div id=\"df-2e92eccd-3e99-49bd-ab18-72dbc0e3adb4\">\n    <div class=\"colab-df-container\">\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>count</th>\n      <th>average_score</th>\n      <th>title</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>206370</td>\n      <td>2</td>\n      <td>0.703</td>\n      <td>How Mars lost its atmosphere, and why Earth didn’t</td>\n      <td>Mars was once wetter and warmer, and very possibly a congenial environment for life as we know it. Today it looks mighty dead, with all due respect. If there's life, it's cryptic. Mars ju...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>83533</td>\n      <td>2</td>\n      <td>0.679</td>\n      <td>Mars Reconnaissance Orbiter celebrates 10 years at red planet</td>\n      <td>[Sign in to comment!, NASA’s Mars Reconnaissance Orbiter (MRO) arrived at the red planet 10 years ago today and has since completed 45,000 orbits and generated a vast amount of scientific data., O...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>83531</td>\n      <td>2</td>\n      <td>0.664</td>\n      <td>Buzz Aldrin eyes 2040 for manned Mars mission</td>\n      <td>[Sign in to comment!, Former astronaut Buzz Aldrin is eyeing 2040 for the first manned mission to Mars, noting that the red planet’s moon Phobos could play a vital role for astronauts., “I think t...</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>16615</td>\n      <td>2</td>\n      <td>0.641</td>\n      <td>NASA orbiters watch as comet flies safely past Mars - LA Times</td>\n      <td>Comet Siding Spring sailed past Mars on Sunday, coming 10 times closer to the Red Planet than any comet on record has come to Earth.', \"At the time of the comet's closest approach at 11:27 a.m., i...</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>158293</td>\n      <td>2</td>\n      <td>0.640</td>\n      <td>Mars makes closest approach to Earth for 11 years</td>\n      <td>Mars reaches its closest approach to Earth for 11 years this evening at 21:35 GMT. The red planet will be just 75 million kilometres away., Mars has been steadily approaching, tripling its apparen...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n## Summary\nWe demonstrated a simple textual semantic search approach that aggregates results from two different news article representations:  for titles only and content only. We do that by utilizing Pinecone's namespace feature to create two namespaced indexes. The aggregation mechanism is simple. We use the query's title and content representations to query both namespaces and weight results by their occurrences. Our example queries illustrate the effectiveness of this approach. \n\nWe encourage you to try the code with your data. You might want to try other embedding or aggregation mechanisms. Working with a similarity search service makes such experimentations easy. Have fun, and [let us know](https://www.pinecone.io/contact/) if you have any questions or interesting findings. \n\n## Delete the index\n\nDelete the index once you are sure that you do not want to use it anymore. Once the index is deleted, you cannot use it again. Use it as a cleanup step if you are done working with a specific index.\n\n\n```python\npinecone.delete_index(index_name)\n```\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccea"
  },
  "filename": "time-series.md",
  "title": "Time Series Search",
  "category": "630fc5235d91a70054705fb7",
  "content": "---\ntitle: Time Series Search\ncategory: 630fc5235d91a70054705fb7\n---\n\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/analytics-and-ml/time-series/time-series-stocks-pattern-example.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/analytics-and-ml/time-series/time-series-stocks-pattern-example.ipynb) [![Open github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/pinecone-io/examples/blob/master/analytics-and-ml/time-series/time-series-stocks-pattern-example.ipynb)\n\n\nTime series, a sequence of values ordered by time, is one of the fundamental data forms. Consequently, there are plentiful time-series analysis methods and tools, ranging from forecasting to anomaly detection. \n\nHere we demonstrate how to perform time-series \"pattern\" matching using a similarity search service. Wherein we want to retrieve all historical time series that match a particular pattern. Such matching capability serves as a core ingredient for time series applications such as clustering, labeling, and recommendation. For example, consider a time series describing web page visitors and a need to retrieve all historical peak surges, drops, or trends. \n\nWe will walk you through a simple approach that utilizes the time series raw data as-is. In other words, it does not require any modeling heavy lifting. Such an approach is very appealing because it does not require any domain-specific technical knowledge nor extra model generation resources. Sounds too good to be true?\n\nOur demo indicates that this simple approach provides satisfying results. We will show you how to index and search a set of stock market daily prices time series. Then we will compare the simple approach with an alternative that utilizes a comprehensive time-series library recently published by Facebook AI. \n\nWhat we'll cover:\n* Prerequisites\n* Simple Time-Series Embeddings\n    * Prepare data\n    * Index\n    * Search\n* Facebook's [Kats](https://engineering.fb.com/2021/06/21/open-source/kats/) Time-Series Embeddings\n    * Index\n    * Search\n* Conclusion\n\n## Prerequisites\nInstall and import relevant python packages\n\n\n```python\n!pip install -qU convertdate kaggle matplotlib==3.1.3 \n!pip install -q git+https://github.com/facebookresearch/Kats.git\n!pip install -qU pinecone-client\n```\n\n*If you are using Google Colab, please restart the runtime in order to use newly installed package versions.*\n\n\n```python\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pprint\nfrom sklearn.preprocessing import MinMaxScaler\nfrom kats.consts import TimeSeriesData\nfrom kats.tsfeatures.tsfeatures import TsFeatures\nimport itertools\nfrom decimal import Decimal\nfrom IPython.display import clear_output\n\nimport warnings\nwarnings.simplefilter(action='ignore')\n```\n\n## Simple Time-Series Embeddings\n\n### Upload Time Series to Pinecone's Similarity Search Service\nIn the steps below how to set up Pinecone's similarity search service and upload the time series into the service's *Index* data structure. Pinecone stores and searches [vector embeddings](https://www.pinecone.io/learn/vector-embeddings/). These embeddings or feature vectors are a numerical representation of raw data semantics. \n\nRecall that we want to create two indexes: \n* An index that contains vectors representing the raw data of historical prices of different stocks. In other words, vector embedding is simply the time-series sequence of numbers. \n* An index that stores feature embeddings calculated using Facebook's [Kats toolkit](https://engineering.fb.com/2021/06/21/open-source/kats/). Kats is a powerful time-series analysis tool that includes a time-series [embedding functionality](https://github.com/facebookresearch/Kats#tsfeatures). \n\n### Configure Pinecone\nLet's start by configuring the Pinecone service. \n\n#### Pinecone Setup\n\n\n```python\nimport pinecone\n```\n\n\n```python\n# Load Pinecone API key\napi_key = os.getenv('PINECONE_API_KEY') or 'YOUR_API_KEY'\n# Set Pinecone environment. Default environment is YOUR_ENVIRONMENT\nenv = os.getenv('PINECONE_ENVIRONMENT') or 'YOUR_ENVIRONMENT'\npinecone.init(api_key=api_key, environment=env)\n```\n\n[Get your API key](https://www.pinecone.io/start/) and try this example yourself! You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**.\n\n#### Create a New Service\n\nLet's start with the simple approach and create an index.\n\n\n```python\n# Pick a name for the new index\nsimple_index_name = 'stocks-trends'\n```\n\n\n```python\n# Check whether the index with the same name already exists\nif simple_index_name in pinecone.list_indexes():\n    pinecone.delete_index(simple_index_name)\n```\n\n\n```python\n# Create a new index\npinecone.create_index(name=simple_index_name, dimension=128)\n```\n\n\n```python\n# Establish a connection\nsimple_index = pinecone.Index(index_name=simple_index_name)\n```\n\n### Prepare data\nStarting with the simple embedding approach described earlier. Wherein we represent a time series as a vector of the time series sequence of numbers. \n\nThroughout the demo, we use a [Stock Market Dataset](https://www.kaggle.com/jacksoncrow/stock-market-dataset). This dataset contains historical daily prices for all tickers trading on NASDAQ, up to April 2020. The dataset is defined in Kaggle and requires either a manual download or a Kaggle API to download it.\n\nThe data processing (i.e., [ETL part](https://en.wikipedia.org/wiki/Extract,_transform,_load)) here is heavy lifting and includes:\n* Downloading the data from [Kaggle](https://www.kaggle.com/jacksoncrow/stock-market-dataset). (Recall, you will need a [Kaggle API key](https://www.kaggle.com/docs/api).) \n* Define the time series raw data. \n* Extract the time series from the relevant files. \n* Transform the raw data into vectors and upload the vectors into Pinecone's service.\n\n#### Download Kaggle Stock Market Dataset\n\nIn order to use the Kaggle’s public API, you must first authenticate using an API token. Please replace the username and key values in the following cell with the values from your [Kaggle API token](https://www.kaggle.com/docs/api#getting-started-installation-&-authentication).\n\n\n```python\n%%writefile kaggle.json\n{\"username\":\"KAGGLE_USERNAME\",\"key\":\"KAGGLE_KEY\"}\n```\n\n    Writing kaggle.json\n\n\n\n```python\n#Check Kaggle username and key\n! cat ./kaggle.json\n```\n\n\n```python\n!mkdir ~/.kaggle\n!cp kaggle.json ~/.kaggle/\n!chmod 600 ~/.kaggle/kaggle.json\n!kaggle datasets download -d jacksoncrow/stock-market-dataset\n!unzip -q stock-market-dataset.zip -d data\n```\n\n    Downloading stock-market-dataset.zip to /content\n    100% 521M/522M [00:04<00:00, 126MB/s]\n    100% 522M/522M [00:04<00:00, 125MB/s]\n\n\n#### Set up Time Series Hyperparameters\nWe set two hyperparameters defining how we extract the time series:\n* sliding window, which controls the length of the time series in consecutive day periods.\n* step size, which defines a gap in the start dates of two consecutive vectors.\n\nFeel free to set the window or step size to a different value. \n\n\n```python\n# Define sliding window and step size\nSLIDING_WINDOW_SIZE = 64\nSTEP = 10\n```\n\n#### Define Extract-Transform-Load Functions\nBefore we do all of the other steps, we will define utility functions to help us extract, transform, and upload the time series data.\n\nNote that we will:\n* Work only with the stock prices and disregard the ETF data folder.\n* Load data for a set of symbols from the stock folder for the simplicity of the example.\n* When creating vectors, we will include daily *Open* and *Close* prices. This way, our vectors will be double the size of a sliding window. Feel free to try different prices when creating vectors.\n\n\n```python\ndef windows(data, window_size, step):\n    r = np.arange(len(data))\n    s = r[::step]\n    z = list(zip(s, s + window_size))\n    f = '{0[0]}:{0[1]}'.format\n    g = lambda t: data.iloc[t[0]:t[1]]\n    return pd.concat(map(g, z), keys=map(f, z))\n\ndef get_feature_embedding_for_window(df, stock):\n    ts_name = f\"{stock.strip('.csv')}_{str(df.Date.min())}_{str(df.Date.max())}\"\n    scaler=MinMaxScaler()\n    df[['Open', 'Close']] = scaler.fit_transform(df[['Open', 'Close']])\n    prices = df[['Open', 'Close']].values.tolist()\n    flat_values = [item for sublist in prices for item in sublist]\n    df = df.rename(columns={\"Date\":\"time\"}) \n    ts_df = pd.DataFrame({'time':df.time.repeat(2), \n                          'price':flat_values})\n    ts_df.drop_duplicates(keep='first', inplace=True)  \n\n    # Use Kats to extract features for the time window\n    try:\n        if not (len(np.unique(ts_df.price.tolist())) == 1 \\\n            or len(np.unique(ts_df.price.tolist())) == 0):\n            timeseries = TimeSeriesData(ts_df)\n            features = TsFeatures().transform(timeseries)\n            feature_list = [float(v) if not pd.isnull(v) else float(0) for _, v in features.items()]\n            if Decimal('Infinity') in feature_list or Decimal('-Infinity') in feature_list:\n                return None\n            return (ts_name, feature_list)\n    except np.linalg.LinAlgError as e:\n        print(f\"Can't process {ts_name}:{e}\")\n    return None\n\ndef get_simple_pair_for_window(df, stock):\n    ts_name = f\"{stock.strip('.csv')}_{str(df.Date.min())}_{str(df.Date.max())}\"\n    prices = df[['Open', 'Close']].values.tolist()\n    flat_values = [item for sublist in prices for item in sublist]\n    return (ts_name, flat_values)\n\ndef chunks(iterable, batch_size=100):\n    it = iter(iterable)\n    chunk = tuple(itertools.islice(it, batch_size))\n    while chunk:\n        yield chunk\n        chunk = tuple(itertools.islice(it, batch_size))\n```\n\n\n```python\ndef upload_data_to_index(index, create_pair_func, verbose=False):\n    # Define path to the folder\n    stocks = sorted(os.listdir('./data/stocks'))\n    \n    # Iterate over files, create vectors and upload data\n    for stock in stocks[::50]:\n        print(stock.strip('.csv'))\n        data = pd.read_csv(os.path.join('./data/stocks', stock))\n        data = data.sort_index(axis=0, ascending=True)\n        data[\"Date\"] = pd.to_datetime(data[\"Date\"]).dt.date\n\n        # Interpolate data for missing dates\n        data.set_index('Date', inplace=True)\n        data = data.reindex(pd.date_range(start=data.index.min(),\n                                          end=data.index.max(),\n                                          freq='1D'))\n        data = data.interpolate(method='linear')\n        data = data.reset_index().rename(columns={'index': 'Date'})\n        data[\"Date\"] = pd.to_datetime(data[\"Date\"]).dt.date\n        \n        # Create sliding windows dataset\n        wdf = windows(data, SLIDING_WINDOW_SIZE, STEP)\n        \n        # Prepare sequences for upload \n        items_to_upload = []\n        for window, new_df in wdf.groupby(level=0):\n            if new_df.shape[0] == SLIDING_WINDOW_SIZE:\n                pair = create_pair_func(new_df, stock)\n                if pair:\n                    items_to_upload.append(pair)\n\n        # Upload data for the symbol\n        for batch in chunks(items_to_upload, 500):\n            index.upsert(vectors=batch)\n```\n\n### Index\nLet's upsert data into the simple index.\n\n\n```python\nupload_data_to_index(simple_index, get_simple_pair_for_window)\nclear_output()\n```\n\n\n```python\n# Check the index size\nsimple_index.describe_index_stats()\n```\n\n\n\n\n    {'dimension': 128, 'namespaces': {'': {'vector_count': 61212}}}\n\n\n\n### Search\nNow that we have uploaded the items into the vector index, it is time to check the similarities between vectors.\n\nIn this section, we will:\n* Define stocks and their windows for the query.\n* Fetch these query items from the index to retrieve their vectors.\n* Query the index using these vectors. Pinecone will return top K most similar vectors for each query item.\n* Show the results.\n\nBelow we define utility functions for data preparation and display.\n\n\n```python\ndef prepare_items_for_graph(data):    \n    scaler = MinMaxScaler()\n    result_list = []\n    \n    for _, row in data.iterrows():\n        id = row['id']\n        vec = row['values']\n        scaled_vec = scaler.fit_transform(np.array(vec).reshape(-1,1))\n        result_list.append((id, (vec, scaled_vec)))\n    return result_list\n```\n\n\n```python\ndef show_query_results(query_item, data):\n    data_prepared = prepare_items_for_graph(data)\n    graph_index = pd.Float64Index(np.arange(start=0, stop=SLIDING_WINDOW_SIZE, step=0.5))\n\n    print('\\n The most similar items from the vector index:')\n    data.reset_index(inplace=True, drop=True)\n    display(data)\n      \n    fig = plt.figure(figsize=(20,7))\n    for item in data_prepared:\n        _id, vectors = item\n        ax1 = plt.subplot(1, 2, 1)\n        graph = plt.plot(graph_index, vectors[0], label = _id, marker='o' if _id == query_item else None)\n        ax2 = plt.subplot(1, 2, 2)\n        graph = plt.plot(graph_index, vectors[1], label = _id, marker='o' if _id == query_item else None)    \n    ax1.set_xlabel(\"Days in time window\")\n    ax2.set_xlabel(\"Days in time window\")\n    ax1.set_ylabel(\"Stock values\")\n    ax2.set_ylabel(\"Normalized Stock Values\")\n    ax1.title.set_text(f'Similar stock patterns and their market values')\n    ax2.title.set_text(f'Similar stock patterns and their normalized market values')\n    plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n    plt.show()\n```\n\nNote that we will filter the retrieved results to make sure we present a diverse set of stocks. Otherwise, we might get consecutive time windows for the same stock.\n\n\n```python\ndef filter_results(query_item, data, historical_only=False):\n    already_present = []\n    \n    # Remove symbol that is already included\n    for i, row in data.iterrows():\n        check_name = row.id.split('_')[0]\n        if check_name not in already_present:\n            already_present.append(check_name)\n        else:\n            data.drop(i,axis=0,inplace=True)\n            \n    # Include only data prior to query interval\n    if historical_only:\n        _, start_dt, end_dt = query_item.split('_')\n        start_dt = pd.to_datetime(start_dt).date()\n        data['final_date'] = data.id.apply(lambda x: x.split('_')[2])\n        data['final_date'] =  data.final_date.apply(lambda x: pd.to_datetime(x).date())\n        data = data[data.final_date <= start_dt]\n        del data['final_date']\n       \n    return data\n```\n\n#### Numerical Examples\nLet's examine a few interesting price patterns and their corresponding best matches.\n\nHere we define the query items, fetch them and prepare vectors for the query.\n\n\n```python\n# Define query examples\nitems_to_query = ['BORR_2019-10-18_2019-12-20', 'HCCO_2020-01-28_2020-03-31', 'PUMP_2019-11-22_2020-01-24']\n\n# Fetch vectors from the index\nfetch_res = simple_index.fetch(ids=items_to_query)\n\n# Create a list of ids and vectors for the fetched items\nquery_ids = [res.id for res in fetch_res.vectors.values()]\nquery_vectors = [res.values for res in fetch_res.vectors.values()]\n```\n\nThe next step is to perform the query for the query vectors. \n\n\n```python\n# Query the pinecone index\nquery_results = []\nfor xq in query_vectors:\n    res = simple_index.query(xq, top_k=100, include_values=True)\n    query_results.append(res)\n```\n\nFinally, iterate over the results, get all vectors needed for the graphs and display them.\n\nNote that graphs on the left show the absolute price values for the query items selected, while graphs on the right show each vector on a 0-1 scale. The price normalization ignores the magnitude of stock prices and thus focuses on the time series pattern only. \n\nIt is more likely that similar trends appear in the same time interval. There is a flag **historical_only** that lets you choose whether you want to look only at the time intervals prior to query time interval or any time interval that exists.\n\n\n```python\n# Iterate and show query results for each query item\nfor query_item, q_res in zip(query_ids, query_results):\n    print(f'\\nQueried: {query_item}')\n    res_df = pd.DataFrame(\n        {\n            'id': [res.id for res in q_res.matches], \n            'score': [res.score for res in q_res.matches],\n            'values': [res.values for res in q_res.matches]\n         }\n    )\n    res_df = filter_results(query_item, res_df, historical_only=False)\n    show_query_results(query_item, res_df.head(6))\n```\n\n    \n    Queried: BORR_2019-10-18_2019-12-20\n    \n     The most similar items from the vector index:\n\n\n\n\n  <div id=\"df-fe076651-f3f5-4592-a734-74801a96d394\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n      <th>values</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BORR_2019-10-18_2019-12-20</td>\n      <td>1.000000</td>\n      <td>[6.22, 6.4, 6.45, 6.4666667, 6.68, 6.5333333, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>OSK_1990-11-25_1991-01-27</td>\n      <td>0.999467</td>\n      <td>[1.23611116, 1.25, 1.25, 1.25, 1.25, 1.2916666...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>BH_2009-10-21_2009-12-23</td>\n      <td>0.999386</td>\n      <td>[197.943146, 192.080719, 207.771332, 206.90921...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>MLAB_2007-03-30_2007-06-01</td>\n      <td>0.999307</td>\n      <td>[19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19....</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SORL_2008-03-19_2008-05-21</td>\n      <td>0.999293</td>\n      <td>[5.1, 4.93, 4.9, 4.93, 4.9425, 4.9525, 4.985, ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>PKBK_2012-12-02_2013-02-03</td>\n      <td>0.999265</td>\n      <td>[3.78662658, 3.77160025, 3.77160025, 3.7716002...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe076651-f3f5-4592-a734-74801a96d394')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 0 24 24\"\n       width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg>\n      </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n  <script>\n    const buttonEl =\n      document.querySelector('#df-fe076651-f3f5-4592-a734-74801a96d394 button.colab-df-convert');\n    buttonEl.style.display =\n      google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n    async function convertToInteractive(key) {\n      const element = document.querySelector('#df-fe076651-f3f5-4592-a734-74801a96d394');\n      const dataTable =\n        await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                  [key], {});\n      if (!dataTable) return;\n\n      const docLinkHtml = 'Like what you see? Visit the ' +\n        '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n        + ' to learn more about interactive tables.';\n      element.innerHTML = '';\n      dataTable['output_type'] = 'display_data';\n      await google.colab.output.renderOutput(dataTable, element);\n      const docLink = document.createElement('div');\n      docLink.innerHTML = docLinkHtml;\n      element.appendChild(docLink);\n    }\n  </script>\n  </div>\n</div>\n\n\n\n\n    \n![Example of vector similarity search for time series data](https://raw.githubusercontent.com/pinecone-io/img/main/time-series-example-40_2.png)\n    \n\n\n\n\n  <div id=\"df-8a2668ba-c4ce-4306-af59-ef0251c55690\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n      <th>values</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HCCO_2020-01-28_2020-03-31</td>\n      <td>1.000000</td>\n      <td>[9.95, 10.0, 10.05, 10.05, 10.05, 10.05, 10.05...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TEI_2016-09-22_2016-11-24</td>\n      <td>0.999958</td>\n      <td>[10.98, 11.0, 10.99, 10.96, 10.9933329, 10.956...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>BSD_2002-01-31_2002-04-04</td>\n      <td>0.999945</td>\n      <td>[14.04, 14.0, 14.04, 14.0, 14.04, 13.9933329, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HPI_2016-07-26_2016-09-27</td>\n      <td>0.999943</td>\n      <td>[23.21, 23.23, 23.25, 23.26, 23.2, 23.27, 23.2...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>MUE_2007-09-23_2007-11-25</td>\n      <td>0.999943</td>\n      <td>[12.2566671, 12.26, 12.25, 12.26, 12.26, 12.35...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>AMCI_2016-10-04_2016-12-06</td>\n      <td>0.999942</td>\n      <td>[0.488, 0.488, 0.488, 0.488, 0.488, 0.488, 0.4...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a2668ba-c4ce-4306-af59-ef0251c55690')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 0 24 24\"\n       width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg>\n      </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n  <script>\n    const buttonEl =\n      document.querySelector('#df-8a2668ba-c4ce-4306-af59-ef0251c55690 button.colab-df-convert');\n    buttonEl.style.display =\n      google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n    async function convertToInteractive(key) {\n      const element = document.querySelector('#df-8a2668ba-c4ce-4306-af59-ef0251c55690');\n      const dataTable =\n        await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                  [key], {});\n      if (!dataTable) return;\n\n      const docLinkHtml = 'Like what you see? Visit the ' +\n        '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n        + ' to learn more about interactive tables.';\n      element.innerHTML = '';\n      dataTable['output_type'] = 'display_data';\n      await google.colab.output.renderOutput(dataTable, element);\n      const docLink = document.createElement('div');\n      docLink.innerHTML = docLinkHtml;\n      element.appendChild(docLink);\n    }\n  </script>\n  </div>\n</div>\n\n\n\n    \n    Queried: HCCO_2020-01-28_2020-03-31\n    \n     The most similar items from the vector index:\n\n\n\n    \n![Example of vector similarity search for time series data](https://raw.githubusercontent.com/pinecone-io/img/main/time-series-example-40_5.png)\n    \n\n\n    \n    Queried: PUMP_2019-11-22_2020-01-24\n    \n     The most similar items from the vector index:\n\n\n\n\n  <div id=\"df-d08ca8ae-0d74-4478-9b54-73f69dec0ff6\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n      <th>values</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>PUMP_2019-11-22_2020-01-24</td>\n      <td>1.000000</td>\n      <td>[8.34, 8.27, 8.31333351, 8.42, 8.28666687, 8.5...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AEL_2003-12-14_2004-02-15</td>\n      <td>0.999547</td>\n      <td>[9.06333351, 9.04, 9.06, 9.01, 9.06, 9.02, 9.0...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>THS_2006-10-21_2006-12-23</td>\n      <td>0.999510</td>\n      <td>[24.503334, 24.376667, 24.3266659, 24.4233322,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>RUBY_2008-07-05_2008-09-06</td>\n      <td>0.999508</td>\n      <td>[5.02380943, 5.02380943, 4.97619057, 4.9761905...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SRI_2010-11-11_2011-01-13</td>\n      <td>0.999495</td>\n      <td>[12.82, 12.78, 12.61, 12.71, 12.6799994, 12.67...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>BH_2017-11-18_2018-01-20</td>\n      <td>0.999465</td>\n      <td>[343.266663, 341.956665, 342.693329, 340.48333...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d08ca8ae-0d74-4478-9b54-73f69dec0ff6')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 0 24 24\"\n       width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg>\n      </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n  <script>\n    const buttonEl =\n      document.querySelector('#df-d08ca8ae-0d74-4478-9b54-73f69dec0ff6 button.colab-df-convert');\n    buttonEl.style.display =\n      google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n    async function convertToInteractive(key) {\n      const element = document.querySelector('#df-d08ca8ae-0d74-4478-9b54-73f69dec0ff6');\n      const dataTable =\n        await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                  [key], {});\n      if (!dataTable) return;\n\n      const docLinkHtml = 'Like what you see? Visit the ' +\n        '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n        + ' to learn more about interactive tables.';\n      element.innerHTML = '';\n      dataTable['output_type'] = 'display_data';\n      await google.colab.output.renderOutput(dataTable, element);\n      const docLink = document.createElement('div');\n      docLink.innerHTML = docLinkHtml;\n      element.appendChild(docLink);\n    }\n  </script>\n  </div>\n</div>\n\n\n\n\n    \n![Example of vector similarity search for time series data](https://raw.githubusercontent.com/pinecone-io/img/main/time-series-example-40_8.png)\n    \n\n\nNotice that we found patterns that look alike and are related to different stocks and different time windows.\n\n## Facebook's Kats Time-Series Embeddings\nIt is time to test another approach. This time we create feature embeddings and upload them using the same stocks and windows as in our previous index. Here we utilize Facebook's [Kats toolkit](https://engineering.fb.com/2021/06/21/open-source/kats/). Kats is a powerful time-series analysis tool that includes a time-series [embedding functionality](https://github.com/facebookresearch/Kats#tsfeatures). \n\nLet's create a new index first.\n\n### Create a New Pinecone Service\n\n\n```python\n# Pick a name for the new index\nkats_index_name = 'stocks-trends-with-features'\n```\n\n\n```python\n# Check whether the index with the same name already exists\nif kats_index_name in pinecone.list_indexes():\n    pinecone.delete_index(kats_index_name)\n```\n\n\n```python\n# Create a new index\npinecone.create_index(name=kats_index_name, dimension=40)\n```\n\n\n```python\n# Establish a connection\nkats_index = pinecone.Index(index_name=kats_index_name)\n```\n\n### Index\nWe will use Kats and its time-series feature extraction module to create feature embeddings for each stock and corresponding time window. These feature embeddings include the following types: seasonality, autocorrelation, modeling parameter, changepoints, moving statistics, and raw statistics of time series array as the ad-hoc features. We used the default set of features for our example and created 40-dimensional feature embeddings.\n\n*Note: Ignore the Kats warning message that appears in the output.*\n\n\n```python\nupload_data_to_index(kats_index, get_feature_embedding_for_window)\nclear_output()\n```\n\n\n```python\nkats_index.describe_index_stats()\n```\n\n\n\n\n    {'dimension': 40, 'namespaces': {'': {'vector_count': 61105}}}\n\n\n\n*Note* that the Kats-based index has fewer vectors compared to the simple embeddings index. It happens because Kats fails to calculate some features for some patterns. E.g., it happens if the time series has a constant value each day in a time window.\n\n\n### Search\n\nWe will use the same query item that we used to query the simple index.\n\n\n```python\n# Fetch vectors from the index\nfetch_res = kats_index.fetch(ids=items_to_query)\n\n# Create a list of ids and vectors for the fetched items\nquery_ids = [res.id for res in fetch_res.vectors.values()]\nquery_vectors = [res.values for res in fetch_res.vectors.values()]\n```\n\n\n```python\n# Query the pinecone index\nquery_results = []\nfor xq in query_vectors:\n    res = kats_index.query(xq, top_k=100, include_values=True)\n    query_results.append(res)\n```\n\n\n```python\n# Iterate and show query results for each query item\nfor query_item, q_res in zip(query_ids, query_results):\n    print(f'\\nQueried: {query_item}')\n    res_df = pd.DataFrame(\n        {\n            'id': [res.id for res in q_res.matches], \n            'score': [res.score for res in q_res.matches]\n        }\n    )\n\n    # Use simple index to retrieve historical prices for query result items\n    res_df['values'] = res_df.id.apply(lambda x: [res.values for res in simple_index.fetch(ids=[x]).vectors.values()][0])\n    res_df = filter_results(query_item, res_df, historical_only=False)\n    show_query_results(query_item, res_df.head(6))\n```\n\n    WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /vectors/fetch?ids=BORR_2019-10-18_2019-12-20\n\n\n    \n    Queried: BORR_2019-10-18_2019-12-20\n    \n     The most similar items from the vector index:\n\n\n\n\n  <div id=\"df-322ecd0e-9264-4133-9240-e6eb1a144323\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n      <th>values</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BORR_2019-10-18_2019-12-20</td>\n      <td>1.000000</td>\n      <td>[6.22, 6.4, 6.45, 6.4666667, 6.68, 6.5333333, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NEON_2004-08-08_2004-10-10</td>\n      <td>0.999648</td>\n      <td>[4541.6665, 4500.0, 4500.0, 4512.5, 4487.5, 44...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AGTC_2019-10-27_2019-12-29</td>\n      <td>0.999632</td>\n      <td>[2.87000012, 2.92, 2.88, 2.95, 3.0, 3.03, 3.01...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DUO_2014-07-14_2014-09-15</td>\n      <td>0.999611</td>\n      <td>[1.57, 1.57, 1.57, 1.63, 1.64, 1.64, 1.64, 1.6...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>MLAB_2016-04-11_2016-06-13</td>\n      <td>0.999604</td>\n      <td>[96.13, 96.93, 97.68, 97.04, 98.12, 99.29, 98....</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>GENC_2004-01-11_2004-03-14</td>\n      <td>0.999585</td>\n      <td>[2.0333333, 2.07777762, 2.0333333, 2.1, 2.0666...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-322ecd0e-9264-4133-9240-e6eb1a144323')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 0 24 24\"\n       width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg>\n      </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n  <script>\n    const buttonEl =\n      document.querySelector('#df-322ecd0e-9264-4133-9240-e6eb1a144323 button.colab-df-convert');\n    buttonEl.style.display =\n      google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n    async function convertToInteractive(key) {\n      const element = document.querySelector('#df-322ecd0e-9264-4133-9240-e6eb1a144323');\n      const dataTable =\n        await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                  [key], {});\n      if (!dataTable) return;\n\n      const docLinkHtml = 'Like what you see? Visit the ' +\n        '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n        + ' to learn more about interactive tables.';\n      element.innerHTML = '';\n      dataTable['output_type'] = 'display_data';\n      await google.colab.output.renderOutput(dataTable, element);\n      const docLink = document.createElement('div');\n      docLink.innerHTML = docLinkHtml;\n      element.appendChild(docLink);\n    }\n  </script>\n  </div>\n</div>\n\n\n\n\n    \n![Example of vector similarity search for time series data](https://raw.githubusercontent.com/pinecone-io/img/main/time-series-example-57_2.png)\n    \n\n\n    \n    Queried: HCCO_2020-01-28_2020-03-31\n    \n     The most similar items from the vector index:\n\n\n\n\n  <div id=\"df-d686a314-5075-4fd3-864b-a1b4f822f82e\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n      <th>values</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HCCO_2020-01-28_2020-03-31</td>\n      <td>1.000000</td>\n      <td>[9.95, 10.0, 10.05, 10.05, 10.05, 10.05, 10.05...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TEI_2002-04-29_2002-07-01</td>\n      <td>0.999724</td>\n      <td>[12.0, 12.08, 12.1, 12.1, 12.13, 12.2, 12.2, 1...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>BY_2018-06-25_2018-08-27</td>\n      <td>0.999667</td>\n      <td>[23.76, 23.13, 23.13, 23.11, 23.2, 22.71, 22.7...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>OSK_2016-12-18_2017-02-19</td>\n      <td>0.999616</td>\n      <td>[66.9233322, 66.8233337, 66.27, 66.97, 66.98, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>PKBK_2019-10-27_2019-12-29</td>\n      <td>0.999610</td>\n      <td>[22.0121212, 21.9757576, 22.09091, 22.0181828,...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>RGR_2012-02-08_2012-04-11</td>\n      <td>0.999577</td>\n      <td>[43.05, 42.26, 42.22, 42.41, 42.03, 41.8, 42.0...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d686a314-5075-4fd3-864b-a1b4f822f82e')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 0 24 24\"\n       width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg>\n      </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n  <script>\n    const buttonEl =\n      document.querySelector('#df-d686a314-5075-4fd3-864b-a1b4f822f82e button.colab-df-convert');\n    buttonEl.style.display =\n      google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n    async function convertToInteractive(key) {\n      const element = document.querySelector('#df-d686a314-5075-4fd3-864b-a1b4f822f82e');\n      const dataTable =\n        await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                  [key], {});\n      if (!dataTable) return;\n\n      const docLinkHtml = 'Like what you see? Visit the ' +\n        '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n        + ' to learn more about interactive tables.';\n      element.innerHTML = '';\n      dataTable['output_type'] = 'display_data';\n      await google.colab.output.renderOutput(dataTable, element);\n      const docLink = document.createElement('div');\n      docLink.innerHTML = docLinkHtml;\n      element.appendChild(docLink);\n    }\n  </script>\n  </div>\n</div>\n\n\n\n\n    \n![Example of vector similarity search for time series data](thttps://raw.githubusercontent.com/pinecone-io/img/main/ime-series-example-57_5.png)\n    \n\n\n    \n    Queried: PUMP_2019-11-22_2020-01-24\n    \n     The most similar items from the vector index:\n\n\n\n\n  <div id=\"df-41786a1a-e34c-42d9-addc-f42dad37f9ff\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n      <th>values</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>PUMP_2019-11-22_2020-01-24</td>\n      <td>1.000000</td>\n      <td>[8.34, 8.27, 8.31333351, 8.42, 8.28666687, 8.5...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>RGR_2014-07-07_2014-09-08</td>\n      <td>0.999919</td>\n      <td>[59.78, 58.95, 58.7, 58.64, 58.65, 58.46, 57.7...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TVIX_2015-09-15_2015-11-17</td>\n      <td>0.999819</td>\n      <td>[322500.0, 266500.0, 246250.0, 234500.0, 23175...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>STNE_2018-12-14_2019-02-15</td>\n      <td>0.999745</td>\n      <td>[18.4, 17.71, 18.14, 17.6833324, 17.8800011, 1...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SCVL_2004-03-18_2004-05-20</td>\n      <td>0.999720</td>\n      <td>[10.373333, 10.2333336, 10.2266665, 10.2066669...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>AWI_2015-12-20_2016-02-21</td>\n      <td>0.999673</td>\n      <td>[45.876667, 45.3466644, 45.77, 45.17, 45.19, 4...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41786a1a-e34c-42d9-addc-f42dad37f9ff')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 0 24 24\"\n       width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg>\n      </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n  <script>\n    const buttonEl =\n      document.querySelector('#df-41786a1a-e34c-42d9-addc-f42dad37f9ff button.colab-df-convert');\n    buttonEl.style.display =\n      google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n    async function convertToInteractive(key) {\n      const element = document.querySelector('#df-41786a1a-e34c-42d9-addc-f42dad37f9ff');\n      const dataTable =\n        await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                  [key], {});\n      if (!dataTable) return;\n\n      const docLinkHtml = 'Like what you see? Visit the ' +\n        '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n        + ' to learn more about interactive tables.';\n      element.innerHTML = '';\n      dataTable['output_type'] = 'display_data';\n      await google.colab.output.renderOutput(dataTable, element);\n      const docLink = document.createElement('div');\n      docLink.innerHTML = docLinkHtml;\n      element.appendChild(docLink);\n    }\n  </script>\n  </div>\n</div>\n\n\n\n\n    \n![Example of vector similarity search for time series data](thttps://raw.githubusercontent.com/pinecone-io/img/main/ime-series-example-57_8.png)\n    \n\n\n## Conclusion\nPattern matching of time series data is an important task affecting time series clustering, labeling, classification, and recommendation. \n\nWe used the similarity search to find the most similar patterns in stocks data. We tried two different approaches to create vector representations of time series. First, we used the raw data of historical prices, and then we represented time-series as a set of statistical features. In both cases, we retrieved the top 100 best matches from the Pinecone similarity search service. Then we further filtered and showed only the top 5 most similar stock trends. (We did that to make sure we retrieve a diverse set of stocks. Otherwise, we might get consecutive time windows for the same stock.)\n\nThe simple approach turned out to give good results. When using Kats' time series features, we got somehow mixed results. We noticed that the most similar feature embeddings sometimes retrieve reverse patterns. \n\nYet, we note that the literature has plentiful advanced time series representation techniques. Starting from sequential deep neural network approaches such as LSTMs and RNNs, combining frequency-domain representations with convolutional neural networks, and even applying deep neural graphs embeddings. We encourage you to explore this fascinating domain. Feel free to try it along with the Pinecone service, and [share](https://www.pinecone.io/contact/) your findings with us!\n\n## Delete indexes\n\nDelete the indexes once you are sure that you do not want to use it anymore. Once the index is deleted, you cannot use it again.\n\n\n\n\n```python\npinecone.delete_index(simple_index_name)\npinecone.delete_index(kats_index_name)\n```\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1cceb"
  },
  "filename": "ner-search.md",
  "title": "NER-Powered Semantic Search",
  "category": "630fc5235d91a70054705fb7",
  "content": "---\ntitle: NER-Powered Semantic Search\ncategory: 630fc5235d91a70054705fb7\n---\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/search/semantic-search/ner-search/ner-powered-search.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/search/semantic-search/ner-search/ner-powered-search.ipynb) [![Open github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/pinecone-io/examples/blob/master/search/semantic-search/ner-search/ner-powered-search.ipynb)\n\nThis notebook shows how to use Named Entity Recognition (NER) for hybrid metadata + vector search with Pinecone. We will:\n\n1. Extract named entities from text.\n2. Store them in a Pinecone index as metadata (alongside respective text vectors).\n3. We extract named entities from incoming queries and use them to filter and search only through records containing these named entities.\n\nThis is particularly helpful if you want to restrict the search score to records that contain information about the named entities that are also found within the query.\n\nLet's get started.\n\n# Install Dependencies\n\n\n```python\n!pip install sentence_transformers pinecone-client datasets\n```\n\n# Load and Prepare Dataset\n\nWe use a dataset containing ~190K articles scraped from Medium. We select 50K articles from the dataset as indexing all the articles may take some time. This dataset can be loaded from the HuggingFace dataset hub as follows:\n\n\n```python\nfrom datasets import load_dataset\n\n# load the dataset and convert to pandas dataframe\ndf = load_dataset(\n    \"fabiochiu/medium-articles\",\n    data_files=\"medium_articles.csv\",\n    split=\"train\"\n).to_pandas()\n```\n    \n\n\n```python\n# drop empty rows and select 50k articles\ndf = df.dropna().sample(50000, random_state=32)\ndf.head()\n```\n\n\n\n\n\n<div id=\"df-4dd0304d-b1f1-4ac5-8c2c-b84168801b0a\">\n  <div class=\"colab-df-container\">\n    <div><div class=\"table-wrapper\" markdown=\"block\">\n\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>text</th>\n      <th>url</th>\n      <th>authors</th>\n      <th>timestamp</th>\n      <th>tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4172</th>\n      <td>How the Data Stole Christmas</td>\n      <td>by Anonymous\\n\\nThe door sprung open and our t...</td>\n      <td>https://medium.com/data-ops/how-the-data-stole...</td>\n      <td>[]</td>\n      <td>2019-12-24 13:22:33.143000+00:00</td>\n      <td>[Data Science, Big Data, Dataops, Analytics, D...</td>\n    </tr>\n    <tr>\n      <th>174868</th>\n      <td>Automating Light Switch using the ESP32 Board ...</td>\n      <td>A story about how I escaped the boring task th...</td>\n      <td>https://python.plainenglish.io/automating-ligh...</td>\n      <td>['Tomas Rasymas']</td>\n      <td>2021-09-14 07:20:52.342000+00:00</td>\n      <td>[Programming, Python, Software Development, Ha...</td>\n    </tr>\n    <tr>\n      <th>100171</th>\n      <td>Keep Going Quotes Sayings for When Hope is Lost</td>\n      <td>It’s a very thrilling thing to achieve a goal....</td>\n      <td>https://medium.com/@yourselfquotes/keep-going-...</td>\n      <td>['Yourself Quotes']</td>\n      <td>2021-01-05 12:13:04.018000+00:00</td>\n      <td>[Quotes]</td>\n    </tr>\n    <tr>\n      <th>141757</th>\n      <td>When Will the Smoke Clear From Bay Area Skies?</td>\n      <td>Bay Area cities are contending with some of th...</td>\n      <td>https://thebolditalic.com/when-will-the-smoke-...</td>\n      <td>['Matt Charnock']</td>\n      <td>2020-09-15 22:38:33.924000+00:00</td>\n      <td>[Bay Area, San Francisco, California, Wildfire...</td>\n    </tr>\n    <tr>\n      <th>183489</th>\n      <td>The ABC’s of Sustainability… easy as 1, 2, 3</td>\n      <td>By Julia DiPrete\\n\\n(according to the Jackson ...</td>\n      <td>https://medium.com/sipwines/the-abcs-of-sustai...</td>\n      <td>['Sip Wines']</td>\n      <td>2021-03-02 23:39:49.948000+00:00</td>\n      <td>[Wine Tasting, Sustainability, Wine]</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4dd0304d-b1f1-4ac5-8c2c-b84168801b0a')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n       width=\"24px\">\n    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n  </svg>\n      </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n  <script>\n    const buttonEl =\n      document.querySelector('#df-4dd0304d-b1f1-4ac5-8c2c-b84168801b0a button.colab-df-convert');\n    buttonEl.style.display =\n      google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n    async function convertToInteractive(key) {\n      const element = document.querySelector('#df-4dd0304d-b1f1-4ac5-8c2c-b84168801b0a');\n      const dataTable =\n        await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                  [key], {});\n      if (!dataTable) return;\n\n      const docLinkHtml = 'Like what you see? Visit the ' +\n        '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n        + ' to learn more about interactive tables.';\n      element.innerHTML = '';\n      dataTable['output_type'] = 'display_data';\n      await google.colab.output.renderOutput(dataTable, element);\n      const docLink = document.createElement('div');\n      docLink.innerHTML = docLinkHtml;\n      element.appendChild(docLink);\n    }\n  </script>\n</div>\n  </div>\n\n\n\n\nWe will use the article title and its text for generating embeddings. For that, we join the article title and the first 1000 characters from the article text.\n\n\n```python\n# select first 1000 characters\ndf[\"text\"] = df[\"text\"].str[:1000]\n# join article title and the text\ndf[\"title_text\"] = df[\"title\"] + \". \" + df[\"text\"]\n```\n\n# Initialize NER Model\n\nTo extract named entities, we will use a NER model finetuned on a BERT-base model. The model can be loaded from the HuggingFace model hub as follows:\n\n\n```python\nimport torch\n\n# set device to GPU if available\ndevice = torch.cuda.current_device() if torch.cuda.is_available() else None\n```\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\n\nmodel_id = \"dslim/bert-base-NER\"\n\n# load the tokenizer from huggingface\ntokenizer = AutoTokenizer.from_pretrained(\n    model_id\n)\n# load the NER model from huggingface\nmodel = AutoModelForTokenClassification.from_pretrained(\n    model_id\n)\n# load the tokenizer and model into a NER pipeline\nnlp = pipeline(\n    \"ner\",\n    model=model,\n    tokenizer=tokenizer,\n    aggregation_strategy=\"max\",\n    device=device\n)\n```\n\n\n\n\n\n```python\ntext = \"London is the capital of England and the United Kingdom\"\n# use the NER pipeline to extract named entities from the text\nnlp(text)\n```\n\n\n\n\n    [{'entity_group': 'LOC',\n      'score': 0.9996493,\n      'word': 'London',\n      'start': 0,\n      'end': 6},\n     {'entity_group': 'LOC',\n      'score': 0.9997588,\n      'word': 'England',\n      'start': 25,\n      'end': 32},\n     {'entity_group': 'LOC',\n      'score': 0.9993923,\n      'word': 'United Kingdom',\n      'start': 41,\n      'end': 55}]\n\n\n\nOur NER pipeline is working as expected and accurately extracting entities from the text.\n\n# Initialize Retriever\n\nA retriever model is used to embed passages (article title + first 1000 characters) and queries. It creates embeddings such that queries and passages with similar meanings are close in the vector space. We will use a sentence-transformer model as our retriever. The model can be loaded as follows:\n\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\n# load the model from huggingface\nretriever = SentenceTransformer(\n    'flax-sentence-embeddings/all_datasets_v3_mpnet-base',\n    device=device\n)\nretriever\n```\n\n# Initialize Pinecone Index\n\nNow we need to initialize our Pinecone index. The Pinecone index stores vector representations of our passages which we can retrieve using another vector (the query vector). We first need to initialize our connection to Pinecone. For this, we need a free [API key](https://app.pinecone.io/); you can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**. We initialize the connection like so:\n\n\n```python\nimport pinecone\n\n# connect to pinecone environment\npinecone.init(\n    api_key=\"YOUR_API_KEY\",\n    environment=\"YOUR_ENVIRONMENT\"\n)\n```\n\nNow we can create our vector index. We will name it `ner-search` (feel free to chose any name you prefer). We specify the metric type as `cosine` and dimension as `768` as these are the vector space and dimensionality of the vectors output by the retriever model.\n\n\n```python\nindex_name = \"ner-search\"\n\n# check if the ner-search index exists\nif index_name not in pinecone.list_indexes():\n    # create the index if it does not exist\n    pinecone.create_index(\n        index_name,\n        dimension=768,\n        metric=\"cosine\"\n    )\n\n# connect to ner-search index we created\nindex = pinecone.Index(index_name)\n```\n\n# Generate Embeddings and Upsert\n\nWe generate embeddings for the `title_text` column we created earlier. Alongside the embeddings, we also include the named entities in the index as metadata. Later we will apply a filter based on these named entities when executing queries.\n\nLet's first write a helper function to extract named entities from a batch of text.\n\n\n```python\ndef extract_named_entities(text_batch):\n    # extract named entities using the NER pipeline\n    extracted_batch = nlp(text_batch)\n    entities = []\n    # loop through the results and only select the entity names\n    for text in extracted_batch:\n        ne = [entity[\"word\"] for entity in text]\n        entities.append(ne)\n    return entities\n```\n\nNow we create the embeddings. We do this in batches of `64` to avoid overwhelming machine resources or API request limits.\n\n\n```python\nfrom tqdm.auto import tqdm\n\n# we will use batches of 64\nbatch_size = 64\n\nfor i in tqdm(range(0, len(df), batch_size)):\n    # find end of batch\n    i_end = min(i+batch_size, len(df))\n    # extract batch\n    batch = df.iloc[i:i_end]\n    # generate embeddings for batch\n    emb = retriever.encode(batch[\"title_text\"].tolist()).tolist()\n    # extract named entities from the batch\n    entities = extract_named_entities(batch[\"title_text\"].tolist())\n    # remove duplicate entities from each record\n    batch[\"named_entities\"] = [list(set(entity)) for entity in entities]\n    batch = batch.drop('title_text', axis=1)\n    # get metadata\n    meta = batch.to_dict(orient=\"records\")\n    # create unique IDs\n    ids = [f\"{idx}\" for idx in range(i, i_end)]\n    # add all to upsert list\n    to_upsert = list(zip(ids, emb, meta))\n    # upsert/insert these records to pinecone\n    _ = index.upsert(vectors=to_upsert)\n \n# check that we have all vectors in index\nindex.describe_index_stats()\n```\n\n\n      100%|██████████| 782/782 [58:24<00:00, 2.24it/s]\n\n    {'dimension': 768,\n     'index_fullness': 0.1,\n     'namespaces': {'': {'vector_count': 50000}},\n     'total_vector_count': 50000}\n\n\n\nNow we have indexed the articles and relevant metadata. We can move on to querying.\n\n# Querying\n\nFirst, we will write a helper function to handle the queries.\n\n\n```python\nfrom pprint import pprint\n\ndef search_pinecone(query):\n    # extract named entities from the query\n    ne = extract_named_entities([query])[0]\n    # create embeddings for the query\n    xq = retriever.encode(query).tolist()\n    # query the pinecone index while applying named entity filter\n    xc = index.query(xq, top_k=10, include_metadata=True, filter={\"named_entities\": {\"$in\": ne}})\n    # extract article titles from the search result\n    r = [x[\"metadata\"][\"title\"] for x in xc[\"matches\"]]\n    return pprint({\"Extracted Named Entities\": ne, \"Result\": r})\n```\n\nNow try a query.\n\n\n```python\nquery = \"What are the best places to visit in Greece?\"\nsearch_pinecone(query)\n```\n \n\n    {'Extracted Named Entities': ['Greece'],\n     'Result': ['Budget-Friendly Holidays: Visit The Best Summer Destinations In '\n                'Greece | easyGuide',\n                'Exploring Greece',\n                'Santorini Island. The power of this volcanic island creates an '\n                'energy that overwhelms the senses…',\n                'All aboard to Greece: With lifting travel restrictions, what is '\n                'the future of the Greek tourism industry?',\n                'The Search for Best Villas in Greece for Rental Ends Here | '\n                'Alasvillas | Greece',\n                'Peripéteies in Greece — Week 31. Adventures in Greece as we '\n                'pursue the…',\n                '‘City of Waterfalls’ Home to Stunning Natural Scenery',\n                'Skiathos — The small paradise in the Sporades, Greece.',\n                'Greece has its own Dominic Cummings — and things are about to get '\n                'scary',\n                'One Must-Visit Attraction in Each of Europe’s Most Popular '\n                'Cities']}\n    \n\n\n```python\nquery = \"What are the best places to visit in London?\"\nsearch_pinecone(query)\n```\n    \n\n    {'Extracted Named Entities': ['London'],\n     'Result': ['Historical places to visit in London',\n                'Never Die Without Seeing London-1',\n                'London LOOP: walk all the way around the capital',\n                'To London, In London',\n                'You’ll never look at London the same way again after playing '\n                'Pokemon GO',\n                'Recommendation system to start a restaurant business in London',\n                'Primrose and Regent’s Park London Walk — Portraits in the City',\n                'Parliaments, picnics and social cleansing: a walk in London',\n                'Universities’ role in building back London',\n                'The Building of London']}\n    \n\n\n```python\nquery = \"Why does SpaceX want to build a city on Mars?\"\nsearch_pinecone(query)\n```\n\n    {'Extracted Named Entities': ['SpaceX', 'Mars'],\n     'Result': ['Elon Musk: First Mars City Will Take 1,000 Starships, 20 Years',\n                'Elon Musk, SpaceX and NASA Are Taking The Second Step In The '\n                'Direction Of Mars',\n                'WHAT ETHICS FRAMEWORK IS NEEDED TO DEVELOP SPACE?',\n                'What is the SpaceX- Starship Mission? What is the SpaceX Mars '\n                'Architecture?',\n                'There is a 100% chance of dying on Mars. Why Elon Musk’s plans '\n                'are suicide for astronauts?',\n                'The Mars Conundrum',\n                'Tesla’s “Starman” or 2001’s “Star Child”—Which One Should Guide '\n                'Our Species Into Space?',\n                'Mars Habitat: NASA 3D Printed Habitat Challenge',\n                'Reusable rockets and the robots at sea: The SpaceX story',\n                'Mars Is Overrated and Going There Isn’t Progress']}\n\n    \n\nThese all look like great results, making the most of Pinecone's advanced vector search capabilities while limiting search scope to relevant records only with a named entity filter.\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccec"
  },
  "filename": "extreme-classification.md",
  "title": "Extreme Classification",
  "category": "630fc5235d91a70054705fb7",
  "content": "---\ntitle: Extreme Classification\ncategory: 630fc5235d91a70054705fb7\n---\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/analytics-and-ml/extreme-classification/extreme-classification.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/analytics-and-ml/extreme-classification/extreme-classification.ipynb) [![Open Github](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/github-shield.svg)](https://github.com/pinecone-io/examples/tree/master/analytics-and-ml/extreme-classification/extreme-classification.ipynb)\n\nThis demo aims to label new texts automatically when the number of possible labels is enormous. This scenario is known as extreme classification, a supervised learning variant that deals with multi-class and multi-label problems involving many choices. \n\nExamples for applying extreme classification are labeling a new article with Wikipedia's topical labels, matching web content with a set of relevant advertisements, classifying product descriptions with catalog labels, and classifying a resume into a collection of pertinent job titles. \n\nHere's how we'll perform extreme classification:\n\n1. We'll transform 250,000 labels into vector embeddings using a publicly available embedding model and upload them into a [managed vector index](https://www.pinecone.io/). \n2. Then we'll take an article that requires labeling and transform it into a [vector embedding](https://www.pinecone.io/learn/vector-embeddings/) using the same model.\n3. We'll use that article's vector embedding as the query to search the vector index. In effect, this will retrieve the most similar labels to the article's semantic content.\n4. With the most relevant labels retrieved, we can automatically apply them to the article.\n\nLet's get started!\n\n## Dependencies\n\n\n```python\n!pip install -qU pinecone-client ipywidgets setuptools>=36.2.1 wikitextparser unidecode\n!pip install -qU sentence-transformers --no-cache-dir\n```\n\n\n```python\nimport os\nimport re\nimport gzip\nimport json\nimport pandas as pd\nimport numpy as np\nfrom wikitextparser import remove_markup, parse\nfrom sentence_transformers import SentenceTransformer\nfrom unidecode import unidecode\n```\n\n## Setting up Pinecone's Similarity Search Service\nHere we set up our similarity search service. We assume you are familiar with Pinecone's [quick start tutorial](https://www.pinecone.io/docs/quickstart-python/).\n\n\n```python\nimport pinecone\n```\n\n\n```python\n# Load Pinecone API key\napi_key = os.getenv(\"PINECONE_API_KEY\") or \"YOUR_API_KEY\"\npinecone.init(api_key=api_key, environment='YOUR_ENVIRONMENT')\n\n# List all existing indices for you API key\npinecone.list_indexes()\n```\n\n    []\n    \n\n[Get a Pinecone API key](https://www.pinecone.io/start/) if you don’t have one. You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**.\n\n\n```python\n# Pick a name for the new index\nindex_name = 'extreme-ml'\n```\n\n\n```python\n# Check whether the index with the same name already exists\nif index_name in pinecone.list_indexes():\n    pinecone.delete_index(index_name)\n```\n\n\n```python\n# Create a new vector index\npinecone.create_index(name=index_name, dimension=300)\n```\n\n\n```python\n# Connect to the created index\nindex = pinecone.Index(index_name)\n\n# Print index statistics\nindex.describe_index_stats()\n```\n\n\n\n\n    {'dimension': 300, 'namespaces': {'': {'vector_count': 139500}}}\n\n\n\n## Data Preparation \nIn this demo, we classify Wikipedia articles using a standard dataset from an extreme classification benchmarking [resource](http://manikvarma.org/downloads/XC/XMLRepository.html). The data used in this example is [Wikipedia-500k](https://drive.google.com/drive/folders/12HiiGWmbLfTEEObs2Y2jiTETZfXDowrn) which contains around 500,000 labels. Here, we will download the raw data and prepare it for the classification task.\n\n\n\n```python\n# Download train dataset\n!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=10RBSf6nC9C38wUMwqWur2Yd8mCwtup5K' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=10RBSf6nC9C38wUMwqWur2Yd8mCwtup5K\" -O 'trn.raw.json.gz' && rm -rf /tmp/cookies.txt \n\n# Download test dataset\n!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1pEyKXtkwHhinuRxmARhtwEQ39VIughDf' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1pEyKXtkwHhinuRxmARhtwEQ39VIughDf\" -O 'tst.raw.json.gz' && rm -rf /tmp/cookies.txt\n\n# Download categories labels file\n!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ZYTZPlnkPBCMcNqRRO-gNx8EPgtV-GL3' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1ZYTZPlnkPBCMcNqRRO-gNx8EPgtV-GL3\" -O 'Yf.txt' && rm -rf /tmp/cookies.txt\n\n# Create and move downloaded files to data folder\n!mkdir data\n!mv 'trn.raw.json.gz' 'tst.raw.json.gz' 'Yf.txt' data\n```\n\n    --2022-02-09 17:13:45--  https://docs.google.com/uc?export=download&confirm=arfw&id=10RBSf6nC9C38wUMwqWur2Yd8mCwtup5K\n    Resolving docs.google.com (docs.google.com)... 142.251.107.101, 142.251.107.113, 142.251.107.100, ...\n    Connecting to docs.google.com (docs.google.com)|142.251.107.101|:443... connected.\n    HTTP request sent, awaiting response... 302 Moved Temporarily\n    Location: https://doc-14-5k-docs.googleusercontent.com/docs/securesc/mi75es4ss9f8cbmlkasp0714ekfl64em/oiufcg4j2mku0ucr6a89me2tql3td1v1/1644426825000/06283569454216238406/01276505903269316155Z/10RBSf6nC9C38wUMwqWur2Yd8mCwtup5K?e=download [following]\n    --2022-02-09 17:13:45--  https://doc-14-5k-docs.googleusercontent.com/docs/securesc/mi75es4ss9f8cbmlkasp0714ekfl64em/oiufcg4j2mku0ucr6a89me2tql3td1v1/1644426825000/06283569454216238406/01276505903269316155Z/10RBSf6nC9C38wUMwqWur2Yd8mCwtup5K?e=download\n    Resolving doc-14-5k-docs.googleusercontent.com (doc-14-5k-docs.googleusercontent.com)... 173.194.210.132, 2607:f8b0:400c:c0f::84\n    Connecting to doc-14-5k-docs.googleusercontent.com (doc-14-5k-docs.googleusercontent.com)|173.194.210.132|:443... connected.\n    HTTP request sent, awaiting response... 302 Found\n    Location: https://docs.google.com/nonceSigner?nonce=l2fmr07iln170&continue=https://doc-14-5k-docs.googleusercontent.com/docs/securesc/mi75es4ss9f8cbmlkasp0714ekfl64em/oiufcg4j2mku0ucr6a89me2tql3td1v1/1644426825000/06283569454216238406/01276505903269316155Z/10RBSf6nC9C38wUMwqWur2Yd8mCwtup5K?e%3Ddownload&hash=46dcmopdem1mp2anvp98snh4203mij96 [following]\n    --2022-02-09 17:13:45--  https://docs.google.com/nonceSigner?nonce=l2fmr07iln170&continue=https://doc-14-5k-docs.googleusercontent.com/docs/securesc/mi75es4ss9f8cbmlkasp0714ekfl64em/oiufcg4j2mku0ucr6a89me2tql3td1v1/1644426825000/06283569454216238406/01276505903269316155Z/10RBSf6nC9C38wUMwqWur2Yd8mCwtup5K?e%3Ddownload&hash=46dcmopdem1mp2anvp98snh4203mij96\n    Connecting to docs.google.com (docs.google.com)|142.251.107.101|:443... connected.\n    HTTP request sent, awaiting response... 302 Found\n    Location: https://doc-14-5k-docs.googleusercontent.com/docs/securesc/mi75es4ss9f8cbmlkasp0714ekfl64em/oiufcg4j2mku0ucr6a89me2tql3td1v1/1644426825000/06283569454216238406/01276505903269316155Z/10RBSf6nC9C38wUMwqWur2Yd8mCwtup5K?e=download&nonce=l2fmr07iln170&user=01276505903269316155Z&hash=jgf078p8n3lll6ulirsoqj13g645cf9q [following]\n    --2022-02-09 17:13:45--  https://doc-14-5k-docs.googleusercontent.com/docs/securesc/mi75es4ss9f8cbmlkasp0714ekfl64em/oiufcg4j2mku0ucr6a89me2tql3td1v1/1644426825000/06283569454216238406/01276505903269316155Z/10RBSf6nC9C38wUMwqWur2Yd8mCwtup5K?e=download&nonce=l2fmr07iln170&user=01276505903269316155Z&hash=jgf078p8n3lll6ulirsoqj13g645cf9q\n    Connecting to doc-14-5k-docs.googleusercontent.com (doc-14-5k-docs.googleusercontent.com)|173.194.210.132|:443... connected.\n    HTTP request sent, awaiting response... 200 OK\n    Length: 5292805889 (4.9G) [application/x-gzip]\n    Saving to: ‘trn.raw.json.gz’\n    \n    trn.raw.json.gz     100%[===================>]   4.93G   120MB/s    in 57s     \n    \n    2022-02-09 17:14:42 (89.0 MB/s) - ‘trn.raw.json.gz’ saved [5292805889/5292805889]\n    \n    --2022-02-09 17:14:43--  https://docs.google.com/uc?export=download&confirm=uE_3&id=1pEyKXtkwHhinuRxmARhtwEQ39VIughDf\n    Resolving docs.google.com (docs.google.com)... 142.251.107.101, 142.251.107.113, 142.251.107.100, ...\n    Connecting to docs.google.com (docs.google.com)|142.251.107.101|:443... connected.\n    HTTP request sent, awaiting response... 302 Moved Temporarily\n    Location: https://doc-0c-2o-docs.googleusercontent.com/docs/securesc/5m4vneh8ah734pahp3qbjncg6mmp89ta/d5qkmrodksuck3tqaeh82prkj3v26vfe/1644426825000/06283569454216238406/08808106369581203619Z/1pEyKXtkwHhinuRxmARhtwEQ39VIughDf?e=download [following]\n    --2022-02-09 17:14:43--  https://doc-0c-2o-docs.googleusercontent.com/docs/securesc/5m4vneh8ah734pahp3qbjncg6mmp89ta/d5qkmrodksuck3tqaeh82prkj3v26vfe/1644426825000/06283569454216238406/08808106369581203619Z/1pEyKXtkwHhinuRxmARhtwEQ39VIughDf?e=download\n    Resolving doc-0c-2o-docs.googleusercontent.com (doc-0c-2o-docs.googleusercontent.com)... 173.194.210.132, 2607:f8b0:400c:c0f::84\n    Connecting to doc-0c-2o-docs.googleusercontent.com (doc-0c-2o-docs.googleusercontent.com)|173.194.210.132|:443... connected.\n    HTTP request sent, awaiting response... 302 Found\n    Location: https://docs.google.com/nonceSigner?nonce=fv89ild8hgp3u&continue=https://doc-0c-2o-docs.googleusercontent.com/docs/securesc/5m4vneh8ah734pahp3qbjncg6mmp89ta/d5qkmrodksuck3tqaeh82prkj3v26vfe/1644426825000/06283569454216238406/08808106369581203619Z/1pEyKXtkwHhinuRxmARhtwEQ39VIughDf?e%3Ddownload&hash=algva8fi1m74v18nhdhve6o38458h8bo [following]\n    --2022-02-09 17:14:43--  https://docs.google.com/nonceSigner?nonce=fv89ild8hgp3u&continue=https://doc-0c-2o-docs.googleusercontent.com/docs/securesc/5m4vneh8ah734pahp3qbjncg6mmp89ta/d5qkmrodksuck3tqaeh82prkj3v26vfe/1644426825000/06283569454216238406/08808106369581203619Z/1pEyKXtkwHhinuRxmARhtwEQ39VIughDf?e%3Ddownload&hash=algva8fi1m74v18nhdhve6o38458h8bo\n    Connecting to docs.google.com (docs.google.com)|142.251.107.101|:443... connected.\n    HTTP request sent, awaiting response... 302 Found\n    Location: https://doc-0c-2o-docs.googleusercontent.com/docs/securesc/5m4vneh8ah734pahp3qbjncg6mmp89ta/d5qkmrodksuck3tqaeh82prkj3v26vfe/1644426825000/06283569454216238406/08808106369581203619Z/1pEyKXtkwHhinuRxmARhtwEQ39VIughDf?e=download&nonce=fv89ild8hgp3u&user=08808106369581203619Z&hash=d3m02ho8p665cjtl094bjkqk6g1qftj1 [following]\n    --2022-02-09 17:14:43--  https://doc-0c-2o-docs.googleusercontent.com/docs/securesc/5m4vneh8ah734pahp3qbjncg6mmp89ta/d5qkmrodksuck3tqaeh82prkj3v26vfe/1644426825000/06283569454216238406/08808106369581203619Z/1pEyKXtkwHhinuRxmARhtwEQ39VIughDf?e=download&nonce=fv89ild8hgp3u&user=08808106369581203619Z&hash=d3m02ho8p665cjtl094bjkqk6g1qftj1\n    Connecting to doc-0c-2o-docs.googleusercontent.com (doc-0c-2o-docs.googleusercontent.com)|173.194.210.132|:443... connected.\n    HTTP request sent, awaiting response... 200 OK\n    Length: 2297151115 (2.1G) [application/x-gzip]\n    Saving to: ‘tst.raw.json.gz’\n    \n    tst.raw.json.gz     100%[===================>]   2.14G   130MB/s    in 15s     \n    \n    2022-02-09 17:14:59 (141 MB/s) - ‘tst.raw.json.gz’ saved [2297151115/2297151115]\n    \n    --2022-02-09 17:15:01--  https://docs.google.com/uc?export=download&confirm=&id=1ZYTZPlnkPBCMcNqRRO-gNx8EPgtV-GL3\n    Resolving docs.google.com (docs.google.com)... 173.194.210.139, 173.194.210.138, 173.194.210.102, ...\n    Connecting to docs.google.com (docs.google.com)|173.194.210.139|:443... connected.\n    HTTP request sent, awaiting response... 302 Moved Temporarily\n    Location: https://doc-04-7s-docs.googleusercontent.com/docs/securesc/55p9v9r892nth323knj0kpu3c0fh68iu/jne4sjlttqu450ikt8fph58j505oapok/1644426900000/06283569454216238406/07409829577848409351Z/1ZYTZPlnkPBCMcNqRRO-gNx8EPgtV-GL3?e=download [following]\n    --2022-02-09 17:15:02--  https://doc-04-7s-docs.googleusercontent.com/docs/securesc/55p9v9r892nth323knj0kpu3c0fh68iu/jne4sjlttqu450ikt8fph58j505oapok/1644426900000/06283569454216238406/07409829577848409351Z/1ZYTZPlnkPBCMcNqRRO-gNx8EPgtV-GL3?e=download\n    Resolving doc-04-7s-docs.googleusercontent.com (doc-04-7s-docs.googleusercontent.com)... 173.194.210.132, 2607:f8b0:400c:c0f::84\n    Connecting to doc-04-7s-docs.googleusercontent.com (doc-04-7s-docs.googleusercontent.com)|173.194.210.132|:443... connected.\n    HTTP request sent, awaiting response... 302 Found\n    Location: https://docs.google.com/nonceSigner?nonce=erf8t6vb6o29s&continue=https://doc-04-7s-docs.googleusercontent.com/docs/securesc/55p9v9r892nth323knj0kpu3c0fh68iu/jne4sjlttqu450ikt8fph58j505oapok/1644426900000/06283569454216238406/07409829577848409351Z/1ZYTZPlnkPBCMcNqRRO-gNx8EPgtV-GL3?e%3Ddownload&hash=tjpqbk2dp469l934sb9627cs6d9dq9ht [following]\n    --2022-02-09 17:15:02--  https://docs.google.com/nonceSigner?nonce=erf8t6vb6o29s&continue=https://doc-04-7s-docs.googleusercontent.com/docs/securesc/55p9v9r892nth323knj0kpu3c0fh68iu/jne4sjlttqu450ikt8fph58j505oapok/1644426900000/06283569454216238406/07409829577848409351Z/1ZYTZPlnkPBCMcNqRRO-gNx8EPgtV-GL3?e%3Ddownload&hash=tjpqbk2dp469l934sb9627cs6d9dq9ht\n    Connecting to docs.google.com (docs.google.com)|173.194.210.139|:443... connected.\n    HTTP request sent, awaiting response... 302 Found\n    Location: https://doc-04-7s-docs.googleusercontent.com/docs/securesc/55p9v9r892nth323knj0kpu3c0fh68iu/jne4sjlttqu450ikt8fph58j505oapok/1644426900000/06283569454216238406/07409829577848409351Z/1ZYTZPlnkPBCMcNqRRO-gNx8EPgtV-GL3?e=download&nonce=erf8t6vb6o29s&user=07409829577848409351Z&hash=m0ja3hdvfkrvsfr6kol8k4bktej41mje [following]\n    --2022-02-09 17:15:02--  https://doc-04-7s-docs.googleusercontent.com/docs/securesc/55p9v9r892nth323knj0kpu3c0fh68iu/jne4sjlttqu450ikt8fph58j505oapok/1644426900000/06283569454216238406/07409829577848409351Z/1ZYTZPlnkPBCMcNqRRO-gNx8EPgtV-GL3?e=download&nonce=erf8t6vb6o29s&user=07409829577848409351Z&hash=m0ja3hdvfkrvsfr6kol8k4bktej41mje\n    Connecting to doc-04-7s-docs.googleusercontent.com (doc-04-7s-docs.googleusercontent.com)|173.194.210.132|:443... connected.\n    HTTP request sent, awaiting response... 200 OK\n    Length: 33740692 (32M) [text/plain]\n    Saving to: ‘Yf.txt’\n    \n    Yf.txt              100%[===================>]  32.18M  --.-KB/s    in 0.1s    \n    \n    2022-02-09 17:15:02 (248 MB/s) - ‘Yf.txt’ saved [33740692/33740692]\n    \n    \n\n\n```python\n# Define paths\nROOT_PATH = os.getcwd()\nTRAIN_DATA_PATH = (os.path.join(ROOT_PATH, 'data/trn.raw.json.gz'))\nTEST_DATA_PATH = (os.path.join(ROOT_PATH, 'data/tst.raw.json.gz'))\n```\n\n\n```python\n# Load categories\nwith open('./data/Yf.txt',  encoding='utf-8') as f:\n    categories = f.readlines()\n\n# Clean values\ncategories = [cat.split('->')[1].strip('\\n') for cat in categories]\n\n# Show frist few categories\ncategories[:3]\n```\n\n\n\n\n    ['!!!_albums', '+/-_(band)_albums', '+44_(band)_songs']\n\n\n\n### Using a Subset of the Data\nFor this example, we will select and use a subset of wikipedia articles. This will save time for processing and consume much less memory than the complete dataset.\nWe will select a sample of 200,000 articles that contains around 250,000 different labels. \n\nFeel free to run the notebook with more data. \n\n\n```python\nWIKI_ARTICLES_INDEX = range(0, 1000000, 5)\n\nlines = []\n\nwith gzip.open(TRAIN_DATA_PATH) as f:\n    for e, line in enumerate(f):\n        if e >= 1000000:\n            break\n        if e in WIKI_ARTICLES_INDEX:\n            lines.append(json.loads(line))\n        \ndf = pd.DataFrame.from_dict(lines)\ndf = df[['title', 'content', 'target_ind']]\ndf.head()\n```\n\n\n\n\n\n  <div id=\"df-c158e1ee-532a-41cb-8be0-bc56f09f22e1\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>content</th>\n      <th>target_ind</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Anarchism</td>\n      <td>{{redirect2|anarchist|anarchists|the fictional...</td>\n      <td>[81199, 83757, 83805, 193030, 368811, 368937, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Academy_Awards</td>\n      <td>{{redirect2|oscars|the oscar|the film|the osca...</td>\n      <td>[19080, 65864, 78208, 96051]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Anthropology</td>\n      <td>{{about|the social science}} {{use dmy dates|d...</td>\n      <td>[83605, 423943]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>American_Football_Conference</td>\n      <td>{{refimprove|date=september 2014}} {{use dmy d...</td>\n      <td>[76725, 314198, 334093]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Analysis_of_variance</td>\n      <td>{{use dmy dates|date=june 2013}} '''analysis o...</td>\n      <td>[81170, 168516, 338198, 441529]</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c158e1ee-532a-41cb-8be0-bc56f09f22e1')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n       width=\"24px\">\n    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n  </svg>\n      </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n      <script>\n        const buttonEl =\n          document.querySelector('#df-c158e1ee-532a-41cb-8be0-bc56f09f22e1 button.colab-df-convert');\n        buttonEl.style.display =\n          google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n        async function convertToInteractive(key) {\n          const element = document.querySelector('#df-c158e1ee-532a-41cb-8be0-bc56f09f22e1');\n          const dataTable =\n            await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                     [key], {});\n          if (!dataTable) return;\n\n          const docLinkHtml = 'Like what you see? Visit the ' +\n            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n            + ' to learn more about interactive tables.';\n          element.innerHTML = '';\n          dataTable['output_type'] = 'display_data';\n          await google.colab.output.renderOutput(dataTable, element);\n          const docLink = document.createElement('div');\n          docLink.innerHTML = docLinkHtml;\n          element.appendChild(docLink);\n        }\n      </script>\n    </div>\n  </div>\n\n\n\n\n\n```python\nprint(df.shape)\n```\n\n    (200000, 3)\n    \n\n### Remove Wikipedia Markup Format\nWe are going to use only the first part of the articles to make them comparable in terms of length. Also, Wikipedia articles have a certain format that is not so readable, so we will remove the markup to make the content as clean as possible.\n\n\n```python\n# Reduce content to first 3000 characters\ndf['content_short'] = df.content.apply(lambda x: x[:3000])\n\n# Remove wiki articles markup\ndf['content_cleaned'] = df.content_short.apply(lambda x: remove_markup(x))\n\n# Keep only certain columns\ndf = df[['title', 'content_cleaned', 'target_ind']]\n\n# Show data\ndf.head()\n```\n\n\n\n\n\n  <div id=\"df-5f3fce25-2f7d-40d1-a109-335c1649ab71\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>content_cleaned</th>\n      <th>target_ind</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Anarchism</td>\n      <td>anarchism is a political philosophy that a...</td>\n      <td>[81199, 83757, 83805, 193030, 368811, 368937, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Academy_Awards</td>\n      <td>the academy awards or the oscars (the offi...</td>\n      <td>[19080, 65864, 78208, 96051]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Anthropology</td>\n      <td>anthropology  is the scientific study of hu...</td>\n      <td>[83605, 423943]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>American_Football_Conference</td>\n      <td>the american football conference (afc) is o...</td>\n      <td>[76725, 314198, 334093]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Analysis_of_variance</td>\n      <td>analysis of variance (anova) is a collection ...</td>\n      <td>[81170, 168516, 338198, 441529]</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f3fce25-2f7d-40d1-a109-335c1649ab71')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n       width=\"24px\">\n    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n  </svg>\n      </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n      <script>\n        const buttonEl =\n          document.querySelector('#df-5f3fce25-2f7d-40d1-a109-335c1649ab71 button.colab-df-convert');\n        buttonEl.style.display =\n          google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n        async function convertToInteractive(key) {\n          const element = document.querySelector('#df-5f3fce25-2f7d-40d1-a109-335c1649ab71');\n          const dataTable =\n            await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                     [key], {});\n          if (!dataTable) return;\n\n          const docLinkHtml = 'Like what you see? Visit the ' +\n            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n            + ' to learn more about interactive tables.';\n          element.innerHTML = '';\n          dataTable['output_type'] = 'display_data';\n          await google.colab.output.renderOutput(dataTable, element);\n          const docLink = document.createElement('div');\n          docLink.innerHTML = docLinkHtml;\n          element.appendChild(docLink);\n        }\n      </script>\n    </div>\n  </div>\n\n\n\n\n\n```python\n# Keep all labels in a single list\nall_categories = []\nfor i, row in df.iterrows():\n    all_categories.extend(row.target_ind)\nprint('Number of labels: ',len(list(set(all_categories))))\n```\n\n    Number of labels:  256899\n    \n\n### Create Article Vector Embeddings\n\nRecall, we want to index and search all possible (250,000) *labels*. We do that by averaging, for each label, the corresponding article vector embeddings that contain that label. \n\nLet's first create the article vector embeddings. Here we use the [Average Word Embeddings Models](https://www.sbert.net/docs/pretrained_models.html#average-word-embeddings-models).  In the next section, we will aggregate these vectors to make the final label embeddings. \n\n\n```python\n# Load the model\nmodel = SentenceTransformer('average_word_embeddings_komninos')\n\n# Create embeddings\nencoded_articles = model.encode(df['content_cleaned'], show_progress_bar=True)\ndf['content_vector'] = pd.Series(encoded_articles.tolist())\n```\n\n\n    Downloading:   0%|          | 0.00/690 [00:00<?, ?B/s]\n\n\n\n    Downloading:   0%|          | 0.00/2.13k [00:00<?, ?B/s]\n\n\n\n    Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]\n\n\n\n    Downloading:   0%|          | 0.00/248 [00:00<?, ?B/s]\n\n\n\n    Downloading:   0%|          | 0.00/267M [00:00<?, ?B/s]\n\n\n\n    Downloading:   0%|          | 0.00/2.59M [00:00<?, ?B/s]\n\n\n\n    Downloading:   0%|          | 0.00/164 [00:00<?, ?B/s]\n\n\n\n    Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]\n\n\n\n    Batches:   0%|          | 0/6250 [00:00<?, ?it/s]\n\n\n## Upload articles\n\n\nIt appears that using the article embeddings per se doesn't provide good enough accuracies. Therefore, we chose to index and search the labels directly. \n\nThe label embedding is simply the average of all its corresponding article embeddings. \n\n\n```python\n# Explode the target indicator column\ndf_explode = df.explode('target_ind')\n\n# Group by label and define a unique vector for each label\nresult = df_explode.groupby('target_ind').agg(mean=('content_vector', lambda x: np.vstack(x).mean(axis=0).tolist()))\nresult['target_ind'] = result.index\nresult.columns = ['content_vector', 'ind']\n\nresult.head()\n```\n\n\n\n\n\n  <div id=\"df-eda4ccdc-de8e-4dac-9282-b989392d0727\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content_vector</th>\n      <th>ind</th>\n    </tr>\n    <tr>\n      <th>target_ind</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>[0.0704750344157219, -0.007719345390796661, 0....</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[0.05894148722290993, -0.03119848482310772, 0....</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>[0.18302207440137863, 0.061663837544620036, 0....</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>[0.1543595753610134, 0.03904660418629646, 0.03...</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>[0.22310754656791687, 0.1524289846420288, 0.09...</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eda4ccdc-de8e-4dac-9282-b989392d0727')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n       width=\"24px\">\n    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n  </svg>\n      </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n      <script>\n        const buttonEl =\n          document.querySelector('#df-eda4ccdc-de8e-4dac-9282-b989392d0727 button.colab-df-convert');\n        buttonEl.style.display =\n          google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n        async function convertToInteractive(key) {\n          const element = document.querySelector('#df-eda4ccdc-de8e-4dac-9282-b989392d0727');\n          const dataTable =\n            await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                     [key], {});\n          if (!dataTable) return;\n\n          const docLinkHtml = 'Like what you see? Visit the ' +\n            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n            + ' to learn more about interactive tables.';\n          element.innerHTML = '';\n          dataTable['output_type'] = 'display_data';\n          await google.colab.output.renderOutput(dataTable, element);\n          const docLink = document.createElement('div');\n          docLink.innerHTML = docLinkHtml;\n          element.appendChild(docLink);\n        }\n      </script>\n    </div>\n  </div>\n\n\n\n\n\n```python\n# Create a list of items to upsert\nitems_to_upsert = [(unidecode(categories[int(row.ind)])[:64], row.content_vector) for i, row in result.iterrows()]\n```\n\n\n```python\nimport itertools\n\ndef chunks(iterable, batch_size=100):\n    it = iter(iterable)\n    chunk = tuple(itertools.islice(it, batch_size))\n    while chunk:\n        yield chunk\n        chunk = tuple(itertools.islice(it, batch_size))\n```\n\n\n```python\n# Upsert data\nfor batch in chunks(items_to_upsert, 250):\n    index.upsert(vectors=batch)\n```\n\nLet's validate the number of indexed labels.\n\n\n```python\nindex.describe_index_stats()\n```\n\n\n\n\n    {'dimension': 300, 'namespaces': {'': {'vector_count': 256899}}}\n\n\n\n## Query \n\nNow, let's test the vector index and examine the classifier results. Observe that here we retrieve a fixed number of labels. Naturally, in an actual application, you might want to calculate the size of the retrieved label set dynamically. \n\n\n```python\nNUM_OF_WIKI_ARTICLES = 3\nWIKI_ARTICLES_INDEX = range(1111, 100000, 57)[:NUM_OF_WIKI_ARTICLES]\n\nlines = []\n\nwith gzip.open(TEST_DATA_PATH) as f:\n    for e, line in enumerate(f):\n        if e in  WIKI_ARTICLES_INDEX:\n            lines.append(json.loads(line)) \n        if e > max(WIKI_ARTICLES_INDEX):\n            break\n            \ndf_test = pd.DataFrame.from_dict(lines)\ndf_test = df_test[['title', 'content', 'target_ind']]\ndf_test.head()\n```\n\n\n\n\n\n  <div id=\"df-05a3b113-37f2-403c-b0e2-f5c4dd0dac99\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>content</th>\n      <th>target_ind</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Discrimination</td>\n      <td>{{otheruses}} {{discrimination sidebar}} '''di...</td>\n      <td>[170479, 423902]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Erfurt</td>\n      <td>{{refimprove|date=june 2014}} {{use dmy dates|...</td>\n      <td>[142638, 187156, 219262, 294479, 329185, 38243...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ETA</td>\n      <td>{{about|the basque organization|other uses|eta...</td>\n      <td>[83681, 100838, 100849, 100868, 176034, 188979...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-05a3b113-37f2-403c-b0e2-f5c4dd0dac99')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n       width=\"24px\">\n    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n  </svg>\n      </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n      <script>\n        const buttonEl =\n          document.querySelector('#df-05a3b113-37f2-403c-b0e2-f5c4dd0dac99 button.colab-df-convert');\n        buttonEl.style.display =\n          google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n        async function convertToInteractive(key) {\n          const element = document.querySelector('#df-05a3b113-37f2-403c-b0e2-f5c4dd0dac99');\n          const dataTable =\n            await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                     [key], {});\n          if (!dataTable) return;\n\n          const docLinkHtml = 'Like what you see? Visit the ' +\n            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n            + ' to learn more about interactive tables.';\n          element.innerHTML = '';\n          dataTable['output_type'] = 'display_data';\n          await google.colab.output.renderOutput(dataTable, element);\n          const docLink = document.createElement('div');\n          docLink.innerHTML = docLinkHtml;\n          element.appendChild(docLink);\n        }\n      </script>\n    </div>\n  </div>\n\n\n\n\n\n```python\n# Reduce content to first 3000 characters\ndf_test['content_short'] = df_test.content.apply(lambda x: x[:3000])\n\n# Remove wiki articles markup\ndf_test['content_cleaned'] = df_test.content_short.apply(lambda x: remove_markup(x))\n\n# Keep only certain columns\ndf_test = df_test[['title', 'content_cleaned', 'target_ind']]\n\n# Show data\ndf_test.head()\n```\n\n\n\n\n\n  <div id=\"df-bf29f1a6-0986-4c74-bb33-d82361095999\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>content_cleaned</th>\n      <th>target_ind</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Discrimination</td>\n      <td>discrimination is action that denies social ...</td>\n      <td>[170479, 423902]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Erfurt</td>\n      <td>erfurt () is the capital city of thuringia ...</td>\n      <td>[142638, 187156, 219262, 294479, 329185, 38243...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ETA</td>\n      <td>eta (, ), an acronym for euskadi ta askatas...</td>\n      <td>[83681, 100838, 100849, 100868, 176034, 188979...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf29f1a6-0986-4c74-bb33-d82361095999')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n       width=\"24px\">\n    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n  </svg>\n      </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n      <script>\n        const buttonEl =\n          document.querySelector('#df-bf29f1a6-0986-4c74-bb33-d82361095999 button.colab-df-convert');\n        buttonEl.style.display =\n          google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n        async function convertToInteractive(key) {\n          const element = document.querySelector('#df-bf29f1a6-0986-4c74-bb33-d82361095999');\n          const dataTable =\n            await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                     [key], {});\n          if (!dataTable) return;\n\n          const docLinkHtml = 'Like what you see? Visit the ' +\n            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n            + ' to learn more about interactive tables.';\n          element.innerHTML = '';\n          dataTable['output_type'] = 'display_data';\n          await google.colab.output.renderOutput(dataTable, element);\n          const docLink = document.createElement('div');\n          docLink.innerHTML = docLinkHtml;\n          element.appendChild(docLink);\n        }\n      </script>\n    </div>\n  </div>\n\n\n\n\n\n```python\n# Create embeddings for test articles\ntest_vectors = model.encode(df_test['content_cleaned'], show_progress_bar=True).tolist()\n```\n\n\n    Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n\n\n\n```python\n# Query the vector index\nquery_results = []\n\nfor xq in test_vectors:\n    query_res = index.query(xq, top_k=10)\n    query_results.append(query_res)\n```\n\n\n```python\n# Show results\nfor term, labs, res in zip(df_test.title.tolist(), df_test.target_ind.tolist(), query_results):\n    print()\n    print('Term queried: ',term)\n    print('Original labels: ')\n    for l in labs:\n        if l in all_categories:\n            print('\\t', categories[l])\n    print('Predicted: ')\n    df_result = pd.DataFrame({\n                'id': [res.id for res in res.matches],\n                'score': [res.score for res in res.matches],})\n    display(df_result)\n```\n\n    \n    Term queried:  Discrimination\n    Original labels: \n    \t Discrimination\n    \t Social_justice\n    Predicted: \n    \n\n\n\n  <div id=\"df-98bd35a3-d003-446e-beec-909fd26b91ac\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Discrimination</td>\n      <td>0.972958</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Sociological_terminology</td>\n      <td>0.971606</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Identity_politics</td>\n      <td>0.970097</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Social_concepts</td>\n      <td>0.967534</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sexism</td>\n      <td>0.967476</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Affirmative_action</td>\n      <td>0.967288</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Political_correctness</td>\n      <td>0.966926</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Human_behavior</td>\n      <td>0.966475</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Persecution</td>\n      <td>0.965421</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Social_movements</td>\n      <td>0.964394</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-98bd35a3-d003-446e-beec-909fd26b91ac')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n       width=\"24px\">\n    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n  </svg>\n      </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n      <script>\n        const buttonEl =\n          document.querySelector('#df-98bd35a3-d003-446e-beec-909fd26b91ac button.colab-df-convert');\n        buttonEl.style.display =\n          google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n        async function convertToInteractive(key) {\n          const element = document.querySelector('#df-98bd35a3-d003-446e-beec-909fd26b91ac');\n          const dataTable =\n            await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                     [key], {});\n          if (!dataTable) return;\n\n          const docLinkHtml = 'Like what you see? Visit the ' +\n            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n            + ' to learn more about interactive tables.';\n          element.innerHTML = '';\n          dataTable['output_type'] = 'display_data';\n          await google.colab.output.renderOutput(dataTable, element);\n          const docLink = document.createElement('div');\n          docLink.innerHTML = docLinkHtml;\n          element.appendChild(docLink);\n        }\n      </script>\n    </div>\n  </div>\n\n\n\n    \n    Term queried:  Erfurt\n    Original labels: \n    \t Erfurt\n    \t German_state_capitals\n    \t Members_of_the_Hanseatic_League\n    \t Oil_Campaign_of_World_War_II\n    \t Province_of_Saxony\n    \t University_towns_in_Germany\n    Predicted: \n    \n\n\n\n  <div id=\"df-6849b581-67be-451f-87c2-c297429607dc\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>University_towns_in_Germany</td>\n      <td>0.966058</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Province_of_Saxony</td>\n      <td>0.959731</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Populated_places_on_the_Rhine</td>\n      <td>0.958737</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Imperial_free_cities</td>\n      <td>0.957159</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Hildesheim_(district)</td>\n      <td>0.956927</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>History_of_the_Electoral_Palatinate</td>\n      <td>0.956800</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Towns_in_Saxony-Anhalt</td>\n      <td>0.956501</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Towns_in_Lower_Saxony</td>\n      <td>0.955259</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Halle_(Saale)</td>\n      <td>0.954934</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Cities_in_Saxony-Anhalt</td>\n      <td>0.954934</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6849b581-67be-451f-87c2-c297429607dc')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n       width=\"24px\">\n    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n  </svg>\n      </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n      <script>\n        const buttonEl =\n          document.querySelector('#df-6849b581-67be-451f-87c2-c297429607dc button.colab-df-convert');\n        buttonEl.style.display =\n          google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n        async function convertToInteractive(key) {\n          const element = document.querySelector('#df-6849b581-67be-451f-87c2-c297429607dc');\n          const dataTable =\n            await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                     [key], {});\n          if (!dataTable) return;\n\n          const docLinkHtml = 'Like what you see? Visit the ' +\n            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n            + ' to learn more about interactive tables.';\n          element.innerHTML = '';\n          dataTable['output_type'] = 'display_data';\n          await google.colab.output.renderOutput(dataTable, element);\n          const docLink = document.createElement('div');\n          docLink.innerHTML = docLinkHtml;\n          element.appendChild(docLink);\n        }\n      </script>\n    </div>\n  </div>\n\n\n\n    \n    Term queried:  ETA\n    Original labels: \n    \t Anti-Francoism\n    \t Basque_conflict\n    \t Basque_history\n    \t Basque_politics\n    \t ETA\n    \t European_Union_designated_terrorist_organizations\n    \t Far-left_politics\n    \t Francoist_Spain\n    \t Government_of_Canada_designated_terrorist_organizations\n    \t Irregular_military\n    \t Military_wings_of_political_parties\n    \t National_liberation_movements\n    \t Nationalist_terrorism\n    \t Organizations_designated_as_terrorist_by_the_United_States_government\n    \t Organizations_designated_as_terrorist_in_Europe\n    \t Organizations_established_in_1959\n    \t Politics_of_Spain\n    \t Resistance_movements\n    \t Secession_in_Spain\n    \t Secessionist_organizations_in_Europe\n    \t Terrorism_in_Spain\n    \t United_Kingdom_Home_Office_designated_terrorist_groups\n    Predicted: \n    \n\n\n\n  <div id=\"df-02b4760b-a840-4c3d-b778-64b3de6f35bf\">\n    <div class=\"colab-df-container\">\n      <div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Organizations_designated_as_terrorist_in_Europe</td>\n      <td>0.948875</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Terrorism_in_Spain</td>\n      <td>0.948431</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Basque_politics</td>\n      <td>0.942670</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Politics_of_Spain</td>\n      <td>0.941830</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>European_Union_designated_terrorist_organizations</td>\n      <td>0.940194</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Irregular_military</td>\n      <td>0.938163</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Political_parties_disestablished_in_1977</td>\n      <td>0.936437</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Algerian_Civil_War</td>\n      <td>0.936311</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Republicanism_in_Spain</td>\n      <td>0.935577</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Guerrilla_organizations</td>\n      <td>0.935506</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-02b4760b-a840-4c3d-b778-64b3de6f35bf')\"\n              title=\"Convert this dataframe to an interactive table.\"\n              style=\"display:none;\">\n\n  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n       width=\"24px\">\n    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n  </svg>\n      </button>\n\n  <style>\n    .colab-df-container {\n      display:flex;\n      flex-wrap:wrap;\n      gap: 12px;\n    }\n\n    .colab-df-convert {\n      background-color: #E8F0FE;\n      border: none;\n      border-radius: 50%;\n      cursor: pointer;\n      display: none;\n      fill: #1967D2;\n      height: 32px;\n      padding: 0 0 0 0;\n      width: 32px;\n    }\n\n    .colab-df-convert:hover {\n      background-color: #E2EBFA;\n      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n      fill: #174EA6;\n    }\n\n    [theme=dark] .colab-df-convert {\n      background-color: #3B4455;\n      fill: #D2E3FC;\n    }\n\n    [theme=dark] .colab-df-convert:hover {\n      background-color: #434B5C;\n      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n      fill: #FFFFFF;\n    }\n  </style>\n\n      <script>\n        const buttonEl =\n          document.querySelector('#df-02b4760b-a840-4c3d-b778-64b3de6f35bf button.colab-df-convert');\n        buttonEl.style.display =\n          google.colab.kernel.accessAllowed ? 'block' : 'none';\n\n        async function convertToInteractive(key) {\n          const element = document.querySelector('#df-02b4760b-a840-4c3d-b778-64b3de6f35bf');\n          const dataTable =\n            await google.colab.kernel.invokeFunction('convertToInteractive',\n                                                     [key], {});\n          if (!dataTable) return;\n\n          const docLinkHtml = 'Like what you see? Visit the ' +\n            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n            + ' to learn more about interactive tables.';\n          element.innerHTML = '';\n          dataTable['output_type'] = 'display_data';\n          await google.colab.output.renderOutput(dataTable, element);\n          const docLink = document.createElement('div');\n          docLink.innerHTML = docLinkHtml;\n          element.appendChild(docLink);\n        }\n      </script>\n    </div>\n  </div>\n\n\n\n## Summary\nWe demonstrated a similarity search approach for performing extreme classification of texts. We took a simple approach representing labels as the average of their corresponding texts' vector embeddings. In classification time, we match between a new article embedding and its nearest label embeddings. Our result examples indicate the usefulness of this approach. \n\nYou can take this forward by exploring advanced ideas. For example, you can utilize the hierarchical relationship between labels or improve the label representations.  Just have fun, and feel free to [share](https://www.pinecone.io/contact/) your thoughts. \n\n## Delete the index\n\nDelete the index once you do not want to use it anymore. Once the index is deleted, you cannot use it again.\n\n\n```python\npinecone.delete_index(index_name)\n```\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1cced"
  },
  "filename": "organizations.md",
  "title": "Organizations",
  "category": "630fc5235d91a70054705fb8",
  "content": "---\ntitle: Organizations\ncategory: 630fc5235d91a70054705fb8\n---\n\n## Overview\n\nA Pinecone organization is a set of [projects](projects) that use the same billing. Organizations allow one or more users to control billing and project permissions for all of the projects belonging to the organization. Each project belongs to an organization. \n\nFor a guide to adding users to an organization, see [Add users to a project or organization](add-users-to-projects-and-organizations/).\n\n\n## Projects in an organization\n\nEach organization contains one or more projects that share the same organization owners and billing settings. Each project belongs to exactly one organization. If you need to move a project from one organization to another, contact [Pinecone support](https://support.pinecone.io). \n\n## Billing settings\n\nAll of the projects in an organization share the same billing method and settings. The billing settings for the organization are controlled by the organization owners.\n\n## Organization roles\n\nThere are two organization roles: organization owner and organization user.\n\n### Organization owners\n\nOrganization owners manage organization billing, users, and projects. Organization owners are also [project owners](projects#project-roles) for every project belonging to the organization. This means that organization owners have all permissions to manage project members, API keys, and quotas for these projects.\n\n### Organization users\n\nUnlike organization owners, organization users cannot edit billing settings or invite new users to the organization. Organization users can create new projects, and project owners can add organization members to a project. New users have whatever role the organization owners and project owners grant them. Project owners can add users to a project if those users belong to the same organization as the project.\n\n**Table 1: Organization roles and permissions**\n\n| Organization role   | Permissions in organization    |\n| ------------------- | ------------------------------ |\n| Organization owner  | Project owner for all projects |\n|                     | Create projects                |\n|                     | Manage billing                 |\n|                     | Manags organization members    |\n| Organization member | Create projects                |\n|                     | Join projects when invited     |\n|                     | Read access to billing         |\n\n## Next steps\n\n* [Add users to an organization](add-users-to-projects-and-organizations/)\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccee"
  },
  "filename": "collections.md",
  "title": "Collections",
  "category": "630fc5235d91a70054705fb8",
  "content": "---\ntitle: Collections\ncategory: 630fc5235d91a70054705fb8\n---\n\n## Overview\n\nThis document explains the concepts related to collections in Pinecone.\n\n> ⚠️  Warning\n>\n> This is a **public preview** (\"Beta\") feature. Test thoroughly before\n> using this feature for production workloads. No SLAs or technical support\n> commitments are provided for this feature.\n\n**A collection is a static copy of an index.** It is a non-queryable representation of a set of vectors and metadata. You can create a collection from an index, and you can create a new index from a collection. This new index can differ from the original source index: the new index can have a different number of pods, a different pod type, or a different similarity metric.\n\n## Use cases for collections\n\nCreating a collection from your index is useful when performing tasks like the following:\n\n+ Temporarily shutting down an index\n+ Copying the data from one index into a different index;\n+ Making a backup of your index\n+ Experimenting with different index configurations\n\nTo learn about creating backups with collections, see [Back up indexes](back-up-indexes/#create-a-backup-using-a-collection).\n\nTo learn about creating indexes from collections, see [Manage indexes](manage-indexes/#create-an-index-from-a-collection).\n\n\n## Public collections contain real world data\n\nPublic collections contain vectorized data from real-world datasets that you can use to [create\nindexes](manage-indexes/#create-an-index-from-a-public-collection). You can use these indexes to try out Pinecone with realistic example data and queries. \n\nPinecone offers public collections containing data from the following datasets:\n\n+ [OpenAI TREC](https://huggingface.co/datasets/trec)\n+ [Cohere TREC](https://huggingface.co/datasets/trec)\n+ [SQuAD](https://huggingface.co/datasets/squad)\n\n## Performance \n\nCollections operations perform differently with different pod types.\n\n+ Creating a collection from an index takes approximately 10 minutes. \n+ Creating a p1 or s1 index from a collection takes approximately 10 minutes.\n+ Creating a p2 index from a collection can take several hours.\n\n## Limitations\n\nYou cannot query or write to a collection after its creation. For this reason, a collection only incurs storage costs.\n\nYou can only perform operations on collections in the current Pinecone project.\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccef"
  },
  "filename": "indexes.md",
  "title": "Indexes",
  "category": "630fc5235d91a70054705fb8",
  "content": "---\ntitle: Indexes\ncategory: 630fc5235d91a70054705fb8\n---\n\n## Overview\n\nThis document describes concepts related to Pinecone indexes. To learn how to create or modify an index, see [Manage indexes](manage-indexes).\n\nAn index is the highest-level organizational unit of vector data in Pinecone. It accepts and stores vectors, serves queries over the vectors it contains, and does other vector operations over its contents. Each index runs on at least one **pod**. \n\n## Pods, pod types, and pod sizes\n\nPods are pre-configured units of hardware for running a Pinecone service. Each index runs on one or more pods. Generally, more pods mean more storage capacity, lower latency, and higher throughput. You can also create pods of different sizes.\n\nOnce an index is created using a particular pod type, you cannot change the pod type for that index. However, you can [create a new index from that collection](manage-indexes/#create-an-index-from-a-collection) with a different pod type.\n\nUsers on the Starter (free) plan are limited to 1 p1 pod or 1 s1 pod.\n\nDifferent pod types are priced differently. See [pricing](https://www.pinecone.io/pricing/) for more details.\n\n### s1 pods\n\nThese storage-optimized pods provide large storage capacity and lower overall costs with slightly higher query latencies than p1 pods. They are ideal for very large indexes with moderate or relaxed latency requirements.\n\nEach s1 pod has enough capacity for around 5M vectors of 768 dimensions.\n\n### p1 pods\n\nThese performance-optimized pods provide very low query latencies, but hold fewer vectors per pod than s1 pods. They are ideal for applications with low latency requirements (<100ms).\n\nEach p1 pod has enough capacity for around 1M vectors of 768 dimensions.\n\n### p2 pods\n\nThe p2 pod type provides greater query throughput with lower latency. For vectors with fewer than 128 dimension and queries where `topK` is less than 50, p2 pods support up to 200 QPS per replica and return queries in less than 10ms. This means that query throughput and latency are better than s1 and p1.\n\nEach p2 pod has enough capacity for around 1M vectors of 768 dimensions. However, capacity may vary with dimensionality.\n\nThe data ingestion rate for p2 pods is significantly slower than for p1 pods; this rate decreases as the number of dimensions increases. For example, a p2 pod containing vectors with 128 dimensions can upsert up to 300 updates per second; a p2 pod containing vectors with 768 dimensions or more supports  upsert of 50 updates per second. Because query latency and throughput for p2 pods vary from p1 pods, test p2 pod performance with your dataset.\n\n### Pod size and performance\n\nPod performance varies depending on a variety of factors. To observe how your workloads perform on a given pod type, experiment with your own data set.\n\nEach pod type supports four pod sizes: `x1`, `x2`, `x4`, and `x8`. Your index storage and compute capacity doubles for each size step. The default pod size is `x1`. You can increase the size of a pod after index creation.\n\nTo learn about changing the pod size of an index, see [Manage indexes](manage-indexes/#changing-pod-sizes).\n\n### Distance metrics\n\nYou can choose from different metrics when creating a vector index:\n\n- `euclidean`\n  - This is used to calculate the distance between two data points in a plane. It is one of the most commonly used distance metric. For an example, see our [image similarity search example](image-similarity-search/).\n  - When you use `metric='euclidean'`, the most similar results are those with the **lowest score**.\n- `cosine`\n  - This is often used to find similarities between different documents. The advantage is that the scores are normalized to [-1,1] range.\n- `dotproduct`\n  - This is used to multiply two vectors. You can use it to tell us how similar the two vectors are. The more positive the answer is, the closer the two vectors are in terms of their directions.\n\nFor the full list of parameters available to customize an index, see the [create_index API reference](reference/create_index/).\n\nDepending on your application, some metrics have better recall and precision performance than others. For more information, see: [What is Vector Similarity Search?](https://www.pinecone.io/learn/what-is-similarity-search/)\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccf0"
  },
  "filename": "projects.md",
  "title": "Projects",
  "category": "630fc5235d91a70054705fb8",
  "content": "---\ntitle: Projects\ncategory: 630fc5235d91a70054705fb8\n---\n\n## Overview\n\nThis document explains the concepts related to Pinecone projects.\n\n## Projects contain indexes and users\n\nEach Pinecone project contains a number of [indexes](/indexes) and users. Only a user who belongs to the project can access the indexes in that project. Each project also has at least one project owner. All of the pods in a single project are located in a single environment. \n\n\n## Project settings\n\nWhen you create a new project, you can choose the **name**, **deployment environment**, and **pod limit**.\n\n### Project environment\n\nGCP US-East is the default environment on default projects, and the only available environment for new users on the Starter (free) plan.\n\nUsers on the Standard and Enterprise plans can choose from GCP US-West, GCP US-East, GCP EU-West, or AWS US-East.\n\nThese regions correspond to the following values of the `environment` parameter for the [init() operation](concepts):\n\n| Cloud region                   | `environment` value |\n| -------------------------------| ------------------- |\n| GCP US-West-1 (N. California)  | us-west1-gcp        |\n| GCP US-East-1 (South Carolina) | us-east1-gcp        |\n| GCP EU-West-1 (Ireland)        | eu-west1-gcp        |\n| AWS US-East-1 (Virginia)       | us-east1-aws        |\n\n [Contact us](http://www.pinecone.io/contact/) if you need a dedicated deployment in other regions.\n\nThe environment cannot be changed after the project is created.\n\n### Project pod limit\n\nYou can set the maximum number of pods that can be used in total across all indexes in a project. Use this to control costs.\n\nThe pod limit can be changed only by the project owner.\n\n### Project roles\n\nThere are two project roles: **Project owner** and **project member.** Table 1 below summarizes the permissions for each role.\n\n**Table 1: Project roles and permissions**\n\n| Project role        | Permissions in organization    |\n| ------------------- | ------------------------------ |\n| Project owner       | Manage project members         |\n|                     | Manage project API keys        |\n|                     | Manage pod limits              |\n| Project member      | Access API keys                |\n|                     | Create indexes in project      |\n|                     | Use indexes in project         |\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccf1"
  },
  "filename": "hybrid-search.md",
  "title": "Hybrid Search",
  "category": "630fc5235d91a70054705fb8",
  "content": "---\ntitle: Hybrid Search\ncategory: 630fc5235d91a70054705fb8\n---\n\n> ⚠️  Warning\n> This is an early access feature, which is available on an invitation basis and is not intended for production workloads. No SLAs or technical support commitments are provided. [Sign up](https://www.pinecone.io/hybrid-search-early-access/) for early access.\n\n## Overview\n\nPinecone supports **hybrid search**, which allows you to perform semantic and keyword search over your data in one query and combine the results for more relevant results. This topic describes what hybrid search does, why it is useful, and how it works in Pinecone.\n\n## Pinecone hybrid search allows keyword-aware semantic search\n\nPinecone hybrid search allows you to perform keyword-aware semantic search. Semantic\nsearch results for out-of-domain queries can be less relevant; [combining these\nwith keyword search results can improve relevance](https://arxiv.org/abs/2210.11934).\n\nBecause Pinecone allows you to create your own sparse vectors, you can use hybrid search to solve the Maximum Inner Product Search (MIPS) problem for hybrid vectors of any real values. This includes emerging use-cases such as retrieval over learnt sparse representations for text data using [SPLADE](https://arxiv.org/abs/2107.05720).\n\n## Hybrid search workflow\n\nHybrid search involves the following general steps:\n\n1. Create dense vectors using an external embedding model.\n1. Create sparse vectors using an external tokenizer.\n1. [Create a hybrid index](manage-indexes/#creating-an-index).\n1. Upsert dense and sparse vectors to the hybrid upsert endpoint.\n1. Search the hybrid index using the hybrid query endpoint.\n1. Pinecone returns ranked hybrid vectors.\n\nFigure 1 below illustrates these steps.\n\n**Figure 1: Hybrid search workflow**\n\n![Hybrid search workflow](https://raw.githubusercontent.com/pinecone-io/img/main/hybrid-search-architecture.png) \n\n## Sparse versus dense vectors in Pinecone \n\nHybrid search combines [dense and sparse vectors](https://www.pinecone.io/learn/dense-vector-embeddings-nlp/#dense-vs-sparse-vectors); these types of vectors represent different types of information and enable distinct kinds of search. [Dense vectors](https://www.pinecone.io/learn/dense-vector-embeddings-nlp/) enable semantic search. Semantic search returns the most similar results according to a specific distance metric even if no exact matches are present. This is possible because dense vectors generated by embedding models such as [SBERT](https://huggingface.co/sentence-transformers) are numerical representations of semantic meaning.\n\nSparse vectors have very large number of dimensions, generally over 1M, where only a small proportion of values are non-zero. When used for keywords search, each sparse vector represents a document; the dimensions represent words from a dictionary, and the values represent the frequency of these words in the document. Keyword search algorithms like the [BM25](https://en.wikipedia.org/wiki/Okapi_BM25) algorithm compute the relevance of text documents based on the number of keyword matches, their frequency, and other factors.\n\n## Creating sparse vectors for use in hybrid search\n\nKeyword-aware semantic search requires vector representations of documents. Because Pinecone hybrid indexes accept sparse indexes rather than documents, you can control the generation of sparse vectors to represent documents. You can choose a tokenizer or analyzer, such as [Hugging Face](https://huggingface.co/docs/tokenizers/index), [spaCy](https://spacy.io/api/tok2vec), [Lucene]() to convert documents into sparse vectors, such as term-frequency vectors. The result is a dictionary that maps token IDs to term frequencies.\n\n**Example**\n\n```\nsparse_vector = dict(Counter(tokenizer.encode(doc)))  # {5:1, 10500:1, 7:1, ... }\n```\n\n## Pinecone creates hybrid vectors from your sparse and dense vectors\n\nHybrid vectors combine dense and sparse vectors. In Pinecone, each hybrid vector consists of a sparse vector and a dense vector. Your hybrid index accepts vector upserts and queries containing both dense and sparse vector parameters and combines these into hybrid vectors.\n\nWhen you upsert a hybrid vector using the hybrid upsert endpoint, your index normalizes the sparse vector for BM25 ranking and stores the normalized version. If you upsert hybrid vectors using the standard `upsert` operation, your index stores them without normalization. \n\n## Hybrid indexes store hybrid vectors and keyword search parameters\n\nPinecone stores hybrid vectors in hybrid indexes. A hybrid index has all of the features of a default dense vector index as well as a set of parameters for BM25 ranking. Your hybrid index uses these parameters to perform BM25 ranking of keyword search results and combines these keyword results with semantic search results to produce hybrid results. \n\nHybrid indexes use the `s1h` [pod type](https://www.pinecone.io/docs/indexes/#pods-pod-types-and-pod-sizes).\n\n## Hybrid queries include sparse and dense vectors with weighting parameter\n\nTo query your hybrid index, you provide a hybrid query vector and a weight parameter `alpha` that determines the relative weight of similarity and keyword relevance in hybrid query results. Your index performs both a semantic or similarity search and a keyword search; then, your index ranks the vectors in your index based on a combination of similarity and keyword matching and returns the most relevant results. Hybrid query results contain both dense and sparse vector values.\n\nIf you query your hybrid index using the hybrid query endpoint, your hybrid index denormalizes the sparse component of the hybrid result vectors before returning them in query results, so that they match the upserted sparse vectors.\n\n## Sparse vector search returns BM25 ranked results\n\nIf you query the hybrid index through the hybrid query endpoint, the sparse rankings are similar to those produced by the BM25 algorithm. \n\nIf you query the hybrid index directly, then the hybrid index returns the sparse vectors with the highest dot product across the sparse component of the hybrid query vector.\n\n## Hybrid queries specify weight of dense and sparse rankings\n\nWhen you query a hybrid index, you provide both dense and sparse vectors, which the hybrid query endpoint combines to create a hybrid query vector. Your index performs similarity search using the dense component of the hybrid vector and keyword search using the sparse component. The hybrid query endpoint normalizes the sparse vector component before searching. Your hybrid query also contains a parameter called `alpha` that determines the relative weight of the relevance rankings from the dense vector searches. You can adjust `alpha` to adjust the relative weight of semantic and keyword search rankings.\n\nThe equation in Figure 1 below expresses how alpha affects the relative weighting of lexical or keyword ranking and semantic ranking in hybrid query results.\n\n**Equation 1: Linear combination with weighting parameter `alpha`**\n\n<sub>hybrid</sub>(q,d)=(1-&alpha;)f<sub>lexical</sub>(q,d) + &alpha;f<sub>semantic</sub>(q,d)\n\nValues for `alpha` between `.7` and `.9` result in the best performance for in-domain models. When using a model that is not trained for the corpus, or is out-of-domain, downweight the semantic score with lower values of `alpha` in the range 0.3-0.6. When the model is fine-tuned or in-domain, use values closer to 1. \n\nFigure 2 below shows the relationship between the value of `alpha` and the NDCG relevance metric.\n\n![Relevance by alpha value for in- and out-of-domain models](https://raw.githubusercontent.com/pinecone-io/img/main/alpha-relevance.png)\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccf2"
  },
  "filename": "release-notes.md",
  "title": "Release notes",
  "category": "630fc5235d91a70054705fb9",
  "content": "---\ntitle: Release notes\ncategory: 630fc5235d91a70054705fb9\n---\n\nThis document contains details about Pinecone releases. For information about using specific features, see our [API reference](/reference/describe_index_stats/).\n\n## January 3, 2023\n\n#### Pinecone Python client version 2.1.0 is now available on GitHub.\n\nThe [latest release of the Python client](https://github.com/pinecone-io/pinecone-python-client/releases/tag/2.1.0) makes the following changes:\n\n* Fixes \"Connection Reset by peer\" error after long idle periods\n* Adds typing and explicit names for arguments in all client operations\n* Adds docstrings to all client operations\n* Adds Support for batch upserts by passing `batch_size` to the upsert method\n* Improves gRPC query results parsing performance\n\n## December 22, 2022\n\n#### Pinecone is now available in GCP Marketplace\n\nYou can now [sign up for Pinecone billing through Google Cloud Platform Marketplace](setting-up-gcp-marketplace-billing).\n\n\n## December 6, 2022\n\n#### Organizations are generally available\n\nPinecone now features [organizations](/organizations), which allow one or more users to control billing and project settings across multiple projects owned by the same organization.\n\n#### p2 pod type is generally available\n\nThe [p2 pod type](indexes#p2-pods) is now generally available and ready for production workloads. p2 pods are now available in the Starter plan and support the [`dotproduct` distance metric](indexes#distance-metrics).\n\n#### Performance improvements\n\n* [Bulk vector_deletes](https://www.pinecone.io/docs/manage-data/#deleting-vectors) are now up to 10x faster in many circumstances.\n\n* [Creating collections](https://www.pinecone.io/docs/back-up-indexes/) is now faster.\n\n\n## October 31, 2022\n\n#### Hybrid search (Early access)\n\nPinecone now supports keyword-aware semantic search with the new [hybrid search](hybrid-search) indexes and endpoints. Hybrid search enables improved relevance for semantic search results by combining them with keyword search.\n\nThis is an **early access** feature and is available only by [signing up](https://www.pinecone.io/hybrid-search-early-access/).\n\n## October 17, 2022\n\n#### Status page\n\nThe new [Pinecone Status Page](https://status.pinecone.io/) displays information about the status of the Pinecone service, including the status of individual cloud regions and a log of recent incidents.\n\n## September 16, 2022\n\n#### Public collections \n\nYou can now [create indexes from public collections](/collections#public-collections), which are collections containing public data from real-world data sources. Currently, public collections include the Glue - SSTB collection, the TREC Question classification collection, and the SQuAD collection.\n\n## August 16, 2022\n\n#### Collections (Public Preview)(\"Beta\")\n\nYou can now [make static copies of your index](back-up-indexes) using [collections](collections). After you create a collection from an index, you can create a new index from that collection. The new index can use any pod type and any number of pods. Collections only consume storage.\n\nThis is a **public preview** feature and is not appropriate for production workloads.\n\n#### Vertical scaling\n\nYou can now [change the size of the pods](manage-indexes/#changing-pod-sizes) for a live index to accommodate more vectors or queries without interrupting reads or writes. The p1 and s1 pod types are now available in [4 different sizes](indexes/#pods-pod-types-and-pod-sizes): `1x`, `2x`, `4x`, and `8x`. Capacity and compute per pod double with each size increment.\n\n#### p2 pod type (Public Preview)(\"Beta\")\n\nThe new [p2 pod type](indexes/#p2-pods) provides search speeds of around 5ms and throughput of 200 queries per second per replica, or approximately 10x faster speeds and higher throughput than the p1 pod type, depending on your data and network conditions. \n\nThis is a **public preview** feature and is not appropriate for production workloads.\n\n#### Improved p1 and s1 performance\n\nThe [s1](indexes/#s1-pods) and [p1](indexes/#p1-pods) pod types now offer approximately 50% higher query throughput and 50% lower latency, depending on your workload.\n\n## July 26, 2022\n\nYou can now specify a [metadata filter](metadata-filtering/) to get results for a subset of the vectors in your index by calling [`describe_index_stats`](https://www.pinecone.io/docs/api/operation/describe_index_stats/) with a [`filter`](/reference/describe_index_stats/#!path=filter&t=request) object.\n\nThe `describe_index_stats` operation now uses the `POST` HTTP request type. The `filter` parameter is only accepted by `describe_index_stats` calls using the `POST` request type. Calls to `describe_index_stats` using the [`GET` request type](/reference/describe_index_stats1/) are now deprecated. \n\n## July 12, 2022\n\n#### Pinecone Console Guided Tour\n\nYou can now choose to follow a guided tour in the [Pinecone Console](https://app.pinecone.io). This interactive tutorial walks you through creating your first index, upserting vectors, and querying your data. The purpose of the tour is to show you all the steps you need to start your first project in Pinecone.\n\n## June 24, 2022\n\n#### Updated response codes\n\nThe [`create_index`](/reference/create_index/), [`delete_index`](/reference/delete_index/), and [`scale_index`](/reference/scale_index/) operations now use more specific HTTP response codes that describe the type of operation that succeeded.\n\n## June 7, 2022\n\n#### Selective metadata indexing\n\nYou can now store more metadata and more unique metadata values! [Select which metadata fields you want to index for filtering](manage-indexes/#selective-metadata-indexing) and which fields you only wish to store and retrieve. When you index metadata fields, you can filter vector search queries using those fields. When you store metadata fields without indexing them, you keep memory utilization low, especially when you have many unique metadata values, and therefore can fit more vectors per pod.\n\n#### Single-vector queries\n\nYou can now [specify a single query vector using the `vector` input](https://www.pinecone.io/docs/api/operation/query/#!path=vector&t=request). We now encourage all users to query using a single vector rather than a batch of vectors, because batching queries can lead to long response messages and query times, and single queries execute just as fast on the server side.\n\n#### Query by ID\n\nYou can now [query your Pinecone index using only the ID for another vector](https://www.pinecone.io/docs/api/operation/query/#!path=id&t=request). This is useful when you want to search for the nearest neighbors of a vector that is already stored in Pinecone. \n\n#### Improved index fullness accuracy\n\nThe index fullness metric in [`describe_index_stats()`](https://www.pinecone.io/docs/api/operation/describe_index_stats/#!c=200&path=indexFullness&t=response) results is now more accurate.\n\n## April 25, 2022\n\n#### Partial updates (Public Preview)\n\nYou can now perform a [partial update](https://www.pinecone.io/docs/manage-data/#partial-update) by ID and individual value pairs. This allows you to update individual metadata fields without having to upsert a matching vector or update all metadata fields at once. \n\n#### New metrics \n\nUsers on all plans can now see metrics for the past one (1) week in the Pinecone console. Users on the Enterprise and Enterprise Dedicated plan now have access to the following metrics via the [Prometheus metrics endpoint](monitoring/):\n\n* `pinecone_vector_count`\n* `pinecone_request_count_total`\n* `pinecone_request_error_count_total`\n* `pinecone_request_latency_seconds`\n* `pinecone_index_fullness` (Public Preview)\n\n**Note:** The accuracy of the `pinecone_index_fullness` metric is improved. This may result in changes from historic reported values. This metric is in public preview.\n\n#### Spark Connector\n\nSpark users who want to manage parallel upserts into Pinecone can now use the [official Spark connector for Pinecone](https://github.com/pinecone-io/spark-pinecone#readme) to upsert their data from a Spark dataframe.\n\n#### Support for Boolean and float metadata in Pinecone indexes\n\nYou can now add `Boolean` and `float64` values to [metadata JSON objects associated with a Pinecone index.](https://www.pinecone.io/docs/metadata-filtering/#adding-metadata-in-pinecone-indexes) \n\n#### New state field in describe_index results\n\nThe [`describe_index`](https://www.pinecone.io/docs/api/operation/describe_index/) operation results now contain a value for `state`, which describes the state of the index. The possible values for `state` are `Initializing`, `ScalingUp`, `ScalingDown`, `Terminating`, and `Ready`.\n\n#### Delete by metadata filter\n\nThe [`Delete`](https://www.pinecone.io/docs/api/operation/delete/) operation now supports filtering my metadata.\n"
},{
  "_id": {
    "$oid": "63dff5e87309c2d89ae1ccf3"
  },
  "filename": "limits.md",
  "title": "Limits",
  "category": "630fc5235d91a70054705fb9",
  "content": "---\ntitle: Limits\ncategory: 630fc5235d91a70054705fb9\n---\n\n\nThis is a summary of current Pinecone limitations. For many of these, there is a workaround or we're working on increasing the limits.\n\n## Upserts\n\nMax vector dimensionality is 20,000.\n\nMax size for an upsert request is 2MB. Recommended upsert limit is 100 vectors per request.\n\nVectors may not be visible to queries immediately after upserting. You can check if the vectors were indexed by looking at the total with `describe_index_stats()`, although this method may not work if the index has multiple replicas. The database is eventually consistent.\n\n## Queries\n\nMax value for `top_k`, the number of results to return, is 10,000. Max value for `top_k` for queries with `include_metadata=True` or `include_data=True` is 1,000.\n\n## Fetch and Delete\n\nMax vectors per fetch or delete request is 1,000.\n\n## Namespaces\n\nThere is no limit to the number of [namespaces](namespaces) per index.\n\n## Pod storage capacity\n\nEach **p1** pod has enough capacity for 1M vectors with 768 dimensions.\n\nEach **s1** pod has enough capacity for 5M vectors with 768 dimensions.\n\n## Metadata\n\nMax metadata size per vector is 10 KB.\n\nNull metadata values are not supported. Instead of setting a key to hold a null value, we recommend you remove that key from the metadata payload.\n\nMetadata with high cardinality, such as a unique value for every vector in a large index, will take up more memory than expected and cause the pods to become full.\n\n## Retention\n\nIndexes of users on the Starter (free) plan are deleted after 7 days of inactivity. To prevent this, send any API request to Pinecone to reset the counter.\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fb8b"
  },
  "filename": "pricing.md",
  "title": "\"Pricing\"",
  "category": "pricing",
  "content": "---\ntitle: \"Pricing\"\nlayout: pricing\nhero2:\n  title: Start free, scale effortlessly.\n  description: Pinecone runs on fully managed infrastructure that scales with you, while you pay only for what you use.\nmarketplaceLink:\n  text: Available on the Google Cloud Marketplace\n  url: https://console.cloud.google.com/marketplace/product/pinecone-public/pinecone\npricing:\n  - tier: Starter\n    description: For trying out and for small applications.\n    price: <span>Free</span>\n    hardware:\n      - Single pod\n    features:\n      - Single project\n      - Single environment\n      - Single Availability Zone\n    supportTitle: Community Support\n    support:\n      - |\n        [Community Support](https://community.pinecone.io/c/support/)\n    ctaText: Get Started\n    ctaLink: https://app.pinecone.io/\n    ctaInfo: No credit card required\n\n  - tier: Standard\n    description: For production applications at any scale.\n    price: From <span>$0.096/hour</span>\n    hardware:\n      - Any number of pods and replicas\n      - Zero-downtime scaling\n    features:\n      - Save vector data in Collections ($0.025/GB/month)\n      - Multiple projects and users\n      - Choose your environment (GCP US-West, GCP EU-West, AWS US-East)\n      - Single AZ\n    supportTitle: Standard Support\n    support:\n      - Email\n      - <span class=\"tooltip-item\">Business hours<span class=\"tooltip\">Monday–Friday 9am–6pm EST</span></span>\n      - Up to 2 technical contacts\n      - \"Response-time SLA (<span class='tooltip-item'>Sev-1<span class='tooltip'>Production system is down or is severely impacted such that routine operation is impossible.</span></span>: 4 hours; <span class='tooltip-item'>Sev-2<span class='tooltip'>Production issue where system is functional but offers service in degraded or restricted capacity.</span></span>: 1 business day; <span class='tooltip-item'>Sev-3<span class='tooltip'>Production system has minor impact or has issue in development.</span></span>: 1 business day; <span class='tooltip-item'>Sev-4<span class='tooltip'>No production impact, questions or request for feature.</span></span>: 2 business days)\"\n    ctaText: Get Started\n    ctaLink: https://app.pinecone.io/\n    ctaInfo: Upgrade after creating account\n\n  - tier: Enterprise\n    description: For mission-critical production applications.\n    price: From <span>$0.144/hour</span>\n    hardware:\n      - Any number of pods and replicas\n      - Zero-downtime scaling\n    features:\n      - Save vector data in Collections ($0.025/GB/month)\n      - Multiple projects and users\n      - Choose your environment (GCP US-West, GCP EU-West, AWS US-East)\n      - Multi-AZ\n      - Prometheus Metrics\n      - Multiple payment options\n    supportTitle: Premium Support\n    support:\n      - Slack and Email\n      - 24/7/365 for <span class='tooltip-item'>Sev-1<span class='tooltip'>Production system is down or is severely impacted such that routine operation is impossible.</span></span> and <span class='tooltip-item'>Sev-2<span class='tooltip'>Production issue where system is functional but offers service in degraded or restricted capacity.</span></span>, <span class=\"tooltip-item\">Business hours<span class=\"tooltip\">Monday–Friday 9am–6pm EST</span></span> for <span class='tooltip-item'>Sev-3<span class='tooltip'>Production system has minor impact or has issue in development.</span></span> and <span class='tooltip-item'>Sev-4<span class='tooltip'>No production impact, questions or request for feature.</span></span>\n      - Up to 4 technical contacts\n      - \"Response-time SLA (<span class='tooltip-item'>Sev-1<span class='tooltip'>Production system is down or is severely impacted such that routine operation is impossible.</span></span>: 1 hour; <span class='tooltip-item'>Sev-2<span class='tooltip'>Production issue where system is functional but offers service in degraded or restricted capacity.</span></span>: 4 hours; <span class='tooltip-item'>Sev-3<span class='tooltip'>Production system has minor impact or has issue in development.</span></span>: 1 business day; <span class='tooltip-item'>Sev-4<span class='tooltip'>No production impact, questions or request for feature.</span></span>: 2 business days)\"\n      - <span class='tooltip-item'>Uptime<span class='tooltip'>Defined as total minutes in that month minus downtime, divided by total minutes in that month.</span></span> SLA (99.9%)\n    ctaText: Get Started\n    ctaLink: https://app.pinecone.io/\n    ctaInfo: Upgrade after creating account\n---\n\n<a name=\"cost\"><h3>Usage Pricing</h3></a>\n\nWhen you [create a Pinecone Index](/docs/manage-indexes/), you choose the pod type and number of pods to use for the index. Each pod comes with a fixed amount of vCPU, RAM, and SSD coupled with the software. You will be billed at the end of the month for total pod-hours used. Pod usage is rounded up to 15-minute increments.\n\nOverview of pod types (see [documentation](/docs/indexes/) for more details):\n\n* s1: Optimized for storage and cost, with low overall cost and low throughput.\n* p1: Optimized for balanced performance, with moderate search latency and throughput.\n* p2: Optimized for high performance, with very low search latency and high throughput.\n\n\n\n{{< tabs totalTabs=\"2\">}}\n{{< tab tabName=\"Google Cloud Platform\" tabId=\"gcp_tab\" tabIcon=\"/images/gcp_logo.svg\" >}}\n\n<div class=\"responsive-table pricing-table\">\n\n| Pod Type |       Starter       |       Standard       |      Enterprise      |\n| :------: | :-----------------: | :------------------: | :------------------: |\n|    s1    |    Free <br>(Limit: 1)    | $0.0960 <br>/pod-hour | $0.1440 <br>/pod-hour |\n|    p1    | Free <br>(Limit: 1) | $0.0960 <br>/pod-hour | $0.1440 <br>/pod-hour |\n|    p2    |    Free <br>(Limit: 1)    | $0.1440 <br>/pod-hour | $0.2160 <br>/pod-hour |\n\n</div>\n\n{{< /tab >}}\n\n{{< tab tabName=\"Amazon Web Services\" tabId=\"aws_tab\" tabIcon=\"/images/aws_logo.svg\" >}}\n\n<div class=\"responsive-table pricing-table\">\n\n| Pod Type |       Starter       |       Standard       |      Enterprise      |\n| :------: | :-----------------: | :------------------: | :------------------: |\n|    s1    |    Not available    | $0.1110 <br>/pod-hour | $0.1665 <br>/pod-hour |\n|    p1    | Not available | $0.1110 <br>/pod-hour | $0.1665 <br>/pod-hour |\n|    p2    |    Not available    | $0.1665 <br>/pod-hour | $0.24975 <br>/pod-hour |\n\n\n</div>\n\n{{< /tab >}}\n{{< /tabs >}}\n\n[Scaling pod sizes](/docs/manage-indexes/#changing-pod-sizes) to x2, x4, and x8 will change the pricing proportionally. For instance, a p1.x2 pod is twice the price of a p1 pod.\n\n[Contact us](/contact/) for pre-commitment discounts and Enterprise Dedicated pricing.\n\n### Common Questions\n\n**What will I be charged for?**\n\nYou will be charged for \"pod hours\" in 15-minute increments, which is the time an index is running. Pod-hours are counted even when you're not sending queries to the index.\n\n**How many pods do I need?**\n\nSince each pod is a bundle of hardware resources (vCPU, RAM, and disk), the required number of pods is affected by multiple factors such as data size (volume, dimensionality, and metadata), metadata cardinality, and desired latency or throughput. We strongly recommend testing with *your own data* on different pod types and number of pods, and following our [tips for performance tuning](https://www.pinecone.io/docs/performance-tuning/).\n\nAs a *very rough* starting guideline: each s1 pod fits 5M 768-dim vectors, each p1 pod fits 1M 768-dim vectors, and each p2 pod fits 1M 768-dim vectors.\n\nYou can also [contact us](/contact/) for help with sizing and testing.\n\n**How secure is Pinecone?**\n\nPinecone is SOC2 Type II compliant, and we take security seriously for all users on all plans. Read about [our security practices](/security/).\n\n**Where is the user license or the terms of service?**\n\nUsers accept the [End-User License Agreement](/user-agreement/) when creating an account.\n\n**Do you offer pre-commitment discounts?**\n\nYes we do. [Contact us](/contact/) for more details.\n\n**How do I cancel or downgrade?**\n\nYou can manage your subscription directly in the console. If you attempt to downgrade to Starter (Free) while exceeding the limits of the Starter plan, you will first need to delete projects or indexes until you are within the limits.\n\n---\n\nFor other questions, [contact us](/contact/) or [ask the community](https://community.pinecone.io).\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fb8c"
  },
  "filename": "dpa.md",
  "title": "info-page",
  "category": "\"Customer Data Protection Addendum\"",
  "content": "---\nlayout: info-page\ntitle: \"Customer Data Protection Addendum\"\nheadline: \"Customer Data Protection Addendum\"\ncta: false\nintro: \"\"\n---\n\nThis Customer Data Protection Addendum (“**DPA**”) is incorporated into and forms part of the Pinecone Services Agreement (the “**Agreement**”) by and between you (“**Customer**”) and Pinecone Systems, Inc. (“**Pinecone**”), under which Pinecone has agreed to provide the certain Hosted Services described therein (“**Hosted Services**”) to Customer.\n\nThis DPA forms part of the Agreement and specifies the data protection obligations of the parties that may arise from the Processing of Personal Data by Pinecone on behalf of Customer in the course of providing Hosted Services to Customer under the Agreement.\n\n1. **Definitions**\n\n    For purposes of this DPA, the terms below have the meanings set forth below. Capitalized terms that are used but not defined in this DPA have the meanings given in the Agreement.\n\n    1. **Affiliate** means any entity that directly or indirectly controls, is controlled by, or is under common control with the subject entity, where “control” refers to the power to direct or cause the direction of the subject entity, whether through ownership of voting securities, by contract or otherwise.\n\n    2. **Applicable Data Protection Laws** means European Data Protection Laws and the CCPA, in each case, to the extent applicable to the relevant Personal Data or Processing thereof under the Agreement.\n\n    3. **CCPA** means the California Consumer Privacy Act of 2018 and any regulations promulgated thereunder, in each case, as amended from time to time.\n\n    4. **Data Subject** means an identified or identifiable natural person.\n\n    5. **EEA** means the European Economic Area.\n\n    6. **EU** means the European Union.\n\n    7. **European Data Protection Laws** means the GDPR and other data protection laws of the EU, its Member States, Switzerland, Iceland, Liechtenstein, Norway and the United Kingdom, in each case, to the extent it applies to the relevant Personal Data or Processing thereof under the Agreement.\n\n    8. **GDPR** means Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016, as amended from time to time.\n\n    9. **Information Security Incident** means a breach of Pinecone’s security leading to the accidental or unlawful destruction, loss, alteration, unauthorized disclosure of, or access to, Personal Data in Pinecone’s possession, custody or control. Information Security Incidents do not include unsuccessful attempts or activities that do not compromise the security of Personal Data, including unsuccessful log-in attempts, pings, port scans, denial of service attacks, or other network attacks on firewalls or networked systems.\n\n    10. **Personal Data** means the Customer Data that constitutes “personal data”, “personal information” or similar information governed by Applicable Data Protection Laws. For purposes of this DPA, Personal Data does not include Personal Data of representatives of Customer with whom Pinecone has business relationships independent of the Hosted Services where Pinecone acts as a controller of such Personal Data.\n\n    11. **Process** or **Processing** means any operation or set of operations that is performed on Personal Data or on sets of Personal Data, whether or not by automated means, such as collection, recording, organization, structuring, storage, adaptation or alteration, retrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, restriction, erasure or destruction.\n\n    12. **Security Measures** has the meaning given in Section 4.1 (Pinecone’s Security Measures).\n\n    13. **Standard Contractual Clauses** means the mandatory provisions of the standard contractual clauses for the transfer of personal data to processors established in third countries in the form set out by European Commission Decision 2010/87/EU.\n\n    14. **Subprocessors** means third parties authorized under this DPA to Process Personal Data in relation to the Hosted Services.\n\n    15. **Third Party Subprocessors** has the meaning given in Section 5 (Subprocessors) of Annex 1.\n\n    16. The terms **controller**, **processor** and **supervisory authority** as used in this DPA have the meanings given in the GDPR.\n\n2. **Duration and Scope of DPA**\n\n    1. This DPA will, notwithstanding the expiration of the Agreement, remain in effect until, and automatically expire upon, Pinecone’s deletion or return of all Personal Data.\n\n    2. <u>Annex 1</u> (EU Annex) to this DPA applies to Personal Data or the Processing thereof subject to European Data Protection Laws. Annex 2 (California Annex) to this DPA, applies to Personal Data or the Processing thereof subject to the CCPA with respect to which Customer is a Business (as defined in the CCPA).\n\n3. **Customer Instructions**\n\n    Pinecone will Process Personal Data only in accordance with Customer’s instructions. By entering into this DPA, Customer instructs Pinecone to Process Personal Data to provide the Hosted Services. Customer acknowledges and agrees that such instruction authorizes Pinecone to Process Personal Data (a) to perform its obligations and exercise its rights under the Agreement; (b) perform its legal obligations and to establish, exercise or defend legal claims in respect of the Agreement; (c) pursuant to any other written instructions given by Customer and acknowledged in writing by Pinecone as constituting instructions for purposes of this DPA; and (d) as reasonably necessary for the proper management and administration of Pinecone’s business.\n\n4. **Security**\n\n    1. <u>Pinecone Security Measures</u>. Pinecone will implement and maintain technical and organizational measures designed to protect Personal Data against accidental or unlawful destruction, loss, alteration, unauthorized disclosure of or access to Personal Data as described in Annex 3 (the “Security Measures”). Pinecone may update the Security Measures from time to time, provided the updated measures do not decrease the overall protection of Personal Data.\n\n    2. <u>Information Security Incidents</u>. If Pinecone becomes aware of an Information Security Incident, Pinecone will (a) notify Customer of the Information Security Incident without undue delay after becoming aware of the Information Security Incident and (b) take reasonable steps to identify the cause of such Information Security Incident and prevent a recurrence. Notifications made pursuant to this Section 4.2 will describe, to the extent possible, details of the Information Security Incident, including steps taken to mitigate the potential risks and steps Pinecone recommends Customer take to address the Information Security Incident. Pinecone’s notification of or response to an Information Security Incident under this Section 4.2 will not be construed as an acknowledgement by Pinecone of any fault or liability with respect to the Information Security Incident.\n\n    3. <u>Customer’s Security Responsibilities and Assessment</u>\n\n        1. <u>Customer’s Security Responsibilities</u>. Customer agrees that, without limitation of Pinecone’s obligations under Section 4.1 (Pinecone Security Measures) and Section 4.2 (Information Security Incidents), Customer is solely responsible for its use of the Hosted Services, including (a) making appropriate use of the Hosted Services to ensure a level of security appropriate to the risk in respect of the Personal Data; (b) securing the account authentication credentials, systems and devices Customer uses to access the Hosted Services; (c) securing Customer’s systems and devices that Pinecone uses to provide the Hosted Services; and (d) backing up Personal Data.\n\n        2. <u>Customer’s Security Assessment</u>. Customer is solely responsible for evaluating for itself whether the Hosted Services, the Security Measures and Pinecone’s commitments under this DPA will meet Customer’s needs, including with respect to any security obligations of Customer under Applicable Data Protection Laws or other laws. Customer acknowledges and agrees that (taking into account the state of the art, the costs of implementation and the nature, scope, context and purposes of the Processing of Personal Data as well as the risks to individuals) the Security Measures implemented and maintained by Pinecone provide a level of security appropriate to the risk in respect of the Personal Data.\n\n5. **Data Subject Rights**\n\n    1. <u>Customer’s Responsibility for Requests</u>. If Pinecone receives any request from a Data Subject in relation to the Data Subject’s Personal Data, Pinecone will advise the Data Subject to submit the request to Customer and Customer will be responsible for responding to any such request.\n\n    2. <u>Pinecone’s Data Subject Request Assistance</u>. Pinecone will (taking into account the nature of the Processing of Personal Data) provide reasonable assistance as necessary for Customer to perform its obligation under Applicable Data Protection Laws to fulfill requests by Data Subjects to exercise their rights under Applicable Data Protection Laws, including if applicable, Customer’s obligation to respond to requests for exercising the Data Subject’s rights set out in Chapter III of the GDPR. Customer shall reimburse Pinecone for any such assistance, at Pinecone’s then-current professional Hosted Services rates, which shall be made available to Customer upon request.\n\n6. **Customer Responsibilities**\n\n    Customer represents and warrants to Pinecone that (a) all notices have been given to, and consents and rights have been obtained from, the relevant Data Subjects and any other party as may be required by Applicable Data Protection Laws and any other laws for Processing under the Agreement; and (b) Personal Data does not and will not contain any (i) Social Security numbers or other government-issued identification numbers, (ii) protected health information subject to the Health Insurance Portability and Accountability Act (HIPAA) or other information regarding a Data Subject’s medical history, mental or physical condition or medical treatment or diagnosis by a health care professional, (iii) health insurance information, (iv) biometric information, (v) passwords to any online accounts, (vi) credentials to any financial accounts, (vii) tax return data, (viii) payment card information subject to the Payment Card Industry Data Security Standard, (ix) personal data of children under 16 years of age, or (x) other information that falls within any special categories of data (as defined in the GDPR).\n\n7. **Notices**\n\n    Notwithstanding anything to the contrary in the Agreement, any notices required or permitted to be given by Pinecone to Customer may be given (a) in accordance with any notice clause of the Agreement; (b) to Pinecone’s primary points of contact with Customer; or (c) to any email provided by Customer for the purpose of providing it with Hosted Services-related communications or alerts. Customer is solely responsible for ensuring that such email addresses are valid.\n\n8. **Miscellaneous**\n\n    1. <u>Liability Cap</u>.\n\n    The total combined liability of either party and its Affiliates towards the other party and its Affiliates, whether in contract, tort or any other theory of liability, under or in connection with this DPA and the Standard Contractual Clauses if entered into as described in Annex 1, Section 4 (Transfers out of the EEA) combined will be limited to limitations on liability or other liability caps agreed to by the parties in the Agreement, subject to Section 8.2 of this DPA (Liability Cap Exclusions)\n\n\n    2. <u>Liability Cap Exclusions</u>.\n\n    Nothing in Section 8.1 of this DPA (Liability Cap) will affect any party’s liability to Data Subjects under the third-party beneficiary provisions of the Standard Contractual Clauses to the extent limitation of such rights is prohibited by European Data Protection Laws, where applicable.\n\n    3. <u>Conflict</u>.\n\n    Except as expressly modified by the DPA, the terms of the Agreement remain in full force and effect. To the extent of any conflict or inconsistency between this DPA and the other terms of the Agreement, this DPA will govern.\n\n## Annex 1\n\n### EU Annex\n\n1. **Processing of Data**\n\n    1. <u>Subject Matter and Details of Processing</u>. The parties acknowledge and agree that (a) the subject matter of the Processing under the Agreement is Pinecone’s provision of the Hosted Services; (b) the duration of the Processing is from Pinecone’s receipt of Personal Data until deletion of all Personal Data by Pinecone in accordance with the Agreement; (c) the nature and purpose of the Processing is to provide the Hosted Services; (d) the data subjects to whom the Processing pertains are Customer’s employees, personnel and other individuals to whom Customer has a relationship; and (e) the categories of Personal Data are provided by Customer or its Authorized Users in connection with the Hosted Services.\n\n    2. <u>Roles and Regulatory Compliance; Authorization</u>. The parties acknowledge and agree that (a) Pinecone is a processor of that Personal Data under European Data Protection Laws; (b) Customer is a controller (or a processor acting on the instructions of a controller) of that Personal Data under European Data Protection Laws; and (c) each party will comply with the obligations applicable to it in such role under the European Data Protection Laws with respect to the Processing of that Personal Data. If Customer is a processor, Customer represents and warrants to Pinecone that Customer’s instructions and actions with respect to Personal Data, including appointment of Pinecone as another processor, have been authorized by the relevant controller.\n\n    3. <u>Pinecone’s Compliance with Instructions</u>. Pinecone will only Process Personal Data in accordance with Customer’s instructions described in this DPA unless European Data Protection Laws requires otherwise, in which case Pinecone will notify Customer (unless that law prohibits Pinecone from doing so on important grounds of public interest).\n\n    4. <u>Data Deletion</u>. Upon termination of Customer’s access to the Hosted Services, Customer instructs Pinecone to delete all Personal Data from Pinecone’s systems as soon as reasonably practicable, unless European Data Protection Laws requires otherwise.\n\n2. **Data Security**\n\n    1. <u>Pinecone Security Measures, Controls and Assistance</u>\n\n        1. <u>Pinecone Security Assistance</u>. Pinecone will (taking into account the nature of the Processing of Personal Data and the information available to Pinecone) provide Customer with reasonable assistance necessary for Customer to comply with its obligations in respect of Personal Data under European Data Protection Laws, including Articles 32 to 34 (inclusive) of the GDPR, by (a) implementing and maintaining the Security Measures; (b) complying with the terms of Section 4.2 (Information Security Incidents) of the DPA; and (c) complying with this Annex 1.\n\n        2. <u>Security Compliance by Pinecone Staff</u>. Pinecone will grant access to Personal Data only to personnel who need such access for the scope of their job duties, and are subject to appropriate confidentiality arrangements.\n\n    2. <u>Reviews and Audits of Compliance</u>\n\n        1. Customer may audit Pinecone’s compliance with its obligations under this DPA up to once per year and on such other occasions as may be required by European Data Protection Laws, including where mandated by Customer’s supervisory authority. Pinecone will contribute to such audits by providing Customer or Customer’s supervisory authority with the information and assistance reasonably necessary to conduct the audit.\n\n        2. If a third party is to conduct the audit, Pinecone may object to the auditor if the auditor is, in Pinecone’s reasonable opinion, not independent, a competitor of Pinecone, or otherwise manifestly unsuitable. Such objection by Pinecone will require Customer to appoint another auditor or conduct the audit itself.\n\n        3. To request an audit, Customer must submit a detailed proposed audit plan to Pinecone at least two weeks in advance of the proposed audit date and any third party auditor must sign a customary non-disclosure agreement mutually acceptable to the parties (such acceptance not to be unreasonably withheld) providing for the confidential treatment of all information exchanged in connection with the audit and any reports regarding the results or findings thereof. The proposed audit plan must describe the proposed scope, duration, and start date of the audit. Pinecone will review the proposed audit plan and provide Customer with any concerns or questions (for example, any request for information that could compromise Pinecone security, privacy, employment or other relevant policies). Pinecone will work cooperatively with Customer to agree on a final audit plan. Nothing in this Section 2.2 shall require Pinecone to breach any duties of confidentiality.\n\n        4. If the controls or measures to be assessed in the requested audit are addressed in an SOC 2 Type 2, ISO, NIST or similar audit report performed by a qualified third party auditor within twelve (12) months of Customer’s audit request and Pinecone has confirmed there are no known material changes in the controls audited, Customer agrees to accept such report lieu of requesting an audit of such controls or measures.\n\n        5. The audit must be conducted during regular business hours, subject to the agreed final audit plan and Pinecone’s safety, security or other relevant policies, and may not unreasonably interfere with Pinecone business activities.\n\n        6. Customer will promptly notify Pinecone of any non-compliance discovered during the course of an audit and provide Pinecone any audit reports generated in connection with any audit under this Section 2.2, unless prohibited by European Data Protection Laws or otherwise instructed by a supervisory authority. Customer may use the audit reports only for the purposes of meeting Customer’s regulatory audit requirements and/or confirming compliance with the requirements of this DPA.\n\n        7. Any audits are at Customer’s expense. Customer shall reimburse Pinecone for any time expended by Pinecone or its Third Party Subprocessors in connection with any audits or inspections under this Section 2.2 at Pinecone’s then-current professional Hosted Services rates, which shall be made available to Customer upon request. Customer will be responsible for any fees charged by any auditor appointed by Customer to execute any such audit. Nothing in this DPA shall be construed to require Pinecone to furnish more information about its Third Party Subprocessors in a connection with such audits than such Third Party Subprocessors make generally available to their customers.\n\n3. **Impact Assessments and Consultations**\n\n    Pinecone will (taking into account the nature of the Processing and the information available to Pinecone) reasonably assist Customer in complying with its obligations under Articles 35 and 36 of the GDPR, by (a) making available documentation describing relevant aspects of Pinecone’s information security program and the security measures applied in connection therewith; and (b) providing the other information contained in the Agreement including this DPA.\n\n4. **Data Transfers**\n\n    1. <u>Data Processing Facilities</u>. Pinecone may, subject to Section 4.2 to this Annex 1 (Transfers out of the EEA, Switzerland or United Kingdom), store and process Personal Data in the United States or anywhere Pinecone or its Subprocessors maintains facilities.\n\n    2. <u>Transfers out of the EEA, Switzerland or United Kingdom</u>. If Customer transfers Personal Data out of the EEA, Switzerland or United Kingdom to Pinecone in a country not deemed by the European Commission to have adequate data protection, such transfer will be governed by the Standard Contractual Clauses, the terms of which are hereby incorporated into this DPA. In furtherance of the foregoing, the parties agree that:\n\n        1. for purposes of the Standard Contractual Clauses, (a) Customer will act as the data exporter and (b) Pinecone will act as the data importer;\n\n        2. for purposes of Appendix 1 to the Standard Contractual Clauses, the categories of data subjects, data, special categories of data (if appropriate), and the Processing operations shall be as set out in Section 1.1 to this Annex 1 (Subject Matter and Details of Processing);\n\n        3. for purposes of Appendix 2 to the Standard Contractual Clauses, the technical and organizational measures shall be the Security Measures;\n\n        4. upon data exporter’s request under the Standard Contractual Clauses, data importer will provide the copies of the subprocessor agreements that must be sent by the data importer to the data exporter pursuant to Clause 5(j) of the Standard Contractual Clauses, and that data importer may remove or redact all commercial information or clauses unrelated the Standard Contractual Clauses or their equivalent beforehand;\n\n        5. Customer agrees that the provisions of Section 4.2 of the DPA (Information Security Incidents) satisfy the requirements of the Standard Contractual Clauses between Customer and Pinecone under Clause 5(d)(ii);\n\n        6. the audits described in Clause 5(f) and Clause 12(2) of the Standard Contractual Clauses shall be performed in accordance with Section 2.2 of this Annex 1 (Reviews and Audits of Compliance);\n\n        7. Customer’s authorizations in Section 5 of this Annex 1 (Subprocessors) will constitute Customer’s prior written consent to the subcontracting by Pinecone of the Processing of Personal Data if such consent is required under Clause 5(h) and 11(1) of the Standard Contractual Clauses; and\n\n        8. certification of deletion of Personal Data as described in Clause 12(1) of the Standard Contractual Clauses shall be provided only upon Customer’s request.\n\n       Notwithstanding the foregoing, the Standard Contractual Clauses (or obligations the same as those under the Standard Contractual Clauses) will not apply to the extent an alternative recognized compliance standard for the lawful transfer of Personal Data outside the EEA, Switzerland or United Kingdom (e.g., binding corporate rules) applies to the transfer.\n\n5. **Subprocessors**\n\n    1. <u>Consent to Subprocessor Engagement</u>. Customer specifically authorizes the engagement of Pinecone’s Affiliates as Subprocessors. In addition, Customer generally authorizes the engagement of any other third parties as Subprocessors (“**Third Party Subprocessors**”).\n\n    2. <u>Information about Subprocessors</u>. Information about Subprocessors, including their functions and locations, is available at: https://www.pinecone.io/subprocessors/ (as may be updated by Pinecone from time to time) or such other website address as Pinecone may provide to customer from time to time (the “Subprocessor Site”).\n\n    3. <u>Requirements for Subprocessor Engagement</u>. When engaging any Subprocessor, Pinecone will enter into a written contract with such Subprocessor containing data protection obligations not less protective than those in this DPA with respect to Personal Data to the extent applicable to the nature of the Hosted Services provided by such Subprocessor. Pinecone shall be liable for all obligations subcontracted to, and all acts and omissions of, the Subprocessor.\n\n    4. <u>Opportunity to Object to Subprocessor Changes</u>. When any new Third Party Subprocessor is engaged during the term of the Agreement, Pinecone will notify Customer of the engagement (including the name and location of the relevant Subprocessor and the activities it will perform) by updating the website listed in Section 5.2 of this Annex 1 (Information about Subprocessors). If Customer objects to such engagement in a written notice to Pinecone within 15 days of being informed thereof on reasonable grounds relating to the protection of Personal Data, Customer and Pinecone will work together in good faith to find a mutually acceptable resolution to address such objection. If the parties are unable to reach a mutually acceptable resolution within a reasonable timeframe, Customer may, as its sole and exclusive remedy, terminate the Agreement and cancel the Hosted Services by providing written notice to Pinecone.\n\n## Annex 2\n\n### California Annex\n\n1. For purposes of this Annex 2, the terms “business,” “commercial purpose,” “sell” and “service provider” shall have the respective meanings given thereto in the CCPA, and “personal information” shall mean Personal Data that constitutes personal information governed by the CCPA.\n\n2. Pinecone shall not retain, use, or disclose any Personal Data that constitutes “personal information” under the CCPA (“**CA Personal Information**”) for any purpose other than for the specific purpose of providing the Hosted Services, or as otherwise permitted by CCPA, including retaining, using, or disclosing the CA Personal Information for a commercial purpose other than providing the Hosted Services.\n\n3. Pinecone shall not (a) sell any CA Personal Information; (b) retain, use or disclose any CA Personal Information for any purpose other than for the specific purpose of providing the Service, including retaining, using, or disclosing the CA Personal Information for a commercial purpose other than provision of the Hosted Services; or (c) retain, use or disclose the CA Personal Information outside of the direct business relationship between Pinecone and Customer. Pinecone hereby certifies that it understands its obligations under this Section 3 and will comply with them.\n\n4. It is the parties’ intent that with respect to any CA Personal Information, Pinecone is a service provider.\n\n5. Provision of the Hosted Services encompasses the Processing authorized by Customer’s instructions described in Section 3 of the DPA (Customer Instructions). The parties acknowledge that Pinecone’s retention, use and disclosure of CA Personal Information authorized by Customer’s instructions are integral to Pinecone’s provision of the Hosted Services and the business relationship between the parties.\n\n6. Notwithstanding anything in the Agreement or any order form entered in connection therewith, the parties acknowledge and agree that Pinecone’s access to CA Personal Information or any other Personal Data does not constitute part of the consideration exchanged by the parties in respect of the Agreement.\n\n## Annex 3\n\n### Security Measures\n\nAs from the DPA Effective Date, Pinecone will implement and maintain the Security Measures set out in this <u>Annex 3</u>. Pinecone may update or modify such Security Measures from time to time provided that such updates and modifications do not materially decrease the overall security of the Hosted Services.\n\n1. Organizational management and dedicated staff responsible for the development, implementation and maintenance of Pinecone’s information security program.\n\n2. Audit and risk assessment procedures for the purposes of periodic review and assessment of risks to Pinecone’s organization, monitoring and maintaining compliance with Pinecone’s policies and procedures, and reporting the condition of its information security and compliance to internal senior management.\n\n3. Data security controls which include at a minimum, but may not be limited to, logical segregation of data, restricted (e.g. role-based) access and monitoring, and utilization of commercially available and industry standard encryption technologies for Personal Data that is transmitted over public networks (i.e. the Internet) or when transmitted wirelessly.\n\n4. Logical access controls designed to manage electronic access to data and system functionality based on authority levels and job functions, (e.g. granting access on a need-to-know basis, use of unique IDs and passwords for all users, periodic review and revoking/changing access when employment terminates or changes in job functions occur).\n\n5. Password controls designed to manage and control password strength, expiration and usage including prohibiting users from sharing passwords and requiring that Pinecone passwords that are assigned to its employees: (i) be at least eight (8) characters in length, (ii) not be stored in readable format on Pinecone’s computer systems; (iii) must be changed every ninety (90) days; must have defined complexity; (iv) must have a history threshold to prevent reuse of recent passwords; and (v) newly issued passwords must be changed after first use.\n\n6. Physical and environmental security of data center, server room facilities and other areas containing Personal Data designed to:  (i) protect information assets from unauthorized physical access, (ii) manage, monitor and log movement of persons into and out of Pinecone facilities, and (iii) guard against environmental hazards such as heat, fire and water damage.\n\n7. Change management procedures and tracking mechanisms designed to test, approve and monitor all changes to Pinecone’s technology and information assets.\n\n8. Incident / problem management procedures design to allow Pinecone to investigate, respond to, mitigate and notify of events related to Pinecone’s technology and information assets.\n\n9. Business resiliency/continuity and disaster recovery procedures designed to maintain service and/or recovery from foreseeable emergency situations or disasters.\n\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fb8d"
  },
  "filename": "careers.md",
  "title": "\"Careers\"",
  "category": "careers",
  "content": "---\ntitle: \"Careers\"\nlayout: careers\nheadline: \"We're hiring!\"\nintro: \"Join our growing team and help shape the future of our industry.\"\ndescription: \"We are hiring! Join our growing team and help shape the future of our industry.\"\nhero:\n  title: We're <span>hiring</span>!\n  description: |\n    Join our growing team and help shape the future of our industry.\n\n    Pinecone is pioneering search infrastructure to power AI/ML for the next decade and beyond. We provide customers with capabilities that until now have only been in the hands of a few tech giants.\n  img:\n    src: /images/pinecone-careers-hero.png\n    alt: Pinecone careers\n  cta:\n    anchorLink:\n      anchorID: open-roles\n      text: See Open Roles\nsection1:\n  title: Who We Are\n  text: |\n    We are a dedicated, open-minded, and kind group of people who love working together on incredibly challenging yet rewarding problems. At Pinecone you will work with world-class scientists and engineers who have built large scale ML applications and platforms at leading companies and cloud providers. Pinecone is a distributed team with offices in New York, Tel Aviv and San Francisco.\nimageGrid:\n  list:\n    - image: /images/pinecone-team-collage-1.png\n      alt: The Pinecone team\n    - image: /images/pinecone-team-collage-2.png\n      alt: Working at Pinecone\n    - image: /images/pinecone-team-collage-3.png\n      alt: Pinecone careers\n    # - image: /images/working-at-pinecone.jpg\n    #   alt: Pinecone team member\nsection2:\n  title: Our <span>values</span>\n  text: |\n    Great results come from working well together, which is why we actively support both personal growth and good team work. We strive to help our employees flourish, while promoting a strong sense of community.\n  list:\n    - title: Be a pro\n      text: Own your work, push boundaries, and strive for simplicity.\n    - title: Be yourself\n      text: Be authentic, speak your truth, seek feedback, and know you are accepted.\n    - title: Be a friend\n      text: Earn trust, extend help, listen intently, and commit to shared decisions.\ntestimonials:\n  list:\n    - text: |\n        Working at Pinecone has been great for my career growth. I get to work on features that put me outside my comfort zone, which strengthens my technical skills. I get to take ownership of projects and make a true impact on the product.\n\n        Collaborating with an international team is a fun and culturally enriching experience. The management team is trusting and caring, and I'm constantly impressed by the faith in the engineering team.\n      name: Ai Lin\n      title: Frontend Engineer\n      image: \"/images/ai-lin.png\"\n    - text: |\n        Working at Pinecone has helped me grow exponentially as an engineer. With a culture that values taking ownership and getting things done, I've been able to design real-world applications and bring them to production.\n\n        Engineers at Pinecone get a lot of control over the product roadmap, and there’s always something new to learn and build. Working with top-tier engineers on bleeding-edge technologies has been a truly exciting experience, and the breadth of engineering talent means it's easy to find help and mentorship.\n      name: Daniel Margulis\n      title: Full-Stack Engineer\n      image: \"/images/daniel-margulis.jpg\"\nsection3:\n  title: Perks &<br><span>benefits</span>\n  list:\n    - Competitive salaries\n    - Company stock options\n    - Flexible PTO\n    - Medical, dental & vision plans\n    - Paid parental leave\n    - Mental health resources\n    - Commuter benefits\n    - FSA\n    - Flexible work hours\n    - Hybrid in-office/remote\n    - Annual team retreat\n    - WFH Equipment Stipend\nsection4:\n  text: At Pinecone, we’re proud to be an equal opportunity employer. We realize the key to creating a company with a world-class culture and employee experience comes from whom we hire and from creating a workplace that celebrates everyone. We proudly consider qualified applicants without regard to race, religion, gender, sexual identity, national origin, age, disability, or any other basis.\n  title: Open Roles\n---"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fb8e"
  },
  "filename": "community.md",
  "title": "\"Community\"",
  "category": "community",
  "content": "---\ntitle: \"Community\"\nlayout: community\nheadline: \"Pinecone Community\"\ndescription: \"We believe that search should be accurate, intelligent, and helpful. That’s why we created the Pinecone vector search community: to help you learn more, succeed, and be part of a community that pushes search forward.\"\nhero:\n  title: Hi! We’re happy <span>you’re here</span>.\n  description: |\n    The Pinecone Community is for engineers, data scientists, and anyone else involved in the new frontier of (vector) search.\n  emailSubmit: true\ninfo:\n  title: Ways to get and <span>stay involved</span>\n  forum:\n    description: |\n      [Join the forum](https://community.pinecone.io/) for:\n    list:\n      - title: Announcements\n        text: \"- News and updates from the Pinecone team.\"\n      - title: Support\n        text: \"- Ask questions about and get help using Pinecone.\"\n      - title: Search, No Filter\n        text: \"- General discussions about search, models, data, use cases, and anything else. Share your projects, tutorials, and ideas with the community.\"\n    ctaText: Join the forum\n    ctaLink: https://community.pinecone.io/\n  events:\n    description: Live presentations and workshops featuring experts from Pinecone and other organizations.\n    list:\n      - date: \"2022-02-16\"\n        event: Pinecone Office Hours\n        url: https://pinecone-io.zoom.us/webinar/register/WN_kd6WSbQ5TSONOSqMF0vRdg\n      - date: \"2022-02-24\"\n        event: \"Ask Like You Mean It: Build a custom ML-powered Q&A app in a day\"\n        url: https://pinecone-io.zoom.us/webinar/register/WN_31NCjwJjRBOxZy1AnKTArw\n      - date: \"2022-03-09\"\n        event: \"Question, Answered: How to build AI-powered Q&A applications with Haystack and Pinecone\"\n        url: https://pinecone-io.zoom.us/webinar/register/WN_EOJJDPrbTc2yyH9KpbTcIQ\n  showcase:\n    description: |\n      Want to see your Pinecone project here? Tell us: info@pinecone.io\n    list:\n      - title: All-In On AI\n        description: A semantic search index built with Pinecone and OpenAI that lets you search across every episode of the All-In podcast.\n        url: https://all-in-on-ai.vercel.app/\n      - title: NFT Semantic Search\n        description: Visual semantic search for a million NFTs with Alchemy, OpenAI's CLIP & Pinecone.\n        url: https://abhaykashyap.com/blog/visual-semantic-search-nfts-alchemy-openai-clip-pinecone/\n      - title: Parent Resemblence Detection\n        description: Use Pinecone to detect which parent looks like their children the most.\n        url: https://medium.com/@timtullydevnull/use-pinecone-to-see-which-parent-most-resembles-your-kids-862588d1c1a\n      - title: Bible Semantic Search\n        description: A streamlit app for performing semantic search on the King James Bible.\n        url: https://share.streamlit.io/chrislee973/bible-semantic-search/main/app.py\n      - title: Mood Surf\n        description: A discovery engine to explore topics that intrigue you. Made by [Re:Search](https://re-search.xyz/).\n        url: https://mood.surf/\n      - title: FlixRec\n        description: A similar movies recommendation system built using Pinecone. Test it out at the link above and [read the article here](https://web.navan.dev/posts/2022-05-21-Similar-Movies-Recommender.html).\n        url: https://flixrec.navan.dev/\n      - title: Not Slack\n        description: Not Slack chatbot.\n        url: https://share.streamlit.io/pinecone-io/playground/not_slack_chatbot/src/server.py\n      - title: Pinecone.jl\n        description: Pinecone.jl is a Julia API for the Pinecone vector database.\n        url: https://github.com/tullytim/Pinecone.jl\n      - title: ML Q&A\n        description: The Q&A tool takes discussions and docs from some of the best Python ML libraries and collates their content into a natural language search and Q&A tool.\n        url: https://share.streamlit.io/pinecone-io/playground/beyond_search_openai/src/server.py\n      - title: Semantic Search with Sentence Transformers\n        description: A demo on semantic search with sentence transformers and Pinecone.\n        url: https://colab.research.google.com/github/pinecone-io/examples/blob/master/semantic_search/light_demo.ipynb\n      - title: Semantic Search with Pinecone and OpenAI\n        description: A demo example of building a SOTA semantic search tool with OpenAI vector embeddings and the Pinecone vector DB.\n        url: https://colab.research.google.com/drive/1t15pC65wFcynCD-VcuRjutysItZM2iII?usp=sharing\n      - title: ML Doc Q&A\n        description: The ML Doc Q&A tool takes discussions and docs from some of the best Python ML libraries and collates their content into a natural language search tool.\n        url: https://share.streamlit.io/pinecone-io/playground/doc_search/src/server.py\n      - title: Face Recognition\n        description: Facial recognition using Pinecone.\n        url: https://github.com/serengil/tensorflow-101/blob/master/python/Pinecone-Face-Recognition.ipynb\n      - title: GPL for Semantic Search by James Briggs\n        description: Domain Adaptation for Dense Retrieval.\n        url: https://gist.github.com/jamescalam/d2c888775c87f9882bb7c379a96adbc8#file-gpl-domain-adaptation-ipynb\n      - title: Hacker News Doppelgänger\n        description: This app compares the semantic meaning of your comment history with those of all other users, and finds the top ten users whose comment histories are most similar to yours.\n        url: https://share.streamlit.io/pinecone-io/playground/hacker_news/src/server.py\n      - title: Haystack-Pinecone Integration Live App by James Briggs and Brandon Chan\n        description: Explore the world - Ask any question on this topic and see if Haystack can find the correct answer to your query!\n        url: https://haystack-demo.deepset.ai/\n      - title: ODQA Pipeline\n        description: A question answering demonstration by James Briggs to show how useful Pinecone can be in developing ML-powered applications. Take a look at his work at the link above, and [try out the demo](https://share.streamlit.io/pinecone-io/playground/doc_search/src/server.py).\n        url: https://colab.research.google.com/drive/1sZmvJ7qpuX7uzD3f8gb7PhxCA9VPkqSF#scrollTo=WRkkhlQOz5yL\n  newsletter:\n    description: Don't miss community news and events.\npioneers:\n  title: Pinecone <span>Pioneers</span>\n  intro: We want to celebrate those who are revolutionizing search technology. The people below are leading the way on adopting, advancing, and advocating for vector search technology.\n  list:\n    - name: Romana Dorfer\n      position: Co-Founder, Factinsect\n      image: /images/romana-dorfer.jpeg\n      bio: Romana is an experienced software developer and AI expert, whose company makes an AI-based tool for fact-checking online content.\n    - name: Dan Whalen\n      position: Manager R&D, Expel.io\n      image: /images/dan-whalen.png\n      bio: Dan is a Principal Researcher at Expel, a transparent Managed Detection and Response vendor. Being in the information security field for 8+ years, he’s passionate about building tech solutions that help protect people from security breaches. Lately, he's focused on applying data science solutions to unique security problems.\n    - name: André Mourão\n      position: Senior Software Engineer, Mem Labs\n      image: /images/andre-mourao.jpeg\n      bio: \"André is at the forefront of Information Retrieval and Machine Learning. The Principal Senior Engineer at Searchable.ai, making it easy for you to find your files that are spread across multiple cloud services, and founder of Revionista.PT, uncovering post-publication changes in Portuguese news, he’s spent his career ensuring technology makes life easier and more accessible for others. He holds a Computer Science PhD in multimodal search systems. \"\n    - name: Isabella Fulford\n      position: Solutions Architect, OpenAI\n      image: /images/isabella-fulford.png\n      bio: \"Isabella is a software engineer at Mem Labs, the world's first self-organizing workspace to help you find information and store it better, faster. While pursuing her Bachelor's and completing her Master's in Computer Science at Stanford as a Mayfield Fellow, she also worked at Amazon Web Services and Inspire AI.\"\n    - name: Craig Schmidt\n      position: ML Engineer, Delomore\n      image: /images/craig-schmidt.png\n      bio: Craig is a former Principal Machine Learning Engineer at TripAdvisor. In his seven years there, he worked extensively with the search group. Before that, he worked at several startups in greater Boston in various Machine Learning (ML) roles. His current startup, Delomore, is a search engine for Shopify shops. It is a perfect application for an embedding-based vector search.\n    - name: Diego Lopez Yse\n      position: ML Engineer, Moody's\n      image: /images/diego-lopez-yse.png\n      bio: \"Diego is a data scientist working on applied Machine Learning solutions in the Life Sciences. He’s also a writer and content creator, helping others expand their digital skills. He enjoys hackathons and every opportunity to tackle challenges differently: If we've always done it this way, let's change it.\"\n    - name: George Mathew\n      position: Founder, Nyckel\n      image: /images/george-mathew.png\n      bio: \"George is a computer engineering veteran. After working at companies like Microsoft and Oracle, he founded Nyckel, which makes machine learning accessible to software developers.\"\n    - name: Aiden Lee\n      position: Co-Founder & CTO, Twelve Labs\n      image: /images/aiden-lee.png\n      bio: \"Aiden has been in the AI/Deep Learning space for years, including work as an AI Researcher, a Deep Learning Research Scientist, and now as the Founder and CTO of Twelve Labs. Twelve Labs is building a video understanding AI infrastructure where users can easily train and integrate SOTA video understanding models into their system. Their AI team won 1st place in the video retrieval track of IEEE ICCV VALUE Challenge 2021 hosted by Microsoft and UNC.\"\n    - name: George Williams\n      position: Head of AI, Smile Identity\n      image: /images/george-williams.png\n      bio: \"George is the Head of AI at Smile Identity, an identity management and biometrics provider. He has held senior leadership roles at Apple's New Product Architecture group and NYU's Courant Institute.  He is the author of several research papers in computer vision and deep learning and regularly presents at meetups and tech conferences. He has also held the position of Embedded AI Chair for the Valleyml.ai conference, serves on the content committee for the Open Data Science Conference, and is a chair at the Neural Information Processing Conference.\"\n    - name: Pratik Bhavsar\n      position: Senior NLP Scientist, Enterpret\n      image: /images/pratik-bhavsar.png\n      bio: \"Pratik has worked as a data scientist and NLP engineer throughout his entire career. He’s the founding engineer and senior NLP scientist at Enterpret and is the founder of Maxpool, a community for data scientists to discuss practical ML problems. Enterpret provides automated customer feedback to help bridge the gap between product builders and consumers.\"\n    - name: Alex Lee\n      position: Engineering Manager, Goodnotes\n      image: /images/alex-lee.jpg\n      bio: \"Alex is currently an Engineering Manager at GoodNotes, where he works on GoodNotes Community, a platform that enables students to explore and exchange lecture notes and other study materials.\"\n    - name: Oded Kalev\n      position: Detection Group Leader, Perception Point\n      image: /images/oded-kalev.jpeg\n      bio: \"Oded is the ML and Data team lead at Perception Point cyber security where he is leveraging data to improve detection in Files, Emails, URLs and more. He's been interested in science and coding since he was eight years old.\"\n    - name: Chaymae Chaali\n      position: Data Scientist, Sealk\n      image: /images/chaymae-chaali.jpg\n      bio: \"Chaymae is a Data Scientist at Sealk, a french Fintech offering an artificial intelligence solution to facilitate deal sourcing for M&A and Private Equity.\"\n    - name: Alvise Sembenico\n      position: ML Engineer, Klue\n      image: /images/alvise-sembenico.png\n      bio: \"Alvise is a machine learning engineer at Klue, an AI-powered Competitive Enablement platform. He’s also the founder of Intrical AI, which ensures due diligence in the AI age. \"\n    - name: Özge Karakaya\n      position: ML Engineer, Klue\n      image: /images/ozge-karakaya.png\n      bio: \"Ozge is a machine learning engineer at Klue. He’s worked as an engineer in many industries at major companies including Sony, GE Aviation, ING Nederland, and more before joining Klue in July 2021. \"\n    - name: Nichita Diaconu\n      position: ML Engineer, Klue\n      image: /images/nichita-diaconu.png\n      bio: \"Nichita is a machine learning engineer at Klue. Prior to joining, he was a research engineer at Philips where he wrote his MSc thesis on self-attention in vision models, using data dependent filters that also take advantage of the rotational and translational symmetries of images.\"\n    - name: Björn Burscher\n      position: ML Engineering Manager, Klue\n      image: /images/bjorn-burscher.png\n      bio: \"Björn is an engineering manager in machine learning at Klue. Before that, he was a machine learning engineer at Klue and received a PhD in Information Science from the University of Amsterdam.\"\n    - name: Morgan Gallant\n      position: Co-Founder, Operand\n      image: /images/morgan-gallant.png\n      bio: Morgan is the co-founder of Operand, a startup working to \"make knowledge come alive\" by leveraging the latest and greatest in ML-powered search technologies. Specifically, he is interested in the idea of \"[human]-computer symbiosis\", where machines are constantly working on users' behalf to surface information proactively when it's needed.\n    - name: Bhairav Mehta\n      position: CEO & Co-Founder, Buzzle.ai\n      image: /images/bhairav-mehta.png\n      bio: Bhairav, CEO/Co-founder at Buzzle.ai, uses his background in deep learning research to build innovative NLP solutions that extract strategic insight for product and marketing teams, directly from the sales conversations they are already recording. Before starting Buzzle, Bhairav was a PhD student at MIT studying the theoretical foundations of deep and multi-task learning. His work as a Masters student at Mila (fka the Montreal Institute for Learning Algorithms) is heavily-cited in the fields of learning-based robotics and empirical applications of Stein’s Method.\n    - name: Adithya Ramanathan\n      position: Co-Founder, Buzzle.ai\n      image: /images/adithya-ramanathan.png\n      bio: Adithya is a co-founder at Buzzle.ai where they leverage cutting edge NLP techniques to turn recorded sales and customer success conversations into strategic insight for product and marketing teams. Prior to building Buzzle, Adithya was a Principal Machine Learning Engineer at Capital One’s Center for Machine Learning where he built and deployed several state of the art NLP solutions.\n    - name: Michael Staunton\n      position: Co-Founder, Buzzle.ai\n      image: /images/michael-staunton.png\n      bio: Michael, a co-founder at Buzzle.ai, leads development of the core Buzzle services where they apply NLP modeling to take a company's customer conversations and transform them into actionable insight for product and marketing teams. Previously, Michael worked at Capital One where he helped build their CCPA portal.\n    - name: Val Jones\n      position: CTO, StoryFile\n      image: /images/val-jones.png\n      bio: Val is the CTO of StoryFile, a cloud-based, no-code, automatic platform that would bring the power of conversational video into everyone’s hands. They were one of the first employees Raxium, is the author of over 20 academic publications and five patents, and comes to StoryFile with more than 20 years of experience helping develop cutting edge tech.\n    - name: Rafal Cycon\n      position: Chief Data Scientist, Form.com\n      image: /images/rafal-cycon.png\n      bio: Rafal is the Chief Data Scientist at Form.com, the digital assistant for the frontline. According to Kaggle, he is the highest rated data science competitor in Poland and one of the Top 30 in the world. Previously, he led ShelfWise and FORNAX and was a computer vision and ML engineer at a variety of companies, as well.\n    - name: Matt Sonnati\n      position: CEO & Co-Founder, Inokufu\n      image: /images/matt-sonnati.png\n      bio: Matt is the co-founder, CEO and CTO of Inokufu. He has a very successful educational background, holding a PhD in chemistry, a MBA, and awarded Innovator Under 35 France by the MIT Technology Review in 2012. Having two dyslexic brothers, he quickly realized that this educational and career path was not equally accessible to everyone. He then co-founded Inokufu, which offers a semantic search engine and a multi-view recommendation engine, with the aim of offering personalized learning on a large scale.\n    - name: Guillaume Lefebvre\n      position: Head of R&D, Inokufu\n      image: /images/guillaume-lefebvre.jpg\n      bio: Guillaume has always been passionate about artificial intelligence and more particularly NLP, which he discovered in computer science engineering school and deepened in his master's degree in data science. Today, he continues to work in the field of NLP as a PhD student and head of R&D at Inokufu. At Inokufu, Guillaume aims to revolutionize the field of education and professional training with a disruptive technology of semantic search and multi-view recommendation.\n    - name: Gabin Desserprit\n      position: Founder, OriginMatter.com\n      image: /images/gabin-desserprit.jpg\n      bio: As a lead product engineer at Origin Matter, Gabin is developing their reverse content search engine that takes images, videos, texts, and audio and reverses them into searchable content like movies, music, NFTs, products, stores and more.\n    - name: Gabriel Jimenez\n      position: Software Engineer & Architect\n      image: /images/gabriel-jimenez.jpg\n      bio: Gabe is an experienced Software Engineer and Architect, with a passion for learning and software development. With over 15 years of experience in large and small projects, in all kinds of organizations from large multi-nationals to startups, he has developed software in every paradigm from embedded to systems to web development.\n    - name: Hannah Kolbeck\n      position: Software Developer\n      image: /images/hannah-kolbeck.jpg\n      bio: Hannah is a software developer in Portland, Oregon who meddles in microcontroller hardware and laser cutting. She maintains several Twitter bots centered around alt text and the alt-text.org database. In her spare time she plays soccer, volunteers doing mutual aid, and hangs out with her cat and her wife. She is deeply committed to improving the lives of the people around her, and believes that technology can be a force for immense good in the world is freed from the requirement that it be profitable.\n    - name: Henry Mao\n      position: Entrepreneur & Machine Learning Researcher\n      image: /images/henry-mao.jpg\n      bio: Henry is a technology entrepreneur with expertise in deep learning, music generation and natural language processing. He co-founded a startup in 2016 called Altum with the mission to usher in a new era of human creativity through artificial intelligence. They are currently developing [Jenni](https://jenni.ai/), an AI content writer.\n  signUp:\n    text: |\n      Want to be recognized or nominate someone as a pioneer in vector search? Email [info@pinecone.io](mailto:info@pinecone.io) with your or their name, title, and a short bio!\n  perks:\n    title: Perks of <span>Pioneering</span>\n    text: \"As part of this diverse and invite-only group you’ll get:\"\n    list:\n      - Special recognition throughout the Pinecone community.\n      - Early access to new features, with opportunities to provide feedback and help shape the product roadmap.\n      - Shirts and other goodies from Pinecone.\n      - Opportunities to publish content in the <a href=\"https://www.pinecone.io/learn/\">learning center</a> and present at our events.\n# announcements:\n#   text: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nunc vulputate libero et velit interdum, ac aliquet odio mattis. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Curabitur tempus urna at turpis condimentum lobortis.\n#   ctaText: Learn More\n#   ctaLink: /\n---\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fb8f"
  },
  "filename": "thin-client-user-agreement.md",
  "title": "info-page",
  "category": "\"Pinecone Software End User License Agreement\"",
  "content": "---\nlayout: info-page\ntitle: \"Pinecone Software End User License Agreement\"\nheadline: \"Pinecone Software End User License Agreement\"\ncta: false\nintro: \"\"\n---\n\n*Last Revised: May 26, 2022*\n\nIMPORTANT – THIS PINECONE SOFTWARE END USER LICENSE AGREEMENT (EULA) IS A LEGALLY BINDING AGREEMENT BETWEEN YOU (“**YOU**”) AND PINECONE SYSTEMS, INC. (“**PINECONE**” OR “**WE**”). THIS EULA GOVERNS YOUR DOWNLOADING, INSTALLATION AND USE OF SOFTWARE THAT PINECONE MAKES AVAILABLE TO YOU FOR INSTALLATION AS A THIN-CLIENT TO ENABLE YOU TO ACCESS AND USE PINECONE’S PROPRIETARY PLATFORM THAT IT OFFERS AS A HOSTED SOLUTION (SUCH SOFTWARE, THE “**LICENSED SOFTWARE**”, AND THE PINECONE PLATFORM, THE “**PLATFORM**”).\n\nBY DOWNLOADING, INSTALLING OR USING THE LICENSED SOFTWARE,  YOU AGREE TO BE BOUND BY THE TERMS OF THIS EULA AND THE [PINECONE PRIVACY POLICY](/product-privacy/). YOU (A) ACKNOWLEDGE THAT YOU HAVE READ, UNDERSTAND, AND AGREE TO BE BOUND BY THIS EULA; AND (B) REPRESENT THAT YOU HAVE THE AUTHORITY TO ENTER INTO THIS EULA AS AN INDIVIDUAL, OR ON BEHALF OF THE ENTITY LICENSING THE LICENSED SOFTWARE, AND TO BIND SUCH ENTITY TO THE TERMS HEREIN. IF YOU DO NOT AGREE TO ALL TERMS AND CONDITIONS IN THIS EULA, OR IF YOU DO NOT HAVE SUCH AUTHORITY, DISCONTINUE THE DOWNLOAD OF THE LICENSED SOFTWARE.\n\n1. **Scope**. This EULA governs your use of the Licensed Software. If the parties have entered or subsequently enter into a written agreement that purports to govern or that includes provisions governing use of the Licensed Software (“**Other Agreement**”), and the Other Agreement contains any provision that conflicts with any term of this EULA, the conflicting provision in the Other Agreement will govern, but only to extent expressly specified in the Other Agreement.\n2. **License**. Subject to the terms of this EULA, Pinecone grants to you a nontransferable, nonexclusive, royalty-free, fully paid, worldwide license (without the right to sublicense) to install and execute the Licensed Software, in executable object code format only, solely on computers that you own or control and for the sole purpose of obtaining access and use of the Platform as permitted under a separate agreement with Pinecone that permits your access and use of the Platform.\n3. **Restrictions**. The rights granted hereunder are subject to the following restrictions: (a) you shall not license, sell, rent, lease, transfer, assign, distribute, host, outsource, disclose or otherwise commercially exploit the Licensed Software or make the Licensed Software available to any third party (other than the entity on whose behalf you enter into this EULA); (b) you shall not modify, make derivative works of, disassemble, reverse compile or reverse engineer any part of the Licensed Software; (c) you shall not access the Licensed Software in order to build a similar or competitive product or service; (d) except as expressly stated herein, no part of the Licensed Software may be copied, reproduced, distributed, republished, downloaded, displayed, posted or transmitted in any form or by any means, including but not limited to electronic, mechanical, photocopying, recording or other means; and (e) any future release, update, or other addition to the functionality of the Licensed Software provided by Pinecone (if any) shall be subject to the terms of this EULA unless Pinecone expressly states otherwise. You shall preserve all copyright and other proprietary rights notices on the Licensed Software and all copies thereof.\n4. **Responsibility**. You are responsible and liable for all actions and failures to take required actions with respect to the Licensed Software by any party to whom you may provide access to or use of the Licensed Software, whether such access or use is permitted by or in violation of this EULA.\n5. **Ownership**. The Licensed Software and all worldwide copyrights, trade secrets, and other intellectual property rights therein, are the exclusive property of Pinecone and its suppliers. All rights in and to the Licensed Software not expressly granted to you in this EULA are reserved by Pinecone and its suppliers.\n6. **Third Party Software**. Certain items of software included as part of the Licensed Software are licensed from third parties and are subject to terms and conditions provided by such third parties (“**Third Party Software**”). The Third Party Software is not subject to the terms and conditions of Sections 1 and 2 of this EULA. Instead, each item of Third Party Software is licensed under the terms of the license that accompanies such Third Party Software. Nothing in this EULA limits your rights under, or grants you rights that supersede rights available in, the terms and conditions of any applicable license for the Third Party Software.\n7. **Disclaimer of Warranties**. THE LICENSED SOFTWARE AND ANY THIRD PARTY SOFTWARE IS PROVIDED TO YOU ON AN “AS-IS” BASIS. EXCEPT AS EXPRESSLY STATED HEREIN, PINECONE PROVIDES NO TECHNICAL SUPPORT, WARRANTIES OR REMEDIES FOR THE LICENSED SOFTWARE UNDER THIS EULA. PINECONE AND ITS SUPPLIERS, EMPLOYEES, AGENTS, OFFICERS AND PARTNERS (THE “**PINECONE PARTIES**”) DISCLAIM ALL EXPRESS, IMPLIED OR STATUTORY WARRANTIES RELATING TO THE LICENSED SOFTWARE, INCLUDING BUT NOT LIMITED TO, MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, AND NON-INFRINGEMENT. PINECONE DOES NOT WARRANT THAT USE OF THE LICENSED SOFTWARE WILL BE UNINTERRUPTED, OR ERROR-FREE, THAT DEFECTS WILL BE CORRECTED, OR THAT THE LICENSED SOFTWARE IS FREE OF VIRUSES OR OTHER HARMFUL COMPONENTS. IF APPLICABLE LAW REQUIRES ANY WARRANTIES WITH RESPECT TO THE LICENSED SOFTWARE, ALL SUCH WARRANTIES ARE LIMITED IN DURATION TO NINETY (90) DAYS FROM THE DATE OF DOWNLOAD. THE WARRANTY DISCLAIMER SET FORTH ABOVE IS A FUNDAMENTAL ELEMENT OF THE BASIS OF THE AGREEMENT BETWEEN PINECONE AND YOU. PINECONE WOULD NOT BE ABLE TO PROVIDE THE LICENSED SOFTWARE ON AN ECONOMIC BASIS WITHOUT SUCH LIMITATIONS. THE WARRANTY DISCLAIMER INURES TO THE BENEFIT OF THE PINECONE PARTIES.\n8. **Registration Information**. YOU ACKNOWLEDGE AND AGREE THAT WHEN YOU INSTALL AND REGISTER THE LICENSED SOFTWARE, THE LICENSED SOFTWARE TRANSMITS TO PINECONE CERTAIN INFORMATION THAT YOU PROVIDE DURING THE INSTALLATION OR REGISTRATION PROCESS, AS WELL AS COMPUTER OR DEVICE CONFIGURATION INFORMATION. YOU AGREE THAT PINECONE MAY COLLECT AND USE THIS DATA TO FACILITATE THE PROVISION OF SOFTWARE UPDATES, PRODUCT SUPPORT OR OTHER SERVICES TO YOU (IF ANY) RELATED TO THE LICENSED SOFTWARE. PINECONE MAY USE THIS INFORMATION PROVIDED SUCH USE IS IN ACCORDANCE WITH ITS PRIVACY POLICY.\n9. **Limitation on Liability**. TO THE MAXIMUM EXTENT PERMITTED UNDER APPLICABLE LAW, IN NO EVENT SHALL ANY PINECONE PARTY BE LIABLE FOR ANY INDIRECT, EXEMPLARY, SPECIAL, CONSEQUENTIAL OR INCIDENTAL DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, LOST PROFITS, REVENUES OR DATA, OR COSTS OF REPLACEMENT GOODS OR SERVICES, ARISING IN ANY WAY OUT OF THIS EULA OR YOUR USE OF OR INABILITY TO USE THE LICENSED SOFTWARE, HOWEVER CAUSED, REGARDLESS OF THE THEORY OF LIABILITY (CONTRACT, TORT, OR OTHERWISE) AND EVEN IF IT HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES AND NOTWITHSTANDING THE FAILURE OF ANY LIMITED REMEDY OF ITS ESSENTIAL PURPOSE. SOME JURISDICTIONS DO NOT ALLOW THE LIMITATION OF LIABILITY FOR PERSONAL INJURY, OR OF INCIDENTAL OR CONSEQUENTIAL DAMAGES, SO THIS LIMITATION MAY NOT APPLY TO YOU. In no event shall Pinecone’s total liability to you for all damages (other than as may be required by applicable law in cases involving personal injury) exceed the amount of fifty dollars ($50.00).\n10. **Term and Termination**. This EULA and the licenses granted hereunder are effective on the date you download the Licensed Software and shall continue unless this EULA is terminated by either party pursuant to this section. Pinecone may terminate this EULA immediately upon notice to you in the event that you materially breach any of the terms hereof. You may terminate this EULA at any time, with or without cause, by sending to Pinecone written notice indicating your intent to terminate your license (such notice to include your name and the subject “REMOVE”), either via email to info@pinecone.io or via mail or courier service to Pinecone Systems Inc, 548 Market St, PMB 19327, San Francisco, CA 94104-5401. Upon termination, the license granted hereunder shall terminate and you shall immediately destroy any copies of the Licensed Software in your possession, but the terms of Sections 2-15 will remain in effect.\n11. **For U.S. Government End Users**. The Licensed Software is a “commercial item” as that term is defined at 48 C.F.R. 2.101 (OCT 1995), and more specifically is “commercial computer software” and “commercial computer software documentation,” as such terms are used in 48 C.F.R. 12.212 (SEPT 1995). Consistent with 48 C.F.R. 12.212 and 48 C.F.R. 227.7202-1 through 227.7202-4 (JUNE 1995), the Licensed Software is provided to U.S. Government End Users (a) only as a commercial end item and (b) with only those rights as are granted to all other customers pursuant to the terms and conditions herein.\n12. **Export**. The Licensed Software and related technology are subject to U.S. export control laws and may be subject to export or import regulations in other countries. You agree to strictly comply with all such laws and regulations and acknowledge that you have the responsibility to obtain authorization to export, re-export, or import the Licensed Software and related technology, as may be required. You will indemnify and hold the Pinecone Parties harmless from any and all claims, losses, liabilities, damages, fines, penalties, costs and expenses (including attorney’s fees) arising from or relating to any breach by you of your obligations under this section.\n13. **Governing Law and Venue**. This EULA will be governed by the laws of the California without regard to its principles of conflicts of law. Any action or proceeding arising from or relating to this EULA must be brought in a federal or state court located in San Francisco, California, and each party irrevocably submits to the jurisdiction and venue of any such court in any such action or proceeding.\n14. **Miscellaneous**. Neither the rights nor the obligations arising under this EULA are assignable by you, and any such attempted assignment or transfer shall be void and without effect. The United Nations Convention on Contracts for the International Sale of Goods shall not apply to this EULA. Any notice to you may be provided by email. Any modifications of this Agreement must be in writing and agreed to by both parties.\n15. **Questions or Additional information**. If you have questions regarding this EULA, or wish to obtain additional information about the Licensed Software license, please send an e-mail to info@pinecone.io.\n\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fb90"
  },
  "filename": "contact.md",
  "title": "\"Contact\"",
  "category": "contact",
  "content": "---\ntitle: \"Contact\"\nlayout: contact\nheadline: \"Talk to an ML Infrastructure Expert\"\ncta: false\nintro: \"Contact us with questions or to see a personalized demo of Pinecone.\"\nhero2:\n  title: Talk to the <br><span>Vector Search</span> Experts\n  description: Send us your questions about Pinecone or details about your vector search needs. We’ll schedule a time to learn and share more with you.\ncontact:\n  info: |\n    Want to try Pinecone?<br>\n    [Start here.](/start/)\n\n    Press and general inquiries:<br>\n    info@pinecone.io\n\n    Support:<br>\n    support@pinecone.io<br>\n    [Read the Docs](/docs/)\n---\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fb91"
  },
  "filename": "privacy.md",
  "title": "info-page",
  "category": "\"Privacy Policy\"",
  "content": "---\nlayout: info-page\ntitle: \"Privacy Policy\"\nheadline: \"Privacy Policy\"\ncta: false\nintro: \"\"\n---\n\n*Updated: December 3, 2020*\n\nThis Website Privacy Policy (“Privacy Policy”) describes the privacy practices of Pinecone Systems, Inc. and that of its subsidiaries and affiliates (collectively, “Pinecone,” “we,” “us,” or “our”). This Privacy Policy explains how we collect, use, disclose, secure and otherwise process personal information from individuals in connection with our website and any other website that we own or control and which posts or links to this Privacy Policy (collectively, the “Websites”), and the rights and choices available to individuals with respect to their information. Pinecone may provide additional or supplemental privacy policies to individuals for specific products or services that we offer at the time we collect personal information. These supplemental privacy policies will govern how we may process the personal information in the context of the specific product or service.\n\nWe have designed the Websites for businesses and they are not intended for personal or household use. Accordingly, we treat all personal information covered by this Privacy Policy, including information about any visitors to our Websites, as pertaining to individuals acting in their capacity as business representatives (including as representatives our enterprise customers), rather than in their personal capacity.\n\nThis Privacy Policy does not govern how we may process personal information on behalf of our enterprise customers as part of the Pinecone Services. Please review the [Product Privacy Statement](/product-privacy/) to understand how we collect, use and otherwise process customer personal information in connection with our products and services (collectively, the “Hosted Services”).\n\nWe provide important information for individuals located in the European Union, European Economic Area, Switzerland and United Kingdom (collectively, “Europe” or “European”) [below](#notice-to-european-users).\n\n## Information Collected\n\n### _Information Collected Directly from You._\n\nWe collect personal information directly from you through your use of our Websites. This information may include:\n\n- **Business contact data**, such as your name, email address, and phone number.\n- **Employment information**, such as your employer’s name and job title.\n- **Location information**, such as when you authorize our Websites to access your general location information such as city, state or geographic area.\n- **Feedback or correspondence**, such as information you provide when you contact us with questions, feedback, or otherwise correspond with us online.\n- **Usage information**, such as information about how you use the Websites and interact with us, including information associated with any information you provide when you use any features of the Websites.\n- **Marketing information**, such as your preferences for receiving communications from us and details about how you engage with our communications.\n- **Other information** that we may collect that is not specifically listed here but that we will use in accordance with this Privacy Policy or as otherwise disclosed at the time of collection.\n\n### _Information we obtain from social media platforms._\n\nWe may maintain pages about us on social media platforms such as Facebook, LinkedIn, Twitter and other third-party platforms. When you visit or interact with our pages on those platforms, the platform provider’s privacy policy will apply to your interactions and their collection, use and processing of your personal information. You or the platforms may provide us with information through the platforms, and we will treat such information in accordance with this Privacy Policy.\n\n### _Information that we obtain from other third parties._\n\nWe may receive personal information about you from third-party sources. For example, a business partner may share your contact information with us if you have expressed interest in learning specially about our services. We may obtain your personal information from other third parties such as marketing and advertising partners, publicly-available sources and data providers.\n\n### _Referrals._\n\nWebsite visitors may have the opportunity to refer contacts to us. You may only submit a referral if you have permission to provide the referral’s contact information to us so that we may contact them.\n\n### _Automatically Collected Data._\n\nWe and our service providers as well as our advertising and business partners may use cookies, beacons, pixel tags and other tracking technologies to automatically log information about you, your computer or mobile device and your activity over time on the Websites, including:\n\n- **Device Data**, such as your computer or mobile device operating system type and version number, manufacturer and model, browser type, screen resolution, IP address, device identifier (such as the Google Advertising ID or Apple ID for Advertising), the website you visited before browsing our Websites and general location information such as city, state or geographic area.\n- **Online Activity Data**, such as browsing history, search history, whether you clicked on or opened one of our emails, which of our pages or screens you viewed, how long you spent on a page or screen, navigation paths between pages or screens, information about your activity on a page or screen, access times and duration of access.\n\nPlease visit the “[Options](#options)” section of this Privacy Policy for information on how to disable or opt-out of certain cookies and similar technologies.\n\n## Use of Information\n\nWe may use your personal information for the following purposes or as otherwise described in this Privacy Policy or at the time of collection:\n\n**<span id=\"to-operate-the-websites\">To operate the Websites</span>**. We may use personal information to operate the Websites and to provide related services, including to:\n\n- provide, operate and improve the Websites;\n- provide information about our services and product offerings;\n- enable security features of the Websites;\n- understand your needs and interests, and personalize your experience with the Websites and our communications;\n- respond to your requests, questions and feedback;\n- protect the security of the Websites and our systems; and\n- provide you with customer support when needed.\n\n**<span id=\"for-research-and-development\">For research and development</span>**. We may analyze use of the Websites to evaluate and improve the Websites, including by studying user demographics and use of the Websites. We may use Google Analytics for this purpose.\n\n**Advertising**. We may also work with third-party advertising partners who use cookies (such as the DoubleClick cookie) and similar technologies to deliver targeted advertising that is displayed on unaffiliated websites, to measure the effectiveness of advertising on behalf of our advertising partners, and to identify the audience most likely to respond to an advertisement. These advertisements are delivered by our advertising partners and may be targeted based on your use of the Websites or your activity elsewhere online.\n\n**<span id=\"to-comply-with-law\">To comply with law</span>**. We may use your personal information as we believe necessary or appropriate to comply with applicable laws, lawful requests, and legal process, such as to respond to subpoenas or requests from government authorities.\n\n**<span id=\"for-compliance-fraud-prevention-and-safety\">For compliance, fraud prevention and safety</span>**. We may use your personal information and disclose it to law enforcement, government authorities, and private parties as we believe necessary or appropriate to: (a) protect our, your or others’ rights, privacy, safety or property (including by making and defending legal claims); (b) enforce the terms and conditions that govern the Websites; and (c) protect, investigate and deter against fraudulent, harmful, unauthorized, unethical or illegal activity.\n\n**<span id=\"with-your-consent\">With your consent</span>**. In some cases, we may specifically ask for your consent to collect, use or share your personal information, such as when required by law.\n\n**To create anonymous, aggregated or de-identified data**. We may create anonymous, aggregated or de-identified data from your personal information and other individuals whose personal information we collect, by removing the information that makes the data personally identifiable to you. We may use and share such anonymous, aggregated or de-identified data for any purpose we deem appropriate, such as to maintain and improve the Website.\n\n**<span id=\"to-send-you-marketing-and-promotional-communications\">To send you marketing and promotional communications</span>**. We may send you Pinecone-related marketing and promotional communications about the products or services that we offer. You will have the ability to opt-out of our marketing and promotional communications as described in the “Options” section below.\n\n## Sharing of Information\n\n\nWe may share personal information with the following categories of recipients:\n\n**Service providers**. We may share your personal information with third-party companies and individuals that provide services on our behalf or help us operate the Websites (such as customer support, hosting, analytics, email delivery, marketing and database management services).\n\n**Advertising partners**. We may enable advertising partners to automatically collect information directly from our Websites for targeted marketing purposes.\n\n**Professional advisors**. We may share your personal information with professional advisors, such as lawyers, auditors, bankers and insurers, where necessary in the course of the professional services that they render to us.\n\n**Authorities and others**. We may share your personal information with law enforcement, government authorities, and private parties, as we believe in good faith to be necessary or appropriate for the compliance, fraud prevention and safety purposes described above.\n\n**Business transferees**. We may share your personal information with relevant participants in business transactions (or potential transactions) involving a corporate divestiture, merger, consolidation, acquisition, reorganization, sale or other disposition of all or any portion of the business or assets of, or equity interests in, (including, in connection with a bankruptcy or similar proceedings).\n\n## Options\n\n**Access or update information**. If your personal information changes, or if you no longer desire to have a relationship with us, you may correct or update by emailing privacy@pinecone.io. We will respond to your request within a reasonable timeframe.\n\n**Cookies & browser web storage**. Most browsers let you remove or reject cookies. To do this, follow the instructions in your browser settings. To prevent the use of Google Analytics relating to your use of our Websites, you can download and install the browser plug-in available here. Many browsers accept cookies by default until you change your settings. Please note that if you set your browser to disable cookies, the Websites may not work properly. Similarly, your browser settings may allow you to clear your browser web storage.\n\n**Targeted online advertising**. Some of the business partners that collect information about users’ activities on or through the Websites may be members of organizations or programs that provide choices to individuals regarding the use of their browsing behavior for purposes of targeted advertising. Users may opt out of receiving targeted advertising by:\n\n- **Blocking cookies in your browser**. Most browsers let you remove or reject cookies, including cookies used for interest-based advertising. To do this, follow the instructions in your browser settings. Many browsers accept cookies by default until you change your settings. For more information about cookies, including how to see what cookies have been set on your device and how to manage and delete them, visit www.allaboutcookies.org.\n- **Blocking advertising ID use in your mobile settings**. Your mobile device settings may provide functionality to limit use of the advertising ID associated with your mobile device for interest-based advertising purposes.\n- **Using privacy plug-ins or browsers**. You can block our websites from setting cookies used for interest-based ads by using a browser with privacy features, like [Brave](https://brave.com/), or installing browser plugins like [Privacy Badger](https://privacybadger.org/), [Ghostery](https://www.ghostery.com/) or [uBlock Origin](https://ublock.org/), and configuring them to block third party cookies/trackers.\n- **Platform opt-outs**. The following advertising partners offer opt-out features that let you opt-out of use of your information for interest-based advertising:\n\n    - Google: https://adssettings.google.com\n\n**Do Not Track**. Some Internet browsers may be configured to send “Do Not Track” signals to the online services that you visit. We currently do not respond to “Do Not Track” or similar signals. To find out more about “Do Not Track,” please visit http://www.allaboutdnt.com.\n\n**Opt out of marketing communications**. If at any time you wish to opt-out of our marketing and promotional emails, you may click the “unsubscribe” link in the email or otherwise contact us at privacy@pinecone.io. It may take up to 10 business days before you stop receiving promotional emails. This opt-out does not apply to operational communications, for example, confirmation emails and you may continue to receive service-related and other non-marketing emails.\n\n## Information about Children\n\nOur Websites are not directed to children under 16. If a parent or guardian becomes aware that his or her child has provided us with information without their consent, he or she should contact us at privacy@pinecone.io. We will delete such information from our files as soon as reasonably practicable.\n\n## Security\n\nWe maintain various physical, electronic and procedural safeguards designed to protect the personal information we collect. However, security risk is inherent in all internet and information technologies and we cannot guarantee the security of personal information.\n\n## Third-Party Websites\n\nThe Websites may contain links to websites and other online services operated by third parties. These links are not an endorsement of, or representation that we are affiliated with, any third-party and we cannot control, and take no responsibility for, the content or privacy practices of such third-party websites or online services operated by third parties. We encourage you to review the privacy policy and any other applicable terms for such third-party websites or online services operated by third parties.\n\n## International Data Transfers\n\nWe are headquartered in the United States and may have service providers in other countries, and your personal information may be transferred to the United States or other locations outside of your state, province, or country where privacy laws may not be as protective as those in your state, province, or country.\n\nEuropean users should read the important information provided in the “[Notice to European Users](#notice-to-european-users)” section to learn more about transfer of personal information outside of Europe.\n\n## Privacy Policy Updates\n\nWe reserve the right to modify this Privacy Policy at any time. If we make material changes to this Privacy Policy, we will notify you by updating the date of this Privacy Policy and posting it on the Websites. If required by law, we will also provide notification of changes in another way that we believe is reasonably likely to reach you, such as via email or another manner through the Websites.\n\nAny modifications to this Privacy Policy will be effective upon our posting the modified version (or as otherwise indicated at the time of posting). In all cases, your continued use of the Websites after the effective date of any modified Privacy Policy indicates your acceptance of the modified Privacy Policy.\n\n## Contact\n\nIf you have any further questions concerning this Privacy Policy, or would like to request to correct, update, or delete such information, please contact privacy@pinecone.io or mail us at:\n\nPinecone Systems, Inc<br>\n548 Market St<br>\nPMB 19327<br>\nSan Francisco, CA 94104-5401\n\n## Notice to European Users\n\nThe information provided in this “Notice to European Users” section applies only to individuals in Europe.\n\n**Personal information**. References to “personal information” in this Privacy Policy are equivalent to “personal data” governed by European data protection laws.\n\n**Controller and EU Representative**. Pinecone Systems, Inc. is the controller of your personal information covered by this Privacy Policy for purposes of European data protection laws.\n\n**Legal bases for processing**. We use your personal information only as permitted by law. Our legal bases for processing the personal information described in this Privacy Policy are described in the table below.\n\n<table class=\"table table-bordered\">\n  <tbody>\n    <tr>\n      <td style=\"width:50%\"><b>Processing purpose</b> (click link for details)<br>Details regarding each processing purpose listed below are provided in the section above titled “Use of Personal Information”.</td>\n      <td style=\"width:50%\"><b>Legal basis</b></td>\n    </tr>\n    <tr>\n      <td>\n        <ul class=\"my-0\">\n          <li><a href=\"#to-operate-the-websites\">To operate the Websites</a></li>\n        </ul>\n      </td>\n      <td>Processing is necessary to perform the contract governing our provision of services to you or to take steps that you request prior to entering an agreement for our services. If we have not entered into a contract with you, we process your personal information based on our legitimate interest in providing our services (including our Website) to you.</td>\n    </tr>\n    <tr>\n      <td>\n        <ul class=\"my-0\">\n          <li><a href=\"#for-research-and-development\">For research and development</a></li>\n          <li><a href=\"#to-send-you-marketing-and-promotional-communications\">To send you marketing and promotional communications</a></li>\n          <li><a href=\"#for-compliance-fraud-prevention-and-safety\">For compliance, fraud prevention and safety</a></li>\n        </ul>\n      </td>\n      <td>These activities constitute our legitimate interests. We do not use your personal information for these activities where our interests are overridden by the impact on you (unless we have your consent or are otherwise required or permitted to by law).</td>\n    </tr>\n    <tr>\n      <td>\n        <ul class=\"my-0\">\n          <li><a href=\"#to-comply-with-law\">To comply with law</a></li>\n        </ul>\n      </td>\n      <td>Processing is necessary to comply with our legal obligations.</td>\n    </tr>\n    <tr>\n      <td>\n        <ul class=\"my-0\">\n          <li><a href=\"#with-your-consent\">With your consent</a></li>\n        </ul>\n      </td>\n      <td>Processing is based on your consent. Where we rely on your consent you have the right to withdraw it any time in the manner indicated when you consented.</td>\n    </tr>\n  </tbody>\n</table>\n\n**Use for new purposes**. We may use your personal information for reasons not described in this Privacy Policy where permitted by law and the reason is compatible with the purpose for which we collected it. If we need to use your personal information for an unrelated purpose, we will notify you and explain the applicable legal basis.\n\n## Sensitive personal information\n\nWe ask that you not provide us with any sensitive personal information (e.g., information related to racial or ethnic origin, political opinions, religion or other beliefs, health, biometrics or genetic characteristics, criminal background or trade union membership) on or through the Websites, or otherwise to us. If you provide us with any sensitive personal information when you use the Websites, you must consent to our processing and use of such sensitive personal information in accordance with this Privacy Policy. If you do not consent to our processing and use of such sensitive personal information, you must not submit such sensitive personal information through our Websites.\n\n## Retention\n\nWe retain personal information for as long as necessary to fulfill the purposes for which we collected it, including for the purposes of satisfying any legal, accounting, or reporting requirements, to establish or defend legal claims, or for fraud prevention purposes.\nTo determine the appropriate retention period for personal information, we consider the amount, nature, and sensitivity of the personal information, the potential risk of harm from unauthorized use or disclosure of your personal information, the purposes for which we process your personal information and whether we can achieve those purposes through other means, and the applicable legal requirements.\nWhen we no longer require the personal information we have collected about you, we will either delete or anonymize it or, if this is not possible (for example, because your personal information has been stored in backup archives), then we will securely store your personal information and isolate it from any further processing until deletion is possible. If we anonymize your personal information (so that it can no longer be associated with you), we may use this information indefinitely without further notice to you.\n\n## Your rights\n\nEuropean data protection laws give you certain rights regarding your personal information. If you are located within Europe, you may ask us to take the following actions in relation to your personal information that we hold:\n\n- **Access**. Provide you with information about our processing of your personal information and give you access to your personal information.\n- **Correct**. Update or correct inaccuracies in your personal information.\n- **Delete**. Delete your personal information.\n- **Transfer**. Transfer a machine-readable copy of your personal information to you or a third party of your choice.\n- **Restrict**. Restrict the processing of your personal information.\n- **Object**. Object to our reliance on our legitimate interests as the basis of our processing of your personal information that impacts your rights.\n\nYou may submit these requests by email to privacy@pinecone.io or our postal address provided above. We may request specific information from you to help us confirm your identity and process your request. Applicable law may require or permit us to decline your request. If we decline your request, we will tell you why, subject to legal restrictions. If you would like to submit a complaint about our use of your personal information or our response to your requests regarding your personal information, you may contact us or submit a complaint to the data protection regulator in your jurisdiction. You can find your data protection regulator here.\n\n## Cross-Border Data Transfer\n\nIf we transfer your personal information out of Europe to a country not deemed by the European Commission to provide an adequate level of personal information protection, the transfer will be performed:\n\n- Pursuant to the recipient’s compliance with standard contractual clauses, or Binding Corporate Rules;\n- Pursuant to the consent of the individual to whom the personal information pertains; or\n- As otherwise permitted by applicable European requirements.\n\nYou may contact us if you want further information on the specific mechanism used by us when transferring your personal information out of Europe.\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fb92"
  },
  "filename": "user-agreement.md",
  "title": "info-page",
  "category": "\"Pinecone Services Agreement\"",
  "content": "---\nlayout: info-page\ntitle: \"Pinecone Services Agreement\"\nheadline: \"Pinecone Services Agreement\"\ncta: false\nintro: \"\"\n---\n\n*Last updated: May 26, 2022*\n\nPlease read this Pinecone Services Agreement (**“Agreement”**) before clicking the “I accept” button, and/or using the Pinecone Systems, Inc. (“**Pinecone**”) Hosted Services or the underlying Platform (as defined below).  By clicking the “I Accept” button, and by using the Hosted Services in any way, you and the entity that you represent (**“Customer”** or **“you”**) are unconditionally consenting to be bound by and becoming a party to this Agreement with Pinecone and you represent and warrant that you have the authority to bind such entity to this Agreement. If you do not unconditionally agree to all of the terms of this Agreement, use of the Hosted Services is strictly prohibited. If Customer has executed, or subsequently executes, a separate agreement with Pinecone with respect to use of the Hosted Services (“**Other Agreement**”), then the terms and conditions of such Other agreement shall govern and control your use of the Hosted Services.\n\nPLEASE NOTE THAT THIS AGREEMENT IS SUBJECT TO CHANGE BY PINECONE IN ITS SOLE DISCRETION AT ANY TIME.  When changes are made, Pinecone will make a new copy of this Agreement available on the Platform.  We will also update the “Last Updated” date at the top of the Agreement.  If we make material changes to this Agreement, we may (and, where required by law, will) also provide notification of changes in another way that we believe is reasonably likely to reach you, such as via e-mail or another manner through the Platform (which may include posting an announcement on our Platform). Pinecone may require you to provide consent to the updated Agreement in a specified manner before further use of the Hosted Services is permitted.  If you do not agree to any of the changes after receiving a notice of such changes, you shall stop accessing the Platform and using Hosted Services. Otherwise, your continued use of the Hosted Services (or access to the Platform) constitutes your acceptance of such changes.  \n\n1. **PLATFORM AND HOSTED SERVICES**. \n\n\n    1. <span style=\"text-decoration:underline;\">Access</span>.  Pinecone’s proprietary solution is a platform that leverages machine learning to improve serving capacity, including hosting, real time scoring, operations, scaling, reporting, monitoring, and testing to solve problems related to “many-to-many relationships,” such as search, retrieval, ranking, matching, and deduplication (the “**Platform**”). Customer wishes to access and utilize the Platform, and Pinecone desires to make the Platform available to Customer, subject to the following terms and conditions (the provisioning of the Platform, the “**Hosted Services**”). Pinecone hereby grants Customer, during the term of this Agreement, a non-exclusive, non-transferable, non-sublicensable right and license to access and use the Hosted Services, including by installing any downloadable components of the Platform made available by Pinecone and using and accessing any other materials provided to Customer by Pinecone in connection with the Hosted Services (“**Materials**”), solely for Customer’s internal business purposes. Customer agrees to provide only truthful and accurate information in connection with obtaining usernames and passwords (including API access keys) (collectively, “**User IDs**”) to access the Hosted Services. Customer is responsible for all acts and omissions of any users of Customer’s account or User ID and will undertake reasonable efforts to make all such users aware of the provisions of this Agreement as applicable to such users’ use of the Hosted Services, and will cause such users to comply with such provisions. Any act or omission by any such third party which, if undertaken by Customer, would constitute a breach of this Agreement, will be deemed a breach of this Agreement by Customer.  Customer is responsible for maintaining the confidentiality of the User IDs, and is solely responsible for all activities that occur thereunder.  Customer agrees to notify Pinecone promptly of any actual or suspected unauthorized use of its account or User ID, or any other breach or suspected breach of this Agreement. Pinecone reserves the right to terminate any User ID that Pinecone reasonably determines may have been used by an unauthorized third party.\n\n\n    2. <span style=\"text-decoration:underline;\">Free or Trial Subscriptions</span>. From time to time and in its sole discretion, Pinecone may offer limited free access to the Hosted Services (for purposes of this Section, “**Trial Services**”) so that you can test whether the Hosted Services meet your needs.  Because Trial Services are limited and are provided free of charge, to the extent permitted by law, (i) we make them available to you solely “AS IS” without any warranties of any kind (and we are under no obligation to provide you with support for Trial Services); (ii) we may discontinue the Trial Services or your ability to use them at any time, with or without notice and without any further obligations to you; (iii) you agree that our indemnification obligations under Section 12 do not extend to any claims related to your use or inability to use the Trial Services; and (iv) you agree that our support obligations under Section 5 do not apply to your use of the Trial Services. Except as expressly stated in this Section 1.2, the Trial Services shall be subject in all respects to the terms and conditions of this Agreement. Pinecone may allow you to continue using Hosted Services provided as Trial Services on a paid basis, but your continued use is subject to the payment of the applicable Fees in accordance with Section 6.\n\n\n    3. <span style=\"text-decoration:underline;\">Open Source</span>. Certain items of software that may be provided to Customer as part of the Platform  are subject to “open source” or “free software” licenses (**“Open Source Software”**).  Some of the Open Source Software is owned by third parties.  The Open Source Software is not subject to the terms and conditions of Sections 1.1 or 12.  Instead, each item of Open Source Software is licensed under the terms of the end-user license that accompanies such Open Source Software. Nothing in this Agreement limits Customer’s rights under, or grants Customer rights that supersede, the terms and conditions of any applicable end user license for the Open Source Software. If required by any license for particular Open Source Software, Pinecone makes such Open Source Software, and Pinecone’s modifications to that Open Source Software, available by written request at the notice address specified herein.\n\n\n    4. <span style=\"text-decoration:underline;\">Changes</span>.  From time to time, Pinecone reserves the right to release updates to or upgrades of the Hosted Services, including new versions of the Hosted Services, and to otherwise change or discontinue any aspect or feature of the Hosted Services. Changes may not be consistent across all platforms and devices. Pinecone will use commercially reasonable efforts to notify Customer (including posting through the Hosted Services) of changes to the Hosted Services that Pinecone believes will materially affect use of the Hosted Services.\n\n\n2. **RESTRICTIONS ON USE.** Customer shall not itself, or through any parent, subsidiary, affiliate, agent or other third party: (i) license, sublicense, sell, resell, transfer, assign, distribute or make available, in whole or in part, the Hosted Services or the Materials to any third party; (ii) copy, translate, decompile, disassemble, reverse-engineer or otherwise modify or make derivative works based upon the Hosted Services or the Materials; (iii) build a product using similar ideas, features, functions or graphics of the Hosted Services or Materials or otherwise engage in competitive benchmarking; (iv) disclose the results of any benchmark test of the Hosted Services or the Materials to any third party without Pinecone’s prior written approval; (v) use the Hosted Services to (A) send or store infringing, threatening, harassing, defamatory, libelous, obscene, pornographic, indecent or otherwise unlawful or tortious materials, including materials harmful to children or violating third-party privacy rights, (B) send or store materials containing software viruses, worms, Trojan horses or other harmful computer code, files, scripts, agents or programs, or (C) engage in any of Customer’s time-critical, or mission-critical functions; (vi) interfere with or disrupt the integrity or performance of the Hosted Services or the data contained therein, or attempt to probe, scan or test vulnerability of the Hosted Services without prior authorization of Pinecone; or (vii) attempt to gain unauthorized access to the Hosted Services or its related systems or networks, including the Platform itself.\n<br>\n<br>\n3. **DATA**. \n\n    1. <span style=\"text-decoration:underline;\">License to Customer Data</span>. “**Customer Data**” means any data, information, or materials that Customer discloses or submits to Pinecone in the course of using the Hosted Services.  Customer hereby grants Pinecone and its suppliers a non-exclusive, royalty-free license to access, use, reproduce, modify and display the Customer Data for the purposes of (i) providing the Hosted Services, (ii) exercising its rights and obligations under this Agreement; (iii) generating Aggregated and Anonymous Data (as defined below); and (iv) complying with its obligations under law.  All rights in and to the Customer Data not expressly granted herein are retained by Customer. Without limiting the foregoing, Customer will be solely responsible for providing all appropriate notices to third parties (including all employees, agents, and independent contractors (collectively, “**Personnel**”)) and obtaining from third parties (including Personnel) all necessary consents and rights for Pinecone to use the Customer Data submitted by or on behalf of Customer for the purposes set forth in this Agreement, including all consents required in accordance with all applicable privacy laws. Customer shall immediately notify, and address with, Pinecone any complaints or claims by Personnel with respect to the sharing of the Customer Data involving such Personnel.\n\n\n    2. <span style=\"text-decoration:underline;\">Performance Data</span>. “**Performance Data**” means any analytics or similar data collected, generated or processed by Pinecone based on Customer’s access to and use of the Platform or Hosted Services (“**Performance Data**”).  Performance Data will be owned by Pinecone, and Pinecone may collect and use such Performance Data for any lawful purpose, _provided_ Pinecone will only disclose Performance Data to third parties, including its subcontractors, for the purposes of facilitating the Hosted Services, for internal purposes, including to improve its products and services, to perform its other obligations and exercise its rights under this Agreement, or as otherwise required by law.  \n\n\n    7. <span style=\"text-decoration:underline;\">Aggregated and Anonymous Data</span>. Notwithstanding anything to the contrary herein, Customer agrees that Pinecone may obtain and use Customer Data and Performance Data to create aggregated, anonymized or deidentified data or information of similar form that does not permit the identification of Customer or any individual or entity (the “**Aggregated and Anonymous Data**”).  Customer further agrees that Pinecone shall own such Aggregated and Anonymous Data and may retain, use and disclose such data for any lawful business purpose, including to improve its products and services.\n\n\n    8. <span style=\"text-decoration:underline;\">Customer Responsibility</span>. Customer will be responsible for providing all Customer Data to Pinecone and will provide such Customer Data in a format consistent with the requirements set forth in the documentation (or as otherwise specified by Pinecone). Errors in loading Customer Data into the Platform may cause Customer Data to be rejected by the Platform and Pinecone will have no responsibility for any related impact on Customer’s ability to access or use the Platform. \n\n\n4. **PROPRIETARY RIGHTS.**\n\n\n    1. <span style=\"text-decoration:underline;\">Ownership</span>. Pinecone and its suppliers own all right, title and interest in and to the Hosted Services (including the Platform, but excluding any Customer Data hosted therein), Performance Data, Aggregated and Anonymous Data, and any Materials, including but not limited to concepts, specifications, integration scenarios and examples of code, and all intellectual property rights in each of the foregoing.  All rights in and to the Hosted Services, including the Platform, and Materials not expressly granted herein are retained by Pinecone.\n\n\n    10. <span style=\"text-decoration:underline;\">Feedback</span>. Notwithstanding anything to the contrary herein, Pinecone may freely use and incorporate into Pinecone’s products and services any suggestions, enhancement requests, recommendations, corrections, or other feedback provided by Customer or by any users of the Hosted Services, the Platform, and the Materials (“**Feedback**”). Customer acknowledges and agrees that all Feedback and all intellectual property rights therein are the exclusive property of Pinecone, and hereby assigns to Pinecone, all right, title and interest to any and all Feedback.\n\n\n    11. <span style=\"text-decoration:underline;\">Publicity</span>. Pinecone may use Customer’s name and logo (“**Customer** **Marks**”) in its Customer list (including on Pinecone’s website, social media and in sales and marketing materials) in the same manner in which it uses the names of its other customers. Pinecone shall use Customer Marks in accordance with Customer’s applicable branding guidelines and Pinecone may not use Customer’s name in any other way without Customer’s prior written consent (with email consent deemed sufficient).\n\n\n5. **SUPPORT SERVICES.** Subject to the terms and conditions of this Agreement, Pinecone will exercise commercially reasonable efforts to provide support for the use of the Platform and Hosted Services to Customer.<br><br>\n\n6. **Fees, Payment, and Taxes.**\n\n    1. <span style=\"text-decoration:underline;\">Fees</span>. The fees for access to and use of the Hosted Services (“**Fees**”) are based on Platform usage and, unless otherwise specified herein, are charged at the rates set forth on [the Pinecone Pricing webpage](https://www.pinecone.io/pricing/). Pinecone reserves the right to change the Fees or its pricing model at any time during the term.  Any such change to Fees, rates or pricing shall go into effect no earlier than thirty (30) days after the change is posted to the Pinecone Pricing webpage. Unless otherwise expressly specified, the Fees are calculated at the end of each month based on Customer’s usage of the Hosted Services. \n\n\n    13. <span style=\"text-decoration:underline;\">Invoicing and Payment</span>. All Fees are quoted in United States Dollars and, except as set forth otherwise in this Agreement, are non-refundable. Pinecone will invoice Customer monthly for the Fees.  Fees are payable thirty (30) days from the date of invoice and will be deemed overdue if they remain unpaid thereafter.  \n\n\n    14. <span style=\"text-decoration:underline;\">Late Payments</span>. Payments by Customer that are past due will be subject to interest at the rate of one and one-half percent (1½%) per month (or, if less, the maximum allowed by applicable law) on that overdue balance. Customer will be responsible for any costs resulting from collection by Pinecone of any such overdue balance, including, without limitation, reasonable attorneys’ fees and court costs.  Pinecone reserves the right (in addition to any other rights or remedies Pinecone may have) to suspend Customer’s access to the Platform and the Hosted Services if any Fees are more than fifteen (15) days overdue until such amounts are paid in full.\n\n\n    15. <span style=\"text-decoration:underline;\">Taxes</span>. The Fees do not include taxes, duties or charges of any kind.  If Pinecone is required to pay or collect any local, value added, goods and services taxes or any other similar taxes or duties arising out of or related to this Agreement (not including taxes based on Pinecone’s income), then such taxes and/or duties shall be billed to and paid by Customer. \n\n\n    16. <span style=\"text-decoration:underline;\">Withholding Payments</span>.  If any applicable law requires Customer to withhold amounts from any payments to Pinecone hereunder, then Customer will perform such obligations consistent with the provisions of this section.  Customer will effect such withholding, remit such amounts to the appropriate taxing authorities and promptly furnish Pinecone with tax receipts evidencing the payments of such amounts. The sum payable by Customer upon which the deduction or withholding is based will be increased to the extent necessary to ensure that, after such deduction or withholding, Pinecone receives and retains, free from liability for such deduction or withholding, a net amount equal to the amount Pinecone would have received and retained in the absence of such required deduction or withholding. \n\n\n7. **CONFIDENTIALITY.**  \n\n    1. “**Confidential Information**” means any proprietary, confidential and/or trade secret information concerning or relating to the property, business and affairs of the party disclosing such information (the “**Disclosing Party**”) to the other party (the “**Receiving Party**”) under this Agreement, or any other information that the Receiving Party would reasonably understand to be confidential given the nature of the information or the circumstances surrounding disclosure. For the avoidance of doubt, Customer’s Confidential Information includes Customer Data, other than **Personal Data** (as defined in the Customer Data Protection Addendum). Such Personal Data is governed by Section 8 of this Agreement, and the Customer Data Protection Addendum incorporated herein by reference. Pinecone’s Confidential Information includes Performance Data, the proprietary and non-public portions of the Hosted Services, and the Materials provided in connection with this Agreement. Confidential Information shall not include information that a Receiving Party can demonstrate by reasonably sufficient evidence (i) was known to the Receiving Party before receipt thereof under this Agreement, (ii) is disclosed to the Receiving Party by a third party who has a right to make such disclosure without any obligation of confidentiality to the Disclosing Party, (iii) is or becomes generally known to the public or in the trade without violation of either this Agreement by the Receiving Party or any confidentiality obligation owed to the Disclosing Party by any third party, (iv) is furnished by the Disclosing Party to a third party without restriction on subsequent disclosure, or (v) is independently developed by the Receiving Party or its employees or subcontractors without reliance on such Confidential Information.\n\n    18. The Receiving Party shall (i) not disclose Confidential Information to third parties (except to its directors, employees, agents or subcontractors to the extent such disclosure is necessary for the performance of this Agreement and who have agreed to restrictions similar to those set forth in this Section or except as may be required by law), (ii) not use Confidential Information except for the purposes contemplated by this Agreement and (iii) use at least the same degree of care to safeguard Confidential Information that it uses to protect its own confidential and proprietary information, but in no event less than a reasonable degree of care under the circumstances.\n\n\n    19. Upon expiration or termination of this Agreement, or upon request of the Disclosing Party, the Receiving Party shall return to the Disclosing Party or destroy all Confidential Information in the possession of the Receiving Party. Each party acknowledges that it will not obtain any right, title or interest in or to the Confidential Information of the other Party as a result of disclosure under this Agreement.\n\n\n    20. The parties acknowledge that the Confidential Information is unique and valuable, and that the Disclosing Party will have no adequate remedy at law if the Receiving Party does not comply with its obligations under this Agreement.  Therefore, the Disclosing Party shall have the right, in addition to any other rights it may have, to seek in any court of competent jurisdiction temporary, preliminary and permanent injunctive relief to restrain any breach, threatened breach, or otherwise to specifically enforce any obligations of the Receiving Party if the Receiving Party fails to perform any of its obligations under this Agreement. \n\n\n8. **DATA PRIVACY; SECURITY**. \n\n    1. <span style=\"text-decoration:underline;\">Data Privacy</span>. Each party shall comply with their respective obligations under the Customer Data Processing Addendum located at https://www.pinecone.io/dpa/ (or such successor URL as may be designated by Pinecone) (“**DPA**”), which is incorporated herein by this reference. By each party’s acceptance and agreement to the terms and conditions of this Agreement, each party agrees to the terms of the DPA, including the Standard Contractual Clauses as “Data exporter” in the case of Customer, and as “Data importer” in the case of Pinecone.\n\n    22. <span style=\"text-decoration:underline;\">Security</span>. Pinecone will use reasonable technical and organizational measures designed to prevent unauthorized access, use, alteration, or disclosure of, Customer Data. However, Pinecone shall have no responsibility for errors in transmissions or any other causes beyond Pinecone’s reasonable control.\n\n    23. <span style=\"text-decoration:underline;\">Customer Responsibility for Data and Security</span>. Customer will have access to the Customer Data and will be responsible for all changes to and/or deletions of Customer Data and the security of all passwords and other usernames and passwords required in order the access the Platform and the Services. Upon request to Customer’s account manager, Pinecone may facilitate for Customer the ability to export Customer Data from the Platform. Customer will have the sole responsibility for the accuracy, quality, integrity, legality, reliability, and appropriateness of all Customer Data.  Pinecone is not obligated to back up any Customer Data; the Customer is solely responsible for creating backup copies of any Customer Data at Customer’s sole cost and expense. \n\n9. **WARRANTIES.**\n\n    1. <span style=\"text-decoration:underline;\">Pinecone Limited Warranty</span>.  Pinecone warrants to you that during the term of this Agreement the Platform will perform materially in accordance with the functionality described in the documentation that Pinecone makes available for the Platform. Your sole and exclusive remedy for a breach of this warranty will be that Pinecone will use commercially reasonable efforts to modify the applicable Platform to achieve the functionality described above. This warranty is void in the event you are in breach of this Agreement. For clarity, this warranty will not apply to any trial or beta services.\n\n\n    25. <span style=\"text-decoration:underline;\">Customer Warranty</span>. Customer represents and warrants that:\n        * it has procured all applicable consents required to provide the Customer Data to Pinecone for the performance of the Hosted Services, including in accordance with Section 3.1, and all applicable privacy laws;\n        * the Customer Data will not: (a) infringe or misappropriate any third party’s intellectual property rights; (b) be deceptive, defamatory, obscene, pornographic or unlawful; (c) contain any viruses, worms or other malicious computer programming codes intended to damage the Hosted Services, the Platform or Materials; and (d) otherwise violate the rights of a third party (including under all applicable privacy laws); \n        * it will use the Hosted Services, Platform and Materials in accordance with the terms herein and all applicable laws; and \n        * Customer shall not upload to the Hosted Services any Customer Data that contains any sensitive personal information (such as financial, medical or other sensitive personal information such as government IDs, passport numbers or social security numbers).\n\n        Customer agrees that any use of the Hosted Services, Platform or Materials contrary to or in violation of the representations and warranties of Customer in this Section 9.2 constitutes unauthorized and improper use of the Hosted Services, Platform or Materials, as applicable.\n\n10. **DISCLAIMER.** \n\n    1. <span style=\"text-decoration:underline;\">Disclaimer</span>. TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE HOSTED SERVICES, THE PLATFORM, AND ALL OTHER MATERIALS ARE PROVIDED “AS IS” AND WITH ALL FAULTS. EXCEPT FOR THE LIMITED WARRANTY PROVIDED IN SECTION 9.1, PINECONE MAKES NO WARRANTIES WITH RESPECT TO THE HOSTED SERVICES, THE PLATFORM OR THE MATERIALS, WHETHER EXPRESS OR IMPLIED, INCLUDING WARRANTIES OF TITLE, ACCURACY, INTERFERENCE WITH CUSTOMER’S QUIET ENJOYMENT, SYSTEM INTEGRATION, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK ARISING OUT OF THE USE OR PERFORMANCE OF THE HOSTED SERVICES, THE PLATFORM, OR THE MATERIALS IS WITH CUSTOMER.  NO ORAL OR WRITTEN INFORMATION OR ADVICE GIVEN BY PINECONE OR ITS AGENTS OR EMPLOYEES SHALL IN ANY WAY INCREASE THE SCOPE OF THIS WARRANTY. \n    27. CUSTOMER ACKNOWLEDGES AND AGREES THAT PINECONE IS NOT LIABLE, AND CUSTOMER AGREES NOT TO SEEK TO HOLD PINECONE LIABLE, FOR THE CONDUCT OF THIRD PARTIES, INCLUDING PROVIDERS OF THE THIRD-PARTY SERVICES, AND THAT THE RISK OF INJURY  FROM SUCH THIRD-PARTY SERVICES RESTS ENTIRELY WITH CUSTOMER.\n\n    28. FROM TIME TO TIME, PINECONE MAY OFFER NEW “BETA” FEATURES OR TOOLS WITH WHICH CUSTOMER MAY EXPERIMENT. SUCH FEATURES OR TOOLS ARE OFFERED SOLELY FOR EXPERIMENTAL PURPOSES AND WITHOUT ANY WARRANTY OF ANY KIND, AND MAY BE MODIFIED OR DISCONTINUED AT PINECONE’S SOLE DISCRETION.  THE PROVISIONS OF THIS SECTION APPLY WITH FULL FORCE TO SUCH FEATURES OR TOOLS.\n\n\n11. **LIMITATION OF LIABILITY.**\n\n    1. <span style=\"text-decoration:underline;\">Generally</span>. NEITHER PARTY SHALL BE LIABLE TO THE OTHER PARTY NOR TO ANY THIRD PARTIES FOR LOST PROFITS OR LOST DATA OR FOR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, RELIANCE OR PUNITIVE LOSSES OR DAMAGES HOWSOEVER ARISING UNDER THIS AGREEMENT OR IN CONNECTION WITH THE PLATFORM, WHETHER UNDER CONTRACT, TORT OR OTHERWISE, WHETHER FORESEEABLE OR NOT, AND REGARDLESS WHETHER SUCH PARTY HAS BEEN ADVISED OF THE POSSIBILITY THAT SUCH DAMAGES MAY ARISE, OCCUR OR RESULT.  IN NO EVENT SHALL PINECONE BE LIABLE FOR PROCUREMENT COSTS OF SUBSTITUTE PRODUCTS OR SERVICES. EACH PARTY’S AGGREGATE CUMULATIVE LIABILITY ARISING OUT OF OR IN ANY WAY CONNECTED TO THIS AGREEMENT WILL IN NO EVENT EXCEED THE GREATER OF (A) THE AMOUNT OF FEES PAID BY CUSTOMER UNDER THIS AGREEMENT IN THE TWELVE (12) MONTHS IMMEDIATELY PRECEDING THE EVENT GIVING RISE TO THE CLAIM OR (B) ONE HUNDRED UNITED STATES DOLLARS ($100.00). THE PARTIES AGREE THAT THE LIMITATIONS OF LIABILITY SET FORTH IN THIS SECTION SHALL SURVIVE AND CONTINUE IN FULL FORCE AND EFFECT DESPITE ANY FAILURE OF CONSIDERATION OR OF AN EXCLUSIVE REMEDY. THE PARTIES ACKNOWLEDGE THAT THIS AGREEMENT HAS BEEN ENTERED INTO IN RELIANCE UPON THESE LIMITATIONS OF LIABILITY AND THAT ALL SUCH LIMITATIONS FORM AN ESSENTIAL BASIS OF THE BARGAIN BETWEEN THE PARTIES.  \n\n    30. <span style=\"text-decoration:underline;\">Basis of the Bargain</span>. THESE LIMITATIONS OF LIABILITY WILL APPLY NOTWITHSTANDING ANY FAILURE OF ESSENTIAL PURPOSE OF ANY LIMITED REMEDY.  THE PARTIES ACKNOWLEDGE THAT THE PRICES HAVE BEEN SET AND THE AGREEMENT ENTERED INTO IN RELIANCE UPON THESE LIMITATIONS OF LIABILITY AND THAT ALL SUCH LIMITATIONS FORM AN ESSENTIAL BASIS OF THE BARGAIN BETWEEN THE PARTIES.  THE PROVISIONS OF THIS AGREEMENT ALLOCATE THE RISKS UNDER THIS AGREEMENT BETWEEN PINECONE AND CUSTOMER.  PINECONE’S FEES FOR THE HOSTED SERVICES REFLECTS THIS ALLOCATION OF RISK AND THE LIMITATION OF LIABILITY SPECIFIED HEREIN.\n\n    31. <span style=\"text-decoration:underline;\">Exclusions</span>. THESE LIMITATIONS OF LIABILITY DO NOT APPLY TO: (A) A BREACH BY A PARTY OF SECTIONS 1.1, 2,  OR 7; (B) CUSTOMER’S OBLIGATIONS UNDER SECTION 9.2; (C) INDEMNIFICATION OBLIGATIONS; OR (D) ANY DEATH OR PERSONAL INJURY CAUSED BY EITHER PARTY’S NEGLIGENCE, GROSS NEGLIGENCE, OR WILLFUL MISCONDUCT.\n\n12. **INDEMNIFICATION**\n\n    1. <span style=\"text-decoration:underline;\">By Pinecone</span>. Pinecone will defend at its expense any suit brought against Customer, and will pay any settlement Pinecone makes or approves, or any damages finally awarded in such suit, insofar as such suit is based on a claim by any third party alleging that the Platform or the Hosted Services infringes such third party’s patents, copyrights or trade secret rights under applicable laws of any jurisdiction within the United States of America. If any portion of the Platform or Hosted Services becomes, or in Pinecone’s opinion is likely to become, the subject of a claim of infringement (“**Infringing Technology**”), Pinecone may, at Pinecone’s option: (i) procure for Customer the right to continue using the Infringing Technology; (ii) replace the Infringing Technology with non-infringing software or services which do not materially impair the functionality of the Platform or Hosted Services; (iii) modify the Infringing Technology so that it becomes non-infringing; or (iv) terminate this Agreement and refund any unused prepaid Fees for the remainder of the term then in effect, and upon such termination, Customer will immediately cease all use of the Platform and Hosted Services. Notwithstanding the foregoing, Pinecone will have no obligation under this section or otherwise with respect to any infringement claim based upon: (A) any use of the Platform or Hosted Services not in accordance with this Agreement or as specified in the Documentation; (B) any use of the Platform or Hosted Services in combination with other products, equipment, software or data not supplied by Pinecone; or (C) any modification of the Platform or Hosted Services by any person other than Pinecone or its authorized agents (collectively, the “**Exclusions**” and each, an “**Exclusion**”). This section states the sole and exclusive remedy of Customer and the entire liability of Pinecone, or any of the officers, directors, employees, shareholders, contractors or representatives of the foregoing, for infringement claims and actions. \n\n\n    33. <span style=\"text-decoration:underline;\">By Customer</span>. Customer will defend at its expense any suit brought against Pinecone, and will pay any settlement Customer makes or approves, or any damages finally awarded in such suit, insofar as such suit is based on a claim arising out of or relating to: (a) an Exclusion, or (b) Customer’s breach or alleged breach of Section 9.2. This section states the sole and exclusive remedy of Pinecone and the entire liability of Customer, or any of its officers, directors, employees, shareholders, contractors or representatives, for the claims and actions described herein.\n\n\n    34. <span style=\"text-decoration:underline;\">Procedure</span>. The indemnifying Party’s obligations as set forth above are expressly conditioned upon each of the foregoing: (a) the indemnified Party promptly notifying the indemnifying Party in writing of any threatened or actual claim or suit; (b) the indemnifying Party having sole control of the defense or settlement of any claim or suit; and (c) the indemnified Party cooperating with the indemnifying Party to facilitate the settlement or defense of any claim or suit.\n\n\n13. **TERM.**\n\n    1. <span style=\"text-decoration:underline;\">Term and Termination</span>. The term of this Agreement commences on the earlier of your clicking of the “I ACCEPT” button, and when you first access the Hosted Services and shall continue for so long as you access the Hosted Services. If Pinecone becomes aware of any possible violations by Customer of this Agreement, Pinecone may, in its discretion, immediately terminate or suspend Customer’s access to the Hosted Services (including the Platform). Customer can discontinue using the Hosted Services at any time. Upon termination, Customer shall immediately cease all use of the Hosted Services (including the Platform), and delete or destroy all copies of any other Materials in the possession or control of Customer. \n\n\n    36. <span style=\"text-decoration:underline;\">Survival</span>. Sections 1.1, 1.2, 1.3, 2, 3 - 7, 9 - 12, 13.2, and 14 shall survive termination or expiration of this Agreement. \n\n\n14. **GENERAL**. \n\n\n    1. <span style=\"text-decoration:underline;\">Trade Control Laws</span>.  Customer shall comply with all export control and economic sanctions laws and regulations (collectively, “**Trade Control Laws**”) applicable to Customer in the performance of this Agreement.  Pinecone shall not be required under this Agreement to be directly or indirectly involved in the provision of goods, software, services and/or technical data that may be prohibited by applicable Trade Control Laws.  Customer represents and covenants that it (a) is not identified on, or owned or controlled by or acting on behalf of any individuals or entities identified on, applicable government restricted party lists (“**Restricted Parties**”); (b) is not located in, organized under the laws of or ordinarily resident in Cuba, Iran, North Korea, Sudan, Syria or Crimea (region of Ukraine/Russia) (“**Restricted Countries**”); and (c) will not directly or indirectly export, re-export or otherwise transfer any goods, technology or services covered by the Agreement to or for use in or from Restricted Countries or Restricted Parties.\n\n\n    38. <span style=\"text-decoration:underline;\">Assignment</span>.  Customer may not assign or otherwise transfer this Agreement or any of its rights or obligations, in whole or in part, without the prior written consent of Pinecone, and any unauthorized assignment or transfer shall be void, provided, however, that either party shall have the right to assign the Agreement, without the prior written consent of the other party, to the successor entity in the event of merger, corporate reorganization or a sale of all or substantially all of such party’s assets. This Agreement shall be binding upon the parties and their respective successors and permitted assigns.\n\n\n    39. <span style=\"text-decoration:underline;\">Notices</span>.  Where Pinecone requires that you provide an e-mail address, you are responsible for providing Pinecone with your most current e-mail address.  In the event that the last e-mail address you provided to Pinecone is not valid, or for any reason is not capable of delivering to you any notices required/ permitted by the Agreement, Pinecone’s dispatch of the e-mail containing such notice will nonetheless constitute effective notice.  You may give notice to Pinecone at the following address: Pinecone Systems Inc, 548 Market St, PMB 19327, San Francisco, CA 94104-5401, Attn: NOTICE, or info@pinecone.io. Such notice shall be deemed given when received by Pinecone by letter delivered by nationally recognized overnight delivery service or first class postage prepaid mail at the above address, or by electronic mail.\n\n\n    40. <span style=\"text-decoration:underline;\">Choice of Law; Venue</span>.  This Agreement shall be governed by laws of the State of California, without regard to the choice of conflicts of law provisions of any jurisdiction.  Customer submits to the exclusive jurisdiction and venue of the federal and state courts located in Santa Mateo County, California for any disputes arising out of or related to this Agreement.  \n\n\n    41. <span style=\"text-decoration:underline;\">Severability</span>.  If any term of this Agreement is found by competent judicial authority to be unenforceable in any respect, the validity of the remainder of this Agreement will be unaffected, provided that such unenforceability does not materially affect the parties’ rights under this Agreement.\n\n\n    42. <span style=\"text-decoration:underline;\">Waiver</span>.  An effective waiver under this Agreement must be in writing signed by the party waiving its right.  A waiver by either party of any instance of the other party’s noncompliance with any obligation or responsibility under this Agreement will not be deemed a waiver of subsequent instances. \n\n\n    43. <span style=\"text-decoration:underline;\">Independent Contractor</span>.  Neither this Agreement nor the cooperation of the parties contemplated under this Agreement shall be deemed or construed to create any partnership, joint venture or agency relationship between the parties.  Except as otherwise expressly permitted in this Agreement, neither party is, nor will either party hold itself out to be, vested with any power or right to bind the other party contractually or act on behalf of the other party as a broker, agent or otherwise\n\n\n    44. <span style=\"text-decoration:underline;\">Force Majeure</span>.  Any delay in the performance of any duties or obligations of either party (except for the obligation to pay Fees owed) will not be considered a breach of this Agreement if such delay is caused by a labor dispute, shortage of materials, war, fire, earthquake, typhoon, flood, natural disasters, governmental action, pandemic/epidemic, cloud-service provider outages any other event beyond the control of such party, provided that such party uses reasonable efforts, under the circumstances, to notify the other Party of the circumstances causing the delay and to resume performance as soon as possible.\n\n\n    45. <span style=\"text-decoration:underline;\">U.S. Government Restricted Rights</span>. If Customer is a government end user, then this provision also applies to Customer. The software contained within the Platform and the Services and provided in connection with this Agreement has been developed entirely at private expense, as defined in FAR section 2.101, DFARS section 252.227-7014(a)(1) and DFARS section 252.227- 7015 (or any equivalent or subsequent agency regulation thereof), and is provided as “commercial items,” “commercial computer software” and/or “commercial computer software documentation.” Consistent with DFARS section 227.7202 and FAR section 12.212, and to the extent required under U.S. federal law, the minimum restricted rights as set forth in FAR section 52.227-19 (or any equivalent or subsequent agency regulation thereof), any use, modification, reproduction, release, performance, display, disclosure or distribution thereof by or for the U.S. Government shall be governed solely by this Agreement and shall be prohibited except to the extent expressly permitted by this Agreement.\n\n\n    46. <span style=\"text-decoration:underline;\">Entire Agreement</span>.  This Agreement constitutes the entire understanding of the parties with respect to the transactions and matters contemplated hereby and supersedes all previous communications, representations, agreements and understanding relating to the Hosted Services, the Platform, and the Materials.  No representations, inducements, promises or agreements, whether oral or otherwise, between the parties not contained in this Agreement shall be of any force or effect. \n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fb93"
  },
  "filename": "semantic-search.md",
  "title": "\"Pinecone for Semantic Search\"",
  "category": "\"Pinecone for Semantic Search\"",
  "content": "---\ntitle: \"Pinecone for Semantic Search\"\nheadline: \"Pinecone for Semantic Search\"\nlayout: solutions-page\nhero:\n  title: Let your users search like they <span>mean it</span>.\n  description: |\n    Pinecone is a fully-managed vector database that indexes, stores, and retrieves semantic representations of documents. When combined with [Natural Language Processing (NLP)](/learn/nlp/), the vector database makes it easy to add semantic search to production applications.\n  img:\n    src: /images2/pinecone-diagram.svg\n    alt: Pinecone Diagram\n  cta:\n    link:\n      url: https://app.pinecone.io/\n      text: Get Started\nfeatures:\n  title: Why <span>Pinecone</span>\n  list:\n    - eyebrow: Fully Managed\n      title: Focus on your application, not your search infrastructure.\n      text:\n        - Pinecone obsesses over operations and security so that you can focus on your application. Just sign up for an account, and Pinecone manages the infrastructure with high availability, geo-replication, and 24/7 operational support. \n      src: /images/illustration-fully-managed.svg\n      maxWidth: 40%\n    - eyebrow: Production Ready\n      title: Deploy faster - test in a few minutes and get into production in days.\n      text:\n        - Most users can test a subset of their vectors in just minutes and can introduce full-scale semantic search to their production applications in a matter of days.\n      src: /images/illustration-production-ready.svg\n      maxWidth: 60%\n    - eyebrow: Performance at Scale\n      title: Fast semantic search, at any scale, at a low cost.\n      text:\n        - The ability to power semantic search across billions of documents in milliseconds, combined with usage-based pricing, is perfect for high-volume production applications. Optional hybrid storage can be up to 10x more cost-effective compared to in-memory databases.\n      src: /images/illustration-scale-performance.svg\n      maxWidth: 80%\nfeaturesHighlights:\n  title: Features Highlights\n  list:\n    - title: Single-Stage Filtering\n      text: Combine vector similarity searches with metadata filtering for greater accuracy, control, and relevance of results — without any loss of performance.\n    - title: Basic Hybrid Search\n      text: |\n        Combine semantic search with basic keyword filtering. Filter by exact keyword match and use Boolean logic including AND, OR, and NOT. [See example notebook.](https://www.pinecone.io/docs/examples/basic-hybrid-search/)\n    - title: Horizontal Scaling\n      text: Add or remove pods on the fly to adjust search latencies, storage capacity, availability, and cost.\n    - title: Easy-To-Use API\n      text: Initiate the functionality of the database from any environment that can make HTTPS calls. Add semantic search to your application with just a few lines of code.\nlearnMore:\n  list:\n    - title: Solution Brief\n      url: /learn/vector-database/\n      thumbnail: \"/images/solution-brief.jpg\"\n    - title: NLP E-book\n      url: /learn/nlp\n      thumbnail: \"/images/nlp-ebook.png\"\n    - title: NLP Webinar\n      url: https://youtu.be/7RF03_WQJpQ\n      thumbnail: \"/images/nlp-webinar.png\"\n    - title: Semantic search example\n      url: /docs/examples/semantic-text-search/\n      thumbnail: \"/images/semantic-search-example.png\"\n    - title: Question answering example\n      url: /docs/examples/extractive-question-answering/\n      thumbnail: \"/images/question-answering-example.png\"\n---"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fb94"
  },
  "filename": "subprocessors.md",
  "title": "info-page",
  "category": "\"Subprocessors\"",
  "content": "---\nlayout: info-page\ntitle: \"Subprocessors\"\nheadline: \"Subprocessors\"\ncta: false\nintro: \"\"\n---\n\n*Updated: September 7, 2022*\n\nInformation about Subprocessors, including their functions and locations.\n\n| Service/Vendor | Function | Address |\n|--|--|--|\n| Confluent | Event streaming | California, USA |\n| Datadog | Cloud Monitoring Service Provider |  New York, USA |\n| Zendesk | Customer service and support ticketing | California, USA |\n| AWS | Cloud Infrastructure | Washington, USA |\n| Google | Internal collaboration and cloud infrastructure | California, USA |\n| Databricks | Data ingestion | California, USA |\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fb95"
  },
  "filename": "company.md",
  "title": "\"Company\"",
  "category": "company",
  "content": "---\ntitle: \"Company\"\nlayout: company\nheadline: \"We are Engineers, Scientists, and Researchers\"\nintro: \"The vector database for machine learning is brought to you by a dedicated team of engineers, scientists, and researchers with a passion for harnessing AI technology to drive business success.\"\nhero:\n  title: Meet <span>Pinecone</span>\n  description: |\n    We are engineers and scientists on a mission to build the search and database technology to power AI/ML applications for the next decade and beyond. We provide customers with capabilities that until now have only been in the hands of a few tech giants.\n\n    We are a distributed team with offices in New York City, San Francisco, and Tel Aviv. [Join us!](/careers/)\n  img:\n    src: /images2/pinecone-diagram.svg\n    alt: Pinecone Diagram\n    width: 447\n    height: 548\nteam:\n  title: Management Team\n  list:\n    - name: Edo Liberty\n      position: Founder & CEO\n      src: /images/team-edo-liberty-resized.jpg\n    - name: Lior Ehrenfeld\n      position: VP of Finance & Ops\n      src: /images/team-lior-ehrenfeld-resized.jpg\n    - name: Ram Sriharsha\n      position: VP of R&D\n      src: /images/team-ram-sriharsha-resized.jpg\n    - name: Greg Kogan\n      position: VP of Marketing\n      src: /images/team-greg-kogan-resized.jpg\n    - name: Elan Dekel\n      position: VP of Product\n      src: /images/team-elan-dekel.jpg\ninvestors:\n  eyebrow: Our investors\n  title: Backed by <span>industry giants</span>\n  description: \n  list:\n    - name: Tim Tully\n      position: Menlo Ventures\n    - name: Peter Wagner\n      position: Wing\n    - name: Bob Muglia\n      position: ex-CEO Snowflake\n    - name: Gaurav Gupta\n      position: Lightspeed\n    - name: Paul Hsiao\n      position: Canvas Ventures\n    - name: Ilan Stern\n      position: 166 2nd\n    - name: Bucky Moore\n      position: Kleiner Perkins\n    - name: Sebastian Gunningham\n      position: Saks 5th Avenue\n    - name: Will Hayes\n      position: CEO LucidWorks\n    - name: Will Freiberg\n      position: CEO Crux Informatics\nnews:\n  title: In the <span>News</span>\n  cta:\n    href: '#press-kit'\n    text: Download Our Press Kit\n---\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fb96"
  },
  "filename": "product-privacy.md",
  "title": "info-page",
  "category": "\"Product Privacy Statement\"",
  "content": "---\nlayout: info-page\ntitle: \"Product Privacy Statement\"\nheadline: \"Product Privacy Statement\"\ncta: false\nintro: \"\"\n---\n\n*Updated: May 26, 2022*\n\nPinecone Systems, Inc. (“Pinecone”) is a managed database for working with vectors. It provides the infrastructure for machine-learning applications that need to search and rank results based on similarity, such as recommendations, personalization, image and semantic search, and more.\n\nThis Product Privacy Statement explains how we collect, use, disclose, and otherwise process personal information on behalf of our enterprise customers (any, a “Customer”) in connection with our products and Hosted Services (collectively, the “Hosted Services”). As a service provider/data processor, Pinecone provides the Hosted Services to its Customers. Pinecone’s Customers are the data owners/data controllers with respect to such personal information processed through the Hosted Services. This Product Privacy Statement does not apply to any websites or other online Hosted Services that do not link to this Product Privacy Statement. To the extent Pinecone shares information with the Customer as this Product Privacy Statement describes, this Product Privacy Statement does not apply to the Customer’s subsequent use of that information.\nPinecone’s processing of personal information in connection with the Hosted Services is governed by this Product Privacy Statement and the relevant Customer agreement. In the event of any conflict between this Product Privacy Statement and the relevant Customer agreement, the customer agreement will control to the extent permitted by applicable law.\n\nThis Product Privacy Statement is not a substitute for any privacy notice that Pinecone Customers are required to provide to their employees or other authorized users.\n\n## Information We Collect\n\n_Information collected via the Hosted Services_. The Customer is responsible for determining the scope of information that Pinecone may collect and process. The Hosted Services process information through copying the data catalogue – upon your request - from your cloud servers into our systems, transform the data into vectors, store those vectors, and allow you to add, edit or delete them by changing your data catalogue. This processing is augmented by Pinecone’s intelligence tools. Such information may include personal information. We ask that our Customers not provide us with information that may be deemed sensitive under applicable laws (including, without limitation, financial information, health information, and information about children).\n\n_Information provided to us by authorized users in connection with their use of the Hosted Services_. We collect information about the Customer’s “authorized users” — individuals (typically personnel) that Customer authorizes to create an account or use an API key to access the Hosted Services. This may include personal information that authorized users provide when they:\n\n- Register for an account or create a user profile (such as first and last name, email address, physical address, telephone number(s), fax number, employer name, department and job title, device ID, and profile picture);\n- Upload content to the Hosted Services; and\n- Contact customer support or otherwise correspond with us by phone, email, or other means.\n\n_Information collected about authorized users_. We may collect information about authorized users, such as:\n\n- Information provided to us by the Customer about its authorized users. This may include business contact information, such as name, email address, and phone number.\n- Information about authorized users’ use of the Hosted Services. This may include information about authorized users’ use of the Hosted Services, including computer or mobile device operating system type and version number, manufacturer and model, device identifier, browser type, screen resolution, IP address, general location information such as city, state or geographic area; and information about authorized users’ use of and actions on the Hosted Services, such as pages you viewed, how much time was spent on a page, navigation paths between pages, information about activity on a page, access times, and length of access. This information is collected using cookies and similar technologies.\n\n## How We Use Information\n\nWe use the information we collect at the instruction of the relevant Customer and in accordance with the relevant Customer agreement, to provide the Hosted Services and for related purposes, including to:\n\n- enable authorized users to access and use the Hosted Services;\n- provide information about the Hosted Services, such as important updates or changes to the Hosted Services and security alerts;\n- customize the end user experience, such as personalizing content and features to better match interests and preferences;\n- derive anonymized, aggregated or de-identified data for our subsequent use;\n- measure performance of and improve the Hosted Services and develop new products and Hosted Services; and\n- respond to inquiries, complaints, and requests for customer support.\n\nWe may also use personal information as we believe necessary or appropriate to (a) comply with applicable law; (b) enforce the terms and conditions that govern the Service; (c) protect our rights, privacy, safety or property, and/or that of you or others; and (d) protect, investigate and deter against fraudulent, harmful, unauthorized, unethical or illegal activity.\n\n## How We Share Information\n\nWe share the information we collect:\n\n- with the relevant Customer from which we obtained the information;\n- with such third parties as the relevant Customer may direct; and\n- with third-party service providers that help us manage and improve the Hosted Services.\n\nWe may also share personal information with government, law enforcement officials or private parties as required by law, when we believe such disclosure is necessary or appropriate to (a) comply with applicable law; (b) enforce the terms and conditions that govern the Hosted Services; (c) protect our rights, privacy, safety or property, and/or that of you or others; and (d) protect, investigate and deter against fraudulent, harmful, unauthorized, unethical or illegal activity.\n\nWe may sell, transfer or otherwise share some or all of Pinecone’s business or assets, including personal information that we process as part of the Hosted Services, in connection with a business transaction (or potential business transaction) such as a merger, consolidation, acquisition, reorganization or sale of assets or in the event of bankruptcy.\n\n## Information Security\n\nPinecone uses physical, electronic, and procedural safeguards designed to protect personal information from loss, theft, misuse, and unauthorized access, disclosure, alteration, and destruction. We cannot, however, guarantee that any safeguards or security measures will be sufficient to prevent a security problem. We recommend that our Customers take steps to protect against unauthorized access to any devices or networks used to access the Hosted Services. See the relevant Customer agreement for additional information regarding Pinecone’s information security practices.\n\n## Data Subject Rights\n\nThe relevant Customer is the data owner/data controller of authorized users’ or others’ personal information processed through the Hosted Services. As the data owner/data controller, the relevant Customer is responsible for receiving and responding to authorized users’ and others’ requests to exercise any rights afforded to them under applicable data protection law. Pinecone will assist Customers in responding to such requests as set forth in the relevant Customer agreement.\n\n## Cross Border Data Transfer\nPinecone may transfer personal information collected and processed through the Hosted Services outside of the country from which it originated, including to the United States. See the relevant Customer agreement for additional information regarding how Pinecone safeguards the personal information it transfers across borders.\n\n## Data Retention\n\nPinecone retains personal information for as long as necessary to (a) provide the Hosted Services; (b) comply with legal obligations; (c) resolve disputes; and (d) enforce the terms of the relevant Customer agreement. See the relevant Customer agreement for additional information regarding Pinecone’s data retention practices.\n\n## Third-Party Products and Hosted Services\n\nThe Hosted Services may integrate with or enable access to third-party tools. Third-party tools registered, installed, or accessed by authorized users are governed by those third-party providers’ privacy notices. Please review those notices carefully, as Pinecone does not control and cannot be responsible for these providers’ privacy or information security practices.\n\n## Changes to this Privacy Statement\n\nPinecone reserves the right to modify this Product Privacy Statement at any time. Similar to Pinecone’s Hosted Services, laws, regulations and industry standards may evolve which may make changes to this Product Privacy Statement appropriate. Pinecone will post the changes to this page. Pinecone encourages you to review the Product Privacy Statement to stay informed. In accordance with applicable law, Pinecone will notify you of any material changes in its collection, use, or disclose of your information by posting a notice on its website. Any material changes to this Product Privacy Statement will be effective thirty (30) calendar days following notice of the changes on the website. These changes will be effective immediately for new users of the Hosted Services. If you object to any such changes, you must notify Pinecone prior to the effective date of such changes that you wish to deactivate your account. Continued use of the Hosted Services following the effective date of such changes indicates your agreeing to such changes.\n\n## Contact Us\n\nIf you have any question about this Product Privacy Statement, you can contact us at privacy@pinecone.io or mail us at:\n\nPinecone Systems, Inc<br>\n548 Market St<br>\nPMB 19327<br>\nSan Francisco, CA 94104-5401<br>\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fb97"
  },
  "filename": "cookies.md",
  "title": "info-page",
  "category": "\"Cookie Policy\"",
  "content": "---\nlayout: info-page\ntitle: \"Cookie Policy\"\nheadline: \"Cookie Policy\"\ncta: false\nintro: \"\"\n---\n\n*Last Revised: December 3, 2020*\n\nThis Cookie Policy explains how Pinecone Systems, Inc. (“Pinecone”, “we”, “us” or “our”) uses cookies and similar technologies in connection with the https://www.pinecone.io/ website and any other website that we own or control and which posts or links to this Privacy Policy (collectively, the “Websites”).\n\nOur [Privacy Policy](/privacy/) explains how we collect and use information from and about you when you use our Websites. This Cookie Policy explains more about how we use cookies and your related choices.\n\n## What are cookies?\n\nCookies are small data files that are placed on your computer or mobile device when you visit a website.  Cookies serve different purposes, like helping us understand how a website is being used, letting you navigate between pages efficiently, remembering your preferences and generally improving your browsing experience.\n\nOur Websites may use both session cookies (which expire once you close your web browser) and persistent cookies (which stay on your computer or mobile device until you delete them).\n\nWe may use two broad categories of cookies: (1) first party cookies, served directly by us to your computer or mobile device, which we use to recognize your computer or mobile device when it revisits our Websites; and (2) third party cookies, which are served by service providers or business partners on our Websites, and can be used by these parties to recognize your computer or mobile device when it visits other websites. Third party cookies can be used for a variety of purposes, including website analytics and social media features.\n\n## What types of cookies and similar tracking technologies does Pinecone use on the Websites?\n\nOn the Websites, we may use cookies and other tracking technologies in the following categories described in the table below.\n\n<table class=\"table table-bordered\">\n  <thead>\n    <tr>\n      <th scope=\"col\">Type</th>\n      <th scope=\"col\">Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Advertising</td>\n      <td>These cookies are used by advertising companies to collect information about how you use our Websites and other websites over time.  These companies use this information to show you ads they believe will be relevant to you within our Websites and elsewhere, and to measure how the ads perform.</td>\n    </tr>\n    <tr>\n      <td>Analytics</td>\n      <td>These cookies help us understand how our Websites is performing and being used.  These cookies may work with web beacons included in emails we send to track which emails are opened and which links are clicked by recipients.</td>\n    </tr>\n    <tr>\n      <td>Essential</td>\n      <td>These cookies are necessary to allow the technical operation of our Websites (e.g., they enable you to move around on a website and to use its features).</td>\n    </tr>\n    <tr>\n      <td>Functionality/performance</td>\n      <td>These cookies enhance the performance and functionality of our Websites.</td>\n    </tr>\n  </tbody>\n</table>\n\n## Other technologies\n\nIn addition to cookies, our Websites may use other technologies, such as Flash technology and pixel tags to collect information automatically.\n\n### Browser Web Storage\n\nWe may use browser web storage (including via HTML5), also known as locally stored objects (“LSOs”), for similar purposes as cookies. Browser web storage enables the storage of a larger amount of data than cookies. Your web browser may provide functionality to clear your browser web storage.\n\n### Web Beacons\n\nWe may also use web beacons (which are also known as pixel tags and clear GIFs) on our Websites and in our HTML formatted emails to track the actions of users on our Websites and interactions with our emails. Unlike cookies, which are stored on the hard drive of your computer or mobile device by a website, pixel tags are embedded invisibly on webpages or within HTML formatted emails. Pixel tags are used to demonstrate that a webpage was accessed or that certain content was viewed, typically to measure the success of our marketing campaigns or engagement with our emails and to compile statistics about usage of the Website, so that we can manage our content more effectively.\n\n## Your choices\n\nMost browsers let you remove or reject cookies.  To do this, follow the instructions in your browser settings.  Many browsers accept cookies by default until you change your settings.  Please note that if you set your browser to disable cookies, the Websites may not work properly.\nFor more information about cookies, including how to see what cookies have been set on your computer or mobile device and how to manage and delete them, visit https://www.allaboutcookies.org.  If you do not accept our cookies, you may experience some inconvenience in your use of our Websites. For example, we may not be able to recognize your computer or mobile device and you may need to log in every time you visit our Websites.\nUsers may opt out of receiving targeted advertising on websites through members of the Network Advertising Initiative by [clicking here](http://www.networkadvertising.org/choices) or the Digital Advertising Alliance by [clicking here](http://www.aboutads.info/choices). European users may opt out of receiving targeted advertising on websites through members of the European Interactive Digital Advertising Alliance by [clicking here](https://www.youronlinechoices.eu/), selecting the user’s country, and then clicking “Choices” (or similarly-titled link). Please note that we also may work with companies that offer their own opt-out mechanisms and may not participate in the opt-out mechanisms that we linked above.\n\nIf you choose to opt-out of targeted advertisements, you will still see advertisements online but they may not be relevant to you. Even if you do choose to opt out, not all companies that serve online behavioral advertising are included in this list, and so you may still receive some cookies and tailored advertisements from companies that are not listed.\n\n## Changes\n\nInformation about the cookies we use may be updated from time to time, so please check back on a regular basis for any changes.\n\n## Questions\n\nIf you have any questions about this Cookie Policy, please contact us by email at info@pinecone.io or mail us at:\n\nPinecone Systems, Inc.\\\n548 Market St\\\nPMB 19327\\\nSan Francisco, CA 94104-5401"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fb98"
  },
  "filename": "terms.md",
  "title": "info-page",
  "category": "\"Website Terms of Service\"",
  "content": "---\nlayout: info-page\ntitle: \"Website Terms of Service\"\nheadline: \"Website Terms of Service\"\ncta: false\nintro: \"\"\n---\n\n*Last Revised: May 26, 2022*\n\nThe Pinecone website located at https://www.pinecone.io (the “**Site**”) is a copyrighted work belonging to Pinecone Systems Inc., (“**Pinecone**”, “**us**”, “**our**”, and “**we**”). The Site is for informational purposes only; it contains general information about Pinecone and is directed at businesses and their representatives seeking information on our products and services.\n\nCertain features of the Site may be subject to additional guidelines, terms, or rules, which will be posted on the Site in connection with such features. All such additional terms, guidelines, and rules are incorporated by reference into these Terms of Use (“**Terms**”). Your access to and use of the Site is also subject to our [Privacy Policy](/privacy/).\n\nPinecone also provides a proprietary data and analytics platform, (the “**Platform**”, and the provision of which, the “**Hosted Services**”). Access to the Platform and the Hosted Services is subject to the terms and conditions of the Pinecone End User License Agreement, or any other executed agreement between the parties (either of which, a “**Hosted Services Agreement**”).\n\nTHESE TERMS SET FORTH THE LEGALLY BINDING TERMS AND CONDITIONS THAT GOVERN YOUR USE OF THE SITE. BY ACCESSING OR USING THE SITE, YOU ARE ACCEPTING THESE TERMS (ON BEHALF OF YOURSELF OR THE ENTITY THAT YOU REPRESENT), AND YOU REPRESENT AND WARRANT THAT YOU HAVE THE RIGHT, AUTHORITY, AND CAPACITY TO ENTER INTO THESE TERMS (ON BEHALF OF YOURSELF OR THE ENTITY THAT YOU REPRESENT). YOU MAY NOT ACCESS OR USE THE SITE OR ACCEPT THESE TERMS IF YOU ARE NOT AT LEAST 18 YEARS OLD. IF YOU DO NOT AGREE WITH ALL OF THE PROVISIONS OF THESE TERMS, DO NOT ACCESS AND/OR USE THE SITE.\n\nTHESE TERMS REQUIRE THE USE OF ARBITRATION (SECTION 12.2) ON AN INDIVIDUAL BASIS TO RESOLVE DISPUTES, RATHER THAN JURY TRIALS OR CLASS ACTIONS, AND ALSO LIMIT THE REMEDIES AVAILABLE TO YOU IN THE EVENT OF A DISPUTE.\n\n1. **ACCOUNTS**\n\n    1. **Account Creation**. In order to use certain features of the Platform and/or Hosted Services, you may register for an account (“**Hosted Services Account**”) on the Site and provide certain information about yourself as prompted by the account registration form. You represent and warrant that: (a) all required registration information you submit is truthful and accurate; and (b) you will maintain the accuracy of such information. You may delete your Account in accordance with the instructions set forth in the applicable Hosted Services Agreement. Pinecone may suspend or terminate your Hosted Services Account in accordance with the terms of the applicable Hosted Services Agreement.\n\n    2. **Account Responsibilities**. You are responsible for maintaining the confidentiality of your Account login information and any API Keys you are provided in connection with registration, in accordance with the terms set forth herein, and the applicable Hosted Services Agreement.\n\n2. **ACCESS TO THE SITE**\n\n    1. **License**. Subject to these Terms, Pinecone grants you a non-transferable, non-exclusive, revocable, limited license to use and access the Site solely for your own personal or internal business use.\n\n    2. **Certain Restrictions**. The rights granted to you in these Terms are subject to the following restrictions: (a) you shall not license, sell, rent, lease, transfer, assign, distribute, host, or otherwise commercially exploit the Site, whether in whole or in part, or any content displayed on the Site; (b) you shall not modify, make derivative works of, disassemble, reverse compile or reverse engineer any part of the Site; (c) you shall not access the Site in order to build a similar or competitive website, product, or service; and (d) except as expressly stated herein, no part of the Site may be copied, reproduced, distributed, republished, downloaded, displayed, posted or transmitted in any form or by any means. Unless otherwise indicated, any future release, update, or other addition to the functionality of the Site shall be subject to these Terms. All copyright and other proprietary notices on the Site (or on any content displayed on the Site) must be retained on all copies thereof.\n\n    3. **Modification**. Pinecone reserves the right, at any time, to modify, suspend, or discontinue the Site (in whole or in part) with or without notice to you. You agree that Pinecone will not be liable to you or to any third party for any modification, suspension, or discontinuation of the Site or any part thereof.\n\n    4. **No Support or Maintenance**. You acknowledge and agree that Pinecone will have no obligation to provide you with any support or maintenance in connection with the Site.\n\n    5. **Ownership**. You acknowledge that all intellectual property rights, including copyrights, patents, trademarks, and trade secrets, in the Site and its content are owned by Pinecone or Pinecone’s suppliers. Neither these Terms (nor your access to the Site) transfers to you or any third party any rights, title or interest in or to such intellectual property rights, except for the limited access rights expressly set forth in Section 2.1. Pinecone and its suppliers reserve all rights not granted in these Terms. There are no implied licenses granted under these Terms.\n\n3. **ACCEPTABLE USE POLICY**. The following terms constitute our “**Acceptable Use Policy**.” You agree not to: (i) upload, transmit, or distribute to or through the Site any computer viruses, worms, or any software intended to damage or alter a computer system or data; (ii) send through the Site unsolicited or unauthorized advertising, promotional materials, junk mail, spam, chain letters, pyramid schemes, or any other form of duplicative or unsolicited messages, whether commercial or otherwise; (iii) use the Site to harvest, collect, gather or assemble information or data regarding other users, including e-mail addresses, without their consent; (iv) interfere with, disrupt, or create an undue burden on servers or networks connected to the Site, or violate the regulations, policies or procedures of such networks; (v) attempt to gain unauthorized access to the Site (or to other computer systems or networks connected to or used together with the Site); (vi) interfere with any other user’s use and enjoyment of the Site; or (vi) use software or automated agents or scripts to produce multiple accounts on the Site, or to generate automated searches, requests, or queries to (or to strip, scrape, or mine data from) the Site (provided, however, that we conditionally grant to the operators of public search engines revocable permission to use spiders to copy materials from the Site for the sole purpose of and solely to the extent necessary for creating publicly available searchable indices of the materials, but not caches or archives of such materials, subject to the parameters set forth in our robots.txt file).\n\n4. **ENFORCEMENT**. We reserve the right (but have no obligation) to investigate and/or take appropriate action against you in our sole discretion if you violate the Acceptable Use Policy or any other provision of these Terms or otherwise create liability for us or any other person. Such action may include reporting you to law enforcement authorities.\n\n5. **FEEDBACK**. If you provide Pinecone with any feedback or suggestions regarding the Site (“**Feedback**”), you hereby assign to Pinecone all rights in such Feedback and agree that Pinecone shall have the right to use and fully exploit such Feedback and related information in any manner it deems appropriate. Pinecone will treat any Feedback you provide to Pinecone as non-confidential and non-proprietary. You agree that you will not submit to Pinecone any information or ideas that you consider to be confidential or proprietary.\n\n6. **PINECONE COMMUNICATIONS**.\n\n    1. **Generally**. You may have the opportunity to provide us with your phone number or e-mail address. By providing your phone number or email address to us, you consent to receive phone calls, SMS/text messages, and email communications from Pinecone. Communications from us and our affiliated companies may include communications about your use of the Site, and communications containing your API keys in connection with registration for the Pinecone Platform.\n\n    2. **Promotional Email Communications**. If you opt-in to receive marketing or promotional email communications from us, you will have the ability to opt out of receiving such communications by following the unsubscribe instructions in the communication itself. YOU ACKNOWLEDGE THAT YOU ARE NOT REQUIRED TO CONSENT TO RECEIVE PROMOTIONAL EMAILS AS A CONDITION OF USING THE SITE. CONSENT TO THESE PROMOTIONAL MESSAGES IS NOT REQUIRED TO ACCESS THE SITE.\n\n    3. **Electronic Communications**. The communications between you and Pinecone use electronic means, whether you use the Site or send us emails, or whether Pinecone posts notices on the Site or communicates with you via email. For contractual purposes, you (a) consent to receive communications from Pinecone in an electronic form; and (b) agree that all terms and conditions, agreements, notices, disclosures, and other communications that Pinecone provides to you electronically satisfy any legal requirement that such communications would satisfy if they were to be in a hardcopy writing. The foregoing does not affect your non-waivable rights.\n\n7. **INDEMNIFICATION**. You agree to indemnify and hold Pinecone (and its officers, employees, and agents) harmless, including costs and attorneys’ fees, from any claim or demand made by any third party due to or arising out of (a) your use of the Site, (b) your violation of these Terms, or (c) your violation of applicable laws or regulations. Pinecone reserves the right, at your expense, to assume the exclusive defense and control of any matter for which you are required to indemnify us, and you agree to cooperate with our defense of these claims. You agree not to settle any matter without the prior written consent of Pinecone. Pinecone will use reasonable efforts to notify you of any such claim, action or proceeding upon becoming aware of it.\n\n8. **THIRD-PARTY LINKS AND APPLICATIONS; RELEASE**\n\n    1. **Third-Party Links and Applications**. The Site may contain links to third-party websites and services and applications for third parties (collectively, “**Third-Party Links and Applications**”). Such Third-Party Links and Applications are not under the control of Pinecone, and Pinecone is not responsible for any Third-Party Links and Applications. Pinecone provides access to these Third-Party Links and Applications only as a convenience to you, and does not review, approve, monitor, endorse, warrant, or make any representations with respect to Third-Party Links and Applications. You use all Third-Party Links and Applications at your own risk, and should apply a suitable level of caution and discretion in doing so. When you click on any of the Third-Party Links and Applications, the applicable third party’s terms and policies apply, including the third party’s privacy and data gathering practices. You should make whatever investigation you feel necessary or appropriate before proceeding with any transaction in connection with such Third-Party Links and Applications.\n\n    2. **Release**. You hereby release and forever discharge Pinecone (and our officers, employees, agents, successors, and assigns) from, and hereby waive and relinquish, each and every past, present and future dispute, claim, controversy, demand, right, obligation, liability, action and cause of action of every kind and nature (including personal injuries, death, and property damage), that has arisen or arises directly or indirectly out of, or that relates directly or indirectly to, the Site (including interactions with any other Site users or any Third-Party Links and Applications). If you are a California resident, you hereby waive California Civil Code 1542 in connection with the foregoing, which states, “A general release does not extend to claims that the creditor or releasing party does not know or suspect to exist in his or her favor at the time of executing the release and that, if known by him or her, would have materially affected his or her settlement with the debtor or released party.”\n\n9. **DISCLAIMER**\n\n    1. **As Is**. THE SITE IS PROVIDED ON AN “AS-IS” AND “AS AVAILABLE” BASIS, WITH ALL FAULTS AND NO GUARANTEES REGARDING OUTCOMES OR PERFORMANCE. PINECONE (AND OUR SUPPLIERS) EXPRESSLY DISCLAIM ANY AND ALL WARRANTIES AND CONDITIONS OF ANY KIND, WHETHER EXPRESS, IMPLIED, OR STATUTORY, INCLUDING ALL WARRANTIES OR CONDITIONS OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, QUIET ENJOYMENT, ACCURACY, OR NON-INFRINGEMENT. WE (AND OUR SUPPLIERS) MAKE NO WARRANTY THAT THE SITE WILL MEET YOUR REQUIREMENTS, WILL BE AVAILABLE ON AN UNINTERRUPTED, TIMELY, SECURE, OR ERROR-FREE BASIS, OR WILL BE ACCURATE, RELIABLE, FREE OF VIRUSES OR OTHER HARMFUL CODE, COMPLETE, LEGAL, OR SAFE. IF APPLICABLE LAW REQUIRES ANY WARRANTIES WITH RESPECT TO THE SITE, ALL SUCH WARRANTIES ARE LIMITED IN DURATION TO NINETY (90) DAYS FROM THE DATE OF FIRST USE.\n\n    SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES, SO THE ABOVE EXCLUSION MAY NOT APPLY TO YOU. SOME JURISDICTIONS DO NOT ALLOW LIMITATIONS ON HOW LONG AN IMPLIED WARRANTY LASTS, SO THE ABOVE LIMITATION MAY NOT APPLY TO YOU.\n\n10. **LIMITATION ON LIABILITY**\n\n\n    TO THE MAXIMUM EXTENT PERMITTED BY LAW, IN NO EVENT SHALL PINECONE (OR OUR SUPPLIERS) BE LIABLE TO YOU OR ANY THIRD PARTY FOR ANY LOST PROFITS, LOST DATA, COSTS OF PROCUREMENT OF SUBSTITUTE PRODUCTS, OR ANY INDIRECT, CONSEQUENTIAL, EXEMPLARY, INCIDENTAL, SPECIAL OR PUNITIVE DAMAGES ARISING FROM OR RELATING TO THESE TERMS OR YOUR USE OF, OR INABILITY TO USE, THE SITE, EVEN IF PINECONE HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. ACCESS TO, AND USE OF, THE SITE IS AT YOUR OWN DISCRETION AND RISK, AND YOU WILL BE SOLELY RESPONSIBLE FOR ANY DAMAGE TO YOUR DEVICE OR COMPUTER SYSTEM, OR LOSS OF DATA RESULTING THEREFROM.\n\n    TO THE MAXIMUM EXTENT PERMITTED BY LAW, NOTWITHSTANDING ANYTHING TO THE CONTRARY CONTAINED HEREIN, OUR LIABILITY TO YOU FOR ANY DAMAGES ARISING FROM OR RELATED TO THESE TERMS (FOR ANY CAUSE WHATSOEVER AND REGARDLESS OF THE FORM OF THE ACTION), WILL AT ALL TIMES BE LIMITED TO A MAXIMUM OF FIFTY US DOLLARS (U.S. $50.00). THE EXISTENCE OF MORE THAN ONE CLAIM WILL NOT ENLARGE THIS LIMIT. YOU AGREE THAT OUR SUPPLIERS WILL HAVE NO LIABILITY OF ANY KIND ARISING FROM OR RELATING TO THESE TERMS.\n\n    SOME JURISDICTIONS DO NOT ALLOW THE LIMITATION OR EXCLUSION OF LIABILITY FOR INCIDENTAL OR CONSEQUENTIAL DAMAGES, SO THE ABOVE LIMITATION OR EXCLUSION MAY NOT APPLY TO YOU.\n\n11. **TERM AND TERMINATION**.  Subject to this Section, these Terms will remain in full force and effect while you use the Site. We may suspend or terminate your rights to access or use the Site at any time for any reason at our sole discretion, including for any use of the Site in violation of these Terms. Upon termination of your rights under these Terms, your right to access and use the Site will terminate immediately. Pinecone will not have any liability whatsoever to you for any suspension or termination of your rights under these Terms. Even after your rights under these Terms are terminated, the following provisions of these Terms will remain in effect: Sections 1, 2.2-2.5, and 3 through 12.\n\n12. **GENERAL**\n\n    1. **Changes**. These Terms are subject to occasional revision, and if we make any substantial changes, we may notify you by sending you an e-mail to the last e-mail address you provided to us (if any), and/or by prominently posting notice of the changes on our Site. You are responsible for providing us with your most current e-mail address. In the event that the last e-mail address that you have provided us is not valid, or for any reason is not capable of delivering to you the notice described above, our dispatch of the e-mail containing such notice will nonetheless constitute effective notice of the changes described in the notice. Any changes to these Terms will be effective upon the earlier of thirty (30) calendar days following our dispatch of an e-mail notice to you (if applicable) or thirty (30) calendar days following our posting of notice of the changes on our Site. These changes will be effective immediately for new users of our Site. Continued use of our Site following notice of such changes shall indicate your acknowledgement of such changes and agreement to be bound by the terms and conditions of such changes.\n\n    2. **Dispute Resolution. PLEASE READ THIS CAREFULLY. IT AFFECTS YOUR RIGHTS.**\n\n        (a) Any and all controversies, disputes, demands, counts, claims, or causes of action (including the interpretation and scope of this clause, and the arbitrability of the controversy, dispute, demand, count, claim, or cause of action) between you and Pinecone and our employees, agents, successors, or assigns, regarding or relating to the Site or these Terms shall exclusively be settled through binding and confidential arbitration.\n\n        (b) Arbitration shall be subject to the Federal Arbitration Act and not any state arbitration law. The arbitration shall be conducted before one commercial arbitrator with substantial experience in resolving commercial contract disputes from the American Arbitration Association (“**AAA**”) or Judicial Arbitration and Mediations Services (“**JAMS**”). As modified by these Terms, and unless otherwise agreed upon by the parties in writing, the arbitration will be governed by the AAA’s or JAMS’s rules for commercial arbitration and, if the arbitrator deems them applicable, the procedures for consumer-related disputes.\n\n        (c) You are thus GIVING UP YOUR RIGHT TO GO TO COURT to assert or defend your rights EXCEPT for matters that may be taken to small claims court. Your rights will be determined by a NEUTRAL ARBITRATOR and NOT a judge or jury. You are entitled to a FAIR HEARING, BUT the arbitration procedures are SIMPLER AND MORE LIMITED THAN RULES APPLICABLE IN COURT. Arbitrator decisions are as enforceable as any court order and are subject to VERY LIMITED REVIEW BY A COURT.\n\n        (d) You and we must abide by the following rules: (1) ANY CLAIMS BROUGHT BY YOU OR US MUST BE BROUGHT IN THE PARTIES’ INDIVIDUAL CAPACITY, AND NOT AS A PLAINTIFF OR CLASS MEMBER IN ANY PURPORTED CLASS OR REPRESENTATIVE PROCEEDING; (2) THE ARBITRATOR MAY NOT CONSOLIDATE MORE THAN ONE PERSON’S CLAIMS, MAY NOT OTHERWISE PRESIDE OVER ANY FORM OF A REPRESENTATIVE OR CLASS PROCEEDING, AND MAY NOT AWARD CLASS-WIDE RELIEF; (3) in the event that you are able to demonstrate that the costs of arbitration will be prohibitive as compared to costs of litigation, we will pay as much of your filing and hearing fees in connection with the arbitration as the arbitrator deems necessary to prevent the arbitration from being cost-prohibitive as compared to the cost of litigation; (4) we also reserve the right in our sole and exclusive discretion to assume responsibility for all of the costs of the arbitration; (5) the arbitrator shall honor claims of privilege and privacy recognized at law; (6) the arbitrator’s award shall be final and may be enforced in any court of competent jurisdiction; (7) the arbitrator may award any individual relief or individual remedies that are permitted by applicable law; and (8) each side pays its own attorneys’ fees and expenses unless there is a statutory provision that requires the prevailing party to be paid its fees’ and litigation expenses, and then in such instance, the fees and costs awarded shall be determined by the applicable law.\n\n        (e) Notwithstanding the foregoing, either you or we may bring an individual action in small claims court. Further, claims of infringement or misappropriation of the other party’s patent, copyright, trademark, or trade secret rights shall not be subject to this arbitration agreement. Such claims shall be exclusively brought in the state or federal courts located in San Francisco County, California. Additionally, notwithstanding this agreement to arbitrate, either party may seek emergency equitable relief before the state or federal courts located in San Francisco, California in order to maintain the status quo pending arbitration, and hereby agree to submit to the exclusive personal jurisdiction of the courts located within San Francisco County, California for such purpose. A request for interim measures shall not be deemed a waiver of the right to arbitrate.\n\n        (f) With the exception of subparts (1) and (2) in Section 12.2(d) above (prohibiting arbitration on a class or collective basis), if any part of this arbitration provision is deemed to be invalid, unenforceable or illegal, or otherwise conflicts with these Terms, then the balance of this arbitration provision shall remain in effect and shall be construed in accordance with its terms as if the invalid, unenforceable, illegal or conflicting provision were not contained herein. If, however, either subparts (1) and (2) in Section 12.2(d) (prohibiting arbitration on a class or collective basis) is found to be invalid, unenforceable or illegal, then the entirety of this arbitration provision shall be null and void, and neither you nor we shall be entitled to arbitration. If for any reason a claim proceeds in court rather than in arbitration, the dispute shall be exclusively brought in state or federal court in San Francisco County, California.\n\n        (g) Notwithstanding any provision in these Terms to the contrary, if we seek to terminate the Dispute Resolution section as included in these Terms, any such termination shall not be effective until 30 days after the version of these Terms not containing the agreement to arbitrate is posted to the Site, and shall not be effective as to any claim of which you provided Pinecone with written notice prior to the date of termination.\n\n        (h) For more information on AAA, its Rules and Procedures, and how to file an arbitration claim, you may call AAA at 800-778-7879 or visit the AAA website at http://www.adr.org. For more information on JAMS, it’s Rules and Procedures, and how to file an arbitration claim, you may call JAMS at 800-352-5267 or visit the JAMS website at http://www.jamsadr.com.\n\n        (i) Any and all controversies, disputes, demands, counts, claims, or causes of action between you and Pinecone and our employees, agents, successors, or assigns, regarding or relating to these Terms or the Site shall exclusively be governed by the internal laws of the State of California, without regard to its choice of law rules and without regard to conflicts of laws principles except that the arbitration provision shall be governed by the Federal Arbitration Act. The United Nations Convention on Contracts for the International Sale of Goods shall not apply to these Terms.\n\n    3. **Export**. The Site may be subject to U.S. export control laws and may be subject to export or import regulations in other countries. You agree not to export, reexport, or transfer, directly or indirectly, any U.S. technical data acquired from Pinecone, or any products utilizing such data, in violation of the United States export laws or regulations.\n\n    4. **Disclosures**. Pinecone is located at the address in Section 12.7. If you are a California resident, you may report complaints to the Complaint Assistance Unit of the Division of Consumer Product of the California Department of Consumer Affairs by contacting them in writing at 400 R Street, Sacramento, CA 95814, or by telephone at (800) 952-5210.\n\n    5. **Entire Terms**. These Terms constitute the entire agreement between you and us regarding use of the Site. Our failure to exercise or enforce any right or provision of these Terms shall not operate as a waiver of such right or provision. The section titles in these Terms are for convenience only and have no legal or contractual effect. The word “including” means “including without limitation.” If any provision of these Terms is, for any reason, held to be invalid or unenforceable, the other provisions of these Terms will be unimpaired and the invalid or unenforceable provision will be deemed modified so that it is valid and enforceable to the maximum extent permitted by law. These Terms, and your rights and obligations herein, may not be assigned, subcontracted, delegated, or otherwise transferred by you without Pinecone’s prior written consent, and any attempted assignment, subcontract, delegation, or transfer in violation of the foregoing will be null and void. Pinecone may freely assign these Terms. The terms and conditions set forth in these Terms shall be binding upon assignees.\n\n    6. **Copyright/Trademark Information**. Copyright © 2020, Pinecone Systems Corporation. All rights reserved. All trademarks, logos and service marks (“**Marks**”) displayed on the Site are our property or the property of other third parties. You are not permitted to use these Marks without our prior written consent or the consent of such third party which may own the Marks. All goodwill generated from the use of any Pinecone Marks will inure to Pinecone’s benefit.\n\n    7. **Contact Information:**\n\n      Pinecone Systems, Inc<br>\n      548 Market St<br>\n      PMB 19327<br>\n      San Francisco, CA 94104-5401<br>\n      info@pinecone.io\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fb99"
  },
  "filename": "hybrid-search-early-access.md",
  "title": "Hybrid Search Early Access",
  "category": "contact",
  "content": "---\ntitle: Hybrid Search Early Access\nlayout: contact\nclass: early-access\nhero2:\n  title: Hybrid Search <br><span>Early Access</span>\n  description: We'll reach out to you directly within a week of submitting the form.\n\nFAQ:\n  title: Frequently Asked <span>Questions</span>\n  list:\n    - question: How long is the private preview?\n      answer: The private preview will run for 3-4 weeks.\n\n    - question: How do I access the preview / how do I get started?\n      answer: You will receive instructions once approved for early access.\n\n    - question: Is there documentation?\n      answer: You will receive a link to the docs once approved for early access.\n\n    - question: What if I have questions? Or find a bug? \n      answer: Contact [support@pinecone.io](mailto:support@pinecone.io) and mention you are previewing hybrid search.\n\n    - question: Does this cost anything?\n      answer: No, you will not be billed for usage within the beta environment.\n\n    - question: Do I have to be an existing Pinecone customer?\n      answer: No, but you will have to create an account on the beta environment, and existing customers will be given priority for early access.\n\n    - question: How do I know if I’ve been granted early access?\n      answer: If accepted, we will email you to set up a short onboarding call.\n\n---\n\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fb9a"
  },
  "filename": "partners.md",
  "title": "\"Pinecone Partners\"",
  "category": "partners",
  "content": "---\ntitle: \"Pinecone Partners\"\nlayout: partners\nheadline: \"Pinecone Partners\"\ndescription: \"Pinecone partners with external organizations including companies that produce vector embeddings, NLP organizations, individual founders, and beyond.\"\nhero:\n  title: Pinecone <span>Partners</span>\n  description: |\n    Pinecone partners with exceptional organizations in the fields of AI/ML, search, and cloud infrastructure to advance our complementary missions and for the benefit of our users.\n\n  emailSubmit: false\n\ntechnologyPartners:\n  title: Technology Partners\n  description: We partner with software vendors to grow and serve their customer base by leveraging Pinecone integrations and joint go-to-market opportunities.\n  partners:\n    - title: Cohere\n      url: https://www.cohere.ai/\n      image: \"/images/cohere.png\"\n      description: Cohere makes it possible for every developer to harness the power of NLP by allowing them to easily generate, categorize, and organize text at a scale that was previously unimaginable.\n      links:\n        - title: \"Watch: Supercharging Semantic Search with Pinecone and Cohere\"\n          url: https://www.youtube.com/watch?v=e2g5ya4ZFro\n        - title: \"Try: Semantic-search example notebook with Cohere and Pinecone\"\n          url: https://github.com/pinecone-io/examples/blob/master/integrations/cohere/semantic_search_trec.ipynb\n        - title: \"Read: Cohere integration docs\"\n          url: https://www.pinecone.io/docs/integrations/cohere/\n    - title: Deepset\n      url: https://www.deepset.ai/\n      image: \"/images/deepset.png\"\n      description: Deepset provides developers with the right tools to build production-ready NLP systems quickly and efficiently.\n      links:\n        - title: \"Watch: How to Build AI-powered Q&A Applications with Haystack and Pinecone\"\n          url: https://www.youtube.com/watch?v=ZdS_V1A5r44\n        - title: \"Try: Q&A example notebook with Haystack and Pinecone\"\n          url: https://colab.research.google.com/drive/18YAEm1jHr73S5Lo799GdyXR_EayYDWiY?usp=sharing\n        - title: \"Read: Haystack integration docs\"\n          url: https://www.pinecone.io/docs/integrations/haystack/\n    - title: OpenAI\n      url: https://www.openai.com/\n      image: \"/images/openai-logo.png\"\n      description: OpenAI is an AI research and deployment company. Their mission is to ensure that artificial general intelligence benefits all of humanity by building safe and beneficial AGI, as well as helping others achieve this outcome with their work.\n      links:\n        - title: \"Watch: Beyond Semantic Search with OpenAI and Pinecone\"\n          url: https://www.youtube.com/watch?v=HtI9easWtAA\n        - title: \"Demo: Q&A demo app made with OpenAI and Pinecone\"\n          url: https://share.streamlit.io/pinecone-io/playground/beyond_search_openai/src/server.py\n        - title: \"Read: OpenAI integration docs\"\n          url: https://www.pinecone.io/docs/integrations/openai/\n\nserviceProviderPartners:\n  title: Service Provider Partners\n  description: We partner with service providers to achieve success for their customers by leveraging Pinecone solutions and expertise. \n  partners:\n    - title: Strong\n      url: https://www.strong.io\n      image: /images/strong.png\n      description: Strong helps top companies of all sizes in all industries design, engineer, and deploy custom end-to-end machine learning and AI products and solutions with a team that brings diverse, deep knowledge of ML/AI and state-of-the-art techniques to bring these solutions to fruition.\n      # links:\n      #   - title: Youtube Links\n      #     url: /\n      #   - title: Demo apps\n      #     url: /\n      #   - title: Documentation\n      #     url: /\n    - title: SmartCat\n      image: /images/smartcat.png\n      url: https://www.smartcat.io\n      description: SmartCat helps partners develop meaningful data solutions and functional data strategies for challenging business problems. We provide an end-to-end solution by having various expertise under the same roof.\n      # links:\n      #   - title: Youtube Links\n      #     url: /\n      #   - title: Demo apps\n      #     url: /\n      #   - title: Documentation\n      #     url: /\n\nbecomeAPartner:\n  title: Become A Partner\n  list:\n    - title: \"Why partner with Pinecone:\"\n      list:\n        - \"**Accelerate your go-to-market**: Reach, educate, convert, and expand more customers who are interested in vector-embedding and vector-search technology to power real-world applications.\"\n        - \"**Add value to your customers**: Unlock valuable use cases for your customers through compatibility and integration with the leading vector database.\"\n        - \"**Differentiate yourself**: Offer new and powerful solutions for your customers, leveraging cutting-edge vector database technology.\"\n        - \"**Take on any challenge**: Get tools, training, and technical guidance from our experts to help you confidently take on and successfully complete exciting projects.\"\n---\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fb9b"
  },
  "filename": "support-policy.md",
  "title": "info-page",
  "category": "\"Pinecone Technical Support Policy and Service Level Agreement\"",
  "content": "---\nlayout: info-page\ntitle: \"Pinecone Technical Support Policy and Service Level Agreement\"\nheadline: \"Pinecone Technical Support Policy and Service Level Agreement\"\ncta: true\nintro: \"\"\n---\n\nLast updated: September 15, 2022\n\nThis Service Level Agreement (“**SLA**”) describes the terms under which Pinecone Systems, Inc. (“**Pinecone**”) provides technical support for its Platform to customers that have purchased access to the Pinecone Platform pursuant to any agreement providing for access to the Platform between Pinecone and customer (“**Customer**”) that references this SLA (the “**Agreement**”). This SLA will apply only to the extent expressly set forth in the Agreement. All support and other services provided under this SLA are subject to the terms and conditions of the Agreement. Any capitalized terms used but not defined herein will have the meanings prescribed to them in the Agreement. \n\n## 1. Definitions.\n\n“**Applicable Monthly Service Fees**” means the total consumption fees paid by Customer for the Platform during the calendar month in which Service Downtime occurred.\n\n“**Business Days**” means Monday through Friday, excluding US Federal Holidays.\n\n“**Business Hours**” means 8am to 6pm Eastern US Time on Business Days.\n\n“**Initial Response**” means an initial response to a Support Request, which, at a minimum, is made by a human agent and acknowledges receipt of the request.\n\n“**Issue**” means a failure of the Service to conform to the specifications set forth in the Documentation.\n\n“**Monthly Uptime Percentage**” means with respect to any particular calendar month, the total number of minutes in that month, minus the number of minutes of measured Service Downtime during that month, divided by the total number of minutes in that month. Monthly Uptime Percentage is calculated as:\n\nMonthly Uptime Percentage = total number of minutes in a month - Service Downtime / total number of minutes in that month\n\nWhen calculating the Monthly Uptime Percentage for any month in which the Platform is deployed for only part of the month, it is assumed that there was no Service Downtime during the portion of the month in which the Platform was not deployed.\n\n“**Production Index**\" means an environment serving your end users or customers.\n\n“**Service Credit**” is the percentage of the Applicable Monthly Service Fees to be credited to Customer if Pinecone approves Customer’s claim, as set forth in Section 3.1.  \n\n“**Service Downtime**” is measured at each Pinecone index as the total number of full minutes, outside of scheduled downtime for maintenance and upgrades, where continuous attempts to establish a connection within the minute fail as reflected in minute-by-minute logs.\n\n“**Service Hours**” means twenty-four (24) hours per day, seven (7) days per week, including US Federal Holidays\n\n## 2. Service levels.\n\n**Availability**. Pinecone will undertake commercially reasonable measures to ensure that the Monthly Uptime Percentage equals or exceeds 99.9% during each calendar month (the “**Service Standard**”).  To achieve the Monthly Uptime standard, you must maintain at least two replicas of your index.\n\n## 3. Remedy.\n\n**3.1** - **Service Credits**. If Pinecone does not achieve and maintain the Monthly Uptime Percentages set forth in the table below, then Customer may be eligible for Service Credits according to the following table:\n\n<div class=\"responsive-table pricing-table\">\n\n| MONTHLY UPTIME PERCENTAGE |       SERVICE CREDIT       |\n| :------: | :-----------------: |\n|    Less than 99.9% and equal or greater than 99.0%   |    10% of Applicable Monthly Service Fees    |\n|    Less than 99.0% and equal or greater than 95.0%    |    25% of Applicable Monthly Service Fees    | \n|    Less than 95.0%   |    50% of Applicable Monthly Service Fees    | \n\n</div>\n\n**3.2** - **Customer Obligations**. As a condition to Pinecone’s obligation to provide Service Credits to Customer, Customer must meet all the following criteria:\n\n**(a)** Have a qualifying index on the Enterprise tier or above, maintained with at least two replicas of that index, throughout the eligible month.  \n\n**(b)** Log a support ticket with Pinecone within sixty (60) minutes of the start of a Service Downtime. \n\n**(c)** Submit a claim by emailing Pinecone at support@pinecone.io by the end of the calendar month immediately following the month in which Downtime with occurred, with all the following required information: \n\n1. The words \"Pinecone: Request for SLA Credit\" in the subject line;\n2. A detailed description of the events resulting in Service Downtime, including Customer’s request logs that document the errors and corroborate Customer’s claimed outage (with any confidential or sensitive information in the logs removed or replaced with asterisks);\n3. The index name, number of pods, number of replicas, the cloud provider, and the cloud region of the affected Pinecone index;\n4. A description of your affected customers or end users;\n5. Information regarding the dates, time and duration of the Service Downtime;\n6. Descriptions of Customer’s attempts to resolve the Service Downtime at the time of occurrence.\n\n**(d)** Reasonably assist Pinecone in investigating the cause of the Service Downtime and processing the claim.\n\n**(e)** Customer must comply with the Pinecone Services Agreement, applicable Pinecone documentation and any advice from Pinecone’s support team.\n\nIf Pinecone determines that Customer has satisfied the customer obligations above and that none of the below limitations apply to Customer’s claim, Pinecone will grant Customer a Service Credit. Pinecone will apply any Service Credit to a future invoice or payment for the Pinecone Platform. Service Credits will not entitle you to any cash refund or other payment from Pinecone.\n\nCustomer’s rights under this Section 3 (Remedy) are Customer’s sole and exclusive remedy with respect to any Service Downtime or any failure by Pinecone to meet the Service Standard required by Section 2.1 (Availability).\n\n## 4. Exclusions.\n\nService Downtime excludes, and Customer will not be eligible for a Service Credit for, any performance or availability issue that results from:\n\n**4.1** - A Force Majeure Event, a network or device failure at Customer’s site or between Customer’s site and the Pinecone Platform, or any other factors outside of Pinecone’s reasonable control;\n\n**4.2** - Services, hardware, or software provided by a third party, such as cloud platform services (e.g., AWS, GCP) on which the Pinecone Platform runs;\n\n**4.3** - Customer’s failure to timely pay Service Fees owed and due to Pinecone for use of the Services;\n\n**4.4** - Customer’s failure to be in full compliance with the terms of the Agreement with Pinecone for use of the Services, as well as applicable Pinecone documentation, best practices, and advice from the Pinecone support team; or Customer’s or any third party’s (a) improper use, scaling or configuration of the Pinecone Platform, (b) attempts at modifications to the Pinecone Platform, or (c) failure to follow appropriate security practices;\n\n**4.5** - Impact to your ability to access or use the Pinecone console, an interface provided to manage services, is excluded. This includes any component and content linked from the Pinecone console, including Documentation, the Customer Support portal, and Monitoring. These components operate independently from database services and do not impact database availability;\n\n**4.6** - Pinecone’s beta services.\n\n## 5. Pinecone Technical Support Policy.\n\nThis Pinecone Technical Support Policy (“**Policy**”). \n\n**5.1** - **Generally**. Technical support is intended to cover the provision of support to Customer’s authorized support contacts for technical issues with the Pinecone Platform that are not covered in the Pinecone documentation. Customer may designate up to two (2) individuals for Standard Support, and four (4) individuals for Premium Support, at any time with appropriate technical expertise that are familiar with the Platform as Customer’s designated support contacts (“**Customer Representative**”). \n\n**5.2** - Technical support does not include, and Pinecone will not be required to provide any assistance related to: (a) use of the Platform other than in accordance with the documentation; (b) use of the Platform in violation of the Agreement; (c) issues that result from a Force Majeure Event or any other factors beyond Pinecone’s reasonable control; (d) the integration or communication of Customer systems with the Pinecone Platform or Services.\n\n**5.3** - **Severity Levels and Response Time**:\n\nPinecone offers three levels of support policies - Community, Standard and Premium. Each policy is defined by its own Uptime Credit, contact method, support hours, number of allowed customer representatives and the Initial Response time, according to the following table:\n\n<div class=\"responsive-table pricing-table\">\n\n|                   |       Community       |       Standard        |       Premium      |\n| :---------------: | :-------------------: | :-------------------: | :----------------: |\n|    **Uptime Credits** |           N/A         |            N/A        | Production Indexes are entitled to service credits as detailed in Section 3.1. |\n|    **Contact Method** | [Community Forum](https://community.pinecone.io/) | [Web Portal](https://support.pinecone.io/), [Email](mailto:support@pinecone.io) |  [Web Portal](https://support.pinecone.io/), [Email](mailto:support@pinecone.io)|\n|    **Support Hours**  |           N/A         |  Sev 1: Service Hours<br>Sev 2-4: Business Hours | Sev 1-2: Service Hours<br>Sev 3-4: Business Hours  |\n|    **Max Customer Representatives** |    N/A  |            2          |          4         |\n|    **Initial Response Times***   |    N/A    | Sev 1: 4 Hours<br>Sev 2: 8 Business Hours<br>Sev 3: 24 Business Hours<br>Sev 4: 48 Business Hours | Sev 1: 1 Hours<br>Sev 2: 4 Hours<br>Sev 3: 12 Business Hours<br>Sev 4: 24 Business Hours |\n\n</div>\n*For the avoidance of doubt, nothing in these initial response times or in this Support Policy guarantees full resolution of a reported Issue.\n\n**Severity Level Definitions**\n\n- Sev 1: Production Index is down or is severely impacted such that routine operation is impossible, with no workaround available.\n- Sev 2: Production Index Issue where system is functional but offers service in degraded or restricted capacity. \n- Sev 3: Production Index has minor impact or has Issue in development, but Customer can still access and use most functionality of the Service, or situation may be temporarily circumvented using an available workaround.\n- Sev 4: No production impact, questions or requests for features.\n\n**5.4** - **Support Requests**. Pinecone will provide 24x7, 365 days support For Sev 1 and Sev 2 requests under Premium Support and Sev 1 requests under Standard Support. All other requests will be responded to during Business Hours. Pinecone provides a variety of ways for Customer to request help or otherwise make inquiries, including: \n\n- Web Portal: an online support module that may be used to report and track issues. Pinecone requests that Customer use this system as the first method of reporting issues and requesting support. It can be accessed through the url https://support.pinecone.io/ or via the Pinecone console by clicking ‘Help Desk’.\n\n- Email: the online support may also be accessed via the email support@pinecone.io. However, requests received via email will be assumed to be Sev 4 at time of creation.\n\nSupport Requests should include the following information:\n\n1. A description of the Issue you are experiencing, sufficiently detailed to allow Pinecone Support to effectively assess the Issue, including index name, and any relevant error message, and assumed Severity Level in accordance with the Support Definitions. \n2. Information regarding the time and duration of the issue. \n3. Descriptions of Customer’s attempts to resolve the issue at the time of occurrence.\n4. Give Pinecone any other important Support Case information in a timely manner.\n\n**5.5** - **Measurement**. Support Services response times are measured by Pinecone from the time the Support Request is received by Pinecone via the contact methods above.\n\n**5.6** - While Pinecone will make commercially reasonable efforts to correct defects or other errors in the Platform and respond to reported incidents of Service Downtime in accordance with Pinecone’s policies and procedures for correcting verified, reproducible errors in the Platform, Customer acknowledges that it may not be possible for Pinecone to correct every or any defect, error, or problem reported by Customer or of which Pinecone is otherwise made aware.\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fb9c"
  },
  "filename": "_index-old.md",
  "title": "true",
  "category": "\"Vector Database for Similarity Search\"",
  "content": "---\ndraft: true\ntitle: \"Vector Database for Similarity Search\"\nintro: \"Search through billions of items for similar matches to any object, in milliseconds. It’s the next generation of search, an API call away.\"\n# preload: /images/pinecone-product-diagram-wide.png\n# preloadsetX1: /images/pinecone-product-diagram-wide.png\n# preloadsetX2: /images/pinecone-product-diagram-wide-2x.png\nhero:\n  title: Search like you <span>mean it</span>\n  description: Pinecone is a fully managed vector database that makes it easy to add semantic search to production applications. It combines vector search libraries, capabilities such as filtering, and distributed infrastructure to provide high performance and reliability at any scale.\n  cta1:\n    href: https://app.pinecone.io/\n    text: Start for Free\n  cta2:\n    href: /contact/\n    text: or ask us a question\n  img:\n    src: /images2/pinecone-diagram.svg\n    alt: Pinecone Diagram\n    width: 447\n    height: 548\nusecases:\n  eyebrow: Use Cases\n  title: What can you do with <span>vector search</span>?\n  description: |\n    Machine Learning teams combine <a href=\"/learn/vector-embeddings/\">vector embeddings</a> and <a href=\"/learn/what-is-similarity-search/\">vector search</a> to create better applications that impact business results.\n  list:\n    - href: '/docs/examples/semantic-text-search/'\n      title: Semantic Search\n    - href: '/docs/examples/image-similarity-search/'\n      title: Unstructured Data Search\n    - href: '/docs/examples/document-deduplication/'\n      title: Deduplication and Record Matching\n    - href: '/docs/examples/personalized-content-recommendations/'\n      title: Recommendations and Ranking\n    - href: '/docs/examples/it-threat-detection/'\n      title: Detection and Classification\n\nfeatures:\n  eyebrow: Why Pinecone\n  title: Add vector search to production applications in less time than it takes to train a model.\n  list:\n    - eyebrow: Production-Ready\n      title: Go to production with a few lines of code, without breaking a sweat or slowing down\n      src: /images/illustration-production-ready.svg\n      maxWidth: 60%\n      features:\n        - Deploy and start using the service with a few lines of code. The REST API, clients (Python, Java, Go), and web console make it easy and quick to integrate into production applications.\n        - Approximate Nearest Neighbor (ANN) search with filtering, live index updates, namespacing, string IDs, batch queries, vector fetch operations, and more.\n    - eyebrow: Scale and Performance\n      title: Search through billions of vectors in tens of milliseconds.\n      src: /images/illustration-scale-performance.svg\n      maxWidth: 80%\n      left: true\n      features:\n        - Automatic scaling with data shards and replicas, eventual consistency, and data persistence on distributed infrastructure.\n        - Sub-100ms query latency and high recall rates at scale, even with billions of vectors and tens of thousands of queries per second.\n        - Maximum throughput (QPS) increases linearly with added replicas, without limits.\n        - Hybrid in-memory/on-disk storage is up to 10x more cost-effective for large data volumes compared to in-memory databases.\n    - eyebrow: Fully Managed\n      title: We obsess over operations and security so you don't have to.\n      src: /images/illustration-fully-managed.svg\n      maxWidth: 50%\n      features:\n        - Just create an account and we'll manage the infrastructure with high availability, geo-replication, and 24/7 operational support.\n        - Pinecone runs on secure AWS or GCP environments in multiple regions, with dedicated deployments available. Your data is secured in isolated containers and encrypted in transit.\nproduct:\n  eyebrow: Product\n  title: Designed for <span>speed, <br>scale, and ease of use</span>.\n  cta:\n    href: '/pdfs/pinecone-factsheet.pdf'\n    text: Download datasheet\n  list:\n    - title: Managed Service\n      description: Launch vector similarity search services on-demand. Each service runs on distributed cloud infrastructure, fully managed by Pinecone.\n      src: /images/pinecone-product-diagram-2.png\n    - title: Vector Database\n      description: Load vector embeddings and metadata from anywhere. Full CRUD operations with live index updates and hybrid in-memory/disk storage.\n      src: /images/pinecone-product-diagram-3.png\n    - title: Vector Search Index\n      description: Find nearest neighbors to any vector embedding, with optional filters, in <100ms with >95% recall.\n      src: /images/pinecone-product-diagram-4.png\n    - title: Orchestration\n      description: Scalability, fault tolerance, and high availability for billions of vectors, with Kafka and Kubernetes.\n      src: /images/pinecone-product-diagram-5.png\n    - title: Distributed Infrastructure\n      description: Choose between multi-tenant or dedicated environments in AWS or GCP.\n      src: /images/pinecone-product-diagram-6.png\nmetrics:\n  eyebrow: Customer Success\n  title: One of the <span>world's largest social media platforms</span> increased user enagement with Pinecone\n  description: Content recommendation engine powered by Pinecone vector search.\n  list:\n    - title: 1 Billion+\n      description: queries served\n    - title: 3,400\n      description: queries per second\n    - title: 5ms\n      description: search latency (p99)\n    - title: 99.9%\n      description: uptime\n---\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fb9d"
  },
  "filename": "newsletter-signup.md",
  "title": "newsletter-signup",
  "category": "Newsletter Signup",
  "content": "---\nlayout: newsletter-signup\ntitle: Newsletter Signup\nhero:\n  title: Newsletter Signup\n  description: |\n    Subscribe to our newsletter for the latest updates!\n  emailSubmit: true\n  inputText: Email Address...\n  buttonText: Submit\n---\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fb9e"
  },
  "filename": "_index.md",
  "title": "index",
  "category": "\"Vector Database for Vector Search\"",
  "content": "---\nlayout: index\ntitle: \"Vector Database for Vector Search\"\nintro: \"The #1 vector database. Search through billions of items for similar matches to any object, in milliseconds. It’s the next generation of search, an API call away.\"\n\n# Hero\nhero:\n  announcement: ⭐️ <a href=\"/learn/pinecone-gcp-marketplace/\">Now available in GCP Marketplace</a>\n  title: Long-term Memory for AI\n  description: The Pinecone <a href=\"/learn/vector-database/\">vector database</a> makes it easy to build high-performance vector search applications. Developer-friendly, fully managed, and easily scalable without infrastructure hassles.\n  cta1:\n    href: https://app.pinecone.io/\n    text: Sign Up for Free\n  cta2:\n    href: /contact/\n    text: or contact us\n  img:\n    src: /images2/pinecone-diagram.svg\n    alt: Pinecone Diagram\n    width: 447\n    height: 548\n\n# Use cases\nusecases:\n  eyebrow: Use Cases\n  title: What can you do with <span>vector search</span>?\n  description: |\n    Once you have [vector embeddings](/learn/vector-embeddings/), manage and search through them in Pinecone to power [semantic search](/ai-search/), recommenders, and other applications that rely on relevant information retrieval.\n  list:\n    - title: Search\n      icon: \"/images/semantic-text-search.svg\"\n      children:\n        - Semantic search\n        - Product search\n        - Multi-modal search\n        - Question-Answering\n\n    - title: Generation\n      icon: \"/images/deduplication2.svg\"\n      children:\n        - Chatbots\n        - Text generation\n        - Image generation\n\n    - title: Security\n      icon: \"/images/fraud-detection.svg\"\n      children:\n        - Anomaly Detection\n        - Fraud Detection\n        - Bot/threat detection\n        - Identity verification\n\n    - title: Personalization\n      icon: \"/images/personalization-alt.svg\"\n      children:\n        - Recommendations\n        - Feed ranking\n        - Ad targeting\n        - Candidate selection\n\n    - title: Analytics & ML\n      icon: \"/images/model-training.svg\"\n      children:\n        - Data labeling\n        - Model training\n        - Molecular search\n        - Generative AI\n\n    - title: Data Management\n      icon: \"/images/data-labeling.svg\"\n      children:\n        - Pattern matching\n        - Deduplication\n        - Grouping\n        - Tagging\n\n# Why Pinecone\nfeatures:\n  eyebrow: Why Pinecone\n  title: Fast, fresh, and filtered vector search.\n  list:\n    - title: Fast\n      text:\n        - Ultra-low query latency, even with billions of items. Give users a great experience.\n    - title: Fresh\n      text:\n        - Live index updates when you add, edit, or delete data. Your data is ready right away.\n    - title: Filtered\n      text:\n        - Combine vector search with metadata filters for more relevant and faster results.\n\n# Features\nfeatures2:\n  eyebrow: Managed Vector Database\n  title: |\n    Fully managed,</br><span>production-ready</span>\n  text: Launch, use, and scale your vector search service with our easy API, without worrying about infrastructure or algorithms. We'll keep it running smoothly and securely.\n  list:\n    - title: Easy to use\n      text: Get started on the free plan with an easy-to-use API or the Python client.\n      icon: \"/images/easy-to-use.svg\"\n    - title: Scalable\n      text: Scale from zero to billions of items, with no downtime and minimal latency impact.\n      icon: \"/images/scalable.svg\"\n    - title: Pay for what you use\n      text: Start free, then pay only for what you use with usage-based pricing.\n      icon: \"/images/usage.svg\"\n    - title: No ops overhead\n      text: No need to maintain infrastructure, monitor services, or troubleshoot algorithms.\n      icon: \"/images/gear.svg\"\n    - title: Reliable\n      text: Choose a cloud provider and region — we'll take care of uptime, consistency, and the rest.\n      icon: \"/images/cloud.svg\"\n    - title: Secure\n      text: Pinecone is SOC 2 Type II certified, GDPR-ready, and built to keep data secure.\n      icon: \"/images/secure.svg\"\n\n# Customers\ncustomers:\n  testimonial:\n    title: Perception Point\n    logo: /images/perception-point-logo.svg\n    url: https://perception-point.io\n    description: \"Pinecone made it fast and easy to take vector search from research to production, without DevOps. Thanks to their performance and scalability we can provide accurate, reliable, and real-time threat detection to our customers.\"\n    authorName: Oded K.\n    authorTitle: R&D Group Lead\n    authorImage: /images/oded-kalev.jpeg\n    authorUrl: https://www.linkedin.com/in/oded-kalev-9a4667ba/\n  logos:\n    - logo: /images/gong-logo.png\n      alt: Gong\n    - logo: /images/clubhouse-logo.png\n      alt: Clubhouse\n    - logo: /images/klue-logo.svg\n      alt: Klue\n    - logo: /images/expel-logo.png\n      alt: Expel\n    - logo: /images/course-hero.png\n      alt: Course Hero\n    - logo: /images/circleup-logo.svg\n      alt: CircleUp\n    - logo: /images/xandr-logo.png\n      alt: Xandr\n    - logo: /images/bamboohr-logo.svg\n      alt: BambooHR\n    - logo: /images/mem-logo.svg\n      alt: Mem\n    - logo: /images/cohere.png\n      alt: \"co:here\"\n---\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fb9f"
  },
  "filename": "security.md",
  "title": "info-page",
  "category": "\"Trust and Security\"",
  "content": "---\nlayout: info-page\ntitle: \"Trust and Security\"\nheadline: \"Trust and Security\"\ncta: true\nintro: \"\"\n---\n\nSome of the largest enterprises trust Pinecone to store their data and power their critical applications. We work hard to earn and maintain that trust by treating security and reliability as a cornerstone of our company and product. We extend this to all our users and customers, regardless of size and industry.\n\nSince its founding in 2019, Pinecone has been developed by engineers and scientists experienced in building secure and reliable systems at companies like Amazon, Yahoo, and Google. Pinecone is based in California, United States.\n\n---\n\n## Data Safeguards\nPinecone runs on fully managed and secure AWS infrastructure as a multi-tenant Kubernetes cluster.\n* Customer data is stored in isolated containers.\n* Customer data is encrypted at rest and in transit.\n* Customer data is never used for any reason other than servicing API calls.\n* Pinecone only monitors operational metrics to support the operational health and performance of the system.\n* Strict role based access control (RBAC) for service engineers.\n\n\n## Additional Safeguards for Dedicated-Cloud Deployments\nEnterprise customers enjoy all the safeguard above and additional security measures \n* A dedicated AWS account for complete resource isolation.\n* A dedicated, single-tenant, Kubernetes cluster.\n* Complete network isolation from the internet.\n* AWS CloudTrail is enabled for audit logging.\n* The above holds for all data including vector data and metadata.\n\n\n[Contact us](/contact/) for complete deployment options.\n\n## SOC2 Type II\nPinecone is SOC2 Type II certified. The certification is based on the COSO framework and has been audited by an external Big4 CPA firm (EY). The scope of the program includes Information Security, Availability, and Confidentiality.\n\n## GDPR\nPinecone is committed to supporting customers in their GDPR compliance efforts, and has undertaken the necessary steps to be GDPR-ready. The [Website Privacy Policy](/privacy/), [Customer Data Protection Addendum](/dpa/), and [Product Privacy Statement](/product-privacy/) detail how Pinecone collects, uses, and protects information.\n\n## Penetration Tests\nPinecone routinely undergoes third-party security reviews and remediates findings according to their criticality and prioritization. Security personnel can request executive summaries of findings by contacting info@pinecone.io.\n\n## Policies, Guidelines, and Practices for Protecting Data\nPinecone information assets and systems are classified into public and confidential, including a subset of Pinecone Confidential Information which is “Pinecone Third Party Confidential” information. This is confidential information belonging or pertaining to Pinecone customers or another corporation.\n\nThe use of these assets is subject to an Acceptable Use Policy which includes user accounts, passwords, media use, email and communication activities, and other such procedures.\n\nAccess Control is based on a policy that instructs relevant employees of the company about methods of access control management and user authorizations in the information systems of the company.\n\nHR policies and procedures define the proper ways to address various security issues in Human Resources management, prior to employment (screening, interviewing, background checks), during employment, and at the time of termination of employment (i.e. off-boarding).\n\nPinecone follows Software Development Lifecycle (SDLC) best practices. Pinecone has a procedure that defines the process for change control in Pinecone's systems and services in its production environment, relating to development, implementation, operations, and IT issues.\n\n## Employee Access Lifecycle\nPinecone addresses various security issues in Human Resources management, prior to employment, during employment, and at the termination of employment. Onboarding includes data security training and adherence to the requirements set in the Information Security Policy, Acceptable Use Policy, and Code of Conduct. The business unit owner and Pinecone IT provide the employee only with the relevant access rights, according to his or her work profile and role. \n\nFor an employee or an employee transferred to a new position, the CTO will provide the authorizations accordingly with the Role-Based Access Control Matrix after receiving a trigger from HR. Any change in an employee's position in Pinecone or change in his or her access privileges is reported to HR and documented by HR.\n\n\n## Risk Assessment Process\nPinecone's Risk Assessment process takes place on an annual basis to identify, assess, and manage risks that affect the company's ability to achieve its objectives. The Risk Assessment process involves identifying, assessing, and minimizing risks through ongoing monitoring and risk assessment procedures that are built into the normal recurring activities and include regular management and supervisory activities. Action plans are tracked by the COO and communicated to appropriate personnel.\n\n## Incident Notification\nPinecone has an incident management policy, including effective identification, repairs, investigation, prevention, and follow-up actions. In case of a security incident, Pinecone’s incident management team will act and make decisions as necessary to appropriately respond to security incidents and breaches of personal data in accordance with the applicable laws and regulations.\n\nThe incident management team includes the CEO and COO and all relevant employees as decided by the CEO and COO. Wherever a security incident of either a physical or electronic nature is suspected or confirmed, all parties are expected to follow appropriate procedures and instructions given by the incident management team.\n\n## Monitoring\nPinecone aggregates production environment audit logs from various components such as Kubernetes, storage, and networking. Some of them are analyzed automatically (e.g. GuardDuty) while others are reviewed manually on a regular basis for signs of intrusion.\n\n* Code Vulnerability Scanner: Pinecone performs weekly scans of its code base using a service that provides fix suggestions for any discovered vulnerabilities. Pinecone engineers promptly address any critical issues.\n* External Vulnerability Scanner: Pinecone uses a service to scan production environments at least once a quarter for network vulnerabilities.\n* Events Threat Detections: Pinecone’s production environment audit logs are archived and analyzed.\n\n## Questions\n[Contact us](/contact/) for more information related to Pinecone trust and security.\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fba0"
  },
  "filename": "rust-rewrite.md",
  "title": "post",
  "category": "Rewriting a high performance vector database in Rust",
  "content": "---\nlayout: post\ntitle: Rewriting a high performance vector database in Rust\nheadline: Rewriting a high performance vector database in Rust\ncategories:\n  - Company News\ntoc: >-\nauthor:\n  name: Jack Pertschuk\n  position: Engineering Manager\n  src: /images/jack-pertschuk.jpg\n  href: https://www.linkedin.com/in/jack-pertschuk-833196114/\ndate: \"2022-09-14\"\nthumbnail: \"/images/rust-rewrite.jpg\"\n# Open Graph\ndescription: Rewriting a high performance vector database in Rust\nimages: [\"/images/rust-rewrite.jpg\"] \n---\n\n![August’s Rust NYC Meetup](/images/rust-rewrite.jpg)\n\nI recently spoke at the [Rust NYC meetup group](https://www.meetup.com/rust-nyc/) about the Pinecone engineering team’s experience rewriting our vector database from Python and C++ to Rust. The event was very well attended (178+ registrations), which just goes to show the growing interest in Rust and its applications for real-world products. Below is a recap of what I discussed, but make sure to check out the [full recording](https://www.youtube.com/watch?v=zv6bXXQmMqA) if interested in learning more. \n\n## Introduction to Pinecone - why are we here?\n\nData lakes, ML Ops, feature stores - these are all common buzzwords trying to solve similar sorts of problems. For example, let’s say you have a lot of unstructured data, and in order to gain insights you store it in blob storage. Historically, you would use an ML Ops platform, like a hosted Spark pipeline, for this. However, in many ways, we’re seeing the industry start to transition to the concept of vector databases and specifically approximate nearest neighbor (ANN) search to support similar use cases. \n\n![Vector Databse](/images/rust-rewrite-vector-database.png)\n\n[Pinecone](/) is a fully managed, SaaS solution for this piece of the puzzle - [the vector database](/learn/vector-database/). While the concept of the vector database has been used by many large tech companies for years, these sorts of companies have built their own proprietary, deep learning ANN indexing algorithms to serve news feeds, advertisements, and recommendations. These infrastructures and algorithms require intensive resources and overhead that most companies can’t support. With its strict memory management, efficient multi-threading, and fast, reliable performance, this is where the Pinecone solution comes in.\n\n## Ramping up with Rust\n\nPinecone was originally written in C++ with a connectivity wrapper written in Python. While this worked well for a while, we began to run into issues. \n\nFirst of all, Python is a garbage collected language, which means it can be extremely slow for writing anything high performance at scale. In addition, it’s challenging to find developers with experience in both Python and C++. And so the idea of iterating on the database was born - we wanted to find some way to unify our code base while achieving the performance predictability we needed.\n\nWe looked at and compared several languages - Go, Java, C++, and Rust. We knew that C++ was harder to scale and maintain high quality as you build a dev team; that Java doesn't provide the flexibility and systems programming language we needed; and that Go is also a garbage collected language. This left us with Rust. With Rust, the pros around performance, memory management, and ease of use outweighed the cons of it not yet being a very established language. \n\n## Identifying bottlenecks \n\n### Continuous benchmarking\n\nAs we began ramping up with Rust, we ran into a few bottlenecks. Before shipping the newly rewritten database, we wanted to ensure it continued to scale easily and have predictable performance. How did we test this? With continuous benchmarking. \n\nContinuous benchmarking allowed us to see every commit broken down by the performance of a specific benchmark test. Through HTML reports, we are able to see the exact commit that caused the regression of the debt anytime a code change is merged. \n\n![Continuous benchmarking](/images/continuous-benchmarking.png)\n\nAs you can see in the above graph, a commit was merged that caused a huge spike. However, with Criterion, an open source benchmarking tool, we were easily able to identify it, mitigate it, and push a fix. And over time, we lowered our latency and shipped improvements.\n\n### Building an observability layer \n\nAt this point, we’ve confirmed that the new database is performant, and have benchmarks to run it against. But what happens when you go to production, and things are slower than they should be? This is when you need an observability solution. \n\nAdding an observability layer with Rust can be complicated without the support of a more mature developer community. As a result, we wanted a solution with minimal instrumentation, that’s easy to integrate, and is cloud agnostic. Our end goal was to provide a layer compatible with Datadog, Prometheus or any other metrics provider. \n\nThere are two main components to our observability layer - traces and aggregated metrics. With each of these signals, you can see how each part of the code is performing over time. \n\nHow did we achieve this? For metrics, we used some macros for histogram and counter metrics. We also used a custom Rust macro that hooks into OpenMetrics, and from there we can push metrics to Prometheus or Datadog. For tracing, we took a similar approach. We implemented an OpenTelemetry protocol that allows us to send traces to any observability solution. This way we’re able to see all of our metrics and trace requests as graphs in a single dashboard (see the below example). \n\n![Dashboard graphs](/images/dashboard-graphs.png)\n\n## Optimizing performance with Rust\n\nAfter identifying and addressing the above bottlenecks, we were able to focus on optimizing performance. With Rust, there are several aspects around achieving high performance that we liked - low level optimized instruction sets, memory layout, and running async tasks. \n\n### Optimized instruction sets \n\nOne of the things we considered when choosing Rust was its access to low level optimized instruction sets, which are critical for optimizing the kind of vector based workloads that Pinecone utilizes. So for example, AVX-512 allows us to utilize parallel dot-product to compute high throughput dot-product queries on anything. And Rust gives us direct access to these compiler optimizations.\n\n### Memory layout\n\nIf you're using a higher level language, you're not going to have access to how the memory is laid out. A simple change, like removing indirection in our list, was an order of magnitude improvement in our latencies since there's memory prefetching in the compiler and the CPU can anticipate which vectors are going to be loaded next in order to improve the memory footprint.\n\n### Running async tasks \n\nRust is async, and [Tokio](https://tokio.rs/) is the one of the most popular async providers. It's performant, ergonomic, and has options for running on a single event loop. However, it's not great for running CPU intensive workloads, like with Pinecone. \n\nWhen it comes to running these tasks, there are many options. For example, because Tokio has different runtime modes, you can run it by itself in this async mode with multiple threads. And in that context, you can block on an individual thread in place, which is called 'block_in_place'. You can also use 'spawn_blocking'. \n\nThere are also “smart” work, parallel processing libraries, like [Rayon](https://github.com/rayon-rs/rayon), that maintain a thread pool and implement things like work stealing. And finally there's the option of your own solution. If you want more control, you can use MPSC channels. While you have to write some custom code, they give you the fine grained ability to schedule work and ensure data locality. \n\n## What’s next for Pinecone? \n\nWe are continuing to optimize our codebase to ensure we’re maintaining a highly performant, stable, and fast database. This recap highlights the key points discussed during the meetup, but make sure to [watch the full recording](https://www.youtube.com/watch?v=zv6bXXQmMqA) for more detail. \n\nIf you are interested in learning more about Pinecone and vector databases, check out the resources on our [learn page](/learn/) or [try it out](https://app.pinecone.io/) (it's free). Also, if you’re currently using or interested in working with Rust, [we are hiring](/careers/#open-roles). \n\n{{< youtube zv6bXXQmMqA >}}\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fba1"
  },
  "filename": "organizations.md",
  "title": "post",
  "category": "\"Organizations",
  "content": "---\nlayout: post\ntitle: \"Organizations: New access controls to makes vector search a company-wide capability\"\nheadline: \"Organizations: New access controls to makes vector search a company-wide capability\"\ncategories:\n  - Company News\ntoc: >-\nauthor:\n  name: Gibbs Cullen\n  position: Senior Product Marketing Manager\n  src: /images/gibbs-cullen.jpg\n  href: https://www.linkedin.com/in/gibbscullen/\ndate: \"2022-12-07\"\n# Open Graph\ndescription: With organizations, users and teams can now collaborate across projects in a way that works best for them.\nimages: [\"/images/organizations-announcement-1v2.png\"]\nthumbnail: \"/images/organizations-announcement-thumbnail.png\"\n---\n\nPrior to organizations, projects and billing within Pinecone could only be owned and managed by a single user. But when responsibilities change, you may want more than one user to manage projects and billing so operations continue to run smoothly. Or as new projects arise, you may want additional users to make contributions or test out a new application.\n\nWith organizations, teams can easily define roles to manage access and billing for a set of projects. Every project is assigned to an organization with an organization owner, and all projects within an organization have the same billing method. In addition, a project’s users can easily transfer ownership amongst themselves and consolidate usage across projects within an organization for billing purposes. Users and teams can now collaborate across projects in a way that works best for them.\n\nAs of today, organizations are available to all users and support four predefined roles: Organization Owner, Organization User, Project Owner, and Project User. Note that all existing users and projects have been assigned to an organization by default.\n\n## How it works\n\nTo get started with organizations, navigate to “Settings” in the left-hand menu of the Pinecone Console, then click on “Organization” to see your current organization. You can then set the desired plan type and billing method using the “Billing” tab under the organization view.\n\n![Organization Settings](/images/organizations-announcement-2.png)\n\nIf you are a [project](https://preview.redoc.ly/pinecone/orgs/manage-projects) or [organization](https://preview.redoc.ly/pinecone/orgs/organizations) owner, you can also add users to organizations and projects with a few easy steps:\n\n1. In the Settings view, click the “Users” tab.\n\n2. Click “Invite User”.\n   {{< image src=\"/images/organizations-announcement-3.png\" alt=\"Invite user\" width=\"auto\">}}\n\n3. (Organization owner only) Select an [organization role](https://preview.redoc.ly/pinecone/orgs/organizations#organization-roles).\n\n4. Select a project and [project role](https://preview.redoc.ly/pinecone/orgs/projects#project-roles).\n\n5. Enter the user's email address, and “Invite User”.\n\nEach role has a different level of access - at both an organization and project level. Note that everyone within an organization is an organization user by default.\n\n- **Organization owner**: Manages organization settings and billing, and creates and manages all project and index settings.\n- **Organization user**: Read-only access to organization settings, but can create a project within an organization and manage project permissions.\n- **Project owner**: Manages project settings, API keys, and the index.\n- **Project user**: Read-only access to API keys, but can manage the index.\n\n## Get started\n\nLearn more about this feature and how to get started in our [documentation](/docs/add-users-to-projects-and-organizations/).\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fba2"
  },
  "filename": "reader-models.md",
  "title": "ebook-post",
  "category": "\"Readers for Question-Answering\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Readers for Question-Answering\"\nheadline: \"Reader Models for Open Domain Question-Answering\"\ncategories:\n  - NLP for Semantic Search\ntoc: >-\nweight: 9\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: How to fine-tune reader models to identify answers from relevant contexts.\n# Open graph\nimages: ['/images/reader-models-0.jpg']\n---\n\nOpen-domain question-answering (ODQA) is a wildly popular *pipeline* of databases and language models that allow us to ask a machine human-like questions and return comprehensible and even intelligent answers.\n\nDespite the outward guise of simplicity, ODQA requires a reasonably advanced set of components placed together to enable the *extractive* Q&A functionality.\n\nWe call this *extractive* Q&A because the models are not generating an answer. Instead, the answer already exists but is hidden somewhere within potentially thousands, millions, or even more data sources.\n\nBy enabling extractive Q&A, we enable a more *intelligent* and *efficient* way to retrieve information from what can be massive stores of data.\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/-fzCSPsfMic\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\nODQA relies on three components: the vector database, for storing encoded vector representations of the data we will search, a retriever to handle context and question encoding, and a reader model that consumes relevant *retrieved* contexts and identifies a shorter, more specific answer.\n\nThe reader is the final act in an ODQA pipeline; it takes the contexts returned by the vector database and retriever components and *reads* them. Our reader will then return what it believes to be the *specific answer* to our question.\n\nTo be exact, we don't get the 'specific answer'. The model is reading *input IDs*, which are integers representing words or subwords. So, rather than returning a human-readable text answer, it actually returns a *span* of input ID positions.\n\n![question_context_answer](/images/reader-models-1.jpg)\n<small>Question (grey), context (cyan), and answer (blue). The model doesn't read the strings. It reads token IDs, and so when outputting a prediction for the answer, it outputs a span of token IDs that it believes represent the answer.</small>\n\nTo fine-tune a model, we need two inputs and two labels. The inputs are the question and a relevant context, and the labels are the answer's start and end positions.\n\n![inputs_and_labels](/images/reader-models-2.jpg)\n<small>Inputs (cyan) and target labels (answer start and end positions, blue). The start and end positions are the token positions from the encoded question-context *input_ids* tensor that represent the start and end of the answer (extracted from the context).</small>\n\nThere isn't much more to fine-tuning a reader model. It’s a relatively straightforward process. The most complex part is pre-processing the training data.\n\nWith our overview complete, let's dive into the details and work through an actual training example.\n\n\n\n## Implementation\n\nThere are more steps when training a reader model than just *train the model*. As mentioned, these other steps can prove to be the tricky part. In our case, we have three distinct steps.\n\n1. Download and pre-process Q&A dataset\n2. Fine-tune the model\n3. Evaluation\n\nWithout any further ado, let's begin with the data.\n\n### Download and Pre-process\n\nWe will be using the **S**tanford **Q**uestion and **A**nswering **D**ataset (SQuAD) for fine-tuning. We can download it with HuggingFace *Datasets*.\n\n{{< notebook file=\"download-squad\" height=\"full\" >}}\n\nLooking at this, we have *five* features, of which we only care about `question`, `context` for the inputs, and `answers` for the labels.\n\n{{< notebook file=\"squad-0\" height=\"full\" >}}\n\nWe must make a few transformations to format the `answers` into the start and end token ID positions we need. We have `answer_start`, but this gives us the position within the context *string* that the answer begins. These positions are not what the model needs. Instead, it requires the start position using token ID indexes.\n\n{{< notebook file=\"squad-ans-extract\" height=\"full\" >}}\n\nThat is our main hurdle. To push through it, we will take three steps:\n\n1. Tokenize the context.\n2. Convert `answer_start` to a token ID index.\n3. Find the end token index using the starting position and answer `text`.\n\nStarting with **tokenize the context**, we first initialize a tokenizer using the HuggingFace *Transformers* library.\n\n{{< notebook file=\"tokenize\" height=\"full\" >}}\n\nThen we tokenize our question-context pairs, and this returns *three* tensors by default:\n\n* `input_ids`, the token ID representation of our text.\n* `attention_mask` a list of values telling our model whether to apply the attention mechanism to respective token embeddings with `1` or to ignore padding token positions with `0`.\n* `token_type_ids` indicates sentence A (the question) with the first set of `0` values, sentence B (the context) with `1` values, and remaining padding tokens with the trailing `0` values.\n\nWe have added another tensor called `offset_mapping` by setting `return_offsets_mapping=True`. This tensor is very important for finding our label values for training our model.\n\nEarlier, we found the start and end positions for the *character* positions from our *context* string. As mentioned, we cannot use these. We need the token positions, and the `offset_mapping` tensor is essential in finding the token positions.\n\n{{< notebook file=\"decode-example\" height=\"full\" >}}\n\nAnother consideration when finding the token position is that when we tokenized, we tokenized both the question *and* context as shown above where we follow the format `[CLS] question [SEP] context [SEP] padding`. To find the answer start and end positions, we must shift the values by the length of the question segment.\n\nTo find the question and context segment lengths, we use the `token_type_ids` tensor.\n\n{{< notebook file=\"get-lengths\" height=\"full\" >}}\n\nWe need to consider one additional case where the answer has been truncated or never existed (some records have no answer). In both of these scenarios, we set the start and end positions to `0`.\n\n{{< notebook file=\"char-to-id\" height=\"full\" >}}\n\nOnce we have the start and end positions, we need to define how we will load the dataset into our model for training. At the moment, our dataset will return lists of dictionaries for each training batch.\n\nWe cannot feed lists of dictionaries into our model. Instead, we need to pull these dictionaries into single batch-size tensors. For that, we use the `default_data_collator` function.\n\n{{< notebook file=\"prep-data\" height=\"full\" >}}\n\nWe don't need to do anything else with our dataset or data collator for now, so we move on to the next step of fine-tuning.\n\n### Fine-tuning the Model\n\nAs mentioned, we will be fine-tuning the model using the HuggingFace *Transformers* `Trainer` class. To use this, we first need a model to fine-tune, which we load as usual with transformers.\n\n{{< notebook file=\"init-model\" height=\"full\" >}}\n\nNext, we set up the `Trainer` training parameters.\n\n{{< notebook file=\"training-params\" height=\"full\" >}}\n\nWe use tried and testing training parameters used in the first BERT for QA with SQuADv2 paper *and* Deepset AI's BERT training parameters, we set a learning rate of `2e-5`, `0.1` weight decay, and train in batches of `24` for `3` epochs [1] [2].\n\n{{< notebook file=\"trainer-train\" height=\"full\" >}}\n\nLike we said, fine-tuning the model is the easy part. We can find our model files in the directory defined in the `args` parameter, in this case, `./bert-base-uncased-squad2`. We will see a set of folders named `checkpoint-x` in this directory. The last of those is the *latest* model checkpoint saved during training.\n\n![model_dir](/images/reader-models-3.jpg)\n<small>Model and tokenizer files in the `/bert-reader-squad2` model directory.</small>\n\nBy default, a new checkpoint is saved every 500 steps. These checkpoint saves mean the *final* model (at step 27,150) is not the final model but rather the model at step 27,000.\n\nThere is unlikely to be a noticeable difference between these two states, so we either take the model files from `./bert-base-uncased-squad2/checkpoint-24000` or we manually save our model with:\n\n{{< notebook file=\"save-model\" height=\"full\" >}}\n\nWe can find the model files in the specified directory.\n\n### Inference\n\nBefore moving on to the next step of evaluation, let's take a look at how we can use this model.\n\nFirst, we initialize a transformers `pipeline`.\n\n{{< notebook file=\"init-pipeline\" height=\"full\" >}}\n\nNext, we prepare the evaluation data. Again we will use the `squad_v2` dataset from HuggingFace, taking the *validation* split.\n\n{{< notebook file=\"val-split\" height=\"full\" >}}\n\nThe `pipeline` requires an iterable set of key-value pairs where the only keys are `question` and `context`. We can simply drop the unneeded columns of `id` and `title` to handle this. However, we will need to keep track of the true answers during the next step of *evaluation*, so we store them in a separate `ans` dataset.\n\n{{< notebook file=\"ans-dataset\" height=\"full\" >}}\n\nTo make a prediction, we take a single *question* and *context* and feed them into our pipeline `qa`:\n\n{{< notebook file=\"qa-example\" height=\"full\" >}}\n\nWe'll process the whole dataset like this in the next section.\n\n## Evaluation\n\nWe've technically finished fine-tuning our model, but it's not of much use if we can't validate its performance. We need confidence in the model's performance.\n\nEvaluation of our reader model is a little tricky as we want to identify matches between true and predicted answer labels. The most straightforward approach is to use an **E**xact **M**atch metric. This metric will simply tell us `1` if the true and predicted answers are *precisely* the same or `0` if not.\n\nThere are two reasons we might want to avoid this and try something more flexible. First, we may find that a model predicts the correct answer, but when decoded, the predicted tokens are in a slightly different format.\n\nThe second reason is that our model might predict a *partially correct* answer and partially correct is better than nothing, but this *better than nothing* isn't accounted for by the EM metric.\n\n{{< notebook file=\"em-v-rouge\" height=\"full\" >}}\n\nWe can solve the first issue in *most cases* by normalizing both the true and predicted answers, meaning we lowercase, remove punctuation, and remove any other potential points of conflict.\n\nThe second problem requires a more sophisticated solution, and it is best if we *do not* use the EM metric. Instead, we use *ROUGE*.\n\nThere are a few different ROUGE metrics. We will focus on ROUGE-N, which measures the number of matching *n-grams* between the predicted and true answers, where an n-gram is a grouping of tokens/words.\n\nThe *N* in ROUGE-*N* stands for the number of tokens/words within a single n-gram. This means that ROUGE-1 compares individual tokens/words (unigrams), ROUGE-2 compares tokens/words in chunks of two (bigrams), and so on.\n\n![ngrams](/images/reader-models-4.jpg)\n<small>Example of unigram, bigram, and trigram which are single-token, double-token, and triple-token groupings respectively.</small>\n\nEither way, we return a score of `1` for an exact match, `0` for no match, or any value in between.\n\n{{< notebook file=\"rouge-examples\" height=\"full\" >}}\n\nTo apply ROUGE-1 for measuring reader model performance, we first need to *predict* answers using our model. We can then compare these predicted answers to the true answers.\n\n{{< notebook file=\"get-results\" height=\"full\" >}}\n\nFinally, given the two sets of answers, we can call `rouge.get_scores` to return recall `r`, precision `p`, and F1 `f` scores for both uni and bi-grams.\n\nWe still need to deal with where there is no answer and that the SQuAD evaluation set contains four possible answers for each sample.\n\n{{< notebook file=\"question-examples\" height=\"full\" >}}\n\nWe could check if the model correctly predicted that no answer exists for the ‘no answer’ scenario. If the model correctly identifies that there is no answer, we would return a score of *1.0*. Otherwise, we would return a score of *0.0*.\n\nWe will calculate the ROUGE-1 F1 score for every possible answer to deal with the multiple answers and take the best score.\n\nAfter calculating all scores, we take the average value. This average value is the final ROUGE-1 F1 score for the model.\n\n| Model                              | ROUGE-1 F1 |\n| ---------------------------------- | ---------- |\n| `bert-reader-squad2`               | 0.354      |\n| `deepset/bert-base-uncased-squad2` | 0.450      |\n\nThese scores seem surprisingly low. A big reason for this is the *no answer scenarios*. Let's take a look at a few.\n\n{{< notebook file=\"unanswerable\" height=\"full\" >}}\n\nIf, like me, you're wondering how these are unanswerable, take note of the particular question and context wording. The first example specifies the 1000s and 1100s, but the context is the 10th and 11th centuries, e.g., 1100s and 1200s. The second example question should be *\"**destructive incursions** devolved into **encampments**\"*. The third should be *\"draining **mines**\"*.\n\nEven by humans, each of these questions is easily mistaken as answerable. If we remove unanswerable examples, the model scores are less surprising.\n\n| Model                              | ROUGE-1 F1 |\n| ---------------------------------- | ---------- |\n| `bert-reader-squad2`               | 0.708      |\n| `deepset/bert-base-uncased-squad2` | 0.901      |\n\nThe importance of identifying unanswerable questions varies between use cases. Many will not need to identify unanswerable questions, so question whether your models should prioritize unanswerable question identification or focus on performing well on answerable questions.\n\n\n\nThat’s it for this walkthrough in fine-tuning reader models for ODQA pipelines. By understanding how to fine-tune a QA reader model, we are able to effectively optimize the final step in the ODQA pipeline for our own specific use cases.\n\nPairing this with a custom vector database and retriever components allows us to add highly optimized ODQA capabilities to a variety of possible use cases, such as internal document search, e-commerce product discovery, or anything where a more natural information retrieval experience can be beneficial.\n\n## References\n\n[1] Y. Zhang, Z. Xu, [BERT for Question Answering on SQuAD 2.0](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/default/15848021.pdf) (2019)\n\n[2] [Model Card for *deepset/bert-base-uncased-squad2*](https://huggingface.co/deepset/bert-base-uncased-squad2), HuggingFace Model Hub\n\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fba3"
  },
  "filename": "spotify-podcast-search.md",
  "title": "ebook-post",
  "category": "\"Spotify's Podcast Search Explained\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Spotify's Podcast Search Explained\"\nheadline: \"How Spotify Uses Semantic Search for Podcasts\"\ncategories:\n  - Vector Search in the Wild\ntoc: >-\nweight: 1\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: How Spotify is reimagining podcast discovery with semantic search.\n# Open graph\nimages: [\"/images/spotify-podcast-search-0.jpg\"]\nthumbnail: \"https://www.pinecone.io/images/spotify-podcast-search-0.jpg\"\n---\n\n**Want to add audio search to your applications just like Spotify? You'll need a vector database like [Pinecone](/). [Try it now for free.](https://app.pinecone.io)**\n\nThe market for podcasts has grown tremendously in recent years, with the number of global listeners having increased by 20% annually in recent years [1].\n\nDriving the charge in podcast adoption is Spotify. In a few short years, they have become the undisputed leaders in podcasting. Despite only entering the game in 2018, by late 2021, Spotify had already usurped Apple, the long-reigning leader in podcasts, with more than 28M monthly podcast listeners [2]\n\nTo back their podcast investments, Spotify has worked on making the podcast experience as seamless and accessible as possible. From their all-in-one podcast creation app (Anchor) to podcast APIs and their latest *natural language enabled* podcast search.\n\nSpotify's natural language search for podcasts is a fascinating use case. In the past, users had to rely on keyword/term matching to find the podcast episodes they wanted. Now, they can search in natural language, in much the same way we might ask a real person where to find something.\n\nThis technology relies on what we like to call *semantic search*. It enables a more intuitive search experience because we tend to have an *idea* of what we're looking for, but rarely do we know precisely which terms appear in what we want.\n\nImagine we wanted to find a podcast talking about healthy eating over the holidays. How would we search for that? It might look something like:\n\n![podcast-search](/images/spotify-podcast-search-1.png)\n\nThere is a podcast episode talking about precisely this. Its description is:\n\n```\n\"Alex Straney chats to Dr. Preeya Alexander about how to stay healthy over Christmas and about her letter to patients.\"\n```\n\nWe have zero overlaps between the query and episode description using term matching, so this result would not be returned using keyword search. To make matters worse, there are undoubtedly thousands of episode descriptions on Spotify containing the words *\"eat\"*, *\"better\"*, and *\"holidays\"*. These episodes likely have nothing to do with our intended search query, but we could return them.\n\nSuppose we were to swap that for a semantic search query. We could see much better results because semantic search looks at the meaning of the words and sentences, *not* specific terms.\n\nDespite sharing no words, our query and episode description would be identified as having very similar meanings. They both describe *being or eating healthier over the winter holidays*.\n\nEnabling meaningful search is not easy, but the impact can be huge if done well. As Spotify has proven, it can lead to a much greater user experience. Let's dive into how Spotify built its natural language podcast search.\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/ok0SDdXdat8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n## Semantic Search\n\nThe technology powering Spotify's new podcast search is more widely known as semantic search. Semantic search relies on two pillars, [**N**atural **L**anguage **P**rocessing (NLP)](/learn/nlp/) and [vector search](/learn/vector-search-basics/).\n\nThese technologies act as two steps in the search process. Given a natural language query, a particular NLP model can encode it into a [vector embedding](/learn/vector-embeddings/), also known as a [*dense vector*](/learn/dense-vector-embeddings-nlp/). These dense vectors can numerically represent the meaning of the query. We can visualize this behaviour:\n\n<div>                        <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script src=\"https://cdn.plot.ly/plotly-2.11.1.min.js\"></script>                <div id=\"a86fded0-6adc-4826-86d4-c0f215e40c4c\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a86fded0-6adc-4826-86d4-c0f215e40c4c\")) {                    Plotly.newPlot(                        \"a86fded0-6adc-4826-86d4-c0f215e40c4c\",                        [{\"customdata\":[[\"how do I cook great food\"],[\"interview with cookbook author\"],[\"chat with chef who wrote books\"],[\"podcast about cooking and writing\"],[\"the writing show\"],[\"eat better during xmas holidays\"],[\"superhero film analysis\"],[\"how to tell more engaging stories\"],[\"how to keep readers interested\"],[\"how to make money with online content\"],[\"why is technology so addictive\"]],\"hovertemplate\":\"text=%{customdata[0]}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#1C17FF\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"scene\":\"scene\",\"showlegend\":false,\"x\":[0.4778360334823838,0.3877692637150937,0.3124091180014456,0.389321831339922,-0.05578718560486039,0.19325670270367676,-0.3115741989711601,-0.3135352627882797,-0.35675623425504904,-0.29536054403167183,-0.42757952359150037],\"y\":[-0.17568194026164283,0.14845017300523397,0.16943416332590094,0.11031489848600957,0.22348277549895443,-0.42691002814007695,-0.4709285642813247,0.2510819837829344,0.47394553650664173,-0.023354030051592516,-0.27983496787103823],\"z\":[-0.05635501984033207,0.18334881332576225,0.08162569729764292,0.07194730746878393,0.21446302600133849,-0.5250254378046134,0.5492251168919206,-0.17995528106009603,-0.1520339801885487,-0.029103124835297464,-0.1581371172565605],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"x\"}},\"yaxis\":{\"title\":{\"text\":\"y\"}},\"zaxis\":{\"title\":{\"text\":\"z\"}}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    )                };                            </script>        </div>\n\nThese vectors have been encoded by one of these special NLP models, called [*sentence transformers*](/learn/sentence-embeddings/). We can see that queries with similar meanings cluster together, whereas unrelated queries do not.\n\nOnce we have these vectors, we need a way of comparing them. That is where the *vector search* component is used. Given our new query vector, we perform a vector search and compare it to previously encoded vectors and retrieve those that are nearest or the most similar.\n\n![podcast-vec-search-distance](/images/spotify-podcast-search-2.png)\n\n<small>Given a query vector *xq*, we could calculate the distance between that and other indexed vectors to identify the top two [nearest \"neighbors\"](/learn/k-nearest-neighbor/).</small>\n\nNLP and vector search have been around for some time, but recent advancements have acted as catalysts in the performance increase and subsequent adoption of semantic search. In NLP, we have seen the introduction of high-performance [transformer models](/learn/transformers/). In vector search, the rise of **A**pproximate **N**earest **N**eighbor (ANN) algorithms.\n\nTransformers and ANN search have powered the growth of semantic search, but *why* is not so clear. So, let's demystify how they work and why they've proven so helpful.\n\n### Transformers\n\nTransformer models have become the standard in NLP. These models typically have two components: the core, which focuses on *\"understanding\"* the meaning of a language and/or domain, and a head, which adapts the model for a particular use case.\n\nThere is just one problem, the core of these models requires vast amounts of data and computing power to pretrain.\n\n---\n\n*Pretraining refers to the training step applied to the core transformer component. It is followed by a fine-tuning step where the head and/or the core are trained further for a specific use case.*\n\n---\n\nOne of the most popular transformer models is BERT, and BERT costs a reported $2.5k - 50K to train; this shifts to \\$80K - \\$1.6M for the larger BERT model [4].\n\nThese costs are prohibitive for most organizations. Fortunately, that doesn't stop us from using them. Despite these models being expensive to pretrain, they are an order of magnitude cheaper to *fine-tune*.\n\nThe way that we would typically use these models is:\n\n1. The core of the transformer model is pretrained at great cost by the likes of Google, Microsoft, etc.\n2. This core is made publicly available.\n3. Other organizations take the core, add a task-specific *\"head\"*, and fine-tune the extended model to their domain-specific task. Fine-tuning is less computationally expensive and therefore cheaper.\n4. The model is now ready to be applied to the organization's domain-specific tasks.\n\nIn the case of building a podcast search model, we could take a pretrained model like `bert-base-uncased`. This model already \"understands\" general purpose English language.\n\nGiven a training dataset of *user query* to *podcast episode* pairs, we could add a *\"mean pooling\"* head onto our pretrained BERT model. With both the core and head, we fine-tune it for a few hours on our pairs data to create a *sentence* transformer trained to identify similar query-episode pairs.\n\nWe must choose a suitable pretrained model for our use case. In our example, if our target query-episode pairs were English language only, it would make no sense to take a French pretrained model. It has no base understanding of the English language and could not learn to understand the English query-episode pairs.\n\nAnother term we have mentioned is *\"sentence transformer\"*. This term refers to a transformer model that has been fitted with a pooling layer that enables it to output single vector representations of sentences (or longer chunks of text).\n\n![podcast-pooling](/images/spotify-podcast-search-3.png)\n\n<small>Sentence transformers add a *\"pooling layer\"* to transform the many token embeddings output by a transformer model into a single sentence embedding.</small>\n\nThere are different types of pooling layers, but they all consume the same input and produce the same output. They take many token-level embeddings and merge them in some way to build a single embedding that represents *all* of those token-level embeddings. That single output is called a *sentence embedding*.\n\nThe sentence embedding is a *dense vector*, a numerical representation of the meaning behind some text. These dense vectors enable the *vector search* component of semantic search.\n\n### ANN Search\n\n**A**pproximate **N**earest **N**eighbors (ANN) search allows us to quickly compare millions or even billions of vectors. It is called *approximate* because it does not guarantee that we will find the true nearest neighbors (most similar embeddings).\n\nThe only way we can guarantee that is by exhaustively comparing every single vector. At scale, that's slow.\n\nRather than comparing *every* vector, we approximate with ANN search. If done well, this approximation can be incredibly accurate and super fast. But there is often a trade-off. Some algorithms offer speedier search but poorer accuracy, whereas others may be more accurate but increase search times.\n\n![podcast-search-balance](/images/spotify-podcast-search-4.png)\n\n<small>In vector search, there is often a decision to be made on whether to prioritize latency or accuracy.</small>\n\nIn either case, an approximate solution is required to maintain reasonable query times at scale.\n\n\n\n## How Spotify Did It\n\nTo build this type of semantic search tool, Spotify needed a language model capable of encoding similar *(query, episode)* pairs into a similar vector space. There are existing sentence transformer [models like SBERT](/learn/train-sentence-transformers-softmax/), but Spotify found two issues with using this model:\n\n* They needed a model capable of supporting multilingual queries; SBERT was trained on English text only.\n* SBERT's cross-topic performance *without* further fine-tuning is poor [5].\n\nWith that in mind, they decided to use a different, multilingual model called the **U**niversal **S**entence **E**ncoder (USE). But this still needed fine-tuning.\n\nTo fine-tune their USE model to encode *(query, episode)* pairs in a meaningful way, Spotify needed *(query, episode)* data. They had *four* sources of this:\n\n1. Using their past search logs, they identified *(query, episode)* pairs from successful searches.\n2. They identified unsuccessful searches that were followed by a successful search. The idea is that the unsuccessful query is likely to be a more *natural* query, which was then used as a *(query_prior_to_successful_reformulation, episode)* pair.\n3. Generating synthetic queries using a query generation model produces *(synthetic_query, episode)* pairs.\n4. A small set of curated queries, manually written for episodes.\n\nSources (1 - 3) fine-tune the USE model, with some samples left for evaluation. Source (4) was used for evaluation only.\n\nUnfortunately, we don't have access to Spotify's past search logs, so there's little we can do in replicating sources (1 - 2). However, we can replicate the approach of the building source (3) using query generation models. And, of course, we can manually write queries as per source (4).\n\n### Data Preprocessing\n\nBefore generating any queries, we need episode data. Spotify describes *episodes* as a concatenation of textual metadata fields, including episode title and description, with the podcast show's title and description.\n\nWe can find a [podcast episodes dataset](https://www.kaggle.com/datasets/listennotes/all-podcast-episodes-published-in-december-2017) on Kaggle that contains records for 881k podcast episodes i. Including episode titles and descriptions, with podcast show titles and descriptions.\n\nWe use the Kaggle API to download this data, installed in Python with `pip install kaggle`. An account and API key are needed (find the API key in your *Account Settings*). The *kaggle.json* API key should be stored in the location displayed when attempting to `import kaggle`. If no location or error appears, the API key has already been added.\n\nWe then authenticate access to Kaggle.\n\n```python\nfrom kaggle.api.kaggle_api_extended import KaggleApi\napi = KaggleApi()\napi.authenticate()\n```\n\nOnce authenticated, we can download the dataset using the `dataset_download_file` function, specifying the dataset location (found from its URL), files to download, and where to save them.\n\n```python\napi.dataset_download_file(\n    'listennotes/all-podcast-episodes-published-in-december-2017',\n    file_name='podcasts.csv',\n    path='./'\n)\napi.dataset_download_file(\n    'listennotes/all-podcast-episodes-published-in-december-2017',\n    file_name='episodes.csv',\n    path='./'\n)\n```\n\nBoth *podcasts.csv* and *episodes.csv* will be downloaded as zip files, which we can extract using the `zipfile` library.\n\n```python\nwith zipfile.ZipFile('podcasts.csv.zip', 'r') as zipref:\n    zipref.extractall('./')\nwith zipfile.ZipFile('episodes.csv.zip', 'r') as zipref:\n    zipref.extractall('./')\n```\n\nWe have two CSV files, *podcasts.csv* details the podcast shows themselves, including titles, descriptions, and hosts. The *episodes.csv* data includes data from specific podcast episodes, including episode title, description, and publication date.\n\nTo replicate Spotify's approach of concatenating podcast shows and episode-specific details, we must merge the two datasets. We do this with an inner join on the podcast ID columns.\n\n```python\nepisodes = episodes.merge(\n    podcasts,\n    left_on='podcast_uuid',\n    right_on='uuid',\n    suffixes=('_ep', '_pod')\n)\n```\n\nBefore concatenating the features we want, let's clean up the data. We strip excess whitespace and remove rows where *any* of our relevant features contain null values.\n\n{{< notebook file=\"podcast-clean-data\" height=\"full\" >}}\n\nWe're ready to concatenate, giving us our *episodes* feature.\n\n{{< notebook file=\"podcast-concat-shuffle\" height=\"full\" >}}\n\n### Query Generation\n\nWe now have episodes but no queries, and we need *(query, episode)* pairs to fine-tune a model. Spotify generated synthetic queries from episode text, which we can do.\n\nTo do this, they fine-tuned a query generation BART model using the MS MARCO dataset. We don't need to fine-tune a BART model as plenty of readily available models have been fine-tuned on the exact same dataset. Therefore, we will initialize one of these models using the HuggingFace *transformers* library.\n\n```python\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\n# after testing many BART and T5 query generation models, this seemed best\nmodel_name = 'doc2query/all-t5-base-v1'\n\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name).cuda()\n```\n\nWe tested several T5 *and* BART models for query generation on our episodes data; the [results are here](https://github.com/pinecone-io/examples/blob/master/search/semantic-search/spotify-podcast-search/query-gen.md). The `doc2query/all-t5-base-v1` model was chosen as it produced more reasonable queries and has some multilingual support.\n\nIt's time for us to generate queries. We will generate three queries per episode, in-line with the approach taken by the [GenQ](/learn/genq/) and [GPL](/learn/gpl/) techniques.\n\n{{< notebook file=\"podcast-query-gen\" height=\"full\" >}}\n\nQuery generation can take some time, and we recommend limiting the number of episodes (we used 100k in this example). Looking at the generated queries, we can see some good and some bad. This randomness is the nature of query generation and should be expected.\n\nWe now have *(synthetic_query, episode)* pairs that can be used in fine-tuning a sentence transformer model.\n\n### Models and Fine-tuning\n\nAs mentioned, Spotify considered using pretrained models like BERT and SBERT but found the performance unsuitable for their use case. In the end, they opted for a pretrained **U**niversal **S**entence **E**ncoder (USE) model from TFHub.\n\nWe will use a similar model called DistilUSE that is supported by the *sentence-transformers* library. By taking this approach, we can use the *sentence-transformers* model fine-tuning utilities. After installing the library with `pip install sentence-transformers`, we can initialize the model like so:\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n```\n\nWhen fine-tuning with the sentence-transformers library, we need to reformat our data into a list of `InputExample` objects. The exact format does vary by training task.\n\nWe will be using a ranking function (more on that soon), so we must include two text items, the *(query, episode)* pairs.\n\n```python\nfrom sentence_transformers import InputExample\n\neval_split = int(0.01 * len(pairs))\ntest_split = int(0.19 * len(pairs))\n\n# we separate a number of these for testing\ntest_pairs = pairs[-test_split:]\npairs = pairs[:-test_split]\n         \n# and take a small number of samples for evaluation\neval_pairs = pairs[-eval_split:]\npairs = pairs[:-eval_split]\n\ntrain = []\n\nfor (query, episode) in pairs:\n    train.append(InputExample(texts=[query, episode]))\n```\n\nWe also took a small set of evaluation (`eval_pairs`) and test set pairs (`test_pairs`) for later use.\n\nAs mentioned, we will be using a ranking optimization function. That means that the model is tasked with learning how to identify the *correct episode* from a batch of episodes when given a specific *query*, e.g., *ranking* the correct pair above all others.\n\n![podcast-ranking](/images/spotify-podcast-search-5.png)\n\n<small>Given a query and set of episodes, the model must learn how to embed them so that the relevant episode embedding is the most similar to the query embedding.</small>\n\nThe model achieves this by embedding similar *(query, episode)* pairs as closely as possible in a vector space. We measure the proximity of these embeddings using *cosine similarity*, which is essentially the angle between embeddings (e.g., vectors).\n\n![podcast-vec-search-cosine](/images/spotify-podcast-search-6.png)\n\n<small>When using cosine similarity, we are searching for embeddings that have the shortest angular distance, rather than Euclidean distance.</small>\n\nAs we are using a ranking optimization function, we must make sure no duplicate queries or episodes are placed in the same training batch. If there are duplicates, this will confuse the training process as the model will be told that despite two queries/episodes being identical, one is correct, and the other is not.\n\nThe sentence-transformers library handles the duplication issue using the `NoDuplicatesDataLoader`. As the name would suggest, this data loader ensures no duplicates make their way into a training batch.\n\nWe initialize the data loader with a `batch_size` parameter. A larger batch size makes the ranking task harder for the model as it must identify one correct answer from a higher number of options.\n\nIt is harder to choose an answer from a hundred samples than from four samples. With that in mind, a higher `batch_size` tends to produce higher performance models.\n\n```python\nfrom sentence_transformers.datasets import NoDuplicatesDataLoader\n\nbatch_size = 64\n\nloader = NoDuplicatesDataLoader(train, batch_size=batch_size)\n```\n\nNow we initialize the loss function. As we're using ranking, we choose the `MultipleNegativesRankingLoss`, typically called *MNR loss*.\n\n```python\nfrom sentence_transformers.losses import MultipleNegativesRankingLoss\n\nloss = MultipleNegativesRankingLoss(model)\n```\n\n#### In-Batch Evaluation\n\nSpotify describes two evaluation steps. The first can be implemented before fine-tuning using in-batch metrics. What they did here was calculate two metrics at the batch level (using `64` samples at a time in our case); those are:\n\n* Recall@k tells us if the correct answer is placed in the top *k* positions.\n* **M**ean **R**eciprocal **R**ank (MRR) calculates the average reciprocal rank of a correct answer.\n\nWe will implement a similar approach to in-batch evaluation. Using the sentence-transformers `RerankingEvaluator`, we can calculate the MRR score at the end of each training epoch using our evaluation data, `eval_pairs`.\n\nBefore initializing this evaluator, we need to remove duplicates from the eval data.\n\n{{< notebook file=\"podcast-eval-dedupe\" height=\"full\" >}}\n\nThen, we reformat the data into a list of dictionaries containing a query, its *positive* episode (that it is paired with), and then all other episodes as *negatives*.\n\n```python\nfrom sentence_transformers.evaluation import RerankingEvaluator\n\n# we must format samples into a list of:\n# {'query': '<query>', 'positive': ['<positive>'], 'negative': [<all negatives>]}\neval_set = []\neval_episodes = [pair[1] for pair in eval_pairs]\n\nfor i, (query, episode) in enumerate(eval_pairs):\n    negatives = eval_episodes[:i] + eval_episodes[i+1:]\n    eval_set.append(\n        {'query': query, 'positive': [episode], 'negative': negatives}\n    )\n    \nevaluator = RerankingEvaluator(eval_set, mrr_at_k=5, batch_size=batch_size)\n```\n\nWe set the MRR@5 metric, meaning if the positive episode is returned within the top *five* results, we return a positive score. Otherwise, the score would be zero.\n\n---\n\n<em>If the correct episode appeared at position *three*, the reciprocal rank of this sample would be calculated as 1/**3**. At position *one* we would return 1/**1**.</em>\n\n<em>As we're calculating the **mean** reciprocal rank, we take all sample scores and compute the mean, giving us our final MRR@5 score.</em>\n\n---\n\nUsing our evaluator, we first calculate the MRR@5 performance without any fine-tuning.\n\n{{< notebook file=\"podcast-mrr-zero\" height=\"full\" >}}\n\nReturning an MRR@5 of *0.68*, we will compare this to the post-training MRR@5 score.\n\n#### Fine-Tuning\n\nWith our evaluator ready, we can fine-tune our model. The Spotify article doesn't give any information about the parameters they used, so we will stick with pretty typical training parameters for sentence transformer models using MNR loss. We train for a single epoch and *\"warm up\"* the learning rate for the first 10% of training steps.\n\n{{< notebook file=\"podcast-model-fit\" height=\"full\" >}}\n\nAfter fine-tuning, the model will be saved into the directory specified by `output_path`. In *distiluse-podcast-nq*, we will see all the required model files and a directory called *eval*. Here, we will find a post-training MRR@5 score of *0.89*, a sizeable 21-point improvement from the previous MRR@5 of *0.68*.\n\nThis score looks promising, but there's further evaluation to be performed.\n\n### Evaluation\n\nWe want to emulate a more *real-world* scenario for the final evaluation step. Rather than calculating MRR@5 across small batches of data (as done previously), we should index *many* episodes and recalculate some retrieval metrics.\n\nSpotify details their *full-retrieval setting metrics* as using Recall@30 and MRR@30, performed both on queries from the eval set and on their curated dataset.\n\nOur eval set is small, so we can discard that. Instead, we will use the much larger test set `test_pairs`.\n\nAs before, we must deduplicate the episodes from the dataset.\n\n{{< notebook file=\"podcast-test-dedupe\" height=\"full\" >}}\n\nThis time, rather than keeping all of our embeddings stored in memory, we use a vector database, Pinecone.\n\nWe first [sign up for a free account](https://app.pinecone.io/), enter the default project and retrieve the *default* API key.\n\nBack in Python, we ensure the Pinecone client is installed with `pip install pinecone-client`. Then we initialize our connection to Pinecone and create a new vector index.\n\n```python\nimport pinecone\n\npinecone.init(\n    api_key='YOUR_API_KEY',  # app.pinecone.io\n    environment='YOUR_ENV'  # find next to API key in console\n)\n\n# check if an evaluation index already exists, if not, create it\nif 'evaluation' not in pinecone.list_indexes():\n    pinecone.create_index(\n        'evaluation', dimension=model.get_sentence_embedding_dimension(),\n        metric='cosine'\n    )\n    \n# now connect to the index\nindex = pinecone.Index('evaluation')\n```\n\nThe vector index is where we will store all of our episode embeddings. We must encode the episode text using our fine-tuned `distiluse-podcast-nq` model and insert the embeddings into our index.\n\n```python\nto_upsert = []\nqueries = []\neps_batch = []\nid_batch = []\nupsert_batch = 64\n\nfor i, (query, episode) in enumerate(tqdm(test_pairs)):\n    # create batch\n    queries.append((query, str(i)))\n    eps_batch.append(episode)\n    id_batch.append(str(i))\n    # on reaching batch_size we encode and upsert\n    if len(eps_batch) == upsert_batch:\n        embeds = model.encode(eps_batch).tolist()\n        # insert to index\n        index.upsert(vectors=list(zip(id_batch, embeds)))\n        # refresh batch\n        eps_batch = []\n        id_batch = []\n```\n\n---\n\n*Short on time? Download the fine-tuned model using `model = SentenceTransformer('pinecone/distiluse-podcast-nq')`.*\n\n---\n\nWe will calculate the *Recall@K* score, which differs slightly from the *MRR@K* metric as if the match appears in the top K returned results, we score *1*; otherwise, we score *0*. As before, we take all query scores and compute the mean.\n\n{{< notebook file=\"podcast-synthetic-recall\" height=\"full\" >}}\n\nSo far, this looks great; 88% of the time, we are returning the exact positive episode within the top 30 results. But this does assume that our synthetic queries are perfect, which they are not.\n\nWe should measure model performance on more realistic queries, as Spotify did with their curated dataset. In this example, we have chosen a selection of episodes and manually written queries that fit the episode.\n\n```python\ncurated = {\n    \"funny show about after uni party house\": 1,\n    \"interview with cookbook author\": 8,\n    \"eat better during xmas holidays\": 14,\n    \"superhero film analysis\": 27,\n    \"how to tell more engaging stories\": 33,\n    \"how to make money with online content\": 34,\n    \"why is technology so addictive\": 38\n}\n```\n\nUsing these curated samples, we returned a lower score of 0.57. Compared to 0.88, this seems low, but we must remember that there are likely other episodes that fit these queries. Meaning, we're calculating recall assuming there are no other relevant queries.\n\nWhat we can do is: measure this score against the score of the model before fine-tuning. We create a new Pinecone index and replicate the same steps but using the `distiluse-base-multilingual-cased-v2` sentence transformer. You can find the [full script here](https://github.com/pinecone-io/examples/blob/master/search/semantic-search/spotify-podcast-search/walkthrough.ipynb).\n\nUsing this model, we return a score of just 0.29. By fine-tuning the model on this episode data, despite having no query pairs, we have improved episode retrieval performance by 28-points.\n\n\n\nThe technique we followed, informed by Spotify's very own semantic search implementation, produced significant performance improvements.\n\nCould this performance be better? Of course! Spotify fine-tuned their model using *three* data sources. We can assume that the first two of those, pulled from Spotify's past search logs, are of much higher quality than our synthetic dataset.\n\nMerging the approach we have taken with a real dataset, as done by Spotify, is almost certain to produce a significantly higher-performing model.\n\nThe world of semantic search is already huge, but what is perhaps more exciting is the potential of this field. We will continue seeing new examples of semantic search, like Spotify’s podcast search, applied in many interesting and unique ways.\n\nIf you’re using Pinecone for semantic search and are interested in [showcasing your project](https://www.pinecone.io/community/), let us know! Comment them below or email them to us at *info@pinecone.io*.\n\n\n\n## Resources\n\n[Full Code Walkthrough](https://github.com/pinecone-io/examples/blob/master/search/semantic-search/spotify-podcast-search/walkthrough.ipynb)\n\n[1] [Podcast Content is Growing Audio Engagement](https://www.nielsen.com/us/en/insights/article/2020/podcast-content-is-growing-audio-engagement/) (2020), Nielsen\n\n[2] S. Lebow, [Spotify Poised to Overtake Apple Podcasts This Year](https://www.emarketer.com/content/spotify-poised-overtake-apple-podcasts-this-year?ecid=NL1001) (2021), eMarketer\n\n[3] A. Tamborrino [Introducing Natural Language Search for Podcast Episodes](https://engineering.atspotify.com/2022/03/introducing-natural-language-search-for-podcast-episodes/) (2022), Engineering at Spotify Blog\n\n[4] O. Sharir, B. Peleg, Y. Shoham, [The Cost of Training NLP Models](https://arxiv.org/abs/2004.08900) (2020)\n\n[5] N. Reimers, I. Gurevych, [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084) (2019), EMNLP\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fba4"
  },
  "filename": "unsupervised-training-sentence-transformers.md",
  "title": "ebook-post",
  "category": "\"Unsupervised Training for Sentence Transformers\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Unsupervised Training for Sentence Transformers\"\nheadline: \"Unsupervised Training for Sentence Transformers\"\ncategories:\n  - NLP for Semantic Search\ntoc: >-\nweight: 6\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: How to create sentence transformer models without labelled data.\n# Open graph\nimages: ['/images/unsupervised-training-sentence-transformers-0.jpg']\n---\n\nLanguage represents a way for us to communicate abstract ideas and concepts. It has evolved as a human-only form of interaction for the best part of the past 100 million years. Translating that into something a machine can understand is (unsurprisingly) difficult.\n\nModern(ish) computers appeared during and around WW2. The first application of natural language processing (NLP) came soon after with the Georgetown machine translation (MT) experiment in 1954. In the first decade of research, many expected MT to be solvable within a few short years [1] — they were slightly *too optimistic*.\n\nMT is still not 'solved', but that and the field of NLP have become heavily researched in the past few years, and there have been many breakthroughs. We now have some incredible language models for a stunning array of use-cases.\n\nMuch of this recent success is thanks to dense vector representations of words and sentences. These vectors are produced by language models that translate the semantic meaning of language into a numerical vector space that a computer can understand and process.\n\nAs is the trend with ML models, we need *a lot* of data and *a lot* of compute to build these models.\n\nSentence transformers are the current-best models for producing information-rich representations of sentences and paragraphs. The training process to create this type of model varies but begins with the unsupervised *pretraining* of a transformer model using methods like masked-language modeling (MLM).\n\nTo adapt a pretrained transformer to produce meaningful sentence vectors, we *typically* need a more supervised fine-tuning approach. We can use datasets like natural language inference (NLI) pairs, labeled semantic textual similarity (STS) data, or parallel data (pairs of translations).\n\nFor some domains and languages, such as finance and English, this data is fairly easy to find or gather. But many domains and many languages have *very little* labeled data. If you can find semantic similarity pairs for the agriculture industry, please let me know. There are many languages, such as Dhivehi, where unlabelled data is hard to find and labelled data practically non-existent.\n\nThis means you either spend a very long time gathering tens of thousands of labeled samples or you can try an unsupervised fine-tuning approach.\n\nUnsupervised training methods for sentence transformers are not as effective as their supervised counterparts, but they *do work*. And if you have no other choice, why not?\n\nIn this article, we will introduce the concept of unsupervised fine-tuning for sentence transformers. We will learn to train these models using the unsupervised **T**ransformer-based **S**equential **D**enoising **A**uto-**E**ncoder (TSDAE) approach.\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/pNvujJ1XyeQ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n## Unsupervised Or Not?\n\nThe first thing we need to decipher is: *\"Do we really need to use unsupervised fine-tuning?\"* The answer depends on your use case and available data.\n\nFirst, let's look at a few *supervised* training approaches, their use cases, and required data.\n\n### Natural Language Inference\n\nNatural language inference (NLI) is the most common approach to fine-tuning a generic monolingual sentence transformer. It uses optimization functions like [softmax loss](https://www.pinecone.io/learn/train-sentence-transformers-softmax/) or [multiple negatives ranking](https://www.pinecone.io/learn/fine-tune-sentence-transformers-mnr/) to learn how to distinguish between similar and dissimilar sentences [2] [3].\n\nAn NLI dataset contains sentence pairs labeled as either (1) *entailing/inferring* each other, being (2) *neutral* (e.g., they're not necessarily related), and sometimes (3) *contradictory*.\n\nSome methods like multiple negatives ranking only require positive *entailment* pairs but work better when we add 'hard negatives' (contradictory) pairs.\n\nSuppose you have these 'positive' pairs of sentences. In that case, a good pretrained transformer can usually be fine-tuned to a reasonable performance with just 20,000 pairs. If you have less than this, you may still be able to successfully fine-tune — it depends on the complexity of your domain and language.\n\n### Semantic Textual Similarity\n\nSemantic textual similarity (STS) is another common approach to fine-tuning generic sentence transformers. An STS dataset contains sentence pairs alongside their 'semantic similarity', given as a numeric value within a set range.\n\nOptimization functions like cosine similarity loss can be used. The model will attempt to optimize the sentence vectors of each pair to be more-or-less similar according to a cosine similarity function.\n\n### Multilingual Parallel Data\n\nWhen building multilingual sentence transformers, we take an already trained monolingual sentence transformer and [use a fine-tuning process called multilingual knowledge distillation](https://www.pinecone.io/learn/multilingual-transformers/) which*distills* the monolingual knowledge of the fine-tuned model and adapts it across multiple languages in a pretrained multilingual model.\n\nTo do this, you need:\n\n* A pretrained multilingual model (it does not need to be a sentence transformer)\n* A *fine-tuned* monolingual sentence transformer\n* Parallel data, which are translation pairs from the monolingual model's language, to your multilingual target language(s).\n\n### Unsupervised\n\nIf your data and use-case don't seem to fit into any of the above, unsupervised fine-tuning may be the answer.\n\n## How TSDAE Works\n\nThere are different options for unsupervised fine-tuning of sentence transformers. One of the best performing is the **T**ransformer(-based) and **S**equential **D**enoising **A**uto-**E**ncoder (TSDAE) pretraining method developed by Kexin Wang, Nils Reimers, and Iryna Gurevych in 2021 [4].\n\nTSDAE introduces *noise* to input sequences by deleting or swapping tokens (e.g., words). These *damaged sentences* are encoded by the transformer model into sentence vectors. Another decoder network then attempts to reconstruct the *original input* from the damaged sentence encoding.\n\nAt first glance, this may seem similar to masked-language modeling (MLM). MLM is the most common pretraining approach for transformer models. A random number of tokens are masked using a '*masking token'*, and the transformer must try to guess what is missing,  like a 'fill in the blanks' test in school.\n\n```\nFill in the missing words:\n\nThe miniature pet ________ became the envy of the neighborhood.\n```\n\nIf you guessed `elephant`, you're correct (and possibly unhinged).\n\nTSDAE differs in that the decoder in MLM has access to full-length word embeddings for *every single token*. The TSDAE decoder only has access to the sentence vector produced by the encoder.\n\n![mlm_vs_tsdae](/images/unsupervised-training-sentence-transformers-1.jpg)\n<small>The TSDAE (top) process outputs a *sentence vector*, which the decoder uses to predict the original text. MLM outputs token vectors, providing the decoder with much more information to use in its prediction.</small>\n\nIn the K. Wang, et al. (2021) paper, the best performing *noise* used deletion-only, with a *deletion ratio* of 0.6. To translate from token-level representation to sentence-level, the classifier token `[CLS]` embedding was used.\n\nThe TSDAE paper described five tested approaches to noise in the input data and noise ratios tested from 0.1 to 0.9. Of those, it was deletion at a ratio of 0.6 that performed best.\n\nIf you've read our previous articles on [sentence transformers](https://www.pinecone.io/learn/sentence-embeddings/), you will remember that we usually apply a mean pooling operation across transformer word embeddings to produce a single sentence embedding/vector.\n\n![mean_vs_cls](/images/unsupervised-training-sentence-transformers-2.jpg)\n<small>CLS token pooling (top) takes the CLS token vector and uses this as the sentence vector. Mean pooling (bottom) takes the average value across all token vectors to create a sentence vector.</small>\n\nWhen fine-tuning with TSDAE, the performance difference between the two approaches is tiny.\n\n| CLS | Mean | Max |\n| ----- | ----- | ----- |\n| 78.77 | 78.84 | 78.17 |\n\n<small>Validation scores for the different pooling methods, the difference between using the CLS embedding or mean pooling all token embeddings is minimal, source [4].</small>\n\nRather than averaging vectors, we can take the `[CLS]` vector as is. Usually, this embedding is used to feed information into a classification head of a transformer, which, of course, classifies the *whole* input sequence. So we can see the `[CLS]` embedding as a representation of the whole sequence/sentence.\n\n\n\n## Fine-Tuning Walkthrough\n\nBefore we begin fine-tuning, there are a few things we need to set up:\n\n* Training data\n* A pretrained model prepared for producing sentence vectors\n* Loss function\n\nLet's work through each of these.\n\n#### Training Data\n\nWhen fine-tuning a model for producing sentence vectors, TSDAE requires nothing more than text data. One handy source for text data (in *many* languages) is the OSCAR corpus. We will stick with English, for which OSCAR contains 1.8TB (after deduplication),so we have enough data.\n\nWe won't be training on the entire dataset. Instead, we will gather just 100K (mostly) short sentences. To download OSCAR (not the whole thing), we will use HuggingFace's `datasets` library.\n\n{{< notebook file=\"oscar-en\" height=\"full\" >}}\n\nNote that we added `streaming=True`; this allows us to iteratively download samples from the OSCAR dataset rather than downloading the full 1.8TB.\n\n{{< notebook file=\"print-oscar\" height=\"full\" >}}\n\nEach sample in OSCAR contains an `id` and `text`. The text can be very long, with several sentences and paragraphs. Ideally, we need to split each of these into single sentences. We can do this by using a regex function that covers both period and newline characters.\n\n{{< notebook file=\"regex-splitter\" height=\"full\" >}}\n\nWe will use `splitter` to create a list of 100K sentences to feed into TSDAE fine-tuning.\n\n{{< notebook file=\"splitter-build\" height=\"full\" >}}\n\nThe typical PyTorch process requires creating a `Dataset` object then passing it into a `DataLoader`. In PyTorch, we need to create a function to add noise to the data (usually within the `Dataset` class). Fortunately, sentence-transformers handles this for us via the `DenoisingAutoEncoderDataset` object.\n\n{{< notebook file=\"dataset-and-dataloader\" height=\"full\" >}}\n\nBy default, the `DenoisingAutoEncoderDataset` deletes tokens with a probability of 60% per token. Now that our training data is ready we can move on to the final pretraining phase.\n\n#### Model and Training\n\nAs mentioned, we need a model to fine-tune. This model should already be pretrained, of which there are *plenty* of choices over at [HuggingFace models](https://huggingface.co/models).\n\nNote that BERT seems to outperform other models after fine-tuning with TSDAE. This is possibly thanks to the next sentence prediction (NSP) pretraining task used for BERT, which learns sentence-level contexts [4]. With this in mind, we will go ahead and use `bert-base-uncased`.\n\n{{< notebook file=\"create-model\" height=\"full\" >}}\n\nAlongside the transformer model (BERT), we need a pooling layer to move from the usual output of 512 token vectors to a single sentence vector. The K. Wang, et al. (2021) paper recommends using the `[CLS]` token vector as the sentence vector, which we have used above. The two parts are then merged with `SentenceTransformer`.\n\nAfter this, we define a loss function. Again, sentence-transformers handles this with the `DenoisingAutoEncoderLoss` class.\n\n{{< notebook file=\"loss-func\" height=\"full\" >}}\n\nWe're now ready to begin fine-tuning. We use an Adam optimizer with a constant learning rate (no warm-up) of `3e-5` and *no* weight decay.\n\n{{< notebook file=\"tsdae-train\" height=\"full\" >}}\n\nFine-tuning should not take too long, after which we can move on to model evaluation.\n\n\n\n### Does it Work?\n\nThe training process is undoubtedly easy to set up and run, but does it work? And, if so, how does it compare to other supervised fine-tuning methods?\n\nTo evaluate model performance, we will use the **S**emantic **T**extual **S**imilarity **b**enchmark (STSb) dataset. As before, we will use HuggingFace's `datasets` to retrieve the data.\n\n{{< notebook file=\"get-stsb\" height=\"full\" >}}\n\nThe `label` feature contains a score from `0 -> 5` that describes how similar `sentence1` and `sentence2` are (higher is more similar). The evaluator we will be using requires scores from `0 -> 1`, so we normalize  then reformat the data to use sentence-transformers `InputExample` class.\n\n{{< notebook file=\"format-stsb\" height=\"full\" >}}\n\nDuring evaluation, we want to produce sentence vectors for `sentence1`-`sentence2` pairs and calculate their similarity. If the similarity score is close to the `label` value, great! If not, that's not as great. We can use the `EmbeddingSimilarityEvaluator` to do this.\n\n{{< notebook file=\"eval-new-model\" height=\"full\" >}}\n\nA score of 0.73 seems reasonable, but how does it compare to other models? Let's compare it to an untrained `bert-base-uncased`.\n\n{{< notebook file=\"eval-orig-bert\" height=\"full\" >}}\n\nThere's clearly an improvement from untrained BERT to a TSDAE fine-tuned BERT, which is great to see. However, we know that an unsupervised approach is unlikely to compete with supervised methods.\n\nThe most popular approach (as mentioned earlier) for fine-tuning sentence transformers is with **N**atural **L**anguage **I**nference (NLI) data. The original SBERT `bert-base-nli-mean-tokens` was trained with this, and many of the highest performing models like `flax-sentence-embeddings/all_datasets_v3_mpnet-base` are too. Let's see how these two perform.\n\n{{< notebook file=\"eval-sbert-mpnet\" height=\"full\" >}}\n\nWe can see a big difference here. Fine-tuning with TSDAE simply cannot compete in terms of performance against supervised methods.\n\nHowever, the point and value of TSDAE is that it allows us to fine-tune models for use-cases where we have no data. Specific domains with unique terminology or low resource languages.\n\nFor these use-cases, a score of 0.73 from a TSDAE-trained sentence transformer after just 20 minutes of training  is incredible.\n\n\n\nThat's it for this article on the unsupervised fine-tuning of sentence transformers using TSDAE. We've explained where *not* to use TSDAE and where to use it.\n\nWe worked through the logic being TSDAE and its parallels with MLM pretraining. After working through a TSDAE fine-tuning example, we evaluated the performance of a TSDAE-trained model against models trained using common supervised methods.\n\nDespite TSDAE producing lower performing models than other supervised methods, it opens doors for many previously inaccessible domains and languages. With nothing more than unstructured text, we’re able to build effective sentence transformers. As increasingly effective unsupervised methods are developed, we may find that the future of sentence transformers needs nothing more than unstructured text.\n\n\n\n## References\n\n[1] J. Hutchins, [The History of Machine Translation in a Nutshell](https://www.translationdirectory.com/article411.htm) (2005)\n\n[2] N. Reimers, I. Gurevych, [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084) (2019), EMNLP\n\n[3] N. Reimers, [NLI Fine-Tuning: MultipleNegativesRankingLoss](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/nli#multiplenegativesrankingloss), Sentence Transformers on GitHub\n\n[4] K. Wang, et al., [TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning](https://arxiv.org/abs/2104.06979) (2021), EMNLP\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fba5"
  },
  "filename": "image-search.md",
  "title": "book",
  "category": "\"Embedding Methods for Image Search\"",
  "content": "---\nlayout: book\ntitle: \"Embedding Methods for Image Search\"\ndescription: Image embeddings for content-based information retrieval.\nauthor: James Briggs and Laura Carnevali\nintro: |\n  Learn how to make machines understand images as people do. This free course covers everything you need to build state-of-the-art image retrieval systems; image search, text-to-image, object detection and more. \nemailSubmit: true\nsocialShare: true\nimage: /images/image-search-ebook.png\nimages: ['/images/image-search-ebook.png']\nintroChapter: \n    title: Introduction\n    text: |\n      Image retrieval has a long history, from term-matching manually annotated images in the 70s to today's state-of-the-art deep learning-based approaches.\n\n      In this ebook, we will cover the state-of-the-art methods for image retrieval. We will start with a brief history of the field before diving in to the pillars of image retrieval: similarity search, content-based image retrieval, and multi-modal retrieval.\n\n      Image retrieval relies on two components; image embeddings, and vector search. We will cover how to produce information rich image embeddings with state-of-the-art deep learning architectures, including convolutional neural networks and transformers. Following this, we will learn how to pair our image embeddings with vector search to build powerful image retrieval systems.\n\n      This ebook is for anyone who wants to build amazing image-search applications using the latest methods in deep learning and information retrieval. No prior knowledge in either is necessary!\nchapters:\n  - title: Color Histograms\n    text: A look at one of the earliest content-based embeddings methods.\n    url: /learn/color-histograms\n  - title: Bag of Visual Words\n    text: Content-based information retrieval and classification with visual words.\n    url: /learn/bag-of-visual-words\n  - title: Image-net\n    text: How ImageNet and AlexNet kickstarted the deep learning era of computer vision.\n    url: /learn/imagenet\n  - title: Convolutional Neural Nets\n    text: A visual tour of the long reigning champions of computer vision.\n    url: /learn/cnn\n  - title: Vision Transformers (ViT)\n    text: A deep dive into the unification of NLP and computer vision with the Vision Transformer (ViT).\n    url: /learn/vision-transformers\n  - title: CLIP Explained\n    text: Multi-modality and the future of computer vision with OpenAI's CLIP.\n    url: /learn/clip\n    bonusSection: # Bonus content / further materials\n        title: Bonus Material\n        links:\n          - title: Optimize Image Classifiers with Vector Search\n            url: /learn/classifier-train-vector-search\n  - title: Zero-Shot Image Classification with CLIP\n    text: A deep dive on OpenAI's CLIP for zero-shot image classification.\n    url: /learn/zero-shot-image-classification-clip\n  - title: Object Localization and Detection with CLIP\n    text: How to apply CLIP to object detection in a zero-shot setting.\n    url: /learn/zero-shot-object-detection-clip\n  - title: EMAIL_SUBMIT #email submit form\n  - title: Traditional Image Embeddings Methods\n    text: An overview of the pre-DL methods for image embedding.\n  - title: CNNs and Search\n    text: Extracting meaning with convolutional neural nets (CNNs).\n  - title: Diffusion Explained\n    text: A deep dive on diffuser models\n  - title: And more...\n---\n\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fba6"
  },
  "filename": "vector-embeddings.md",
  "title": "post",
  "category": "\"What are Vector Embeddings?\"",
  "content": "---\nlayout: post\ntitle: \"What are Vector Embeddings?\"\nheadline: \"What are <span>Vector Embeddings?</span>\"\ncategories:\n  - Vector Search 101\ntoc: >-\nweight: 2\nauthor:\n  name: Rajat Tripathi\n  position: Software Engineer\n  src: /images/company-rajat.jpeg\n  href: https://www.linkedin.com/in/rajat-tripathi-08/\ndescription: A gentle introduction to vector embeddings with key terms, use cases, and examples.\n# Open Graph\nimages: ['/images/vector_embeddings.jpg']\n---\n\n## Introduction\n\nVector embeddings are one of the most fascinating and useful concepts in machine learning. They are central to many NLP, recommendation, and search algorithms. If you've ever used things like recommendation engines, voice assistants, language translators, you've come across systems that rely on embeddings.\n\nML algorithms, like most software algorithms, need numbers to work with. Sometimes we have a dataset with columns of numeric values or values that can be translated into them (ordinal, categorical, etc). Other times we come across something more abstract like an entire document of text. We create vector embeddings, which are just lists of numbers, for data like this to perform various operations with them. A whole paragraph of text or any other object can be reduced to a vector. Even numerical data can be turned into vectors for easier operations.\n\n![Vector Embeddings are a list of numbers](/images/vector_embeddings.jpg)\n\nBut there is something special about vectors that makes them so useful. This representation makes it possible to translate [semantic similarity](https://en.wikipedia.org/wiki/Semantic_similarity) as perceived by humans to proximity in a [vector space](https://en.wikipedia.org/wiki/Vector_space).\n\nIn other words, when we represent real-world objects and concepts such as images, audio recordings, news articles, user profiles, weather patterns, and political views as vector embeddings, the semantic similarity of these objects and concepts can be quantified by how close they are to each other as points in vector spaces.  Vector embedding representations are thus suitable for common machine learning tasks such as clustering, recommendation, and classification.\n\n![Semantic similarity in sentence embeddings.](/images/sentence_embeddings.png)\n<small>Source: [DeepAI](https://deepai.org/publication/in-search-for-linear-relations-in-sentence-embedding-spaces)</small>\n\nFor example, in a clustering task, clustering algorithms assign similar points to the same cluster while keeping points from different clusters as dissimilar as possible. In a recommendation task, when making recommendations for an unseen object, the recommender system would look for objects that are most similar to the object in question, as measured by their similarity as vector embeddings. In a classification task, we classify the label of an unseen object by the major vote over labels of the\nmost similar objects.\n\n## Creating Vector Embeddings\n\nOne way of creating vector embeddings is to engineer the vector values using domain knowledge. This is known as feature engineering. For example, in medical imaging, we use medical expertise to quantify a set of features such as shape, color, and regions in an image that capture the semantics. However, engineering vector embeddings requires domain knowledge, and it is too expensive to scale.\n\nInstead of engineering vector embeddings, we often train models to translate objects to vectors. A deep neural network is a common tool for training such models. The resulting embeddings are usually high dimensional (up to two thousand dimensions) and dense (all values are non-zero). For text data, models such as [Word2Vec](https://en.wikipedia.org/wiki/Word2vec), [GLoVE](https://en.wikipedia.org/wiki/GloVe_(machine_learning)),\nand [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) transform words, sentences,\nor paragraphs into vector embeddings.\n\nImages can be embedded using models such as [convolutional neural networks (CNNs)](https://en.wikipedia.org/wiki/Convolutional_neural_network),\nExamples of CNNs include [VGG](https://arxiv.org/abs/1409.1556), and [Inception](https://arxiv.org/abs/1409.4842).\nAudio recordings can be transformed into vectors using image embedding transformations over the audio frequencies visual representation\n(e.g., using its [Spectrogram](https://en.wikipedia.org/wiki/Spectrogram)).\n\n## Example: Image Embedding with a Convolutional Neural Network\n\nConsider the following example, in which raw images are represented as greyscale pixels. This is equivalent to a matrix (or table) of integer values in the range ``0`` to ``255``. Wherein the value ``0`` corresponds to a black color and ``255`` to white color. The image below depicts a greyscale image and its corresponding\nmatrix.\n\n![Grayscale image representation](/images/Image_matrix_source_syyeung_cvweb.png)\n<small>Source: [Serena Young](https://ai.stanford.edu/~syyeung/)</small>\n\nThe left sub-image depicts the grayscale pixels, the middle sub-image contains the pixel grayscale values, and the rightmost sub-image defines the matrix. Notice the matrix values define a vector embedding in which its first coordinate is the matrix upper-left cell, then going left-to-right until the last coordinate which corresponds to the lower-right matrix cell.\n\nSuch embeddings are great at maintaining the semantic information of a pixel’s neighborhood in an image. However, they are very sensitive to transformations like shifts, scaling, cropping and other image manipulation operations. Therefore they are often used as raw inputs to learn more robust embeddings.\n\nConvolutional Neural Network (CNN or ConvNet) is a class of deep learning architectures that are usually applied to visual data transforming images into embeddings.\n\nCNNs are processing the input via hierarchical small local sub-inputs which are termed receptive fields. Each neuron in each network layer processes a specific receptive field from the former layer. Each layer either applies a [convolution](https://en.wikipedia.org/wiki/Convolution) on the receptive field or reduces the input size, which is called subsampling.\n\nThe image below depicts a typical CNN structure. Notice the receptive fields, depicted as sub-squares in each layer, service as an input to a single neuron within the preceding layer. Notice also the subsampling operations reduce the layer size, while the convolution operations extend the layer size. The resulting vector embedding is received via a fully connected layer.\n\n![Typical CNN architecture](/images/Typical_cnn_source_wikipedia.png \"Image source: https://upload.wikimedia.org/wikipedia/commons/6/63/Typical_cnn.png\")\n<small>Source: [Aphex34, CC BY-SA 4.0](https://commons.wikimedia.org/wiki/File:Typical_cnn.png)</small>\n\n\nLearning the network weights (i.e., the embedding model) requires a large set of labeled images. The weights are being optimized in a way that images with the same labels are embedded closer compared to images with different labels. Once we learn the CNN embedding model we can transform the images into vectors and store them with a K-Nearest-Neighbor index. Now, given a new unseen image, we can transform it with the CNN model, retrieve its k-most similar vectors, and thus the corresponding similar images.\n\nAlthough we used images and CNNs as examples, vector embeddings can be created for any kind of data and there are multiple models/methods that we can use to create them.\n\n## Using Vector Embeddings\n\nThe fact that embeddings can represent an object as a dense vector that contains its semantic information makes them very useful for a wide range of ML applications.\n\n[Similarity search](/learn/what-is-similarity-search/) is one of the most popular uses of vector embeddings. Search algorithms like KNN and ANN require us to calculate distance between vectors to determine similarity. Vector embeddings can be used to calculate these distances. Nearest neighbor search in turn can be used for tasks like de-duplication, recommendations, anomaly detection, reverse image search, etc.\n\nEven if we don't use embeddings directly for an application, many popular ML models and methods internally rely on them. For example in [encoder-decoder architectures](https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346), embeddings produced by encoder contain the necessary information for the decoder to produce a result. This architecture is widely used in applications, such as machine translation and caption generation.\n\nCheck out [some applications](/docs/examples/) you can build with vector embeddings and Pinecone.\n\n{{< newsletter text=\"Subscribe for more vector search tutorials!\" inputText=\"Email address...\" buttonText=\"Submit\">}}\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fba7"
  },
  "filename": "youtube-search.md",
  "title": "post",
  "category": "\"Making YouTube Search Better with NLP\"",
  "content": "---\nlayout: post\ntitle: \"Making YouTube Search Better with NLP\"\nheadline: \"Making YouTube Search Better with NLP\"\ncategories:\n  - Projects\ntoc: >-\nweight: 1\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: Learn how to build better video search using NLP semantic search\n# Open graph\nimages: [\"/images/youtube-search-0.png\"]\nthumbnail: \"https://www.pinecone.io/images/youtube-search-0.png\"\n---\n\nYouTube is a cultural phenomenon. The first video *\"Me at the zoo\"* was uploaded in 2005. It is a 19 second clip of YouTube's co-founder Jawed Karim at the zoo. This was a uniquely ordinary insight into another person's life, and, back then, this type of content had not really been seen before.\n\nToday's world is different. 30,000 hours of video are uploaded to YouTube *every hour*, and more than one *billion* hours of video are watched daily \\[1\\]\\[2\\].\n\nTechnology and culture have advanced and become ever more entangled. Some of the most significant technological breakthroughs are integrated so tightly into our culture that we never even notice they’re there.\n\nOne of those is AI-powered search. It powers your Google results, Netflix recommendations, and ads you see everywhere. It is being rapidly weaved throughout all aspects of our lives. Further, this is a new technology; its full potential is unknown.\n\nThis technology weaves directly into the cultural phenomenon of YouTube. Imagine a search engine like Google that allows you to rapidly access the billions of hours of YouTube content. There is no comparison to that level of highly engaging video content in the world [3].\n\n[*All supporting notebooks and scripts can be found here*](https://github.com/pinecone-io/examples/tree/master/search/semantic-search/yt-search).\n\n### Data for Search\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/FzLIIwiaXSU\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\nTo power this technology, we will need data. We will use the [YTTTS Speech Collection dataset from Kaggle](https://www.kaggle.com/datasets/ryanrudes/yttts-speech?resource=download). The dataset is organized into a set of directories containing folders named by video IDs.\n\nInside each video ID directory, we find more directories where each represents a timestamp start and end. Those timestamp directories contain a *subtitles.txt* file containing the text from that timestamp range.\n\n![yttts-dataset-structure](/images/youtube-search-1.png)\n<small>Dataset directory structure. Containing video IDs > timestamps > subtitles.</small>\n\nWe can extract the transcriptions, their start/end timestamps, and even the video URL (using the ID).\n\nThe original dataset is excellent, but we do need to make some changes for it to better suit our use case. The code for downloading and processing this [dataset can be found here](https://github.com/pinecone-io/examples/blob/master/search/semantic-search/yt-search/00_data_build.ipynb).\n\n---\n\n*If you prefer, this step can be skipped by downloading the processed dataset with:*\n\n```python\nfrom datasets import load_dataset  # pip install datasets\n\nytt = load_dataset(\n    \"pinecone/yt-transcriptions\",\n    split=\"train\",\n    revision=\"926a45\"\n)\n```\n\n---\n\nFirst, we need to extract the data from the *subtitles.txt* files. We do this by iterating through the directory names, structured by video IDs and timestamps.\n\n{{< notebook file=\"yt-search-make-data\" height=\"full\" >}}\n\nWe now have the *core* data for building our search tool, but it would be nice to include video titles and thumbnails in search results.\n\nRetrieving this data is as simple as scraping the title and thumbnail for each record using the `url` feature and Python's *BeautifulSoup* package.\n\n{{< notebook file=\"yt-search-get-meta\" height=\"full\" >}}\n\nWe need to merge the data we pulled from the YTTTS dataset and this metadata.\n\n{{< notebook file=\"yt-search-merge-data\" height=\"full\" >}}\n\nThat leaves us with *11298* sentence-to-paragraph length video transcriptions. Using this, we're now ready to move on to developing the video search pipeline.\n\n## Retrieval Pipeline\n\nOur video search relies on a subdomain of NLP called semantic search. There are many approaches to semantic search, at a high-level this is the retrieval of *contexts* (sentences/paragraphs) that seem to answer a *query*.\n\n![indexing-querying](/images/youtube-search-2.png)\n<small>Indexing and querying pipeline with the retriever and vector database components.</small>\n\nRetrieving contexts requires two components, a *vector database* and a *retriever* model, both of which are used for indexing and retrieving data.\n\n### Vector Database\n\nThe vector database acts as our data storage and retrieval component. It stores vector representations of our text data that can be retrieved using another vector. We will use the Pinecone vector database.\n\nAlthough we use a small sample here, any meaningful coverage of YouTube would require us to scale to billions of records. Pinecone's vector database allows this through **A**pproximate **N**earest **N**eighbors **S**earch (ANNS). Using ANNS, we can restrict our search scope to a small subset of the index, avoiding the excessive complexity of comparing (potentially) billions of vectors.\n\nTo initialize the database, we sign up for a [free Pinecone API key](https://app.pinecone.io/) and `pip install pinecone-client`. Once ready, we initialize our index with:\n\n```python\nimport pinecone  # pip install pinecone-client\n\n# connect to pinecone (get API key and env at app.pinecone.io)\npinecone.init(api_key=\"YOUR_API_KEY\", environment=\"YOUR_ENV\")\n# create index\npinecone.create_index(\n\t'youtube-search',\n  \tdimension=768, metric='cosine'\n)\n# connect to the new index\nindex = pinecone.Index('youtube-search')\n```\n\nWhen creating the index, we pass:\n\n* The index name, here we use `'youtube-search'` but it can be anything.\n* Vector `dimension`, the dimensionality of vector embeddings stored in the index, must align with the *retriever* dimensionality (more on this soon).\n* Retrieval `metric`, describing the method for calculating the proximity of vectors here we use `'cosine'` similarity, which aligns to the retriever output (again, more later).\n\nWe have our index, but we're missing a key detail. How do we go from the transcription text we have now to vector representations for our vector database? We need a retriever model.\n\n### Retriever Model\n\nThe retriever is a transformer model specially trained to embed sentences/paragraphs into a meaningful vector space. By meaningful, we expect sentences with similar semantic meaning (like question-answer pairs) to be placed into the model and embedded into a similar vector space.\n\n![retriever_vecs](/images/youtube-search-3.png)\n<small>The retriever model encodes semantically related phrases into a similar vector space.</small>\n\nFrom this, we can place these vectors into our vector database. When we have a query, we use the same retriever model to create a query vector. This query vector is used to retrieve the most similar (already indexed) context vectors.\n\n![sim_search](/images/youtube-search-4.png)\n<small>When given a query vector, the vector database handles the search and retrieval of similar context vectors.</small>\n\nWe can load a [pre-existing retriever model](https://huggingface.co/flax-sentence-embeddings/all_datasets_v3_mpnet-base) from the *sentence-transformers* library (`pip install sentence-transformers`).\n\n{{< notebook file=\"yt-search-init-retriever\" height=\"full\" >}}\n\nNow we can see the model details, including that it outputs vectors of dimensionality `768`. This does not include the similarity metric that the model is optimized to use. That information can often be found via the [model card] (TK link) (if in doubt, cosine is most common).\n\n### Indexing\n\nWe can begin embedding and inserting our vectors into the vector database with both our vector database and retriever initialized. We will do this in batches of `32`.\n\n{{< notebook file=\"yt-search-upsert\" height=\"full\" >}}\n\nOnce we're finished indexing our data, we can check that all records have been added using `index.describe_index_stats()` or via the [Pinecone dashboard](https://app.pinecone.io/).\n\n![pinecone-dashboard](/images/youtube-search-5.png)\n<small>We can see the index details from the [Pinecone dashboard](https://app.pinecone.io).</small>\n\n## Querying\n\nEverything has been initialized and indexed. All that is left to do is query. To do this, we create a query like `\"what is deep learning?\"`, embed it using our retriever, and query via `index.query`.\n\n{{< notebook file=\"yt-search-query\" height=\"full\" >}}\n\nWithin the `index.query` method, we pass our query vector `xq`, the *top_k* number of similar context vectors to return, and that we'd like to return metadata.\n\nInside that metadata, we have several important features: `title`, `url`, `thumbnail`, and `start_second`. We can build a user-friendly interface using these features and a framework like Streamlit with [straightforward code](https://github.com/pinecone-io/examples/blob/master/search/semantic-search/yt-search/app.py).\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/youtube-search-6.mp4\" type=\"video/mp4\">\n</video>\n<small>Streamlit built YouTube search demo, try it yourself <a href=\"https://share.streamlit.io/pinecone-io/playground/yt-search/src/server.py\">here</a>.</small>\n\n\n\n\n\nThe fields of NLP and vector search are experiencing a renaissance as increasing interest and application generate more research, which fuels even greater interest and application of the technology.\n\nIn this walkthrough, we have demoed one use case that, despite its simplicity, can be incredibly useful and engaging. As the adoption of NLP and vector search continues to grow, more use cases will appear and embed themselves into our daily lives, just as Google search and Netflix recommendations have done in the past, becoming an ever-greater influence in the world.\n\n\n\n## Resources\n\n[Article Notebooks and Scripts](https://github.com/pinecone-io/examples/tree/master/search/semantic-search/yt-search)\n\n[1] L. Ceci, [Hours of video uploaded to YouTube every minute](https://www.statista.com/statistics/259477/hours-of-video-uploaded-to-youtube-every-minute/) (2022), Statistica\n\n[2] C. Goodrow, [You know what's cool? A billion hours](https://blog.youtube/news-and-events/you-know-whats-cool-billion-hours/) (2017), YouTube Blog\n\n[3] A. Hayes, [State of Video Marketing report](https://www.wyzowl.com/video-marketing-statistics/) (2022), Wyzowl\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fba8"
  },
  "filename": "hybrid-search-intro.md",
  "title": "post",
  "category": "\"Getting Started with Hybrid Search\"",
  "content": "---\nlayout: post\ntitle: \"Getting Started with Hybrid Search\"\nheadline: \"Getting Started with Hybrid Search\"\ncategories:\n  - Algorithms & Libraries\ntoc: >-\nweight: 16\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: Getting started with hybrid search in Pinecone.\n# Open graph\nimages: [\"/images/hybrid-search-intro-0-alt.png\"]\nthumbnail: \"https://www.pinecone.io/images/hybrid-search-intro-0-alt.png\"\n---\n\nVector search has unlocked the door to another level of relevance and efficiency in information retrieval. In the past year, the number of vector search use cases has exploded, showing no signs of slowing down.\n\nThe capabilities of vector search are impressive, but it isn't a perfect technology. In fact, without big domain-specific datasets to fine-tune models on, a traditional search still has some advantages.\n\n---\n\n⚠️ *Hybrid search is currently in private preview, [sign up for access here](https://www.pinecone.io/hybrid-search-early-access/)!*\n\n---\n\nWe repeatedly see that vector search unlocks incredible and *intelligent* retrieval but struggles to adapt to new domains. Whereas traditional search can cope with new domains but is fundamentally limited to a set performance level.\n\nBoth approaches have pros and cons, but what if we merge them somehow to eliminate a few of those cons? Could we create a *hybrid* search with the heightened performance potential of vector search and the zero-shot adaptability of traditional search?\n\nToday, we will learn how to take our search to a new level. Taking both vector and traditional search and merging them via Pinecone's new hybrid search.\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/0cKtkaR883c\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n---\n\n## Out of Domain Datasets\n\nVector search or *dense retrieval* has been shown to significantly outperform traditional methods *when* the embedding models have been fine-tuned on the target domain. However, this changes when we try using these models for *\"out-of-domain\" tasks*.\n\nThat means if we have a large amount of data covering a specific domain like \"Medical question-answering\", we can fine-tune an embedding model. With that embedding model, we can create dense vectors and get outstanding vector search performance.\n\nThe problem is if we *don't* have data. In this scenario, a pretrained embedding model *may* perform better than traditional BM25, but it is unlikely. Giving us a best-case performance of BM25, an algorithm that we cannot fine-tune and cannot provide intelligent human-like retrieval.\n\nIf we want better performance, we're left with two options; (1) annotate a large dataset to fine-tune the embedding model, or (2) use hybrid search.\n\n## Hybrid Search\n\nCombining dense and sparse search takes work. In the past, engineering teams needed to run different solutions for dense and sparse search engines and another system to combine results in a meaningful way. Typically a dense vector index, sparse inverted index, and reranking step.\n\nThe Pinecone approach to hybrid search uses a *single* hybrid index. It enables search across any modality; text, audio, images, etc. Finally, the weighting of dense vs. sparse can be chosen via the `alpha` parameter, making it easy to adjust.\n\nHow does a hybrid search pipeline look?\n\n![hybrid-pipeline](./images/hybrid-search-intro-1.png)\n\n<small>High-level view of a simple hybrid search pipeline.</small>\n\nEverything within the dotted lines is handled by Pinecone's hybrid index. But before we get there, we still need to create dense and sparse vector representations of our input data.\n\nLet's take a look at how we can do that.\n\n## Implementation of Hybrid Search\n\nThe first step in a hybrid search implementation is preparing a dataset. We will use the [`pubmed_qa`](https://huggingface.co/datasets/pubmed_qa) dataset on Hugging Face *Datasets*. We download it like so:\n\n```python\nfrom datasets import load_dataset  # !pip install datasets\npubmed = load_dataset(\n   'pubmed_qa',\n   'pqa_labeled',\n   split='train'\n)\npubmed\n```\n\n```\nDataset({\n   features: ['pubid', 'question', 'context', 'long_answer', 'final_decision'],\n   num_rows: 1000\n})\n```\n\nThe `context` feature is what we will store in Pinecone. Each `context` record contains multiple contexts within a list. Many lack real significance alone, so we will join them to create larger contexts.\n\n```python\ncontexts = []\n# loop through the context passages\nfor record in pubmed['context']:\n   # join context passages for each question and append to contexts list\n   contexts.append('\\n'.join(record['contexts']))\n# view some of the contexts\nfor context in contexts[:2]:\n   print(f\"{context[:300]}...\")\n```\n\n```\nProgrammed cell death (PCD) is the regulated death of cells within an organism. The lace plant (Aponogeton madagascariensis) produces perforations in its leaves through PCD. The leaves of the plant consist of a latticework of longitudinal and transverse veins enclosing areoles. PCD occurs in the cel...\nAssessment of visual acuity depends on the optotypes used for measurement. The ability to recognize different optotypes differs even if their critical details appear under the same visual angle. Since optotypes are evaluated on individuals with good visual acuity and without eye disorders, differenc...\n```\n\nWe can see the highly-technical language contained within each context. An out-of-the-box model will typically struggle with this *domain-specific* language, making this an ideal use-case for hybrid search.\n\nLet's move on to building the sparse and dense vectors.\n\n### Sparse Vectors\n\nSeveral methods exist for building sparse vector embeddings, from the latest sparse embedding transformer models like SPLADE to rule-based tokenization logic.\n\nWe will stick with a more straightforward tokenization approach to keep things simple. Like the BERT tokenizer hosted by Hugging Face *Transformers*.\n\n```python\nfrom transformers import BertTokenizerFast  # !pip install transformers\n\n# load bert tokenizer from huggingface\ntokenizer = BertTokenizerFast.from_pretrained(\n   'bert-base-uncased'\n)\n```\n\nTo tokenize a single context, we can do this:\n\n```python\n# tokenize the context passage\ninputs = tokenizer(\n   contexts[0], padding=True, truncation=True,\n   max_length=512\n)\ninputs.keys()\n```\n\n```\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n```\n\nThe output from this includes a few arrays that are all important when using transformer models. As we're doing tokenization only, we need the `input_ids`.\n\n```python\ninput_ids = inputs['input_ids']\ninput_ids\n```\n\n```\n[101, 16984, 3526, 2331, 1006, 7473, 2094, ...]\n```\n\nThese input IDs represent a unique word or sub-word token translated into integer ID values. This transformation is done using the BERT tokenizer's rule-based tokenization logic.\n\nPinecone expects to receive sparse vectors in dictionary format. For example, the vector:\n\n```\n[0, 2, 9, 2, 5, 5]\n```\n\nWould become:\n\n```\n{\n       \"0\": 1,\n       \"2\": 2,\n       \"5\": 2,\n       \"9\": 1\n}\n```\n\nEach token is represented by a single *key* in the dictionary, and its frequency is counted by the respective key-*value*. We apply the same transformation to our `input_ids` like so:\n\n```python\nfrom collections import Counter\n\n# convert the input_ids list to a dictionary of key to frequency values\nsparse_vec = dict(Counter(input_ids))\nsparse_vec\n```\n\n```\n{101: 1,\n16984: 1,\n3526: 2,\n2331: 2,\n1006: 10,\n...\n}\n```\n\nWe can reformat all of this logic into two functions; `build_dict` to transform input IDs into dictionaries and `generate_sparse_vectors` to handle the tokenization *and* dictionary creation.\n\n```python\ndef build_dict(input_batch):\n # store a batch of sparse embeddings\n   sparse_emb = []\n   # iterate through input batch\n   for token_ids in input_batch:\n       # convert the input_ids list to a dictionary of key to frequency values\n       d = dict(Counter(token_ids))\n       # remove special tokens and append sparse vectors to sparse_emb list\n       sparse_emb.append({\n           key: d[key] for key in d if key not in [101, 102, 103, 0]\n       })\n   # return sparse_emb list\n   return sparse_emb\n def generate_sparse_vectors(context_batch):\n   # create batch of input_ids\n   inputs = tokenizer(\n           context_batch, padding=True,\n           truncation=True,\n           max_length=512\n   )['input_ids']\n   # create sparse dictionaries\n   sparse_embeds = build_dict(inputs)\n   return sparse_embeds\n```\n\nWe also remove special tokens `101`, `102`, `103`, and `0`. These are all tokens explicitly required by the BERT transformer model but have no meaning when building our sparse vectors.\n\nThis code is all we need to build our sparse vectors, but as usual, we still need to create dense vectors.\n\n### Dense Vectors\n\nOur dense vectors are comparatively simple to generate. We initialize a `multi-qa-MiniLM-L6-cos-v1` sentence transformer model and encode the same context as before like so:\n\n```python\n# !pip install sentence-transformers\nfrom sentence_transformers import SentenceTransformer\n\n# load a sentence transformer model from huggingface\nmodel = SentenceTransformer(\n   'multi-qa-MiniLM-L6-cos-v1'\n)\n\nemb = model.encode(contexts[0])\nemb.shape\n```\n\n```\n(1, 384)\n```\n\nThe model gives us a `384` dimensional dense vector. We can move on to upserting the full dataset with both sparse and dense vectors.\n\n### Upserting\n\nOur upsert operation is almost identical, with the exception that; we are pointing our requests to the `/hybrid/vectors/upsert` endpoint rather than `/vectors/upsert`, and our upsert includes an additional `sparse_values` parameter.\n\nThe private preview of hybrid search does not include a Python Pinecone client so we must interact with the Pinecone API directly. To keep things simple we wrote a [temporary Pinecone client class for hybrid search](https://gist.github.com/jamescalam/37d799cf824e16f0a6337b8a3e25fd34).\n\nWe then initialize our hybrid index like so:\n\n```python\n# choose a name for your index\nindex_name = \"hybrid-test\"\n \n# create the index\npinecone.create_index(\n   index_name = index_name,\n   dimension = 384,\n   metric = \"dotproduct\",\n   pod_type = \"s1h\"\n)\n```\n\nNote that we use a **h**ybrid **s1** pod type by specifying `s1h` and all hybrid indexes are currently restricted to the `dotproduct` similarity metric.\n\nWith all of that ready, we can begin adding all of our data to the hybrid index like so:\n\n{{< notebook file=\"hybrid-upserts\" height=\"full\" >}}\n\nFrom `describe_index_stats`, we should see that *1000* records have been added. With that, we can move on to querying the new index.\n\n## Making Queries\n\nQueries remain very similar to pure dense vector queries, with the exception being that we must include a sparse vector version of our query — alongside the typical dense vector representation.\n\n![hybrid-queries](./images/hybrid-search-intro-2.png)\n\n<small>Queries are made to the `/hybrid/query` endpoint with both dense and sparse vector embeddings.</small>\n\nWe can use the earlier `generate_sparse_vectors` function to build the sparse vector. We will wrap the encode and query operations into a single `hybrid_query` function to keep queries simple.\n\n```python\ndef hybrid_query(question, top_k, alpha):\n   # convert the question into a sparse vector\n   sparse_vec = generate_sparse_vectors([question])\n   # convert the question into a dense vector\n   dense_vec = model.encode([question]).tolist()\n   # set the query parameters to send to pinecone\n   query = {\n     \"topK\": top_k,\n     \"vector\": dense_vec,\n     \"sparseVector\": sparse_vec[0],\n     \"alpha\": alpha,\n     \"includeMetadata\": True\n   }\n   # query pinecone with the query parameters\n   result = pinecone.query(query)\n   # return search results as json\n   return result\n```\n\nNow we query like so:\n\n{{< notebook file=\"hybrid-query-0\" height=\"full\" >}}\n\nHow can we assess the impact of hybrid search vs. vector search with these results? We use the new `alpha` parameter that can be used while making queries.\n\nThe `alpha` parameter controls the weighting between the dense and sparse vector search scores. By default, this is set to `0.5`, making any results a pure hybrid search.\n\nAbove we performed a pure *dense vector search* by using an `alpha` of `1`.\n\nWith a full vector search, we get the same ranking of results. However, the \"best\" context (`711`) is currently in position *two*. We can modify the `alpha` parameter to try and improve this result.\n\n{{< notebook file=\"hybrid-query-1\" height=\"full\" >}}\n\nUsing an `alpha` of `0.3` improves the results and returns the best context (`711`) as the top result.\n\n---\n\nThat's it for our fast introduction to hybrid search and how we can implement it with Pinecone. With this, we can reap the benefits of dense vector search while sidestepping its *out-of-domain* pitfalls.\n\nIf you'd like to get started with hybrid search, it is available via private preview with Pinecone. [Get in touch for early access](https://www.pinecone.io/hybrid-search-early-access )!\n\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fba9"
  },
  "filename": "faster-stable-diffusion.md",
  "title": "post",
  "category": "\"Making Stable Diffusion Faster with Intelligent Caching\"",
  "content": "---\nlayout: post\ntitle: \"Making Stable Diffusion Faster with Intelligent Caching\"\nheadline: \"Making Stable Diffusion Faster with Intelligent Caching\"\ncategories:\n  - Applications of Vector Search\ntoc: >-\nweight: 7\nauthors:\n  - name: James Briggs\n    position: Developer Advocate\n    src: /images/james-briggs.jpeg\n    href: \"https://www.youtube.com/c/jamesbriggs\"\n  - name: Nima Boscarino\n    position: ML Developer Advocate\n    src: /images/nima-boscarino.jpeg\n    href: \"https://twitter.com/NimaBoscarino\"\ndescription: How to speed up stable diffusion by caching previous generations\n# Open graph\nimages: [\"/images/faster-stable-diffusion-0.png\"]\nthumbnail: \"https://www.pinecone.io/images/faster-stable-diffusion-0.png\"\n---\n\nCreative industries are changing. A new wave of *\"AI art\"* tools like DALL-E 2, Imagen, and Midjourney seemed to pop up from nowhere. In a few short months, they have reshaped the way art is made.\n\nThe first of these tools, OpenAI's DALL-E 2, was announced in April 2022. It became the first widespread use of \"diffusion\" models. Since then, diffusion has exploded, reaching far beyond the tech and even creative industries, into common knowledge among people without any ties to either industry.\n\nYet, there is a problem. Diffusion models take a lot of compute to generate images.\n\nThe iterative diffusion process means end users generating images on CPU can expect to wait tens of minutes to produce a single image.\n\n\nDespite the high compute requirements, innovation in the space has blossomed. Several other tools have since been released — one of the most exciting being *stable diffusion*.\n\nStable diffusion didn't bring any fundamental changes to the technology itself. It is exciting because it was the first *high-performance* diffusion model that was *open sourced*.\n\nThe release of stable diffusion alongside the slightly earlier release of Hugging Face *Diffusers* in July 2022 [1] triggered an explosion in new use cases.\n\nGetting high-quality diffusion into the hands of open-source communities quickly produced \"stable diffusion dreams\" that create trippy videos as the model traverses between different prompts [2]. Diffusion has been applied to 3D objects [3] and even to create video and help animators create masterpieces in record time [4].\n\nIt's safe to say that diffusers are here to stay. Yet, since diffusion requires many generations and plenty of prompt tweaking to produce good results — stable diffusion is simply out of reach for most people unless we find a way to make it more efficient.\n\n<script type=\"module\"\nsrc=\"https://gradio.s3-us-west-2.amazonaws.com/3.9.1/gradio.js\">\n</script>\n<gradio-app space=\"jamescalam/dream-cacher\"></gradio-app>\n\nFortunately, there is something we can do to make diffusion more efficient and accessible as demoed above. Whether using the latest GPUs or entry-level CPUs. In this article, we discover how to make stable diffusion more efficient through collaboration and caching with a vector database.\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/YMlzhnlSAww\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n---\n\n## Collaborative Diffusion\n\nHugging Face Spaces has been flush with stable diffusion apps ever since stable diffusion was released. There are thousands of people using these apps daily. Yet, stable diffusion requires a *massive amount of compute*, a problem for both the host’s costs and the user’s patience. Yet, there is no clear solution.\n\n![hf-spaces](./images/faster-stable-diffusion-1.png)\n<small>Hugging Face Spaces allows *a lot* of users to interact with easy-to-build web apps.</small>\n\nServing so many users adds a lot of pressure to the service, but we can use this to our advantage.\n\n![hf-spaces-cacher](./images/faster-stable-diffusion-2.png)\n<small>We can cache user generations in a Pinecone vector database.</small>\n\nRather than users competing for service resources, we help users collaborate. We can find a way to cache the generations of every user in a way that similar generations are *”grouped”*. In that case, we can serve *relevant* past generations to current users and bypass the compute-heavy diffusion process.\n\n![hf-spaces-cacher-retriever](./images/faster-stable-diffusion-3.png)\n<small>After caching past generations, we can use them to quickly serve generated images to new users.</small>\n\nAs more generations are cached, fewer generations are required, speeding up retrieval time for current users *and* freeing up service compute.\n\n## Deciding Between Retrieval or Generation?\n\nWe can't only retrieve past generations; that would miss the value of generating new images with diffusion. However, if a user looks for something highly similar to a previously cached generation, why not return those cached generations?\n\nThe only problem is identifying what *”similar”* actually means.\n\nIdeally, we should enable a backend search through past images based on their *visual* meaning and their *semantic similarity* to the user's text prompt.\n\nWe will handle this through what we call the **dream cacher** component. It consists of two steps:\n\n![hf-spaces-cacher-components-1-2](./images/faster-stable-diffusion-4.png)\n\n1. Embedding the user’s text prompt to a [*meaningful* dense vector](/learn/dense-vector-embeddings-nlp/).\n2. [Vector search](/learn/vector-search-basics/) through past prompt vectors and their respective images.\n\nLet's start with step *1* of creating meaningful embeddings.\n\n## Meaningful Embeddings\n\nOur embeddings must represent the *\"semantic meaning\"* behind past prompts in vector space. This means prompts with similar meanings should be placed close to one another, whereas dissimilar prompts should be separated.\n\n![vector-space](./images/faster-stable-diffusion-5.png)\n<small>Example of a 3D vector space with two clusters: dog-related prompts and surf/wave prompts.</small>\n\nThe typical approach for embedding text prompts uses a [sentence transformer](/learn/sentence-embeddings/) (or another encoder) model. These models all take text as input and output vector representations of that text.\n\n![encoder-vector-space](./images/f6ster-stable-diffusion-6.png)\n<small>An encoder model transforms prompts into meaningful vector space.</small>\n\nNow we need to find suitable vector embeddings produced within the stable diffusion pipeline. Stable diffusion uses an encoder model named [CLIP](/learn/clip/). OpenAI's CLIP is a multi-modal model that places both images *and* text in a similar vector space.\n\nCLIP is not a sentence transformer, but it's pretty close. However, there is a slight misalignment. Stable diffusion does *not* use single CLIP embeddings — it uses a large tensor containing *77* of these embeddings, which *cannot* be used in a vector search.\n\nWe can go ahead and initialize the stable diffusion pipeline from Hugging Face *Diffusers* like so:\n\n```python\n# !pip install torch transformers diffusers\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\n# set the hardware device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# init all of the pipeline models and move them to a given GPU\npipe = StableDiffusionPipeline.from_pretrained(\n   \"CompVis/stable-diffusion-v1-4\",\n   use_auth_token=\"<<ACCESS_TOKEN>>\"\n)\npipe.to(device)\n```\n\n---\n\n*The `StableDiffusionPipeline` requires a Hugging Face user access token. They can be found in user settings. [Follow this guide on Hugging Face access tokens](https://huggingface.co/docs/hub/security-tokens) if you need help.*\n\n---\n\nFrom here, we can use the first two components of the pipeline, the `tokenizer` and the `text_encoder`, to create the CLIP *text embeddings*.\n\n{{< notebook file=\"dream-cacher-prompt-embeds\" height=\"full\" >}}\n\nWe can see the token-level CLIP embeddings with shape *77x768*. Having *77* of these embeddings is problematic as we need a *single* vector  ***vector* search** to work.\n\n![stable-diffusion-pipeline-annotated](./images/faster-stable-diffusion-7.png)\n<small>The stable diffusion pipeline makes use of **77** *768-d* text embeddings output by CLIP.</small>\n\nThanks to CLIP's contrastive pretraining, we can produce a meaningful *768-d* vector by *\"mean pooling\"* the *77* 768-d vectors.\n\n![mean-pooling](./images/faster-stable-diffusion-8.png)\n<small>Mean pooling takes the *mean* value across each dimension in our 2D tensor to create a new 1D tensor (the vector).</small>\n\nWhen we apply mean pooling to our 2D text embeddings tensor, we average the values across each dimension, outputting a single 1D text embedding vector.\n\n![vector-extraction](./images/faster-stable-diffusion-9.png)\n<small>The mean pooling operation occurs before the long stable diffusion steps.</small>\n\nFortunately, CLIP generates a mean pooled version of these text embeddings by default, so we don't need to perform this operation ourselves.\n\n{{< notebook file=\"dream-cacher-pooled-embeds\" height=\"full\" >}}\n\nThese pooled *\"prompt vectors\"* are created before the long diffusion process begins, so they can be built quickly.\n\n![creating-embeddings](./images/faster-stable-diffusion-10.png)\n<small>The user can create meaningful vectors, but what's next?</small>\n\nWe now have meaningful prompt vectors, but how do we use them? For this, we need a **vector database**.\n\n## Vector Database\n\nA [*\"vector database\"*](/learn/vector-database/) is a vector storage and retrieval service that we can use to efficiently search through millions or even billions of vectors.\n\nAfter generating the prompt vectors, we insert them into our vector database. From there, they can be retrieved by querying with *new* prompt vectors.\n\n![making-queries](./images/faster-stable-diffusion-11.png)\n<small>When making queries, a \"query prompt\" is encoded into a query vector, and the most similar already indexed vectors are retrieved.</small>\n\nGiven the prompt `\"A person surfing\"`, we build a prompt vector, search within the vector database for similar items, and find several images that already fit this description:\n\n![surf-search-example](./images/faster-stable-diffusion-12.png)\n<small>Embedding and retrieval of CLIP prompt vectors allow us to quickly return many similar images previously generated.</small>\n\nFor this to work, we need to add a second storage location for our images, like GCP's *Cloud Storage*. Here, we simply save the image using the unique ID assigned to it (we will cover this later) and then retrieve it using this same ID.\n\n### Prompt Recommendation?\n\nAn optional metadata field we can include is the plaintext prompt used to generate the vector. This isn't necessary for direct image retrieval, but it enables a second feature, *prompt recommendations*.\n\n![surf-search-recommendation-example](./images/faster-stable-diffusion-13.png)\n<small>Prompt recommendations can be useful to inspire users' creativity.</small>\n\nThese recommendations are helpful for users struggling with prompt ideas and encourage interesting movement across the vector space.\n\n### Upserting Vectors and Storing Images\n\nTo implement our search component, we will use [Pinecone's vector database](https://www.pinecone.io). To do this, we initialize our connection (using [a free API key](https://app.pinecone.io)) and create a vector *index* (a single instance within the vector database).\n\n{{< notebook file=\"dream-cacher-create-index\" height=\"full\" >}}\n\nWe haven't added any vectors yet, so `'total_vector_count'` should be `0`. We need a unique ID, the `prompt_embeds` vector, and related metadata to create our first record. These will be upserted in the format:\n\n```python\n\"abcd-abcd-abcd-abcd\",  # unique ID\n[0.01, 0.64, 0.27, ...],  # 784-d vector\n{\n \"prompt\": \"A person surfing\"\n}  # metadata dictionary\n```\n\nThe ID can be created using the `uuid` module like so:\n\n```python\nimport uuid\n\n_id = str(uuid.uuid4())  # creates format \"xxxx-xxxx-xxxx-xxxx\"\n```\n\nThe `prompt_embeds` must be reformated into a flat list to satisfy Pinecone client requirements:\n\n```python\nvec = prompt_embeds.cpu().tolist()[0]\n```\n\nThen we use the original prompt text to create the metadata dictionary:\n\n```python\nmeta = {\n   \"prompt\": prompt\n}\n```\n\nNow we upsert everything with `index.upsert`:\n\n{{< notebook file=\"dream-cacher-upsert\" height=\"full\" >}}\n\nThat is our vector and prompt metadata indexed. Now we need to generate the image using the `StableDiffusionPipeline` and store it somewhere to be retrieved later. We will use GCP's *Cloud Storage*.\n\n---\n\n*If you're not familiar with GCP's Cloud Storage and Python's Cloud Storage API, they are explained in [the GCP Cloud Storage docs](https://cloud.google.com/storage/docs).*\n\n---\n\nWe generate the image like so:\n\n{{< notebook file=\"dream-cacher-generate-image\" height=\"full\" >}}\n\nWith the image stored as a PIL object, we save it to file and upload it to a GCP *Cloud Storage* bucket using:\n\n```python\n# !pip install google-cloud-storage\nfrom google.cloud import storage\n\n# set credentials\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'cloud-storage.json'\n# connect to bucket (we named it 'hf-diffusion-images')\nstorage_client = storage.Client()\nbucket = storage_client.get_bucket('hf-diffusion-images')\n# create object blob\nblob = bucket.blob(f'images/{_id}.png')\n# save image to file\nim.save(f'{_id}.png', format='png')\n# upload to blob\nblob.upload_from_filename(f'{_id}.png')\n```\n\nThat covers most of our logic with embeddings, vector search, and the image storage component. The next step is applying this at scale.\n\n## Stable Diffusion Pipeline\n\nWe could build a frontend and let users begin generating images right away. Yet, with just `\"A person surfing\"` currently indexed, the benefits of *retrieval when possible* are unlikely to be used.\n\nFor now, we need far more records indexed to increase the probability of a user entering a prompt similar to those already indexed.\n\nTo get started with this, we build an index using existing prompt datasets found on Hugging Face *Datasets*, like the `bartman081523/stable-diffusion-discord-prompts` dataset.\n\n{{< notebook file=\"dream-cacher-load-dataset\" height=\"full\" >}}\n\nThis dataset alone contains nearly 4M prompts. Many of these can be nonsensical or low-quality — this is particularly noticeable among short prompts. So, to improve quality, we filter for longer prompts.\n\n{{< notebook file=\"dream-cacher-long-prompts\" height=\"full\" >}}\n\nDoing this, we are still left with a huge 3.56M prompts. More than enough to populate our index.\n\n### Generating Records at Scale\n\nOur code for building the index requires the prompt vectors *and* the generated images. As before, we use the `StableDiffusionPipeline`:\n\n{{< notebook file=\"dream-cacher-gen-images\" height=\"full\" >}}\n\n![image-examples](./images/faster-stable-diffusion-14.png)\n<small>Images output by the above code.</small>\n\nBecause we already know what prompts to use, we perform this and successive steps in batches. Allowing us to process many prompts in parallel and speeding up the process.\n\nAfter generating the images, we must give each record a unique ID. This ID is used for storage in both Pinecone and Cloud Storage — as we did for the single `\"a person surfing\"` example earlier.\n\n![generating-images](./images/faster-stable-diffusion-15.png)\n<small>High-level pipeline for generating data.</small>\n\nWe upload the image files to Cloud Storage:\n\n```python\nfor _id, im in zip(ids, out.images):\n       im.save(f\"tmp.png\")\n   # connect to target blob in cloud storage\n   blob = bucket.blob(f\"{_id}.png\")\n   # upload\n   blob.upload_from_filename(f\"{_id}.png\")\n```\n\nThen insert the vectors and respective metadata in Pinecone:\n\n```python\n# tokenize prompts\ntext_inputs = pipe.tokenizer(\n       prompts, padding=True, truncation=True,\n       return_tensors='pt'\n)\n# get embeddings\ntext_embeds = pipe.text_encoder(**text_inputs)\n# mean pool\ntext_embeds = text_embeds.pooler_output.cpu().tolist()\n# create metadata\nmetadata = [{'prompt': prompt} for prompt in prompts]\n# add to pinecone\nindex.upsert(zip(ids, text_embeds, metadata))\n```\n\nWe repeat this over tens of thousands, millions, or more records. After this, new prompt vectors are reasonably likely to collide with existing vectors in the vector space.\n\nAll that remains is a way for users to enter prompts.\n\n## Dream Cacher App\n\n<gradio-app space=\"jamescalam/dream-cacher\"></gradio-app>\n\nThe app is built using Gradio blocks and is relatively simple, consisting of four components:\n\n* `gr.TextInput`\n* `gr.Button`\n* `gr.Dataframe`\n* `gr.Gallery`\n\nFrom these, we have two key features. The prompt recommendation is triggered every time the `gr.TextInput` value changes and is displayed in `gr.Dataframe`. The image search/diffusion is triggered by the search `gr.Button` and displayed in `gr.Gallery`.\n\nBecause both images and prompts are attached as metadata to vectors, the prompt recommendation and image retrieval are pulling records from the same Pinecone index. They differ because the image retrieval process also downloads the stored images from GCP's Cloud Storage.\n\n```python\nfrom PIL import Image\n\n# retrieve most similar records\nxc = index.query(\n   embeds, top_k=9, include_metadata=True\n)\n# get IDs\nids = [match['id'] for match in xc['matches']]\nimages = []\n# begin retrieving images and append to 'images' list\nfor _id in ids:\n   blob = bucket.blob(f'/images/{_id}.png').download_as_string()\n   # convert to 'in-memory file'\n   blob_byes = io.BytesIO(blob)\n   # open image as PIL object\n   im = Image.open(blob_bytes)\n   images.append(im)\n```\n\nThese images are then passed as a list of PIL objects to Gradio's `gr.Gallery` component.\n\nWe have everything needed and can deploy the app using Hugging Face *Spaces*.\n\n![hf-create-space](./images/faster-stable-diffusion-16.png)\n<small>Click **New Space**, found on the Hugging Face homepage after logging in.</small>\n\nTo do so, we sign up for an account at [huggingface.co](https://huggingface.co) > click **Spaces** > enter space details and use *\"Gradio\"* as the *Space SDK*.\n\nFrom here we add our Gradio app code to an `app.py` file:\n\n![hf-new-app-file](./images/faster-stable-diffusion-17.png)\n<small>Creating the `app.py` file in Hugging Face *Spaces*.</small>\n\nWe repeat the **Create a new file** process for a `requirements.txt` file and specify the modules that must be installed:\n\n```\ndiffusers\ntransformers\n--extra-index-url https://download.pytorch.org/whl/cu113\ntorch\ngoogle-cloud-storage\npinecone-client\n```\n\nOur app must communicate with our private Pinecone index and private Cloud Storage. To add API keys or secret tokens to Hugging Face *Spaces*, we open the **Settings** tab and add *Repo secrets*.\n\n![image-20221103121340788](./images/faster-stable-diffusion-18.png)\n\nThese are stored as environment variables in our app deployment, accessible via Python with `os.environ[\"<SECRET_NAME>\"]`.\n\nBecause *Cloud Storage* requires a local JSON file with connection details and auth codes, we cannot enter this information directly inside *Repo secrets*. Instead, we encrypt the file with the `cryptography` module:\n\n```python\nimport json\nfrom cryptography.fernet import Fernet\n\n# load secrets JSON file\nwith open('cloud-storage-secrets.json', 'r', encoding='utf-8') as fp:\n   api = json.load(fp)\n# convert secrets JSON to string\nkeys_str = json.dumps(api, indent=4)\n\n# initialize key to be used for encryption/decryption\nkey = Fernet.generate_key()\nfernet = Fernet(key)\n\n# create encrypted secrets JSON\nencrypted = fernet.encrypt(keys_str.encode())\n# save to file\nwith open('cloud-storage.encrypted', 'wb') as fp:\n   fp.write(encrypted)\n```\n\nWe save the encrypted secrets to file and upload them again to our Hugging Face space. To allow our `app.py` script to decrypt the file, we must add the `key` value to our *Repo secrets* under `DECRYPTION_KEY`.\n\nFrom there, we decrypt the file during deployment and initialize our Cloud Storage connection like so:\n\n```python\nfrom cryptography.fernet import Fernet\nfrom google.cloud import storage\n\n# decrypt Storage Cloud credentials\nfernet = Fernet(os.environ['DECRYPTION_KEY'])\n\nwith open('cloud-storage.encrypted', 'rb') as fp:\n   encrypted = fp.read()\n   creds = json.loads(fernet.decrypt(encrypted).decode())\n# then save creds to file\nwith open('cloud-storage.json', 'w', encoding='utf-8') as fp:\n   fp.write(json.dumps(creds, indent=4))\n# connect to Cloud Storage\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'cloud-storage.json'\nstorage_client = storage.Client()\nbucket = storage_client.get_bucket('hf-diffusion-images')\n```\n\nFrom there, everything is fully prepared, and we simply wait for Hugging Face to build and deploy our app...\n\n![hf-wait-for-build](./images/faster-stable-diffusion-19.png)\n\n<small>The status will start with *Building* and the app is ready-to-go as soon as we see *Running*.</small>\n\nWith that, our app is ready-to-go and accessible to the world.\n\n---\n\nThat's it for this walkthrough to building a diffusion generation and retriever app, or *dream cacher* the latest NLP and vector search technology.\n\n## References\n\n[1] L. Debut, [Diffusers 0.1.2 Release Notes](https://github.com/huggingface/diffusers/releases/tag/0.1.2) (2022), Hugging Face Diffusers Repo\n\n[2] A. Karpathy, [Stable Diffusion dreams of steam punk neural networks](https://www.youtube.com/watch?v=Jv1ayv-04H4) (2022), YouTube\n\n[3] B. Poole, et al., [DreamFusion: Text-to-3D using 2D Diffusion](https://arxiv.org/abs/2209.14988) (2022), ArXiV\n\n[4] Corridor Crew, [Recreating Spiderverse with AI](https://www.youtube.com/watch?v=QBWVHCYZ_Zs) (2022), YouTube\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbaa"
  },
  "filename": "sigir-2022.md",
  "title": "post",
  "category": "\"Pinecone sponsors the 45th annual SIGIR conference\"",
  "content": "---\nlayout: post\ntitle: \"Pinecone sponsors the 45th annual SIGIR conference\"\nheadline: Pinecone sponsors the 45th annual SIGIR conference\ncategories:\n  - Company News\ntoc: >-\nauthor:\n  name: Gibbs Cullen\n  position: Senior Product Marketing Manager\n  src: /images/gibbs-cullen.jpg\n  href: https://www.linkedin.com/in/gibbscullen/\ndate: \"2022-07-22\"\n# Date: July 22, 2022\n# Open Graph\ndescription: Pinecone was a proud sponsor for the conference and for the Reaching Efficiency in Neural Information Retrieval (ReNeuIR) workshop, co-led by our very own Sebastian Bruch, Staff Research Scientist at Pinecone.\nthumbnail: \"/images/sigir-2022-thumbnail.jpg\"\n---\n\n![Sigir 2022](/images/sigir-2022.jpg)\n\nThe 45th annual [SIGIR Conference](https://sigir.org/sigir2022/) on research and development in information retrieval (IR) took place in Madrid, Spain and online last week from July 11-15. Attendance was at capacity as this was the first time attendees were able to gather in person since the start of the pandemic. There was a co-located event [ICTIR](https://www.ictir2022.org/) on the theory of IR that took place on July 11-12. \n\nPinecone was a proud sponsor for the conference and for the [Reaching Efficiency in Neural Information Retrieval (ReNeuIR) workshop](https://reneuir.org/), co-led by our very own Sebastian Bruch, Staff Research Scientist at Pinecone. Below is a recap of some notable themes we took away from the event as well as a summary of the ReNeuIR workshop. \n\n![Sigir 2022 Conference](/images/sigir-conference.jpg)\n\n## Conference themes\n\nThe five day conference was broken out into tutorials, paper presentations, and workshops. The various sessions focused on the latest research and development in IR including recommendation engines, semantic search, and deep learning. There were also some notable talks introducing newer concepts around reinforcement learning and the use of knowledge graphs alongside typical IR systems. \n\n### Increasing interest in vector search and vector databases\n\nIn addition to more traditional IR topics, this year’s event had a larger focus around vector search and databases. As the industry moves more towards deep learning applications, the need for vector search is growing. This increased interest was notable at SIGIR with a number of papers on dense retrieval and a panel on “[Applications and Future of Dense Retrieval in Industry](https://dl.acm.org/doi/abs/10.1145/3477495.3536324).” In order to support this level of similarity search, vector databases like [Pinecone](/) are needed.\n\n### Focus on more sustainable research  \n\nIn general, discussions around sustainability and environmental impact within technology have been on the rise. So naturally, this was a topic of conversation at SIGIR, as well as the motivation behind the ReNeuIR workshop which we’ll cover more below. A notable paper on this topic was “[Reduce, Reuse, Recycle: Green Information Retrieval Research](https://dl.acm.org/doi/abs/10.1145/3477495.3531766)“, which won Best Paper Honorable Mention. Developments in this space have even sparked internal discussions amongst Pinecone’s engineering teams.  \n\n### Retrieval-enhanced machine learning \n\nFinally, there was a focus on a newer notion of retrieval-enhanced machine learning (REML). This theme was highlighted in a conference paper titled “[Retrieval-Enhanced Machine Learning](https://arxiv.org/abs/2205.01230)”. Using the REML framework broadens the scope of conventional IR methods to include task-driven machines, such as machine learning (ML) models. And when a user of a retrieval system is an ML model or system, requirements are introduced such as continual index updates and stricter efficiency constraints, both of which Pinecone strives to deliver. This talk helped to lay the foundation for this new style of information access research with the hope of advancing ML and artificial intelligence (AI) efforts. \n\n## ReNeuIR Workshop\n\nThe ReNeurIR workshop facilitated discussion and collaboration about methods in the new age of neural information retrieval (NIR), specifically around efficiency. NIR models achieve a greater effectiveness than the previous wave of machine learning models (e.g. decision forests on many IR tasks), but with orders of magnitude more learnable parameters and much greater amounts of data.\n\nIn a world where large organizations at the forefront of research in ML and IR have enormous amounts of resources, it is easy for them to deprioritize efficiency and sustainability concerns. This workshop focused on ways to promote more sustainable research by identifying best practices in the development and evaluation of neural models for IR.\n\nThe full day workshop included [two keynotes and a panel](https://reneuir.org/program.html) by experts from leading IR research organizations such as HuggingFace, Microsoft, Georgetown University, and University of Queensland. The event had roughly fifty in-person and twenty virtual attendees. In terms of next steps, the workshop’s organizing committee will be publishing guidelines on how to measure and report the environmental impact of research within IR. Follow the [workshop’s Twitter](https://twitter.com/ReNeuIRWorkshop) for updates. \n\n**About Pinecone:**\n\nThe Pinecone vector database makes it easy to build high-performance vector search applications. Developer-friendly, fully managed, and easily scalable without infrastructure hassles. Visit our [website](/) to create a free account or [contact](/contact) us to learn more. We hope to see you next year in Taipei for SIGIR 2023!\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbab"
  },
  "filename": "what-is-similarity-search.md",
  "title": "post",
  "category": "\"What is Similarity Search?\"",
  "content": "---\nlayout: post\ntitle: \"What is Similarity Search?\"\nheadline: \"What is <span>Similarity Search?</span>\"\ncategories:\n  - Vector Search 101\ntoc: >-\nweight: 1\nauthor:\n  name: Rajat Tripathi\n  position: Software Engineer\n  src: /images/company-rajat.jpeg\n  href: https://www.linkedin.com/in/rajat-tripathi-08/\ndescription: An introduction to similarity search including key terms, use cases, and examples.\n# Open graph\nimages: ['/images/what-is-similarity-search-word2vec.png']\n---\n\n**[Pinecone](/) is a vector database that makes it easy to add similarity search to any application. [Try it free](https://app.pinecone.io), and continue reading to learn what makes similarity search so useful.**\n\n## Introduction\n\nSearching through data for similar items is a common operation in databases, search engines, and many other applications. Finding similar items based on fixed numeric criteria is very straightforward using a query language when we are dealing with traditional databases. For example, finding employees in a database within a fixed salary range.\n\nBut sometimes we have to answer questions like &ldquo;Which objects in our inventory are similar to what a user searched for?&rdquo;\nThe search terms can be vague and can have a lot of variations. For example, a user can search for something generic like &ldquo;shoes&rdquo;,&ldquo;black shoes&rdquo; or something more precise like &ldquo;Nike AF-1 LV8&rdquo;.\n\n![Queries can be vague and varied](/images/what-is-similarity-search-shoes.png)\n\nOur system must be able to discern between these terms and must understand how a black shoe differs from other shoes. To handle such queries we need a representation that captures the deeper conceptual meaning of the objects. On top of that, in scenarios like these, we might have to work with data to the scale of billions of objects.\n\nWhen dealing with data in this scale & context, this problem is quite unlike searching through traditional databases containing symbolic object representations. Hence we need something more powerful that can allow us to search through semantic representations efficiently.\n\nWith similarity search, we can work with semantic representations of our data and find similar items fast. And in the sections below we will discuss how exactly it works.\n\n## What Are Vector Representations?\n\nIn the passage above, we talked about representing objects in a way that captures their deeper meanings. In machine learning, we often represent real-world objects and concepts as a set of continuous numbers, also known as [vector embeddings](/learn/vector-embeddings). This very neat method allows us to translate the similarity between objects as perceived by us into a vector space.\n\nThis means when we represent images or pieces of text as vector embeddings, their semantic similarity is represented by how close their vectors are in the vector space. Hence, what we want to look at is the distance between vectors of the objects.\n\nThese vector representations (embeddings) are often created by training models according to the input data and task. Word2Vec,GLoVE, USE etc. are popular models for generating  embeddings from text data while CNN models like VGG are often used to create image embeddings.\n\n![Word2Vec illustration](/images/what-is-similarity-search-word2vec.png)\n\nThe figure above from [this](https://jalammar.github.io/illustrated-word2vec/) great article on word2vec, can help us visualize how the model can generate similar representations of similar words and is able to capture the semantic meaning.\n\nThis concept can be extended to more complex objects. We can combine information from features in the dataset to create embeddings for every row in the dataset. This is something leveraged by many search & recommendation based algorithms. The point that I want to make is, training a machine learning model on any data can transform the broad abstract concept it can have to something on which we can perform mathematical operations that can give us the insights we need.\n\n## Distance Between Vectors\n\nWe mentioned earlier that we find similarities between objects by calculating the distance between their vectors. We can calculate the distance between these vectors in the vectors space according to the distance metric that fits our problem the best.\n\nSome of the commonly used distance metrics in ML are Euclidean, Manhattan, Cosine, and Chebyshev. The image below will help us understand the intuition behind each of these methods.\n\n![Similarity search distance metrics](/images/what-is-similarity-search-distance-metrics.jpeg)\n\nThe choice of distance metric depends on the use case. A great guide to learn more about distance metrics is [here](https://towardsdatascience.com/importance-of-distance-metrics-in-machine-learning-modelling-e51395ffe60d).\n\n## Performing Search\n\nNow we know we can use vector embeddings to represent our objects, and the distances between vectors represent the similarity between the objects themselves.\n\nThis is where the similarity search, or [vector search](/learn/vector-search-basics/), kicks in. Given a set of vectors and a query vector, we need to find the most similar items in our set for the query. We call this task nearest neighbor search.\n\n### K Nearest Neighbors\n\nK nearest neighbors or [k-NN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) is a very popular algorithm to find nearest vectors in a space for a given query vector. The k here is a hyperparameter set by us which denotes how many nearest neighbors we want to retrieve.\n\nWe can perform k-NN on the vectors we have for our data and retrieve the nearest neighbors for our query vector depending on the distance between the vectors.\n\n![Similarity search with k-NN](/images/what-is-similarity-search-knn.jpg)\n\nA major drawback of k-NN is that to find the nearest vectors for our query we will have to calculate its distance with every vector we have in our database. This will be very inefficient if we have to search through millions of vectors.\n\n### Approximate Neighbor Search\n\nTo reduce the computation complexity added by an exhaustive search like kNN we make use of approximate neighbor search.\n\nInstead of checking distances between each vector in the [database](/learn/vector-database/), we retrieve a \"good guess\" of the nearest neighbor. In some use cases, we would rather lose some accuracy in favor of performance gain, thus allowing us to scale our search. ANN allows us to get a massive performance boost on similarity search when dealing with huge datasets.\n\nIn approximately nearest neighbors (ANN), we build [index structures](https://www.pinecone.io/learn/what-is-a-vector-index/) that narrow down the search space and improve lookup times. Apart from that, most ML models produce vectors that have high dimensionality which is another [hurdle](https://en.wikipedia.org/wiki/Curse_of_dimensionality) to overcome. Approximate search relies on the fact that even though data is represented in a large number of dimensions, their actual complexity is low. It tries to work with the *true intrinsic dimensionality* of data. Hashing is a good example of a method that allows us to do it and is used widely for many applications.\nThere are various algorithms to solve the approximate search problem and to actually dive into how approximate search works warrants another article of its own. I suggest going through [this three-part series of articles](https://mccormickml.com/2017/10/13/product-quantizer-tutorial-part-1/) to understand ANN better.\n\nAs an overview, it is sufficient to understand that ANN algorithms make use of techniques like indexing, clustering, hashing, and quantization to significantly improve computation and storage at the cost of some loss in accuracy.\n\n## Conclusion\n\nWhile we have barely scratched the surface of the complexities of similarity search &mdash; also known as [vector search](/learn/vector-search-basics/) &mdash; with this article, the intent was to introduce some basic concepts and provide resources for a detailed reading on the topic. An increasing number of applications make use of similarity search to untangle problems in search & other domains, I highly encourage diving deeper into some of these concepts to learn more!\n\nWhile you are on the website, why not have a look at how easy it is to build a scalable similarity search system in just a few lines of code?\nYou can find a few [example notebooks](https://www.pinecone.io/learn/) that use Pinecone for solving similarity search and related problems. Grab your free [API key](https://app.pinecone.io/) and we will be happy to help you along the way!\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbac"
  },
  "filename": "langchain-intro.md",
  "title": "post",
  "category": "\"Getting Started with LLMs Using LangChain\"",
  "content": "---\nlayout: post\ntitle: \"Getting Started with LLMs Using LangChain\"\nheadline: \"Getting Started with LLMs Using LangChain\"\ncategories:\n  - Algorithms & Libraries\ntoc: >-\nweight: 3\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: Introduction to the Generative AI and LLM framework for building apps with OpenAI's GPT-3 and open-source alternatives.\n# Open graph\nimages: ['https://www.pinecone.io/images/langchain-intro-0.png']\n---\n\n**L**arge **L**anguage **M**odels (LLMs) entered the world stage with the release of OpenAI's GPT-3 in 2020 [GPT3]. Since then, they've enjoyed a steady growth in popularity.\n\nThat is until late 2022. Interest in LLMs and the broader discipline of generative AI has skyrocketed. The reasons for this are likely the continuous upward momentum of significant advances in LLMs.\n\nWe saw the dramatic news about Google's _\"sentient\"_ LaMDA chatbot. The first high-performance and *open-source* LLM called BLOOM was released. OpenAI released their next-generation text embedding model and the next generation of *\"GPT-3.5\"* models.\n\nAfter all these giant leaps forward in the LLM space, OpenAI released *ChatGPT* — thrusting LLMs into the spotlight.\n\n*LangChain* appeared around the same time. Its creator, Harrison Chase, made the first commit in late October 2022. Leaving a short couple of months of development before getting caught in the LLM wave.\n\nDespite being early days for the library, it is already packed full of incredible features for building amazing tools around the core of LLMs. In this article, we'll introduce the library and start with the most straightforward component offered by LangChain — LLMs.\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/nE2skSRWTTs\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n---\n\n# LangChain\n\nAt its core, LangChain is a framework built around LLMs. We can use it for chatbots, [**G**enerative **Q**uestion-**A**nswering (GQA)](https://www.pinecone.io/learn/openai-gen-qa/), summarization, and much more.\n\nThe core idea of the library is that we can _\"chain\"_ together different components to create more advanced use cases around LLMs. Chains may consist of multiple components from several modules:\n\n* **Prompt templates**: Prompt templates are templates for different types of prompts. Like \"chatbot\" style templates, ELI5 question-answering, etc\n\n* **LLMs**: Large language models like GPT-3, BLOOM, etc\n\n* **Agents**: Agents use LLMs to decide what actions should be taken. Tools like web search or calculators can be used, and all are packaged into a logical loop of operations.\n\n* **Memory**: Short-term memory, long-term memory.\n\nWe will dive into each of these in much more detail in upcoming chapters of the LangChain handbook. You can stay updated for each release via our newsletter:\n\n{{< newsletter text=\"Subscribe to stay updated with LangChain releases!\" inputText=\"Email address...\" buttonText=\"Submit\">}}\n\nFor now, we'll start with the basics behind **prompt templates** and **LLMs**. We'll also explore two LLM options available from the library, using models from *Hugging Face Hub* or *OpenAI*.\n\n# Our First Prompt Templates\n\nPrompts being input to LLMs are often structured in different ways so that we can get different results. For Q&A, we could take a user's question and reformat it for different Q&A styles, like conventional Q&A, a bullet list of answers, or even a summary of problems relevant to the given question.\n\n## Creating Prompts in LangChain\n\nLet's put together a simple question-answering prompt template. We first need to install the `langchain` library.\n\n```\n!pip install langchain\n```\n\n---\n\n*Follow along with the code via [Colab](https://colab.research.google.com/github/pinecone-io/examples/blob/master/generation/langchain/handbook/00-langchain-intro.ipynb)!*\n\n---\n\nFrom here, we import the `PromptTemplate` class and initialize a template like so:\n\n```python\nfrom langchain import PromptTemplate\n\ntemplate = \"\"\"Question: {question}\n\nAnswer: \"\"\"\nprompt = PromptTemplate(\n        template=template,\n    input_variables=['question']\n)\n\n# user question\nquestion = \"Which NFL team won the Super Bowl in the 2010 season?\"\n```\n\nWhen using these prompt template with the given `question` we will get:\n\n```\nQuestion: Which NFL team won the Super Bowl in the 2010 season?\n\nAnswer: \n```\n\nFor now, that's all we need. We'll use the same prompt template across both Hugging Face Hub and OpenAI LLM generations.\n\n# Hugging Face Hub LLM\n\nThe Hugging Face Hub endpoint in LangChain connects to the Hugging Face Hub and runs the models via their free inference endpoints. We need a [Hugging Face account and API key](https://huggingface.co/settings/tokens) to use these endpoints.\n\nOnce you have an API key, we add it to the `HUGGINGFACEHUB_API_TOKEN` environment variable. We can do this with Python like so:\n\n```python\nimport os\n\nos.environ['HUGGINGFACEHUB_API_TOKEN'] = 'HF_API_KEY'\n```\n\nNext, we must install the `huggingface_hub` library via Pip.\n\n```\n!pip install huggingface_hub\n```\n\nNow we can generate text using a Hub model. We'll use [`google/flan-t5-x1`](https://huggingface.co/google/flan-t5-xl).\n\n---\n\n*The default Hugging Face Hub inference APIs do not use specialized hardware and, therefore, can be slow. They are also not suitable for running larger models like `bigscience/bloom-560m` or `google/flan-t5-xxl` (note `xxl` vs. `xl`).*\n\n---\n\n{{< notebook file=\"langchain-00-hf-gen\" height=\"full\" >}}\n\nFor this question, we get the correct answer of `\"green bay packers\"`.\n\n## Asking Multiple Questions\n\nIf we'd like to ask multiple questions, we can try two approaches:\n\n1. Iterate through all questions using the `generate` method, answering them one at a time.\n2. Place all questions into a single prompt for the LLM; this will only work for more advanced LLMs.\n\nStarting with option (1), let's see how to use the `generate` method:\n\n{{< notebook file=\"langchain-00-hf-generate\" height=\"full\" >}}\n\nHere we get bad results except for the first question. This is simply a limitation of the LLM being used.\n\nIf the model cannot answer individual questions accurately, grouping all queries into a single prompt is unlikely to work. However, for the sake of experimentation, let's try it.\n\n{{< notebook file=\"langchain-00-hf-multi-query\" height=\"full\" >}}\n\nAs expected, the results are not helpful. We'll see later that more powerful LLMs can do this.\n\n# OpenAI LLMs\n\nThe OpenAI endpoints in LangChain connect to OpenAI directly or via Azure. We need an [OpenAI account and API key](https://beta.openai.com/account/api-keys) to use these endpoints.\n\nOnce you have an API key, we add it to the `OPENAI_API_TOKEN` environment variable. We can do this with Python like so:\n\n```python\nimport os\n\nos.environ['OPENAI_API_TOKEN'] = 'OPENAI_API_KEY'\n```\n\nNext, we must install the `openai` library via Pip.\n\n```\n!pip install openai\n```\n\nNow we can generate text using OpenAI's GPT-3 generation (or *completion*) models. We'll use [`text-davinci-003`](https://huggingface.co/google/flan-t5-xl).\n\n```python\nfrom langchain.llms import OpenAI\n\ndavinci = OpenAI(model_name='text-davinci-003')\n```\n\n---\n\n*Alternatively, if you're using OpenAI via Azure, you can do:*\n\n```python\nfrom langchain.llms import AzureOpenAI\n\nllm = AzureOpenAI(\n    deployment_name=\"your-azure-deployment\", \n    model_name=\"text-davinci-003\"\n)\n```\n\n---\n\nWe'll use the same simple question-answer prompt template as before with the Hugging Face example. The only change is that we now pass our OpenAI LLM `davinci`:\n\n{{< notebook file=\"langchain-00-openai-gen\" height=\"full\" >}}\n\nAs expected, we're getting the correct answer. We can do the same for multiple questions using `generate`:\n\n{{< notebook file=\"langchain-00-openai-generate\" height=\"full\" >}}\n\nMost of our results are correct or have a degree of truth. The model undoubtedly functions better than the `google/flan-t5-xl` model. As before, let's try feeding all questions into the model at once.\n\n{{< notebook file=\"langchain-00-openai-multi-query\" height=\"full\" >}}\n\nAs we keep rerunning the query, the model will occasionally make errors, but at other times manage to get all answers correct.\n\n---\n\nThat's it for our introduction to LangChain — a library that allows us to build more advanced apps around LLMs like OpenAI's GPT-3 models or the open-source alternatives available via Hugging Face.\n\nAs mentioned, LangChain can do much more than we've demonstrated here. We'll be covering these other features in upcoming articles.\n\n---\n\n# References\n\n[GPT3] [GPT-3 Archived Repo](https://github.com/openai/gpt-3) (2020), OpenAI GitHub"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbad"
  },
  "filename": "gif-search.md",
  "title": "post",
  "category": "\"Using Semantic Search to Find GIFs\"",
  "content": "---\nlayout: post\ntitle: \"Using Semantic Search to Find GIFs\"\nheadline: \"Using Semantic Search to Find GIFs\"\ncategories:\n  - Projects\ntoc: >-\nweight: 2\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: Learn how to enhance GIF discovery using NLP semantic search\n# Open graph\nimages: [\"/images/gif-search-0.png\"]\nthumbnail: \"https://www.pinecone.io/images/gif-search-0.png\"\n---\n\nVector search powers some of the most popular services in the world. It serves your Google results, delivers the [best podcasts on Spotify](/learn/spotify-podcast-search/), and accounts for *at least* 35% of consumer purchases on Amazon \\[1\\]\\[2\\].\n\nIn this article, we will use vector search applied to language, called *semantic* search, to build a GIF search engine. Unlike more traditional search where we rely on *keyword* matching, semantic search enables search based on the *human meaning* behind text and images. That means we can find highly relevant GIFs with natural language prompts.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/gif-search-1.mp4\" type=\"video/mp4\">\n</video>\n<small>Preview of the GIF search app <a href=\"https://share.streamlit.io/pinecone-io/playground/gif-search/src/server.py\">available here</a>.</small>\n\nThe pipeline for a project like this is simple, yet powerful. It can easily be adapted to tasks as diverse as [video search](/learn/youtube-search/) or [answering Super Bowl questions](/learn/question-answering/), or as we'll see, finding GIFs.\n\n[*All supporting notebooks and scripts can be found here*](https://github.com/pinecone-io/examples/tree/master/search/semantic-search/gif-search).\n\n## GIF Dataset\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/xXsDIK9z_fg\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\nWe will be using the TGIF dataset found [on GitHub here](https://github.com/raingo/TGIF-Release). To get the dataset we can use `wget` (alternatively, download it manually), and unzip.\n\n```bash\nwget https://github.com/raingo/TGIF-Release/archive/master.zip\n\nunzip master.zip\n```\n\nIn these unzipped files we should be able to find a file named `tgif-v1.0.tsv` inside the `data` directory. We'll use *pandas* to load the file using `\\t` as the field delimiter.\n\n{{< notebook file=\"gif-search-read-csv\" height=\"full\" >}}\n\nThe dataset contains GIF URLs and their descriptions in natural language. We can take a look at the first five GIFs.\n\n{{< notebook file=\"gif-search-examples\" height=\"full\" >}}\n\nWe will find that there are some duplicate URLs, but these do not necessarily indicate duplicate records as a *single GIF* can be assigned multiple descriptions.\n\n{{< notebook file=\"gif-search-dupes\" height=\"full\" >}}\n\nWith our data, we can move on to building the search pipeline.\n\n## Search\n\nThe search pipeline will at a high level take our natural language query like *\"a dog talking on the phone\"* and search through the existing GIF descriptions for anything that has a similar *meaning* to this query.\n\nIn this context, we describe *meaning* as *semantic similarity*, both of which are loaded terms and could refer to many things. For example, are the two phrases `\"the dog eats lunch\"` and `\"the dog does not eat lunch\"` similar? In this case, it would depend very much on our use case.\n\nAnother example: which of the following two sentences are the most similar?\n\n```\nA: the stock market took a turn for the worse\n\nB: how did the stock market do today?\n\nC: the stock market performed worse than expected\n```\n\nIf we wanted to find phrases with similar meaning then the obvious choice would be `A` and `C`. Matching those with `B` would make little sense. However, this is not the case if we are searching for similar *question-answer pairs*; in that case, `B` should match very closely with `A` and `C`.\n\nIt's important to identify what your use case requires in it's definition of *\"semantic similarity\"*. For us, we really want to identify generic similarity. That is, we want `A` and `C` to match, and `B` to not match either of those.\n\nTo do this, we will transform our phrases into [*dense vector embeddings*](/learn/dense-vector-embeddings-nlp/). These dense vectors can be stored in a [*vector database*](https://www.pinecone.io/learn/vector-database/) where we can very quickly compare vectors and identify those that are most similar based on metrics like Euclidean distance and cosine similarity.\n\n![euclidean-cosine](/images/gif-search-2.png)\n<small>Both of these metrics identify the similarity (proximity) of vectors, but they do it based on distance (left) or angular similarity (right).</small>\n\nThe vector database handles the storage and fast search of our vector embeddings, but we still need a way to create these embeddings. To do that we use NLP transformer models called *retrievers* that are [fine-tuned for creating *sentence embeddings*](https://www.pinecone.io/learn/sentence-embeddings/). These sentence embeddings/vectors are able to *numerically represent* the *meaning* behind the text that they represent.\n\n![retriever-to-vector-space](/images/gif-search-3.png)\n<small>Retriever models are able to take two semantically similar phrases and encode them as similar vectors.</small>\n\nPutting these two components together gives us a semantic search pipeline that we can use to retrieve semantically similar GIF descriptions given a query.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/gif-search-4.mp4\" type=\"video/mp4\">\n</video>\n<small>GIF search pipeline covering the one-time indexing step (left) and querying (right).</small>\n\nLet's take a look at how we can put all of this together.\n\n### Initializing Components\n\nWe will start by initializing our retriever model. Many of the most powerful retrievers use a *sentence transformer* architecture, which are best supported via the `sentence-transformers` library, installed via a `pip install sentence-transformers`.\n\nTo find sentence transformer models we go to [*huggingface.co/models*](https://huggingface.co/models) and search for `sentence-transformers` for the *official* sentence transformer models. However, there are other models we can use like the [`all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) sentence transformer trained during a special event on [over 1B training pairs](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354). We will use this model.\n\n{{< notebook file=\"gif-search-sentence-transformers\" height=\"full\" >}}\n\nThere are a couple of important details here:\n\n* `max_sequence_length=128` means the model can read up to *128* input tokens.\n* `word_embedding_size=384` actually refers to the *sentence* embedding size. This means the model will output a 384-dimensional vector representation of the input text.\n\nFor the short several-word GIF descriptions of our dataset a maximum sequence length of *128* is *more than enough*.\n\nWe need to use the *sentence* embedding size when initializing our vector database, so we store that in the `embed_dim` variable above.\n\nTo initialize our vector database we first need to sign up for a [free Pinecone API key](https://app.pinecone.io/) and install the Pinecone Python client via `pip install pinecone-client`. Once ready, we initialize:\n\n{{< notebook file=\"gif-search-init-index\" height=\"full\" >}}\n\nHere, we are specifying an index name of `'gif-search'`; feel free to choose anything you like. It is simply a name. The `metric` is more important and depends on the model being used. For our chosen model we can see in its [*model card*](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) that it has been trained to use *cosine similarity*, hence why we have specified `metric='cosine'`. Alternative metrics include `euclidean` and `dotproduct`.\n\nWe've initialized both our vector database and retriever components, so we can move on to embedding and indexing our data.\n\n### Indexing\n\nThe embedding and indexing process is much faster when we perform these steps for multiple records *in parallel*. However, we cannot process all of our records at once as the retriever model must shift everything it is embedding into on-chip memory, which is limited.\n\nTo avoid this limit, while keeping indexing times as fast as possible, we process everything in batches of `64`.\n\n{{< notebook file=\"gif-search-indexing\" height=\"full\" >}}\n\nHere we are extracting the `batch` from our data `df`. We encode the descriptions via our retriever model, create metadata (covering both *descriptions* and *url*), and create some string format IDs. From this we have everything we need to create *documents*, which will look like this:\n\n```json\n(\n       \"some-id-value\",\n   [0.1, 0.2, 0.1, 0.4 ...],\n   {\n       'description': \"something descriptive\",\n       'url': \"https://xyz.com\"\n   }\n)\n```\n\nWhen we `upsert` these *documents* to the Pinecone index, we do so in batches of *64*. After all of this, we use `index.describe_index_stats()` to check that we have inserted all *125,782* documents, which we have.\n\n### Querying\n\nThe final step of querying our data covers:\n\n1. Encoding a query like *\"dogs talking on the phone\"* to create a *query vector*,\n2. Retrieval of similar *context vectors* from Pinecone,\n3. Getting relevant GIFs from the URLs found in our metadata fields.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/gif-search-5.mp4\" type=\"video/mp4\">\n</video>\n<small>The querying pipeline.</small>\n\nSteps *one* and *two* will be performed by a function named `search_gif`:\n\n```python\ndef search_gif(query):\n    # Generate embeddings for the query\n    xq = retriever.encode(query).tolist()\n    # Compute cosine similarity between query and embeddings vectors and return top 10 URls\n    xc = index.query(xq, top_k=10,\n                    include_metadata=True)\n    result = []\n    for context in xc['matches']:\n        url = context['metadata']['url']\n        result.append(url)\n    return result\n```\n\nTo display the GIFs we display HTML `<img>` elements using the metadata URLs to point to the correct GIFs. We do this using the `display_gif` function:\n\n```python\ndef display_gif(urls):\n    figures = []\n    for url in urls:\n        figures.append(f'''\n            <figure style=\"margin: 5px !important;\">\n              <img src=\"{url}\" style=\"width: 120px; height: 90px\" >\n            </figure>\n        ''')\n    return HTML(data=f'''\n        <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n        {''.join(figures)}\n        </div>\n    ''')\n```\n\nLet's test some queries.\n\n{{< notebook file=\"gif-search-queries\" height=\"full\" >}}\n\nThat looks pretty accurate so we've managed to put this GIF search pipeline together very easily. With a [little added effort](https://github.com/pinecone-io/examples/blob/master/search/semantic-search/gif-search/app.py) we can translate these steps in creating a web app using something like [*Streamlit*](https://www.youtube.com/watch?v=QpISF8gMsjQ).\n\n\n\nWe've successfully built a GIF search tool using a simple semantic search pipeline with out-of-the-box models and Pinecone. This same pipeline can be applied across a variety of domains with very little tweaking.\n\nThe ease-of-use and potential of both vector and semantic search have led to a lot of research and applications of both technologies beyond the world of big tech. If you're interested in seeing other applications of this technology, or would like to share your own, considering joining the [Pinecone community](https://community.pinecone.io).\n\n\n\n## Resources\n\n[Article Notebooks and Scripts](https://github.com/pinecone-io/examples/tree/master/search/semantic-search/gif-search)\n\n[1] M. Osborne, [How Retail Brands Can Compete And Win Using Amazon's Tactics](https://www.forbes.com/sites/forbesagencycouncil/2017/12/21/how-retail-brands-can-compete-and-win-using-amazons-tactics/) (2017), Forbes\n\n[2] L. Hardesty, [The history of Amazon's recommendation algorithm](https://www.amazon.science/the-history-of-amazons-recommendation-algorithm) (2019), Amazon Science\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbae"
  },
  "filename": "time-series-vectors.md",
  "title": "post",
  "category": "\"Time Series Analysis Through Vectorization\"",
  "content": "---\nlayout: post\ntitle: \"Time Series Analysis Through Vectorization\"\nheadline: \"Time Series Analysis Through Vectorization\"\ncategories:\n  - Applications of Vector Search\ntoc: >-\nweight: 4\nauthor:\n  name: Diego Lopez Yse\n  position: Data Scientist\n  src: /images/diego-lopez-yse.jpeg\n  href: \"https://lopezyse.medium.com/\"\ndescription: Components and complexities of time series, and how vectorization deals with them.\n# Open Graph\nimages: ['/images/time-series-vectors-1.png']\n---\n\n**The components and complexities of time series, and how vectorization can deal with them.**\n\nTime series data is all around us. The daily closing price of JP Morgan’s stock, the monthly sales of your company, the annual GDP value of Spain, or the daily maximum temperature values in a given region, are all examples of times series.\n\nA time series is a sequence of observations of data points measured over a time interval. The concept is not new, but we are witnessing an explosion of this type of data as the world gets increasingly measured. Today, sensors and systems are continuously growing the universe of time series datasets. From wearables to cell phones and self-driving cars, the number of [connected devices worldwide is set to hit 46 billion](https://techjury.net/blog/how-many-iot-devices-are-there/#gref).\n\nDepending on the frequency of observations, a time series may typically be hourly, daily, weekly, monthly, quarterly or annual — the data is in order, with a fixed time difference between the occurrence of successive data points.\n\n![Time series example](/images/time-series-vectors-1.png)\n<small>Example of time series: temperature records. Source: [Influxdata](https://www.influxdata.com/what-is-time-series-data/)</small>\n\n![Time series example](/images/time-series-vectors-2.png)\n<small>Example of time series: stock price evolution. Source: Influxdata</small>\n\n![Time series example](/images/time-series-vectors-3.png)\n<small>Example of time series: health monitoring records. Source: Influxdata</small>\n\nThe concept and applications of time series have become so important, that several tech giants had taken the lead by developing state of the art solutions to ingest, process and analyse them like never before:\n\n1. [Prophet](https://facebook.github.io/prophet/) is open-source software released by Facebook’s Core Data Science team. It’s used in many applications across Facebook for producing reliable forecasts for planning and goal setting, and it includes many possibilities for users to tweak and adjust forecasts. The solution is robust to outliers, missing data, and dramatic changes in the time series.\n2. [Amazon Forecast](https://aws.amazon.com/forecast/) is a fully managed service that uses Machine Learning to deliver highly accurate forecasts based on the same technology that powers Amazon.com. It builds precise forecasts for virtually any business condition, including cash flow projections, product demand and sales, infrastructure requirements, energy needs, and staffing levels.\n3. Uber’s Forecasting Platform team created [Omphalos](https://eng.uber.com/omphalos/), which is a time series back testing framework that generates efficient and accurate comparisons of forecasting models across languages and streamlines the model development process, thereby improving the customer experience.\n4. [SAP Analytics Cloud](https://www.sapanalytics.cloud/resources-your-guide-to-time-series-forecasting/) offers an automatic time series forecasting solution to perform advanced statistical analysis and generate forecasts by analyzing trends, fluctuations and seasonality. The algorithm works by analyzing the historical data to identify the existing patterns in the data and then using those patterns, projects the future values.\n\n## Time Series is Different\n\nThe “time component” in a time series provides an internal structure that must be accounted for, which makes it very different to any other data type, and sometimes more difficult to handle than traditional datasets.\n\nThis is the **main difference with sequential data**, where the order of the data matters, but the timestamp is irrelevant or doesn’t matter (as in the case of a DNA sequence, where the sequence is important but the concept of time is irrelevant).\n\nThis means that, unlike other Machine Learning challenges, you can’t just plug in an algorithm at a time series dataset and expect to have a proper result. Time series data can be transformed into supervised learning problems, but the key step is to consider their temporal structure like trends, seasonality, and forecast horizon.\n\nSince there are so many prediction problems that involve time series, first we need to understand their main components.\n\n## Anatomy of Time Series\n\nUnlike other data types, time series have a strong identity on their own. This means that we can’t use the usual strategies to analyze and predict them, since traditional analytical tools fail at capturing their temporal component.\n\nA good way to get a feel of how a time series pattern behaves is to break the time series into its many distinct components. The **decomposition** of a time series is a task that deconstructs a time series into several pieces, each representing one of the underlying categories of the pattern. Time series decomposition is built on the assumption that data arises as the result of the combination of some underlying components:\n\n* **Base Level**: This represents the average value in the series.\n* **Trend**: is observed when there is a sustained increasing or decreasing slope observed in the time series.\n* **Seasonality**: Occurs when there is a distinct repeated pattern observed between regular intervals due to seasonal factors, whether it is the month of the year, the day of the month, weekdays, or even times of the day. For example, retail stores sales will be high during weekends and festival seasons.\n* **Error**: The random variation in the series.\n\nAll series have a base level and error, while the trend and seasonality may or may not exist. This way, a time series may be imagined as a combination of the base level, trend, seasonality, and error terms.\n\nAnother aspect to consider is **cyclic behavior**. This happens when the rise and fall pattern in the series does not happen in fixed calendar-based intervals, like increases in retail sales that occur around December in response to Christmas or increases in water consumption in summer due to warmer weather.\n\nCare should be taken to not confuse the ‘cyclic’ effect with the ‘seasonal’ effect. So, how to differentiate between a ‘cyclic’ vs ‘seasonal’ pattern?\n\nIf patterns are not of fixed calendar-based frequencies, then it is cyclic. Because, unlike seasonality, cyclic effects are typically influenced by the business and other socio-economic factors.\n\n![Time series components](/images/time-series-vectors-4.png)\n<small>A number of components can be extracted from a time series: Seasonality, Trend, Cycle and Error (Irregular). Source: [QuantDare](https://quantdare.com/decomposition-to-improve-time-series-prediction/)</small>\n\n## Forecasting\n\nWhat if besides analyzing a time series, we could predict it? Forecasting is the process of predicting future behaviors based on current and past data.\n\nA time series represents the relationship between two variables: time is one of them, and the measured value is the second one. From a statistical point of view, we can think of the value we want to forecast as a “random variable”. \n\nA random variable is a variable that is subject to random variations so it can take on multiple different values, each with an associated probability.\n\nA random variable doesn’t have a specific value, but rather a collection of potential values. After a measurement is taken and the specific value is revealed, then the random variable ceases to be a random variable and becomes data.\n\nThe set of values that this random variable could take, along with their relative probabilities, is known as the “probability distribution”. When forecasting, we call this the [forecast distribution](https://otexts.com/fpp2/perspective.html). This way, when referring to the “forecast,” we usually mean the average value of the forecast distribution.\n\n![Time series forecast](/images/time-series-vectors-5.png)\n<small>A forecast example: the black line after the real data (in green) represents the forecasted value, and the shaded grey area the confidence interval of the forecast. Source: [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2020/07/time-series-forecasting-using-microsoft-power-bi/)</small>\n\nThe example on the image above highlights the importance of considering uncertainty. Can we assure how the future will unfold? Of course not, and for this reason it’s important to define prediction intervals (lower and upper boundaries) in which the forecast value is expected to fall.\n\nStatistical time series methods have dominated the forecasting landscape because they are heavily studied and understood, robust, and effective on many problems. Some popular examples of these are **ARIMA** (autoregressive integrated moving average), **Exponential Smoothing** methods such as Holt-Winters, and **Theta**.\n\nHowever, recent impressive results of Machine Learning methods on time series forecasting tasks triggered a big shift towards these types of models. The year 2018 marked a crucial year when the [M4 forecasting competition](https://mofc.unic.ac.cy/m4/) was won for the first time with a model using Machine Learning techniques. Using this kind of approach, models are able to extract patterns not just from a single time series, but from collections of them. Machine Learning models like Artificial Neural Networks can ingest multiple time series and produce tremendous performances. Nevertheless, these models are “black boxes” that become challenging when interpretability is required.\n\nDecomposing a time series into its different elements allows us to perform unbiased forecasting and brings insights into what might happen in the future. Forecasting is a key activity through different industries and sectors, and those who get it right have a competitive advantage over those who don’t.\n\nBy now it becomes clear that we need more than standard techniques to deal with time series. **Time series embeddings** represent a novel way to uncover insights and perform Machine Learning tasks.\n\n## Embeddings to the Rescue\n\nDistance measurement between data examples is a key component of many classification, regression, clustering, and anomaly detection algorithms for time series. For this reason, it’s critical to develop time series representations that can be used to improve the results over these tasks.\n\n**Time series embeddings are a representation of time data in the form of vector embeddings** that can be used by different models, improving their performance. Vector embeddings are well known and pretty successful in domains like Natural Language Processing and Graphs, but uncommon within time series. Why? Because time series can be challenging to vectorize. Sequential data elude a straightforward definition of similarity because of the necessity of alignment between examples, but [time series similarity is also dependent on the task at hand](https://dspace.mit.edu/bitstream/handle/1721.1/119575/1076345253-MIT.pdf?sequence=1), further complicating the matter.\n\nFortunately, there are methods that make time-series vectorization a straightforward process. For example, [Time2Vec](https://www.arxiv-vanity.com/papers/1907.05321/) serves as a vector representation for time series that can be used by many models and Artificial Neural Networks architectures like Long Short Term Memory (LSTM), which excel at time series challenges. Time2Vec can be reproduced and used [with Python](https://ojus1.github.io/posts/time2vec/).\n\n![Time series embedding with time2vec](/images/time-series-vectors-6.png)\n<small>The red dots on the figure represent multiples of 7. In this example, it can be observed that Time2Vec successfully learns the correct period of the time series and oscillates every 7 days. The phase-shifts have been learned in a way that all multiples of 7 are placed on the positive peaks of the signal to facilitate separating them from the other days. Source: [DeepAI](https://deepai.org/publication/time2vec-learning-a-vector-representation-of-time)</small>\n\nOnce your time series are vectorized, you can use [Pinecone](/) to store and search for them in an easy-to-use and efficient environment. Check out [this example](https://www.pinecone.io/docs/examples/time-series/) showing how to perform time-series “pattern” matching to find out the most similar stock trends.\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbaf"
  },
  "filename": "pinecone-v2.md",
  "title": "post",
  "category": "\"Pinecone 2.0",
  "content": "---\nlayout: post\ntitle: \"Pinecone 2.0: Take Vector Search from the Lab to Production\"\nheadline: \"Pinecone 2.0: Take Vector Search from the Lab to Production\"\ncategories:\n  - Company News\ntoc: >-\nauthor:\n  name: Edo Liberty\n  position: Founder and CEO\n  src: /images/company-edo.png\n  href: https://edoliberty.github.io/\ndate: \"2021-09-14\"\n# Date: September 14, 2021\n#Open Graph\ndescription: Pinecone 2.0 helps companies move vector similarity search from R&D labs to production applications.\n# No image in article, default will be used\nthumbnail: \"/images/vector-search-thumbnail.jpg\"\n---\n\nPinecone 2.0 helps companies move [vector similarity search](/learn/what-is-similarity-search/) from R&D labs to production applications. The fully managed vector database now comes with metadata filtering for **greater control over search results** and **hybrid storage for up to 10x lower costs**.\n\nThis update also includes a new REST API for ease of use, a completely new architecture for maximum reliability and availability, and a completed SOC2 Type II audit for enterprise-grade security.\n\n## Single-Stage Filtering\n\nStore metadata with your vector embeddings, and limit the vector similarity search to embeddings that meet your metadata filters.\n\nIn many cases, you want to combine a vector similarity search with some arbitrary filter to provide more relevant results. For example, doing a semantic search on a corpus of documents but only from certain categories, or excluding certain authors.\n\nIn the past, you had two options: The first was pre-filtering, which first filters records by metadata and then must use an inefficient brute-force search through the remaining vectors. The second was post-filtering, where you would first retrieve a large set of nearest neighbors and then apply metadata filters on the results. In that case there is a high latency penalty for retrieving more items than needed, and there is no guarantee the result set would include all the items you actually want.\n\nFor the many companies that require filtering in their search, there was no good option. It’s no wonder vector search has been stuck in R&D labs.\n\nThe metadata filtering introduced in Pinecone v2.0 provides the fine-grained control over vector search results that many search and recommendation applications require, at the ultra-low latencies their users expect. Get the power of vector search with the control of traditional search. It accepts arbitrary filters on metadata and retrieves exactly the number of nearest-neighbor results that match the filters. For most cases the search latency will be even lower than unfiltered searches.\n\nFor example, suppose you want to search through vector embeddings of documents (i.e., semantic search), but only want to include documents labeled as “finance” from this year. You can add the metadata to those document embeddings within Pinecone, and then filter for those criteria when sending the query. Pinecone will search for similar vector embeddings only among those items that match the filter.\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/r5CsJ_S9_w4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n[See documentation for metadata filtering.](/docs/metadata-filtering/)\n\n## Hybrid Storage\n\nVector searches typically run completely in-memory (RAM). For many companies with over a billion items in their catalog, the memory costs alone could make vector search too expensive to consider. Some vector search libraries have the option to store everything on disk, but this could come at the expense of search latencies becoming unacceptably high.\n\nPinecone 2.0 introduces a hybrid configuration, in which a compressed [vector index](/learn/vector-indexes/) is stored in memory and the original, full-resolution vector index is stored on disk. The in-memory index is used to locate a small set of candidates to search within the full index on disk. This method provides the same fast and accurate search results yet **cuts infrastructure costs by up to 10x**.\n\n## Other Updates\n### New Architecture\n\nPinecone now provides fault tolerance, data persistence, and high availability for customers with billions of items and many thousands of operations per second.\n\nBefore, enterprises with strict reliability requirements either had to build and maintain complex infrastructure around vector search libraries to meet those requirements or relax their standards and risk downgraded performance for their users.\n\nNow, the Pinecone platform has been re-architected to use Kafka ingestion and Kubernetes orchestration, in a cloud-native paradigm which separates the read and write paths and disassociates storage and compute. This makes Pinecone’s vector database as reliable, flexible, and performant as top-tier enterprise-grade cloud databases.\n\n\n### REST API and Python Client\n\nPinecone now uses a new [REST API](/docs/api/overview/) based on the OpenAPI spec. This makes Pinecone more flexible and even easier to use for developers from any system and in any language.\n\nUpsert and query vectors using HTTPS and JSON without the need to install anything. The REST API gives you maximum flexibility to use the Pinecone service from any environment that can make HTTPS calls. No need to be familiar with Python.\n\nFor users who prefer Python, the Python client has been rebuilt to use the new API and to use fewer dependencies. Clients for Go and Java are coming soon.\n\nThis update also comes with a completely revamped [documentation portal](/docs/) to make developing with Pinecone even easier.\n\n### SOC2\n\nPinecone is now SOC2 Type II audited, with certification expected soon. Enterprises with even the strictest security requirements can deploy Pinecone to production with confidence and assurance that their data is safe.\n\n[Learn how we keep your data secure](/security/), such as regularly performing third-party penetration tests, keeping data in isolated containers, encryption, and more.\n\n---\n\nWhether you’re already experimenting with vector search or just learning about it, Pinecone 2.0 makes it quicker, easier, and more cost-effective to bring vector search into production applications than ever before.\n\nPinecone 2.0 is available now by request. [Contact us](/contact/) with questions or [request a free trial](/start/) today. It will be generally available to all users within a few weeks. Learn all about the new features and ask us questions in a live webinar, [Introduction to Pinecone 2.0](https://pinecone-io.zoom.us/webinar/register/WN_IIs_xe7NTV2fC5t1nbCUXg).\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbb0"
  },
  "filename": "multilingual-transformers.md",
  "title": "ebook-post",
  "category": "\"Multilingual Sentence Transformers\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Multilingual Sentence Transformers\"\nheadline: \"Tomayto, Tomahto, Transformer: Multilingual Sentence Transformers\"\ncategories:\n  - NLP for Semantic Search\ntoc: >-\nweight: 5\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: How to create multilingual sentence transformers with knowledge distillation.\n#Open Graph\nimages: ['/images/multilingual-transformers-4.jpg']\n---\n\nWe've learned about how [sentence transformers](/learn/sentence-embeddings/) can be used to create high-quality [vector representations](/learn/dense-vector-embeddings-nlp/) of text. We can then use these vectors to find similar vectors, which can be used for many applications such as semantic search or topic modeling.\n\nThese models are *very* good at producing meaningful, information-dense vectors. But they don't allow us to compare sentences across different languages.\n\nOften this may not be a problem. However, the world is becoming increasingly interconnected, and many companies span across multiple borders and languages. Naturally, there is a need for sentence vectors that are language agnostic.\n\nUnfortunately, very few textual similarity datasets span multiple languages, particularly for less common languages. And the standard training methods used for sentence transformers would require these types of datasets.\n\nDifferent approaches need to be used. Fortunately, some techniques allow us to extend models to other languages using more easily obtained language translations.\n\nIn this article, we will cover how multilingual models work and are built. We'll learn how to develop our own multilingual sentence transformers, the datasets to look for, and how to use high-performing pretrained multilingual models.\n\n<p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/NNS5pOpjvAQ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>\n\n## Multilingual Models\n\nBy using multilingual sentence transformers, we can map similar sentences from different languages to similar vector spaces.\n\nIf we took the sentence `\"I love plants\"` and the Italian equivalent `\"amo le piante\"`, the ideal multilingual sentence transformer would view both of these as exactly the same.\n\n![Multilingual language vectors represented in a 3D space](/images/multilingual-transformers-1.jpg)\n<small>A multilingual model will map sentences from different languages into the same vector space. Similar sentences to similar vectors, creating ‘language-agnostic’ vectors (as highlighted).</small>\n\nThe model should identify `\"mi piacciono le piante\"` (*I like plants*) as more similar to `\"I love plants\"` than `\"ho un cane arancione\"` (*I have an orange dog*).\n\nWhy would we need a model like this? For any scenario we might find usual sentence transformers applied; identifying similar documents, finding plagiarism, topic modeling, and so on. But now, used across borders or extended to previously inaccessible populations.\n\nThe lack of suitable datasets means that many languages have limited access to language models. By starting with existing, high-performance monolingual models trained in high resource languages (such as English), we can use multilingual training techniques to extend the performance of these models to other languages using significantly less data.\n\n\n\n## Training approaches\n\nTypical training methods for sentence transformer models use some sort of contrastive training function. Given a high similarity sentence pair, models are optimized to produce high similarity sentence vectors.\n\nTraining data for this is not hard to come by as long as you stick to common languages, mainly English. But it can be hard to find data like this in other languages.\n\nBoth examples below rely *in-part or in full* on having translation pairs rather than similarity pairs, which are easier to find. There are *many* materials in the world that have been translated, but far fewer that compare similar same-language sentences.\n\n\n\n### Translation-based Bridge Tasks\n\nUsing a multi-task training setup, we train on two alternate datasets:\n\n1. An English dataset containing question-answer or anchor-positive) pairs (anchor-positive meaning two high-similarity sentences).\n2. *Parallel data* containing cross-language pairs (English_sentence, German_sentence).\n\nThe idea here is that the model learns monolingual sentence-pair relationships via a (more common) source language dataset. Then learns how to translate that knowledge into a multilingual scope using *parallel data* [2].\n\nThis approach works, but we have chosen to focus on the *next* multilingual training approach for a few reasons:\n\n* The amount of training data required is high. The multilingual universal sentence encoder (mUSE) model was trained on over a billion sentence pairs [3].\n* It uses a multi-task dual-encoder architecture. Optimizing two models in parallel is harder as both training tasks must be balanced (optimizing one is hard enough…).\n* Results can be mediocre without the use of hard negatives [1]. Hard negatives are sentences that *seem similar* (often on a related topic) but are irrelevant/or contradict the *anchor* sentence. Because they're *harder* for a model to identify as dissimilar, by training on these, the model becomes better.\n\nLet’s move on to our preferred approach and the focus of the remainder of the article.\n\n\n\n### Multilingual Knowledge Distillation\n\nAnother approach is to use **multilingual knowledge distillation** — a more recent method introduced by Nils Reimers and Iryna Gurevych in 2020 [1]. With this, we use two models during fine-tuning, the *teacher* and *student* models.\n\nThe teacher model is an already fine-tuned sentence transformer used for creating embeddings in a single language (most likely English). The student model is a transformer that has been *pretrained* on a multilingual corpus.\n\n---\n\n*There are two stages to training a transformer model. Pretraining refers to the initial training of the core model using techniques such as masked-language modeling (MLM), producing a 'language engine'. Fine-tuning comes after — where the core model is trained for a specific task like semantic similarity, Q&A, or classification.*\n\n*However, it is also common to refer to previously fine-tuned models as pretrained.*\n\n---\n\nWe then need a parallel dataset (translation pairs) containing translations of our sentences. These translation pairs are fed into the teacher and student models.\n\n![Fine-tuning process with multilingual knowledge distillation](/images/multilingual-transformers-2.jpg)\n<small>Chart showing the flow of information from parallel pairs through the teacher and student models and the optimization performed using MSE loss. Adapted from [1].</small>\n\nLet's assume we have English-Italian pairs. The English sentence is fed into our teacher and student models, producing two English sentence vectors. Then we feed the Italian sentence into the student model. We calculate the mean squared error (MSE) loss between the one teacher vector and the two student vectors. The student model is optimized using this loss.\n\nThe student model will learn to mimic the monolingual teacher model — but for multiple languages.\n\nUsing multilingual knowledge distillation is an excellent way to extend language options using already trained models. It requires much less data than training from scratch, and the data it uses is widely available — translated pairs of sentences.\n\n\n\n## Fine-tuning with Multilingual Sentence Transformers\n\nThe final question is, how do we build one of these models? We covered multilingual knowledge distillation *conceptually*, but translating concepts into code is never as straightforward as it seems.\n\nLuckily for us, the `sentence-transformers` library makes this process *much* easier. Let's see how we can use the library to build our very own multilingual models.\n\n\n\n### Data Preparation\n\nAs always, we start with data. We need a data source that contains multilingual pairs, split into our *source* language and *target* language(s).\n\nNote that we wrote language(s) — we can fine-tune a model on *many* languages. In fact, some of the multilingual models in [sentence-transformers](https://sbert.net/docs/pretrained_models.html#multi-lingual-models) support more than *50* languages. All of these are trained with multilingual knowledge distillation.\n\nIn the paper from Reimers and Gurevych, one dataset uses translated subtitles from thousands of TED talks. These subtitles also cover a wide range of languages (as we will see). We can access a similar dataset using HF `datasets`.\n\n{{< notebook file=\"load-dataset\" height=\"full\" >}}\n\nThis dataset contains a list of language labels, the translated sentences, and the talk they came from. We only really care about the labels and sentences.\n\n{{< notebook file=\"ted-out\" height=\"full\" >}}\n\nWe need to transform this dataset into a friendlier format. The data we feed into training will consist of nothing more than pairs of *source* sentences and their respective *translations*.\n\nTo create this format, we need to use the language labels to (1) identify the position of our *source* sentence and (2) extract translations of languages we want to fine-tune on. Which will look something like this:\n\n{{< notebook file=\"transform-example\" height=\"full\" >}}\n\nHere we returned *27* pairs from a single row of data. We don't *have* to limit the languages we fine-tune on. Still, unless you plan on using and evaluating every possible language, it's likely a good idea to restrict the range.\n\nWe will use English `en` as our source language. For target languages, we will use Italian `it`, Spanish `es`, Arabic `ar`, French `fr`, and German `de`. *These are ISO language codes, which you can find [here](http://www.mathguide.de/info/tools/languagecode.html)*.\n\nLater we will be using a `ParallelSentencesDataset` class, which expects our pairs to be separated by a tab character `\\t`, and each language pair in a different dataset — so we add that in too.\n\n{{< notebook file=\"pair-build\" height=\"full\" >}}\n\nHopefully, all TED talk subtitles end with `'( Applause )'`. With that, let's save our training data to file ready for the `ParallelSentencesDataset` class to pick it up again later.\n\n{{< notebook file=\"save-pairs\" height=\"full\" >}}\n\nThat's it for data preparation. Now let's move on to set everything up for fine-tuning.\n\n\n\n### Set-up and Training\n\nBefore training, we need *four* things:\n\n* Our `teacher` model.\n* The new `student` model.\n* A loaded `DataLoader` to feed the *(source, translation)* pairs into our model during training.\n* The loss function.\n\nLet's start with our *teacher* and *student* models.\n\n#### Model Selection\n\nWe already know we need a *teacher* and a *student*, but how do we choose a *teacher* and *student*? Well, the *teacher* must be a competent model in producing sentence embeddings, just as we'd like our teachers to be competent in the topic they are teaching us.\n\nThe ideal student can take what the teacher teaches and extend that knowledge beyond the teacher's capabilities. We want the same from our student model. That means that it must be capable of functioning with different languages.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/multilingual-transformers-3.mp4\" type=\"video/mp4\">\n</video>\n\n<small class=\"video\">The knowledge of a monolingual teacher capable of performing in a single language is distilled to a new model capable of working across multiple languages.</small>\n\nNot all models can do this, and of the models that can — some are better than others.\n\nThe first check for a capable student model is its tokenizer. Can the student's tokenizer deal with a variety of languages?\n\nBERT uses a WordPiece tokenizer. That means that it encodes either word-level or sub-word-level chunks of text. The vocabulary of a pretrained BERT tokenizer is already set and limited to (mostly) English tokens. If we begin introducing unrecognizable words/word pieces, the tokenizer will convert them into 'unknown' tokens or small character sequences.\n\nWhen BERT sees the occasional unknown token, it's not a problem. But if we feed many unknowns to BERT — it becomes unmanageable. If I gave you the sentence:\n\n```\nI went to the [UNK] today to buy some milk.\n```\n\nYou could probably fill in the 'unknown' `[UNK]` with an accurate guess of 'shop' or 'store'. What if I gave you:\n\n```\n[UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n```\n\nCan you fill in the blanks? In this sentence, I said *\"I went for a walk in the forest yesterday\"* — if you guessed correct, well done! If not, well, that's to be expected.\n\nBERT works in the same way. It can fill in the occasional blank, but too many, and the task is unsolvable. Let's take a look at how BERTs tokenizer copes with different languages.\n\n{{< notebook file=\"bert-tokenize\" height=\"full\" >}}\n\nThe tokenizer misses most of our Chinese text and all of the Georgian text. Greek is split into character-level tokens, limiting the length of input sequences to just 512 characters. Additionally, character-level tokens carry limited meaning.\n\nA BERT tokenizer is therefore not ideal. There is another transformer model built for multilingual comprehension called XLM-RoBERTa (XLMR). \n\nXLMR uses a *SentencePiece*-based tokenizer with a vocabulary of 250K tokens. This means XLMR already *knows* many more words/characters than BERT. *SentencePiece* also handles new languages much better thanks to language-agnostic preprocessing (it treats all sentences as sequences of Unicode characters) [4].\n\n{{< notebook file=\"xlmr-tokenizer\" height=\"full\" >}}\n\nWe can see straight away that our XLMR tokenizer handles these other languages *much* better. Naturally, we'll use XLMR as our student.\n\nThe student model will be initialized from Hugging Face's transformers. It has *not* been fine-tuned to produce sentence vectors, and we need to initialize a *mean pooling* to convert the 512 token vectors into a single sentence vector.\n\nTo put these two components together, we will use `sentence-transformers` transformer and pooling modules.\n\n{{< notebook file=\"student\" height=\"full\" >}}\n\nThat's our student. Our teacher must be an already fine-tuned monolingual sentence transformer model. We could try the `all-mpnet-base-v2` model:\n\n{{< notebook file=\"teacher-all\" height=\"full\" >}}\n\nBut here, there is a final *normalization* layer. We need to avoid outputting normalized embeddings for our student to mimic. So we either remove that normalization layer or use a model without it. The `paraphrase` models do *not* use normalization. We'll use one of those.\n\n{{< notebook file=\"teacher-paraphrase\" height=\"full\" >}}\n\nAnd with that, we're ready to set everything up for fine-tuning.\n\n\n#### Fine-Tuning\n\nFor fine-tuning, we now need to initialize our data loader and loss function. Starting with the data loader, we first need to initialize a `ParallelSentencesDataset` object.\n\n{{< notebook file=\"dataset-object\" height=\"full\" >}}\n\nAnd once we have initialized the dataset object, we load in our data.\n\n{{< notebook file=\"load-data\" height=\"full\" >}}\n\nWith our dataset ready, all we do is pass it to a PyTorch data loader.\n\n{{< notebook file=\"dataloader\" height=\"full\" >}}\n\nThe final thing we need for fine-tuning is our loss function. As we saw before, we will be calculating the MSE loss, which we initialize like so:\n\n{{< notebook file=\"mseloss\" height=\"full\" >}}\n\nIt's that simple! Now we're onto the fine-tuning itself. As usual with `sentence-transformers` we call the `.fit` method on our student model.\n\n{{< notebook file=\"model-fit\" height=\"full\" >}}\n\nAnd we wait. Once fine-tuning is complete, we find the new model in the `./xlmr-ted` directory. The model can be loaded using the `SentenceTransformer` class as we would any other sentence transformer.\n\nIt would be helpful to understand how our model is performing, so let's take a look at model evaluation.\n\n\n\n### Evaluation\n\nTo evaluate our model, we need a multilingual textual similarity dataset. That is a dataset containing multilingual pairs and their respective similarity scores. A great one is the Sentence Textual Similarity benchmark (STSb) multilingual dataset.\n\nWe can find this dataset on HF `datasets`, named `stsb_multi_mt`. It includes *a lot* of different languages, but we will stick to evaluating two, English and Italian. First, we download both of those.\n\n{{< notebook file=\"load-data-eval\" height=\"full\" >}}\n\nEach row of the different language sets aligns with the same row in the other language sets. Meaning *sentence1* in row *0* of the English dataset is translated to *sentence1* in row *0* of the Italian dataset.\n\n{{< notebook file=\"eval-align\" height=\"full\" >}}\n\nHere the Italian dataset *sentence1* means *'A girl is styling her hair'*. This alignment also applies to *sentence2* and the *similarity_score*.\n\nOne thing we do need to change in this dataset is the *similarity_score*. When we calculate the *positive* cosine similarity between sentence vectors, we will output a zero (no similarity) to one (exact matches) value. The *similarity_score* varies between zero to five. We must normalize this to bring it within the correct range.\n\n{{< notebook file=\"norm\" height=\"full\" >}}\n\nBefore feeding our data into a similarity evaluator, we need to reformat it to use an `InputExample` format. While we do this, we will also merge English and Italian sets to create a new English-Italian dataset for evaluation.\n\n{{< notebook file=\"input-example\" height=\"full\" >}}\n\nWe can use an `EmbeddingSimilarityEvaluator` class to evaluate the performance of our model. First, we need to initialize one of these evaluators for each of our sets.\n\n{{< notebook file=\"eval-init\" height=\"full\" >}}\n\nAnd with that, we just pass our student model through each evaluator to return its performance.\n\n{{< notebook file=\"trained-eval\" height=\"full\" >}}\n\nThat looks pretty good. Let's see how it compares to our untrained student.\n\n{{< notebook file=\"untrained-eval\" height=\"full\" >}}\n\nSome really great results. We can now take the new model and use it with  English `en`, Italian `it`, Spanish `es`, Arabic `ar`, French `fr`, and German `de`.\n\n\n\n### Sentence Transformer Models\n\nFortunately, we rarely need to fine-tune our own model. We can load many high-performing multilingual models as quickly as we initialized our teacher model earlier.\n\nWe can find a list of these multilingual models on the [Pretrained Models](https://sbert.net/docs/pretrained_models.html#multi-lingual-models) page of the `sentence-transformers` docs. A few of which support more than 50 languages.\n\nTo initialize one of these, all we need is:\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n```\n\nAnd that's it, encode your sentences with `model.encode`, and you're good to go.\n\n---\n\nThat's all for this article on multilingual sentence transformers. We've taken a look at the two most common approaches taken to train multilingual sentence transformers; multi-task translation-based bridging and multilingual knowledge distillation.\n\nFrom there, we dived into the tune-tuning process of a multilingual model using multilingual knowledge distillation, covering the required data, loss functions, fine-tuning, and evaluation.\n\nWe've also looked at how to use the existing pretrained multilingual sentence transformers.\n\n## References\n\n[1] N. Reimers, I. Gurevych, [Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation](https://arxiv.org/abs/2004.09813#) (2020), EMNLP\n\n[2] M. Chidambaram, [Learning Cross-Lingual Sentence Representations vis a Multi-task Dual-Encoder Model](https://arxiv.org/abs/1810.12836) (2019), RepL4NLP\n\n[3] Y. Yang, et al., [Multilingual Universal Sentence Encoder for Semantic Retrieval](https://arxiv.org/abs/1907.04307) (2020), ACL\n\n[4] Google, [SentencePiece Repo](https://github.com/google/sentencepiece), GitHub\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbb1"
  },
  "filename": "nyckel-ml-automation.md",
  "title": "ebook-post",
  "category": "How Nyckel Built An API for Semantic Image Search",
  "content": "---\nlayout: ebook-post\ntitle: How Nyckel Built An API for Semantic Image Search\nheadline: How Nyckel Built An API for Semantic Image Search\ncategories:\n  - Vector Search in the Wild\ntoc: >-\nweight: 2\nauthor:\n  name: George Mathew\n  position: Founder, Nyckel\n  src: /images/george-mathew.png\n  href: \"https://www.linkedin.com/in/georgemathew\"\n# Open graph\ndescription: How Nyckel automates ML workflows for image and text classification, object detection, and visual search with the help of vector search.\nimages: [\"/images/nyckel-1.jpg\"]\n---\n\n*Written by [George Mathew](https://www.linkedin.com/in/georgemathew), Co-Founder at Nyckel.*\n\nBusinesses that accumulate large amounts of data eventually run into problems with information retrieval – especially if they’re working with image data. While information retrieval for text is mostly a solved problem, the same cannot be said for images. Recent advances in deep neural networks and in vector search databases make it possible to search through large sets of images. Nyckel implements a [simple API](https://www.nyckel.com/semantic-image-search) for semantic image search – let’s look deeper at how it is implemented. \n\n## What is semantic image search?\n\nSemantic search is the ability to search based on user intent and on the contextual meaning of both the search query and the target content. For images, this means understanding the meaning of search queries (whether in the form of text or images), and then mapping them to images in the search set. This is exactly what Google’s image search engine does. I could use a picture of my dog catching a frisbee on the beach, paste it into Google reverse image search, and it would retrieve public web images that are similar to my picture. Alternatively, I could use the more common means of Google searching by typing in “dog catching a frisbee on the beach”, and Google will retrieve public web images that match those search terms. The latter search case is called a cross-modal search because the search term and the result item are in different modalities: in this case, text and image respectively.\n\n![Dog catching a frisbee](/images/nyckel-1.jpg)\n\n## Applications of Image Search\n\nAs a platform, we are constantly surprised and encouraged by the variety of use-cases we see. Here are a few examples of applications that our customers are using our image search API for:\n\n- Detecting the unauthorized use of images, e.g., copyright infringement or fraud;\n- De-duplicating images in a collection when the duplicates are not exact matches;\n- Image classification into hundreds of thousands of classes where traditional classification mechanisms are difficult;\n- Enabling search functionality over their images for their end-customers or employees. To showcase this, we built a [demo app](https://www.nyckel.com/nft-finder/) to search over the wild world of NFT images:\n\n![NFT image search](/images/nyckel-2.png)\n\n## Nyckel's Image Search Service\n\nNyckel is the lightning fast ML-platform for non-experts. We make ML functionality, such as [image](https://www.nyckel.com/docs/image-classification-quickstart) / [text](https://www.nyckel.com/docs/quickstart) classification, [object detection](https://www.nyckel.com/docs/detection-quickstart), and [image search](https://www.nyckel.com/docs/image-search-quickstart), accessible to developers and companies without any ML expertise. \n\nFor image search, our customers don’t want to deal with model selection, tuning, deployment, and infrastructure management and scaling. They use the following simple API, and we take care of the rest:\n\n![Image search API](/images/nyckel-3.jpg)\n\n## What goes into implementing the API?\n\nSearching within any medium requires two capabilities:\n\n- Extracting semantic information from the data – both the search set and query data;\n- Indexing the semantic information for fast and accurate retrieval.\n\n### Extracting Semantic Information from Images\n\nNeural networks, such as Resnet, are good at extracting a semantic representation of an image in the form of “embeddings”: fixed-length vectors of floating point numbers. Once you have a set of images that have been mapped in this way, then you can use your vector database to index the vector embeddings. If two images have similar embeddings, meaning that they are close to one another in vector space, then they are semantically similar. Conversely, the embeddings for two dissimilar images will be far apart in vector space. You can read more about embeddings and how useful they are in [Peter Gao’s blog post](https://medium.com/aquarium-learning/the-unreasonable-effectiveness-of-neural-network-embeddings-93891acad097).\n\nBut what about cross-modal semantic search, for example when queries for image data come in the form of textual descriptions? What do we do in these cases? We could use a natural language model like BERT to get text embeddings, but these embeddings will be unrelated to the image embeddings. What this means is that the embedding for \"dog catching a frisbee on the beach\" is not guaranteed to be close to the embedding for an image of a dog catching a frisbee on the beach. In the next section we'll talk about how to solve this problem.\n\n### Cross-Modal Embeddings\n\nIn order to support searching for images with a text query, we need a neural network that can provide embeddings for images and text in the same \"embedding space\". OpenAI's [CLIP](https://openai.com/blog/clip/) is one such network. It’s actually two networks: one for images and one for text, trained together to minimize \"[contrastive loss](https://towardsdatascience.com/contrastive-loss-explaned-159f2d4a87ec)\". CLIP is trained on a large corpus of (image, text) pairs from the internet. With the idea that (image, text) pairs that accompany each other are likely to be semantically similar, the training objective was to minimize the distance between vector embeddings for pairs that accompany each other and maximize the distance between embeddings for unrelated pairs. You can read more about CLIP [here](/learn/clip/).\n\nThe result is that CLIP’s embeddings for an image of a dog catching a frisbee on the beach, and the text “dog catching frisbee on beach” will be close together. This gives us the embeddings we need for cross-modal search. \n\n### Indexing\n\nOnce you have embeddings, searching for images becomes a problem of finding the closest vectors to the embedding for that particular query. Finding the closest vectors is a computationally challenging problem for large numbers of high-dimensional vectors. But there are relatively efficient mechanisms for finding approximate closest matches (commonly known as approximate nearest neighbors, or ANN) and several vector-database products that offer this capability. We evaluated several vector databases and picked pinecone because they are a fully managed service with low ingest latency, low query latency, on-demand and scalable provisioning and pricing, and an easy-to-use API. \n\n## Putting it together\n\nNow that we've talked about all the pieces involved in semantic image search, let's look at how Nyckel puts them together for its image search service. The sequence diagram below ignores some of the complexities and asynchronous processing, but provides a general overview of how things work:\n\n![Nyckel's image search service](/images/nyckel-4.png)\n\n## Future work\n\nWhat we’ve described above is just the beginning and we have an exciting roadmap for image search. Here are a couple of items items on our to-do list:\n\n- Support multilingual text queries. CLIP currently only does well with English queries, and we want to support many more languages;\n- Model selection and fine-tuning for specialized use-cases based on search feedback. CLIP does surprisingly well for a wide range of subject matter, but in cases where the set of images is in a very narrow or specialized domain, model fine-tuning will help provide significantly better search results. \n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbb2"
  },
  "filename": "vision-transformers.md",
  "title": "ebook-post",
  "category": "\"Vision Transformers (ViT) Explained\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Vision Transformers (ViT) Explained\"\nheadline: \"Vision Transformers (ViT) Explained\"\ncategories:\n  - Embedding Methods for Image Search\ntoc: >-\nweight: 5\nauthors:\n  - name: James Briggs\n    position: Developer Advocate\n    src: /images/james-briggs.jpeg\n    href: \"https://www.youtube.com/c/jamesbriggs\"\n  - name: Laura Carnevali\n    position: Developer\n    src: /images/laura-carnevali.jpeg\n    href: \"https://www.linkedin.com/in/laura-carnevali-071a01b7/\"\n\ndescription: A deep dive into the Vision Transformer (ViT) and practical implementation\n# Open graph\nimages: ['https://www.pinecone.io/images/vision-transformer-0.png']\n---\n\n**[Pinecone](/) lets you implement semantic, audio, or visual search into your applications using vector search. But first you need to convert your data into vector embeddings, and vision transformers do that for images. This article introduces vision transformers, how they work, and how to use them.**\n\nVision and language are the two big domains in machine learning. Two distinct disciplines with their own problems, best practices, and model architectures. At least, that *was* the case.\n\nThe **Vi**sion **T**ransformer (ViT)<sup>[1]</sup> marks the first step towards the merger of these two fields into a single unified discipline. For the first time in the history of ML, a single model architecture has come to dominate both language *and vision*.\n\nBefore ViT, transformers were *\"those language models\"* and nothing more. Since then, ViT and further work has solidified them as a likely contender for the architecture that merges the two disciplines.\n\nThis article will dive into ViT, explaining and visualizing the intuition behind how and why it works. Later, we'll look at how to implement it ourselves.\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n   <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/qU7wO02urYU\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n---\n\n# Transformers\n\nTransformers were introduced in 2017 by Vaswani et al. in the now-famous paper *\"Attention is All You Need\"* <sup>[2]</sup>. The primary function powering these models is the *attention mechanism*.\n\n## Attention 101\n\nIn NLP, attention allows us to consider the context of words and *focus attention* on the key relationships between different tokens (represented as word or sub-word tokens).\n\nIt works by comparing \"token embeddings\" and calculating an *\"alignment\"* score that describes how similar two tokens are based on their semantic and contextual meaning.\n\nIn the layers preceding the attention layer, each word embedding is encoded into a *\"vector space\"*.\n\nIn this vector space, similar tokens share a similar location. Therefore, when we calculate the dot product between token embeddings (inside the attention mechanism), we return a high *alignment* score when embeddings are aligned in vector space. When embeddings are *not aligned*, we produce a low alignment score.\n\n<img src=\"./images/vision-transformers-1.png\" alt=\"alignment of vectors\" style=\"width:60%;\" center />\n<small>Alignment between vectors is higher where vectors share similar direction and magnitude.</small>\n\nBefore applying attention, our tokens' initial positions are based purely on a \"general meaning\" of a particular word or sub-word token.\n\nAs we go through several encoder blocks (these include the attention mechanism), the position of these embeddings is updated to better reflect the meaning of a token *with respect* to its context. The context being all of the other words within that specific sentence.\n\nSo, given three phrases:\n\n* A plane **banks**\n* The grassy **bank**\n* The **Bank** of England\n\nThe initial embedding for the token *bank* is equal. Yet, the token is pushed towards its context-based meaning through many attention encoder blocks. These blocks might push *bank* towards tokens like [plane, airport, flight], [nature, fields, outdoors], or [finance, England, money].\n\n![contextual-meaning-of-embeddings](./images/vision-transformers-10.png)\n<small>An encoder with attention layers can add contextual meaning to embeddings.</small>\n\nAttention has been used in **C**onvolutional **N**eural **N**etworks (CNNs) over the years. Generally speaking, this has been shown to produce *some benefit* but is often computationally limited.\n\nAttention is a heavy operation and does not scale to large sequences. Therefore, attention can *only* be used in later CNN layers — where the number of pixels has been reduced. This limits the potential benefit of attention as it cannot be applied across the complete set of network layers <sup>[1] [3]</sup>.\n\nTransformer models do not have this limitation and instead apply attention over many layers.\n\nBERT, a well-known transformer architecture, uses several \"encoder\" blocks. Each of these blocks consists of normalization layers, *multi-head* attention (i.e., several parallel attention operations) layers, and a multilayer perceptron (MLP) component.\n\nEach of these encoder *\"blocks\"* encodes *more information* into the token (or patch) embeddings using their context. This operation produces a *deeper* semantic representation of each token.\n\nAt the end of this process, we get super information-rich embeddings. These embeddings are the ultimate output of the *core* of a transformer, including ViT.\n\nAnother set of layers is used to transform these rich embeddings into useful predictions. These final few layers are called the *\"head\"* and a different *head* is used for each task, such as for classification, [**N**amed **E**ntity **R**ecognition (NER)](/docs/examples/ner-search/), [question-answering](/learn/question-answering/), etc.\n\n![model-heads](./images/vision-transformers-2.png)\n<small>Example of a transformer encoder building information-rich final embeddings before passing these on to a task-specific *\"head\"*.</small>\n\nViT works similarly, but rather than consuming *word tokens*, ViT consumes *image patches*. The remainder of the transformer functions in the same way.\n\n# Images to Patch Embeddings\n\nThe new procedure introduced by ViT is limited to the first few processing steps. These first steps take us from images to a set of *patch embeddings*.\n\nIf we didn't split the images into patches, we could alternatively feed in pixel values of the image directly. However, this causes problems with the attention mechanism.\n\nAttention requires the comparison of every input to all other inputs. If we perform that on a *224x224* pixel image, we must perform $224^4$ ($2.5E^9$) comparisons. That's for a single attention layer, of which transformers contain several.\n\nDoing this would be a computational nightmare far beyond the capabilities of even the latest GPUs and TPUs within a reasonable timeframe.\n\nTherefore, we create image patches and embed those as patch embeddings. Our high-level process for doing this is as follows:\n\n![model-prep-steps](./images/vision-transformers-3.png)\n\n1. Split the image into image patches.\n2. Process patches through the linear projection layer to get initial patch embeddings.\n3. Preappend trainable *\"class\"* embedding to patch embeddings.\n4. Sum patch embeddings and *learned positional embeddings*.\n\nAfter these steps, we process the patch embeddings like token embeddings in a typical transformer. Let's dive into each of these components in more detail.\n\n## Image Patches\n\nOur first step is the transformation of images into image patches. In NLP, we do the same thing. Images are sentences and patches are word or sub-word tokens.\n\n![nlp-vs-cv-tokens](./images/vision-transformers-4.png)\n<small>NLP transformers and ViT both split larger sequences (sentences or images) into tokens or patches.</small>\n\nRecall that a *224x224* pixel image requires $2.5E9$ comparisons. If, instead, we split a *224x224* pixel image into 256 *14x14* pixel image patches, a single attention layer requires a more manageable $256 * 14^4$ ($9.8e6$) comparisons.\n\n![image-patches](./images/vision-transformers-5.png)\n<small>Conversion of *224x224* pixel image into *256* *14x14* pixel image patches.</small>\n\nThrough this, these image patches act as a form of *much needed* quantization required for effective use of attention.\n\n## Linear Projection\n\nAfter building the image patches, a *linear projection* layer is used to map the image patch *\"arrays\"* to *patch embedding \"vectors\"*.\n\n![linear-projection](./images/vision-transformers-6.png)\n<small>The linear projection layer attempts to transform arrays into vectors while maintaining their \"physical dimensions\". Meaning similar image patches should be mapped to similar patch embeddings.</small>\n\nBy mapping the patches to embeddings, we now have the correct dimensionality for input into the transformer. However, two more steps remain before the embeddings are fully prepared.\n\n## Learnable Embeddings\n\nOne *feature* introduced to transformers with the popular BERT models was the use of a `[CLS]` (or *\"classification\"*) token. The `[CLS]` token was a *\"special token\"* prepended to every sentence fed into BERT<sup>[4]</sup>.\n\n![bert-cls](./images/vision-transformers-7.png)\n<small>The BERT `[CLS]` token is preappended to every sequence.</small>\n\nThis `[CLS]` token is converted into a token embedding and passed through several encoding layers.\n\nTwo things make `[CLS]` embeddings special. First, it does *not* represent an actual token, meaning it begins as a \"blank slate\" for each sentence. Second, the final output from the `[CLS]` embedding is used as the input into a classification head during pretraining.\n\nUsing a \"blank slate\" token as the sole input to a classification head pushes the transformer to learn to encode a *\"general representation\"* of the entire sentence into that embedding. The model *must* do this to enable accurate classifier predictions.\n\nViT applies the same logic by adding a *\"learnable embedding\"*. This learnable embedding is the same as the `[CLS]` token used by BERT.\n\n![model-class-highlighted](./images/vision-transformers-8.png)\n<small>ViT process with the learnable *class embedding* highlight (left).</small>\n\nThe preferred pretraining function of ViT is based solely on classification, unlike BERT, which uses masked language modeling. Based on that, this learning embedding is *even more* important to the successful pretraining of ViT.\n\n## Positional Embeddings\n\nTransformers do *not* have any default mechanism that considers the \"order\" of token or patch embeddings. Yet, *order* is essential. In language, the order of words can completely change their meaning.\n\nThe same is true for images. If given a jumbled jigsaw set, it's hard-to-impossible for a person to accurately predict what the complete puzzle represents. This applies to transformers too. We need a way of enabling the model to infer the *order* or *position* of the puzzle pieces.\n\nWe enable order with *positional embeddings*. For ViT, these positional embeddings are learned vectors with the same dimensionality as our patch embeddings.\n\nAfter creating the patch embeddings and prepending the \"class\" embedding, we sum them all with positional embeddings.\n\nThese positional embeddings are learned during pretraining and (sometimes) during fine-tuning. During training, these embeddings converge into vector spaces where they show *high similarity* to their neighboring position embeddings — particularly those sharing the same column and row:\n\n<img src=\"./images/vision-transformers-9.png\" alt=\"position embedding similarity\" style=\"width:80%;\" center />\n<small>Cosine similarity between trained positional embeddings. Adapted from [1].</small>\n\nAfter adding the positional embeddings, our *patch embeddings* are complete. From here, we pass the embeddings to the ViT model, which processes them as a typical transformer model.\n\n# Implementation\n\n<div class=\"source\">\n  <a href=\"https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/image-retrieval/vision-transformers/vit.ipynb\" class=\"source-link\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>\n</div>\n\nWe've worked through the logic and innovations introduced by ViT. Let's now work through an example of implementing the model. We start by installing all of the libraries that we'll be using:\n\n```\n!pip install datasets transformers torch\n```\n\nWe will fine-tune with a well-known image classification dataset called CIFAR-10. It can be downloaded via Hugging Face's *Datasets* library, and we'll download both the training *and* validation/test datasets.\n\n{{< notebook file=\"vit-datasets\" height=\"full\" >}}\n\nThe training dataset contains 50K images across 10 classes. To find the human-readable class labels, we can do the following:\n\n{{< notebook file=\"vit-class-labels\" height=\"full\" >}}\n\nEvery record in the dataset contains an `img` and `label` feature. The `img` values are all Python PIL objects with *32x32* pixel resolution and three color channels, **r**ed, **g**reen, and **b**lue (RGB).\n\n{{< notebook file=\"vit-single-record\" height=\"full\" >}}\n\n## Feature Extractor\n\nPreceding the ViT model, we use something called a *feature extractor*. The feature extractor is used to *preprocess* images into normalized and resized image *\"pixel_values\"* tensors. We initialize it from the Hugging Face *Transformers* library like so:\n\n{{< notebook file=\"vit-feature-extractor\" height=\"full\" >}}\n\nThe feature extractor configuration shows that normalization and resizing are set to true. Normalization is performed across the three color channels using the mean and standard deviation values stored in `\"image_mean\"` and `\"image_std\"` respectively. The output size is set by `\"size\"` at *224x224* pixels.\n\nTo process an image with the feature extractor, we do the following:\n\n{{< notebook file=\"vit-feature-extractor-process\" height=\"full\" >}}\n\nLater we'll be fine-tuning our ViT model with these tensors. Although fine-tuning is *not* as computationally heavy as pretraining, it still takes time. Therefore we *ideally* should be running everything on GPU rather than CPU. So, we move these tensors to a CUDA-enabled GPU *if* it is available.\n\n```python\nimport torch\n#  if cuda enabled GPU is available, use it\ndevice = torch.device(\n  \t'cuda' if torch.cuda.is_available() else 'cpu'\n)\npatches = patches.to(device)\n```\n\nFortunately, the `Trainer` utility we will use for fine-tuning later *does handle* this move for our data by default. Still, we will need to later repeat this step for the model.\n\nTo apply this preprocessing step across the entire dataset more efficiently, we will package into a function called `preprocess` and apply the transformations using the `with_transform` method, like so:\n\n```python\ndef preprocess(batch):\n    # take a list of PIL images and turn them to pixel values\n    inputs = feature_extractor(\n        batch['img'],\n        return_tensors='pt'\n    )\n    # include the labels\n    inputs['label'] = batch['label']\n    return inputs\n\n# apply to train-test datasets\nprepared_train = dataset_train.with_transform(preprocess)\nprepared_test = dataset_test.with_transform(preprocess)\n```\n\n## Loading ViT\n\nThe next step is downloading and initializing ViT. Again, we're using Hugging Face *Transformers* with the same `from_pretrained` method used to load the feature extractor.\n\n```python\nfrom transformers import ViTForImageClassification\n\nlabels = dataset_train.features['label'].names\n\nmodel = ViTForImageClassification.from_pretrained(\n    model_name_or_path,\n    num_labels=len(labels)  # classification head\n)\n# move to GPU (if available)\nmodel.to(device)\n```\n\nBecause we are fine-tuning ViT for classification, we use the `ViTForImageClassification` class. By default, this will initialize a classification head with just two outputs.\n\nWe have *10* classes in CIFAR-10, so we must specify that we'd like to initialize the head with *10* outputs. We do this via the `num_labels` parameter.\n\nNow we're ready to move on to fine-tuning.\n\n## Fine-Tuning\n\nWe will implement fine-tuning using Hugging Face's `Trainer` function. `Trainer` is an abstracted training and evaluation loop implemented in PyTorch for transformer models.\n\nThere are several variables that we must define beforehand. First, we start with the collate function. Collate helps us handle the collation of our dataset into batches of tensors that we will be fed into the model during training.\n\n```python\ndef collate_fn(batch):\n    return {\n        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n        'labels': torch.tensor([x['label'] for x in batch])\n    }\n```\n\nAnother important variable is the *evaluation metric* to measure our model performance over time. We will use a simple *accuracy* metric calculated as:\n$$\nAccuracy = \\frac{TP+TN}{TP+TN+FP+FN}\n$$\nWhere:\n\n$TP:$ *True Positives*\n\n$TN: $ *True Negatives*\n\n$FP:$ *False Positives*\n\n$FN:$ *False Negatives*\n\nWe implement this using *Datasets* metrics, defined in the `compute_metrics` function:\n\n```python\nimport numpy as np\nfrom datasets import load_metric\n\n# accuracy metric\nmetric = load_metric(\"accuracy\")\ndef compute_metrics(p):\n    return metric.compute(\n        predictions=np.argmax(p.predictions, axis=1),\n        references=p.label_ids\n    )\n```\n\nThe final variable required by `Trainer` is the `TrainingArguments` configuration. These are simply the training parameters, save settings, and logging settings.\n\n```python\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n  output_dir=\"./cifar\",\n  per_device_train_batch_size=16,\n  evaluation_strategy=\"steps\",\n  num_train_epochs=4,\n  save_steps=100,\n  eval_steps=100,\n  logging_steps=10,\n  learning_rate=2e-4,\n  save_total_limit=2,\n  remove_unused_columns=False,\n  push_to_hub=False,\n  load_best_model_at_end=True,\n)\n```\n\nWith all this, we're ready to initialize `Trainer` and begin the training loop.\n\n```python\nfrom transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=collate_fn,\n    compute_metrics=compute_metrics,\n    train_dataset=prepared_train,\n    eval_dataset=prepared_test,\n    tokenizer=feature_extractor,\n)\n# begin training\nresults = trainer.train()\n```\n\nTraining will take some time, even on GPU. Once complete, the best version of the model will be saved in the `output_dir` we set in the `TrainingArguments` config object.\n\n## Evaluation and Prediction\n\nThe `Trainer` performs evaluation during training but we can also perform a more qualitative check (or make a prediction) by passing a single image through the `feature_extractor` and `model`. We will use this image:\n\n{{< notebook file=\"vit-show-image\" height=\"full\" >}}\n\nThe image isn't very clear, and most people would struggle to correctly classify the image. However, we can see from the label that this is a cat. Let's see what the model predicts.\n\n{{< notebook file=\"vit-inference\" height=\"full\" >}}\n\nLooks like the model is correct!\n\n---\n\nThat concludes our introduction to the Vision Transformer and how to use it via Hugging Face *Transformers*. It's worth noting how quickly transformers have come to dominate NLP and, increasingly likely, computer vision in the near future.\n\nBefore 2021, transformers being used in anything but NLP was unheard of. Yet, despite being known as *\"those language models\"*, they have already found use in some of the most advanced computer vision applications. Transformers are a crucial component of diffusion models<sup>[5]</sup> and even Tesla's Full Self Driving<sup>[6]</sup>.\n\nAs time progresses, we will undoubtedly see both fields continue to merge and more real-world applications of transformers in both domains.\n\n{{< newsletter text=\"Subscribe for the latest ML news!\" inputText=\"Email address...\" buttonText=\"Submit\">}}\n\n---\n\n# Resources\n\n[Vision Transformers in Examples Repo](https://github.com/pinecone-io/examples/blob/master/learn/image-retrieval/vision-transformers/vit.ipynb)\n\n[1] A. Dosovitskiy et al., [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) (2021), ICLR\n\n[2] A. Vaswani et al., [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (2017), NeurIPS\n\n[3] L. Beyer, [Transformers in Vision: Tackling problems in Computer Vision](https://www.youtube.com/watch?v=BP5CM0YxbP8) (2022), Stanford Seminar\n\n[4] J. Devlin et al., [BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) (2019), ACL\n\n[5] [Stable Diffusion](https://github.com/CompVis/stable-diffusion) (2022), CompVis GitHub Repo\n\n[6] A. Kaparthy, [Tesla AI Day 2021 on Transformers in Vision](https://youtu.be/j0z4FweCy4M?t=3554) (2021), Tesla\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbb3"
  },
  "filename": "nlp.md",
  "title": "book",
  "category": "\"Natural Language Processing (NLP) for Semantic Search\"",
  "content": "---\nlayout: book\ntitle: \"Natural Language Processing (NLP) for Semantic Search\"\ndescription: Natural Language Processing (NLP) for Semantic Search.\nauthor: James Briggs\nintro: |\n  Learn how to make machines understand language as people do. This free course covers everything you need to build state-of-the-art language models, from machine translation to question-answering, and more. \nemailSubmit: true\nsocialShare: true\nimage: /images/nlp-ebook.png\nimages: ['/images/nlp-ebook.png']\nintroChapter: \n    title: Introduction\n    text: |\n      Semantic search has long been a critical component in the technology stacks of giants such as Google, Amazon, and Netflix. The recent democratization of these technologies has ignited a search renaissance, and these once guarded technologies are being discovered and quickly adopted by organizations across every imaginable industry.\n\n      Why the explosion of interest in semantic search? It unlocks an essential recipe to many products and applications, the scope of which is unknown but already broad. Search engines, autocorrect, translation, recommendation engines, error logging, and much more are already heavy users of semantic search. Many tools that can benefit from a meaningful language search or clustering function are supercharged by semantic search.\n\n      Two pillars support semantic search; *vector search* and *NLP*. In this course, we focus on the pillar of NLP and how it brings ‘semantic’ to semantic search. We introduce concepts and theory throughout the course before backing them up with real, industry-standard code and libraries.\n\n      You will learn what dense vectors are and why they’re fundamental to NLP and semantic search. We cover how to build state-of-the-art language models covering semantic similarity, multilingual embeddings, unsupervised training, and more. Learn how to apply these in the real world, where we often lack suitable datasets or masses of computing power.\n\n      In short, you will learn everything you need to know to begin applying NLP in your semantic search use-cases.\n\n      Let's begin!\nchapters:\n  - title: Dense Vectors\n    text: An overview of dense vector embeddings with NLP.\n    url: /learn/dense-vector-embeddings-nlp/\n\n  - title: Sentence Transformers and Embeddings\n    text: How sentence transformers and embeddings can be used for a range of semantic similarity applications.\n    url: /learn/sentence-embeddings/\n\n  - title: Training Sentence Transformers with Softmax Loss\n    text: The original way of training sentence transformers like SBERT for semantic search.\n    url: /learn/train-sentence-transformers-softmax/\n\n  - title: Training Sentence Transformers with Multiple Negatives Ranking Loss\n    text: How to create sentence transformers by fine-tuning with MNR loss.\n    url: /learn/fine-tune-sentence-transformers-mnr/\n    bonusSection: # Bonus content / further materials\n      title: \"Bonus Materials: Projects\"\n      links:\n        - title: \"Using Semantic Search to Find GIFs\"\n          url: /learn/gif-search/\n        - title: \"Making YouTube Search Better with NLP\"\n          url: /learn/youtube-search/\n\n  - title: Multilingual Sentence Transformers\n    text: How to create multilingual sentence transformers with knowledge distillation.\n    url: /learn/multilingual-transformers/\n\n  - title: Unsupervised Training for Sentence Transformers\n    text: How to create sentence transformer models without labelled data.\n    url: /learn/unsupervised-training-sentence-transformers/\n\n  - title: An Introduction to Open Domain Question-Answering\n    text: The illustrated overview to open domain question-answering.\n    url: /learn/question-answering\n  \n  - title: Retrievers for Question-Answering\n    text: How to fine-tune retriever models to find relevant contexts in vector databases.\n    url: /learn/retriever-models\n\n  - title: Readers for Question-Answering\n    text: How to fine-tune reader models to identify answers from relevant contexts.\n    url: /learn/reader-models\n    bonusSection:\n      title: \"Bonus Materials: Question-Answering\"\n      links:\n        - title: Generative Question-Answering with OpenAI\n          url: /learn/openai-gen-qa\n        - title: Long-Form Question-Answering with Haystack\n          url: /learn/haystack-lfqa\n\n  - title: Data Augmentation with BERT\n    text: Augmented SBERT (AugSBERT) is a training strategy to enhance domain-specific datasets.\n    url: /learn/data-augmentation/\n\n  - title: Domain Transfer with BERT\n    text: Transfer information from an out-of-domain (or source) dataset to a target domain.\n    url: /learn/domain-transfer/\n\n  - title: Unsupervised Training with Query Generation (GenQ)\n    text: Fine-tune retrievers for asymmetric semantic search using GenQ\n    url: /learn/genq/\n\n  - title: Generative Pseudo-Labeling (GPL)\n    text: A powerful technique for domain adaptation using unstructured text data.\n    url: /learn/gpl/\n    bonusSection: # Bonus content / further materials\n      title: Further Resources\n      links:\n        - title: \"Searching Freely: Using GPL for Semantic Search ft. Nils Reimers\"\n          url: https://www.youtube.com/watch?v=OQhoi1CabWw\n        - title: \"Faiss: The Missing Manual\"\n          url: /learn/faiss\n\n  - title: EMAIL_SUBMIT #email submit form\n\n  - title: Training Sentence Transformers\n    text: The most popular methods for training sentence transformers, and tips for each.\n\n  - title: And more...\n---\n\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbb4"
  },
  "filename": "vector-indexes.md",
  "title": "ebook-post",
  "category": "\"Nearest Neighbor Indexes for Similarity Search\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Nearest Neighbor Indexes for Similarity Search\"\nheadline: \"Nearest Neighbor Indexes for Similarity Search\"\ncategories:\n  - \"Faiss: The Missing Manual\"\ntoc: >-\nweight: 2\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: An overview and comparison of similarity search indexes.\n# Open Graph\nimages: ['/images/similarity-search-indexes26.png']\n---\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/similarity-search-indexes.mp4\" type=\"video/mp4\">\n</video>\n\nVector similarity search is a game-changer in the world of search. It allows us to efficiently search a huge range of media, from GIFs to articles — with incredible accuracy in sub-second timescales for billion+ size datasets.\n\nOne of the key components to efficient search is flexibility. And for that we have a wide range of search indexes available to us — there is no 'one-size-fits-all' in similarity search.\n\nHowever, this great flexibility produces a question — how do we know which size fits our use case?\n\nWhich index do we choose? Should we use multiple indexes, or is one enough?\n\nThis article will explore the pros and cons of some of the most important indexes — Flat, LSH, [HNSW](/learn/hnsw/), and IVF. We will learn how we decide which to use and the impact of parameters in each index.\n\n---\n\n*Note: [Pinecone](/) lets you add vector search to your production applications without knowing anything about vector indexes. However, we know you like seeing how things work, so enjoy learning about these popular vector index types!*\n\n---\n\nCheck out the video walkthrough:\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/B7wmo_NImgM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n## Indexes in Search\n\nBefore jumping into the different indexes available, let's take a look at why we care about similarity search — and how we can use indexes for efficient similarity search.\n\n### Why Similarity Search\n\nLet's start with the most fundamental question to the relevance of this article — why do we care about [similarity search](/learn/what-is-similarity-search/)?\n\nSimilarity search can be used to compare data quickly. Given a query (which could be in any format — text, audio, video, GIFs — you name it), we can use similarity search to return relevant results.\n\nThis is key to a huge number of companies and applications spanning across industries. It's used to identify similar genes in genome databases, deduplication of datasets, or search billions of results to search queries every day.\n\nSearch seems like an easy process — we take one item and compare it to another. But when we have millions (or even billions) of 'other' items to compare against — it begins to get tricky.\n\nFor search to be useful, it needs to be accurate and fast. And it is this more efficient search that we are interested in.\n\n### Indexes For Efficient Search\n\nIn vector similarity search, we use an index to store [vector representations](/learn/vector-embeddings/) of the data we intend to search.\n\nThrough either [statistical methods or machine learning](/learn/semantic-search/) — we can build vectors that encode useful, meaningful information about our original data.\n\n![Using densely encoded vectors, we can show that the equivalent man-King semantic relationship for woman is Queen.](/images/similarity-search-indexes1.jpg)\n\n<small>Using densely encoded vectors, we can show that the equivalent <b>man-King</b> semantic relationship for <b>woman</b> is <b>Queen</b>.</small>\n\nWe take these 'meaningful' vectors and store them inside an index to use for intelligent similarity search.\n\nThere are many index solutions available; one, in particular, is called [Faiss (Facebook AI Similarity Search)](/learn/faiss/). We store our vectors in Faiss and query our new Faiss index using a *'query' vector*. This query vector is compared to other index vectors to find the nearest matches — typically with Euclidean (L2) or inner-product (IP) metrics.\n\nSo, with that introduction to the why and how of similarity search. But what is this about Faiss — and choosing the right indexes?\n\n### Faiss And Indexes\n\nFaiss comes with many different index types — many of which can be mixed and matched to produce multiple layers of indexes.\n\nWe will be focused on a few indexes that prioritize search speed, quality, or index memory.\n\nNow, which one of these indexes we use depends very much on our use case. We must consider factors such as dataset size, search frequency, or search-quality vs. search-speed.\n\n---\n\n## Flat And Accurate\n\n![Flat indexes come with perfect search-quality at the cost of slow search speeds. Memory utilization of flat indexes is reasonable.](/images/similarity-search-indexes2.jpg)\n\n<small>Flat indexes come with perfect search-quality at the cost of slow search speeds. Memory utilization of flat indexes is reasonable.</small>\n\nThe very first indexes we should look at are the simplest — flat indexes.\n\nFlat indexes are 'flat' because we do not modify the vectors that we feed into them.\n\nBecause there is no approximation or clustering of our vectors — these indexes produce the most accurate results. We have perfect search quality, but this comes at the cost of significant search times.\n\nWith flat indexes, we introduce our query vector `xq`and compare it against every other full-size vector in our index — calculating the distance to each.\n\n![With flat indexes, we compare our search query xq to every other vector in the index.](/images/similarity-search-indexes3.png)\n\n<small>With flat indexes, we compare our search query <b>xq</b> to every other vector in the index.</small>\n\nAfter calculating all of these distances, we will return the nearest k of those as our nearest matches. A k-nearest neighbors (kNN) search.\n\n![Once we have calculated all of the distances, we return the k nearest vectors.](/images/similarity-search-indexes4.png)\n\n<small>Once we have calculated all of the distances, we return the <b>k</b> nearest vectors.</small>\n\n### When To Use\n\nSo when should we use a flat index? Well, when search quality is an unquestionably high priority — and search speed is less important.\n\nSearch speed can be an irrelevant factor for smaller datasets — especially when using more powerful hardware.\n\n![Euclidean (L2) and Inner Product (IP) flat index search times using faiss-cpu on an M1 chip. Both using vector dimensionality of 100. IndexFlatIP is shown to be slightly faster than IndexFlatL2.](/images/similarity-search-indexes5.jpg)\n\n<small>Euclidean (L2) and Inner Product (IP) flat index search times using <b>faiss-cpu</b> on an M1 chip. Both using vector dimensionality of 100. <b>IndexFlatIP</b> is shown to be slightly faster than <b>IndexFlatL2</b>.</small>\n\nThe above chart demonstrates Faiss CPU speeds on an M1-chip. Faiss is optimized to run on GPU at significantly higher speeds when paired with CUDA-enabled GPUs on Linux to improve search times significantly.\n\nIn short, use flat indexes when:\n\n-   Search quality is a *very* high priority.\n-   Search time does *not* matter OR when using a small index (<10K).\n\n### Implementing a Flat Index\n\nTo initialize a flat index, we need our data, Faiss, and one of the two flat indexes — `IndexFlatL2` if using Euclidean/L2 distance, or `IndexFlatIP` if using inner product distance.\n\nFirst, we need data. We will be using the [Sift1M dataset](http://corpus-texmex.irisa.fr/), which we can download and load into a notebook with:\n\n{{< notebook file=\"download-sift1m\" height=\"full\" >}}\n\nNow we can index our new data using one of our two flat indexes. We saw that IndexFlatIP is slightly faster, so let’s use that.\n\n```python\nd = 128  # dimensionality of Sift1M data\nk = 10  # number of nearest neighbors to return\n\nindex = faiss.IndexFlatIP(d)\nindex.add(data)\nD, I = index.search(xq, k)\n```\n\nAnd for flat indexes, that is all we need to do — there is no training (as we have no parameters to optimize when storing vectors without transformations or clustering).\n\n### Balancing Search Time\n\nFlat indexes are brilliantly accurate but terribly slow. In similarity search, there is always a trade-off between search-speed and search-quality (accuracy).\n\nWhat we must do is identify where our use-case sweet spot lies. With flat indexes, we are here:\n\n![Flat indexes are 100% search-quality, 0% search-speed.](/images/similarity-search-indexes6.png)\n\n<small>Flat indexes are 100% search-quality, 0% search-speed.</small>\n\nHere we have completely unoptimized search-speeds, which will fit many smaller index use-cases — or scenarios where search-time is irrelevant. But, other use-cases require a better balance speed and quality.\n\nSo, how can we make our search faster? There are two primary approaches:\n\n1. Reduce vector size — through dimensionality reduction or reducing the number of bits representing our vectors values.\n\n2. Reduce search scope — we can do this by clustering or organizing vectors into tree structures based on certain attributes, similarity, or distance — and restricting our search to closest clusters or filter through most similar branches.\n\nUsing either of these approaches means that we are no longer performing an exhaustive nearest-neighbors search but an approximate nearest-neighbors (ANN) search — as we no longer search the entire, full-resolution dataset.\n\nSo, what we produce is a more balanced mix that prioritizes both search-speed and search-time:\n\n![Often, we will want a more balanced mix of both search-speed and search-quality.](/images/similarity-search-indexes7.jpg)\n\n<small>Often, we will want a more balanced mix of both search-speed and search-quality.</small>\n\n---\n\n## Locality Sensitive Hashing\n\n![LSH — a wide range of performances heavily dependent on the parameters set. Good quality results in slower search, and fast search results in worse quality. Poor performance for high-dimensional data. The ‘half-filled’ segments of the bars represent the range in performance encountered while modifying index parameters.](/images/similarity-search-indexes8.jpg)\n\n<small>LSH — a wide range of performances heavily dependent on the parameters set. Good quality results in slower search, and fast search results in worse quality. Poor performance for high-dimensional data. The ‘half-filled’ segments of the bars represent the range in performance encountered while modifying index parameters.</small>\n\nLocality Sensitive Hashing (LSH) works by grouping vectors into buckets by processing each vector through a hash function that maximizes hashing collisions — rather than minimizing as is usual with hashing functions.\n\nWhat does that mean? Imagine we have a Python dictionary. When we create a new key-value pair in our dictionary, we use a hashing function to hash the key. This hash value of the key determines the 'bucket' where we store its respective value:\n\n![A typical hash function for a dictionary-like object will attempt to minimize hash collisions, aiming to assign only one value to each bucket.](/images/similarity-search-indexes9.jpg)\n\n<small>A typical hash function for a dictionary-like object will attempt to minimize hash collisions, aiming to assign only one value to each bucket.</small>\n\nA Python dictionary is an example of a hash table using a typical hashing function that *minimizes* hashing collisions, a hashing collision where two different objects (keys) produce the same hash.\n\nIn our dictionary, we want to avoid these collisions as it means that we would have multiple objects mapped to a single key — but for LSH, we want to *maximize* hashing collisions.\n\nWhy would we want to maximize collisions? Well, for search, we use LSH to group similar objects together. When we introduce a new query object (or vector), our LSH algorithm can be used to find the closest matching groups:\n\n![Our hash function for LSH attempts to maximize hash collisions, producing groupings of vectors.](/images/similarity-search-indexes10.jpg)\n\n<small>Our hash function for LSH attempts to maximize hash collisions, producing groupings of vectors.</small>\n\n### Implementing LSH\n\nImplementing our LSH index in Faiss is easy. We initialize a IndexLSH object, using the vector dimensions d and the nbits argument — and add our vectors like so:\n\n```python\nnbits = d*4  # resolution of bucketed vectors\n# initialize index and add vectors\nindex = faiss.IndexLSH(d, nbits)\nindex.add(wb)\n# and search\nD, I = index.search(xq, k)\n```\n\nOur `nbits` argument refers to the 'resolution' of the hashed vectors. A higher value means greater accuracy at the cost of more memory and slower search speeds.\n\n![Recall score of IndexLSH with d of 128. Note that to get higher recall performance, we need to increase the num_bits value dramatically. For ~90% recall we use 64*d, which is 64*128 = 8192.](/images/similarity-search-indexes11.jpg)\n\n<small>Recall score of <b>IndexLSH</b> with <b>d</b> of <b>128</b>. Note that to get higher recall performance, we need to increase the <b>num_bits</b> value dramatically. For 90% recall we use 64*d, which is 64*128 = <b>8192</b>.</small>\n\nOur baseline `IndexFlatIP` index is our 100% recall performance, using `IndexLSH` we can achieve 90% using a very high `nbits` value.\n\nThis is a strong result — 90% of the performance could certainly be a reasonable sacrifice to performance if we get improved search-times.\n\nHowever, LSH is highly sensitive to the curse of dimensionality when using a larger `d` value we also need to increase `nbits` to maintain search-quality.\n\nSo our stored vectors become increasingly larger as our original vector dimensionality `d` increases. This quickly leads to excessive search times:\n\n![Search time for IndexLSH with varying nbits values, compared against a flat IP index.](/images/similarity-search-indexes12.jpg)\n\n<small>Search time for <b>IndexLSH</b> with varying <b>nbits</b> values, compared against a flat IP index.</small>\n\nWhich is mirrored by our index memory size:\n\n![Index size for IndexLSH with varying nbits values, compared against a flat IP index.](/images/similarity-search-indexes13.jpg)\n\n<small>Index size for <b>IndexLSH</b> with varying <b>nbits</b> values, compared against a flat IP index.</small>\n\nSo `IndexLSH` is *not* suitable if we have large vector dimensionality (`128` is already *too* large). Instead, it is best suited to low-dimensionality vectors — and small indexes.\n\nIf we find ourselves with large `d` values or large indexes — we avoid LSH completely, instead focusing on our next index, HNSW.\n\n---\n\n## Hierarchical Navigable Small World Graphs\n\n![HNSW — great search-quality, good search-speed, but substantial index sizes. The ‘half-filled’ segments of the bars represent the range in performance encountered while modifying index parameters.](/images/similarity-search-indexes14.jpg)\n\n<small>HNSW — great search-quality, good search-speed, but substantial index sizes. The ‘half-filled’ segments of the bars represent the range in performance encountered while modifying index parameters.</small>\n\nHierarchical Navigable Small World (HNSW) graphs are another, more recent development in search. HNSW-based ANNS consistently top out as the highest performing indexes [1].\n\nHNSW is a further adaption of navigable small world (NSW) graphs — where an NSW graph is a graph structure containing vertices connected by edges to their nearest neighbors.\n\nThe 'NSW' part is due to vertices within these graphs all having a very short average path length to all other vertices within the graph — despite not being directly connected.\n\nUsing the example of Facebook — in 2016, we could connect every user (a vertex) to their Facebook friends (their nearest neighbors). And despite the 1.59B active users, the average number of steps (or *hops*) needed to traverse the graph from one user to another was just 3.57 [2].\n\n![Visualization of an NSW graph. Notice that each point is no more than four hops away from another.](/images/similarity-search-indexes15.png)\n\n<small>Visualization of an NSW graph. Notice that each point is no more than four hops away from another.</small>\n\nFacebook is just one example of high connectivity in vast networks — otherwise known as an NSW graph.\n\nAt a high level, HNSW graphs are built by taking NSW graphs and breaking them apart into multiple layers. With each incremental layer eliminating intermediate connections between vertices.\n\n![With HNSW, we break networks into several layers, which are traversed during the search.](/images/similarity-search-indexes16.jpg)\n\n<small>With HNSW, we break networks into several layers, which are traversed during the search.</small>\n\nFor bigger datasets with higher-dimensionality — HNSW graphs are some of the best performing indexes we can use. We'll be covering using the HNSW index alone, but by layering other quantization steps, we can improve search-times even further.\n\n### HNSW Implementation\n\nTo build and search a flat HNSW index in Faiss, all we need is `IndexHNSWFlat`:\n\n```python\n# set HNSW index parameters\nM = 64  # number of connections each vertex will have\nef_search = 32  # depth of layers explored during search\nef_construction = 64  # depth of layers explored during index construction\n\n# initialize index (d == 128)\nindex = faiss.IndexHNSWFlat(d, M)\n# set efConstruction and efSearch parameters\nindex.hnsw.efConstruction = ef_construction\nindex.hnsw.efSearch = ef_search\n# add data to index\nindex.add(wb)\n\n# search as usual\nD, I = index.search(wb, k)\n```\n\nHere, we have three key parameters for modifying our index performance.\n\n- `M` — the number of nearest neighbors that each vertex will connect to.\n- `efSearch` — how many entry points will be explored between layers during the search.\n- `efConstruction` — how many entry points will be explored when building the index.\n\nEach of these parameters can be increased to improve search-quality:\n\n![Recall values for different efConstruction, efSearch, and M values on the Sift1M dataset.](/images/similarity-search-indexes17.jpg)\n\n<small>Recall values for different <b>efConstruction</b>, <b>efSearch</b>, and <b>M</b> values on the Sift1M dataset.</small>\n\n`M` and `efSearch` have a larger impact on search-time — `efConstruction` primarily increases index *construction time* (meaning a slower `index.add`), but at higher `M` values and higher query volume we do see an impact from `efConstruction` on search-time too.\n\n![Search time for different M and efSearch values on the full Sift1M dataset.](/images/similarity-search-indexes18.jpg)\n\n<small>Search time for different <b>M</b> and <b>efSearch</b> values on the full Sift1M dataset.</small>\n\nHNSW gives us great search-quality at very fast search-speeds — but there's always a catch — HNSW indexes take up a significant amount of memory. Using an `M` value of `128` for the Sift1M dataset requires upwards of 1.6GB of memory.\n\n![Index memory usage for different M values on the Sift1M dataset.](/images/similarity-search-indexes19.jpg)\n\n<small>Index memory usage for different <b>M</b> values on the Sift1M dataset.</small>\n\nHowever, we can increase our other two parameters — `efSearch` and `efConstruction` with no effect on the index memory footprint.\n\nSo, where RAM is not a limiting factor HNSW is great as a well-balanced index that we can push to focus more towards quality by increasing our three parameters.\n\n![We can use the lower set of parameters to balance prioritize a slightly faster search-speed with good search-quality — or we use the higher set of parameters for slightly slower search-speed with high search-quality.](/images/similarity-search-indexes20.jpg)\n\n<small>We can use the lower set of parameters to balance prioritize a slightly faster search-speed with good search-quality — or we use the higher set of parameters for slightly slower search-speed with high search-quality.</small>\n\nThat's HNSW in Faiss — an incredibly powerful and efficient index. Now let's move onto our final index — IVF.\n\n---\n\n## Inverted File Index\n\n![IVF — great search-quality, good search-speed, and reasonable memory usage. The ‘half-filled’ segments of the bars represent the range in performance encountered while modifying index parameters.](/images/similarity-search-indexes21.jpg)\n\n<small>IVF — great search-quality, good search-speed, and reasonable memory usage. The ‘half-filled’ segments of the bars represent the range in performance encountered while modifying index parameters.</small>\n\nThe Inverted File Index (IVF) index consists of search scope reduction through clustering. It's a very popular index as it's easy to use, with high search-quality and reasonable search-speed.\n\nIt works on the concept of Voronoi diagrams — also called Dirichlet tessellation (a much cooler name).\n\nTo understand Voronoi diagrams, we need to imagine our highly-dimensional vectors placed into a 2D space. We then place a few additional points in our 2D space, which will become our 'cluster' (Voronoi cells in our case) centroids.\n\nWe then extend an equal radius out from each of our centroids. At some point, the circumferences of each cell circle will collide with another — creating our cell edges:\n\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/similarity-search-indexes2.mp4\" type=\"video/mp4\">\n</video>\n\n<small class=\"video\">How Voronoi cells are constructed — here, we have three centroids, resulting in three Voronoi cells. Each datapoint within a given cell is then assigned to that respective centroid.</small>\n\nNow, every datapoint will be contained within a cell — and be assigned to that respective centroid.\n\nJust as with our other indexes, we introduce a query vector `xq` — this query vector must land within one of our cells, at which point we restrict our search scope to that single cell.\n\nBut there is a problem if our query vector lands near the edge of a cell — there's a good chance that its closest other datapoint is contained within a neighboring cell. We call this the *edge problem*:\n\n![Our query vector xq lands on the edge of the magenta cell. Despite being closer to datapoints in the teal cell, we will not compare these if nprobe == 1 — as this means we would restrict search scope to the magenta cell only.](/images/similarity-search-indexes22.jpg)\n\n<small>Our query vector <b>xq</b> lands on the edge of the magenta cell. Despite being closer to datapoints in the teal cell, we will not compare these if <b>nprobe == 1</b> — as this means we would restrict search scope to the magenta cell only.</small>\n\nNow, what we can do to mitigate this issue and increase search-quality is increase an index parameter known as the `nprobe` value. With `nprobe` we can set the number of cells to search.\n\n![Increasing nprobe increases our search scope.](/images/similarity-search-indexes23.png)\n![Increasing nprobe increases our search scope.](/images/similarity-search-indexes26.png)\n\n<small>Increasing <b>nprobe</b> increases our search scope.</small>\n\n### IVF Implementation\n\nTo implement an IVF index and use it in search, we can use `IndexIVFFlat`:\n\n```python\nnlist = 128  # number of cells/clusters to partition data into\n\nquantizer = faiss.IndexFlatIP(d)  # how the vectors will be stored/compared\nindex = faiss.IndexIVFFlat(quantizer, d, nlist)\nindex.train(data)  # we must train the index to cluster into cells\nindex.add(data)\n\nindex.nprobe = 8  # set how many of nearest cells to search\nD, I = index.search(xq, k)\n```\n\nThere are two parameters here that we can adjust.\n\n-   `nprobe` — the number of cells to search\n-   `nlist` — the number of cells to create\n\nA higher `nlist` means that we must compare our vector to more centroid vectors — but after selecting the nearest centroid's cells to search, there will be fewer vectors within each cell. So, *increase* `nlist` to prioritize *search-speed*.\n\nAs for `nprobe`, we find the opposite. *Increasing* `nprobe` increases the search scope — thus prioritizing *search-quality*.\n\n![Search-time and recall for IVF using different nprobe and nlist values.](/images/similarity-search-indexes24.jpg)\n\n<small>Search-time and recall for IVF using different <b>nprobe</b> and <b>nlist</b> values.</small>\n\nIn terms of memory, `IndexIVFFlat` is reasonably efficient — and modifying `nprobe` will not affect this. The effect of `nlist` on memory usage is small too — higher `nlist` means a marginally larger memory requirement.\n\n![Memory usage of the index is affected only by the nlist parameter. However, for our Sift1M dataset, the index size changed only very slightly.](/images/similarity-search-indexes25.jpg)\n\n<small>Memory usage of the index is affected only by the nlist parameter. However, for our Sift1M dataset, the index size changed only very slightly.</small>\n\nSo, we must decide between greater search-quality with `nprobe` and faster search-speed with `nlist`.\n\n---\n\nWe’ve covered a lot in this article , so let’s wrap everything up with a quick summary of each index's memory, speed, and search-quality performance.\n\n<script src=\"https://gist.github.com/jamescalam/c752e0b5c7a7c8136de5b6167afb7de3.js\"></script>\n\nSo we have four indexes, with equally potent pros and cons — depending on the use-case. Hopefully, with this article, you will now be much better prepared to decide which of these indexes best fits your own use case.\n\nBeyond these metrics, it is possible to improve memory usage and search-speed even further by adding other quantization or compression steps — but that is for another article.\n\n{{< newsletter text=\"Subscribe for the latest in similarity search!\" inputText=\"Email address...\" buttonText=\"Submit\">}}\n\n---\n\n### References\n\n[1] E Bernhardsson, [ANN-benchmarks repo](https://github.com/erikbern/ann-benchmarks), Github\n\n[2] S Edunov, et al., [Three and a half degrees of separation](https://research.fb.com/blog/2016/02/three-and-a-half-degrees-of-separation/) (2016), Facebook Research\n\n---\n\n*All images are by the author except where stated otherwise*\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbb5"
  },
  "filename": "transfer-learning.md",
  "title": "post",
  "category": "\"Introduction to Transfer Learning\"",
  "content": "---\nlayout: post\ntitle: \"Introduction to Transfer Learning\"\nheadline: \"Introduction to Transfer Learning\"\ncategories:\n  - Algorithms & Libraries\ntoc: >-\nweight: 1\nauthor:\n  name: Diego Lopez Yse\n  position: Data Scientist\n  src: /images/diego-lopez-yse.jpeg\n  href: \"https://lopezyse.medium.com/\"\ndescription: An introduction to transfer learning.\nimages: [\"/images/tradition-vs-transfer-learning.jpg\"]\n---\n\nAs humans, we don’t learn tasks in isolation. As we move our knowledge path forward, we build on previously learned and related challenges, which accelerates the learning process.\n\nIf you already know a particular musical, cooking, or dance style, you will not forget that knowledge and start from scratch when learning a different style; you will reuse the knowledge you already have and finetune it to absorb the new features. This is an incredible human capability, which can also be applied to machines through the use of Transfer Learning (TL).\n\nThis way, what has been learned in one setting is built upon to improve generalization on a related setting, which is a huge advantage over traditional approaches that don’t capture this benefit. Traditional Machine Learning models require training from scratch, which is computationally expensive and requires a large amount of data to achieve high performance. They also involve isolated training, where each model is independently trained for a specific purpose without any dependency on past knowledge.\n\n<div class=\"centered-text-section mt\">\n\n**Transfer Learning (TL)** is a Machine Learning method where a model developed for a task is reused as the starting point for a model in another task. \n\n</div>\n\n![Tradition vs transfer learning](/images/tradition-vs-transfer-learning.jpg)\n<small>Traditional Machine Learning vs. Transfer Learning. Source: [datascience.aero](https://datascience.aero/)</small>\n\nTL has become highly popular given the enormous resources required to train Deep Learning models, especially in the areas of:\n\n* **Natural Language Processing (NLP)** to solve problems that use text as input or output. In this discipline, there are several effective algorithms that generate distributed representations or text, such as words or sentences. These solutions are commonly released as **pretrained models** which are trained on a [very large corpus of text documents](https://machinelearningmastery.com/transfer-learning-for-deep-learning/).\n\n* **Computer Vision (CV)** in which it’s very unusual to train an entire [Convolutional Neural Network (CNN)](https://en.wikipedia.org/wiki/Convolutional_neural_network) from scratch (with random initialization), since it’s relatively rare to have a dataset of sufficient size. Instead, [it is common to pretrain a CNN on a very large dataset](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) and then use that model either as an initialization or a fixed feature extractor for the task of interest. This way, the majority of the pretrained CNN can be employed on new models through TL, just retraining a section of it.\n\n## Transfer Learning in action\n\nHow does the transfer learning process work? Although there are [different approaches](https://arxiv.org/abs/1606.09282), two of the main ones are:\n\n* **Feature-based**, which does not modify the original model and allows new tasks to benefit from complex features learned from previous tasks. However, these features are not specialized for the new task and can often be improved by fine-tuning.\n\n* **Fine-tuning**, which modifies the parameters of an existing model to train a new task. The original model is “unfrozen” and retrained on new data, increasing the performance for the new task.\n\n![Transfer Learning in action](/images/transfer-learning-in-action.png)\n\n[These approaches can be integrated in TL](https://keras.io/guides/transfer_learning/), producing the following workflow:\n\n1. Take layers from a previously trained model.\n2. Freeze them to avoid altering any of the information they contain during future training rounds.\n3. Add some new, trainable layers on top of the frozen layers. They will learn to turn old features into predictions on a new dataset.\n4. Train the new layers on your dataset.\n\nAs a last step, one can perform a **fine-tuning of the model**: unfreezing the entire model obtained above (or part of it) and re-training it on the new data with a very low learning rate. This can potentially achieve meaningful improvements by incrementally adapting the pretrained features to the new data.\n\n![The Transfer Learning process](/images/transfer-learning-process.png)\n<small>The Transfer Learning process. Source: [V7 Labs](https://www.v7labs.com/blog/transfer-learning-guide)</small>\n\nUsing this workflow as a generic approach to perform TL, let’s see how specific pretrained models can be used to solve different challenges.\n\n## Computer Vision\n\nIn Computer Vision, the intuition is that if a model is trained on a large and general enough dataset, it will effectively serve as a generic model of the visual world. Some of the main pretrained models used in Computer Vision for TL are:\n\n* Visual Geometry Group **(VGG)** and all its variants like the VGG16 model, which is a CNN of 16 layers trained on a subset of the [ImageNet](https://www.image-net.org/) dataset, a collection of over 14 million images belonging to 22,000 categories. Characterized by its simplicity, VGG was developed to increase the depth of such CNNs in order to increase the model performance. The VGG model is the basis of [ground-breaking object recognition models](https://viso.ai/deep-learning/vgg-very-deep-convolutional-networks/) and became one of the most popular architectures.\n\n![VGG16 Macro Architecture](/images/macro-architecture-vgg16.png)\n<small>Macro Architecture of VGG16. Source: [VGG in TensorFlow](http://www.cs.toronto.edu/~frossard/post/vgg16/)</small>\n\n* Residual Network **(ResNet)**, which relies on the fact that as the number of layers increases in a CNN, the ability of the model to fit more complex functions also increases. ResNet introduces a so-called “*identity shortcut connection*” that skips one or more layers in the CNN architecture, gaining accuracy from considerably increases in depth. This way, the model ensures that the higher layers of the model do not perform any worse than the lower layers, improving the efficiency while minimizing the percentage of errors. Models like Resnet150 can work with more than 150 neural network layers, as deep models generalize well to different datasets.\n\n![Identity short connection](/images/identity-short-connection.png)\n<small>The advantage of adding the “identity shortcut connection” is that if any layer hurts the performance of the model, then it will be skipped. Source: [Geeks for Geeks](https://www.geeksforgeeks.org/residual-networks-resnet-deep-learning/)</small>\n\n## Natural Language Processing (NLP)\n\nLike in Computer Vision, you can take advantage of the learned features from [NLP](/learn/nlp/) pretrained models without having to start from scratch by training a large model on a large dataset. Some of the main pretrained models used in NLP for TL are:\n\n* Universal Language Model Fine-tuning for Text Classification **(ULMFiT)**, an [AWD-LSTM](https://paperswithcode.com/method/awd-lstm#:~:text=ASGD%20Weight%2DDropped%20LSTM%2C%20or,of%20last%20iterations%20of%20weights.) model that was trained on the Wikitext-103 dataset and showed incredible performance on [tasks](https://nlp.fast.ai/classification/2018/05/15/introducing-ulmfit.html) like finding documents relevant to a legal case, identifying spam and offensive comments, classifying reviews of a product, grouping articles by political orientation, and much more. ULMFiT introduces [different techniques](https://arxiv.org/abs/1801.06146) like discriminative fine-tuning (which allows us to tune each layer with different learning rates), *slanted triangular learning rates* (a learning rate schedule that first linearly increases the learning rate and then linearly decays it), and *gradual unfreezing* (unfreezing one layer per epoch) to retain previous knowledge.\n\n![ULMFiT](/images/ulmfit.png)\n<small>ULMFiT requires orders of magnitude less data than previous approaches. Source: [Fast.ai](https://nlp.fast.ai/)</small>\n\n* Bidirectional Encoder Representations from Transformers **(BERT)**, a model that significantly altered the NLP landscape, became a new benchmark useful for almost any task. BERT is a [transformer-based model](/learn/transformers/) trained on 2.5 billion words with the ability to consider the context from both the left and right sides of each word. Its popularity came after [breaking several records](https://jalammar.github.io/illustrated-bert/) for how well it can handle language-based tasks like *question answering, text summarization, sentence prediction, word sense disambiguation, natural language inference, and sentiment classification*, among others.\n\n![BERT](/images/bert.png)\n<small>BERT proved to become the most popular of the modern NLP models. BERT was made possible by two major trends. Firstly, hardware has gotten exponentially better, especially GPUs. Second, the web, mostly composed of text, provided a large, open and diverse dataset. Source: [Techcrunch](https://techcrunch.com/sponsor/nvidia/how-the-revolution-of-natural-language-processing-is-changing-the-way-companies-understand-text/)</small>\n\n## The good and the bad\n\nTransfer learning is a shortcut to saving time, resources and getting better performance. Although it’s not obvious that there will be a benefit to using TL in the domain until after the model has been developed and evaluated, there are some [possible advantages](https://machinelearningmastery.com/transfer-learning-for-deep-learning/) like:\n\n1. **Higher start**. The initial skill (before refining the model) on the source model is higher than it otherwise would be.\n2. **Higher slope**. The rate of improvement of skill during training of the source model is steeper than it otherwise would be.\n3. **Higher asymptote**. The converged skill of the trained model is better than it otherwise would be.\n\n![Transfer learning](/images/transfer-learning.png)\n<small>Three ways in which transfer might improve learning. Source: [Machine Learning Mastery](https://machinelearningmastery.com/transfer-learning-for-deep-learning/)</small>\n\nIdeally, you would see all three benefits from a successful application of TL, which in practical terms, you can try to seek if:\n\n* You identify a related task with abundant data and you have the resources to develop a model for that task and reuse it on your problem, or\n* There is a pre-trained model available that you can use as a starting point for your own model.\n\nThis all sounds great, but although the goal of TL is to improve a model’s performance, it can also degrade it. If the TL task ends up with a decrease in the performance or accuracy of the new model, then it produces an effect called **negative transfer**. Negative transfer can be caused by too [high a dissimilarity of the problem domains](https://developer.ibm.com/articles/transfer-learning-for-deep-learning/) or the inability of the model to train for the new domain’s data set (in addition to the new data set itself).\n\nTransfer learning can also induce **overfitting**, which happens when the new model learns details and noises from the training data that negatively impact its outputs.\n\nTransfer learning can come with a variety of serious problems, including [distributional biases](https://arxiv.org/abs/1911.03064), [social stereotypes](https://arxiv.org/abs/2004.09456), potentially revealing [training samples](https://arxiv.org/abs/2012.07805), and other [possible harms](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922). In the case of NLP language pretrained models, one particular [type of harm](https://deepmind.com/research/publications/2021/Challenges-in-Detoxifying-Language-Models) is the generation of [toxic language](https://arxiv.org/abs/2009.11462), which includes hate speech, insults, profanities, and threats.\n\nThere’s no such thing as free lunch. Although transfer learning has several limitations, it allows for repurposing models for new problems with less data for training, gaining efficiencies in time and resources. \n\nBesides the obvious economic benefits of using TL, some people believe it might also be the gateway to more impactful advancements in AI. As Demis Hassabis (CEO and co-founder of DeepMind) said:\n\n“*I think Transfer Learning is the key to General Intelligence. And I think the key to doing Transfer Learning will be the acquisition of conceptual knowledge that is abstracted away from perceptual details of where you learned it from.*”\n\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbb6"
  },
  "filename": "hnsw.md",
  "title": "ebook-post",
  "category": "\"Hierarchical Navigable Small Worlds (HNSW)\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Hierarchical Navigable Small Worlds (HNSW)\"\nheadline: \"Hierarchical Navigable Small Worlds (HNSW)\"\ncategories:\n  - \"Faiss: The Missing Manual\"\ntoc: >-\nweight: 6\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: Demystifying and experimenting with HNSW for vector search.\n#Open Graph\nimages: ['/images/hnsw-0.jpg']\n---\n\n![Hierarchical Navigable Small Worlds](/images/hnsw-0.jpg)\n\nHierarchical Navigable Small World (HNSW) graphs are among the top-performing indexes for [vector similarity search](/learn/what-is-similarity-search/)<sup>[1]</sup>. HNSW is a hugely popular technology that time and time again produces state-of-the-art performance with super fast search speeds and fantastic recall.\n\nYet despite being a popular and robust algorithm for approximate nearest neighbors (ANN) searches, understanding how it works is far from easy.\n\n---\n\n**Note: [Pinecone](/) lets you build scalable, performant vector search into applications without knowing anything about HNSW or vector indexing libraries. But we know you like seeing how things work, so enjoy the guide!**\n\n---\n\nThis article helps demystify HNSW and explains this intelligent algorithm in an easy-to-understand way. Towards the end of the article, we'll look at how to implement HNSW using [Faiss](/learn/faiss/) and which parameter settings give us the performance we need.\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/QvKMwLjdK-s\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n## Foundations of HNSW\n\nWe can split [ANN algorithms](/learn/what-is-similarity-search/) into three distinct categories; trees, hashes, and graphs. HNSW slots into the *graph* category. More specifically, it is a *proximity graph*, in which two vertices are linked based on their proximity (closer vertices are linked) — often defined in Euclidean distance.\n\nThere is a significant leap in complexity from a *'proximity'* graph to *'hierarchical navigable small world'* graph. We will describe two fundamental techniques that contributed most heavily to HNSW: the probability skip list, and navigable small world graphs.\n\n### Probability Skip List\n\nThe probability skip list was introduced *way back* in 1990 by *William Pugh* <sup>[2]</sup>. It allows fast search like a sorted array, while using a linked list structure for easy (and fast) insertion of new elements (something that is not possible with sorted arrays).\n\nSkip lists work by building several layers of linked lists. On the first layer, we find links that *skip* many intermediate nodes/vertices. As we move down the layers, the number of *'skips'* by each link is decreased.\n\nTo search a skip list, we start at the highest layer with the longest *'skips'* and move along the edges towards the right (below). If we find that the current node 'key' is *greater than* the key we are searching for — we know we have overshot our target, so we move down to previous node in the *next* level.\n\n\n![prob_skip_list](/images/hnsw-1.jpg)\n<small>A probability skip list structure, we start on the top layer. If our current key is greater than the key we are searching for (or we reach `end`), we drop to the next layer.</small>\n\nHNSW inherits the same layered format with longer edges in the highest layers (for fast search) and shorter edges in the lower layers (for accurate search).\n\n### Navigable Small World Graphs\n\nVector search using *Navigable Small World* (NSW) graphs was introduced over the course of several papers from 2011-14 <sup>[4, 5, 6]</sup>. The idea is that if we take a proximity graph but build it so that we have both long-range and short-range links, then search times are reduced to (poly/)logarithmic complexity.\n\nEach vertex in the graph connects to several other vertices. We call these connected vertices *friends*, and each vertex keeps a *friend list*, creating our graph.\n\nWhen searching an NSW graph, we begin at a pre-defined *entry-point*. This entry point connects to several nearby vertices. We identify which of these vertices is the closest to our query vector and move there.\n\n\n![nsw search](/images/hnsw-2.jpg)\n<small>The search process through a NSW graph. Starting at a pre-defined entry point, the algorithm greedily traverses to connected vertices that are nearer to the query vector.</small>\n\nWe repeat the *greedy-routing search* process of moving from vertex to vertex by identifying the nearest neighboring vertices in each friend list. Eventually, we will find no nearer vertices than our current vertex — this is a local minimum and acts as our stopping condition.\n\n---\n\n*Navigable small world models are defined as any network with (poly/)logarithmic complexity using greedy routing. The efficiency of greedy routing breaks down for larger networks (1-10K+ vertices) when a graph is not navigable <sup>[7]</sup>.*\n\n---\n\nThe *routing* (literally the route we take through the graph) consists of two phases. We start with the \"zoom-out\" phase where we pass through low-degree vertices (degree is the number of links a vertex has) — and the later \"zoom-in\" phase where we pass through higher-degree vertices <sup>[8]</sup>.\n\n![vertex degrees](/images/hnsw-3.jpg)\n<small>High-degree vertices have *many* links, whereas low-degree vertices have very *few* links.</small>\n\nOur *stopping condition* is finding no nearer vertices in our current vertex's friend list. Because of this, we are more likely to hit a local minimum and stop too early when in the *zoom-out* phase (fewer links, less likely to find a nearer vertex).\n\nTo minimize the probability of stopping early (and increase recall), we can increase the average degree of vertices, but this increases network complexity (and search time). So we need to balance the average degree of vertices between recall and search speed.\n\nAnother approach is to start the search on high-degree vertices (*zoom-in* first). For NSW, this *does* improve performance on low-dimensional data. We will see that this is also a significant factor in the structure of HNSW.\n\n### Creating HNSW\n\nHNSW is a natural evolution of NSW, which borrows inspiration from hierarchical multi-layers from Pugh's probability skip list structure.\n\nAdding hierarchy to NSW produces a graph where links are separated across different layers. At the top layer, we have the longest links, and at the bottom layer, we have the shortest.\n\n![hnsw](/images/hnsw-4.jpg)\n<small>Layered graph of HNSW, the top layer is our entry point and contains only the *longest links*, as we move down the layers, the link lengths become shorter and more numerous.</small>\n\nDuring the search, we enter the top layer, where we find the longest links. These vertices will tend to be higher-degree vertices (with links separated across multiple layers), meaning that we, by default, start in the *zoom-in* phase described for NSW.\n\nWe traverse edges in each layer just as we did for NSW, greedily moving to the nearest vertex until we find a local minimum. Unlike NSW, at this point, we shift to the current vertex in a lower layer and begin searching again. We repeat this process until finding the local minimum of our bottom layer — *layer 0*.\n\n\n![hnsw search](/images/hnsw-5.jpg)\n<small>The search process through the multi-layer structure of an HNSW graph.</small>\n\n## Graph Construction\n\nDuring graph construction, vectors are iteratively inserted one-by-one. The number of layers is represented by parameter *L*. The probability of a vector insertion at a given layer is given by a probability function normalized by the *'level multiplier' m_L*, where *m_L = ~0* means vectors are inserted at *layer 0* only.\n\n\n![hnsw insertion](/images/hnsw-6.jpg)\n<small>The probability function is repeated for each layer (other than *layer 0*). The vector is added to its insertion layer *and* every layer below it.</small>\n\nThe creators of HNSW found that the best performance is achieved when we minimize the overlap of shared neighbors across layers. *Decreasing m_L* can help minimize overlap (pushing more vectors to *layer 0*), but this increases the average number of traversals during search. So, we use an *m_L* value which balances both. *A rule of thumb for this optimal value is `1/ln(M)` <sup>[1]</sup>*.\n\nGraph construction starts at the top layer. After entering the graph the algorithm greedily traverse across edges, finding the *ef* nearest neighbors to our inserted vector *q* — at this point *ef = 1*.\n\nAfter finding the local minimum, it moves down to the next layer (just as is done during search). This process is repeated until reaching our chosen *insertion layer*. Here begins phase two of construction.\n\nThe *ef* value is increased to `efConstruction` (a parameter we set), meaning more nearest neighbors will be returned. In phase two, these nearest neighbors are candidates for the links to the new inserted element *q* *and* as entry points to the next layer.\n\n*M* neighbors are added as links from these candidates — the most straightforward selection criteria are to choose the closest vectors.\n\nAfter working through multiple iterations, there are two more parameters that are considered when adding links. *M_max*, which defines the maximum number of links a vertex can have, and *M_max0*, which defines the same but for vertices in *layer 0*.\n\n\n![hnsw insert params](/images/hnsw-7.jpg)\n<small>Explanation of the number of links assigned to each vertex and the effect of `M`, `M_max`, and `M_max0`.</small>\n\nThe stopping condition for insertion is reaching the local minimum in *layer 0*.\n\n## Implementation of HNSW\n\nWe will implement HNSW using the Facebook AI Similarity Search (Faiss) library, and test different construction and search parameters and see how these affect index performance.\n\nTo initialize the HNSW index we write:\n\n{{< notebook file=\"hnsw-init\" height=\"full\" >}}\n\nWith this, we have set our `M` parameter — the number of neighbors we add to each vertex on insertion, but we're missing *M_max* and *M_max0*.\n\nIn Faiss, these two parameters are set automatically in the `set_default_probas` method, called at index initialization. The *M_max* value is set to `M`, and *M_max0* set to `M*2` (find further detail in the [notebook](https://github.com/pinecone-io/examples/blob/master/hnsw_faiss/hnsw_faiss.ipynb)).\n\nBefore building our `index` with `index.add(xb)`, we will find that the number of layers (or *levels* in Faiss) are not set:\n\n{{< notebook file=\"hnsw-no-levels\" height=\"full\" >}}\n\nIf we go ahead and build the index, we'll find that both of these parameters are now set.\n\n{{< notebook file=\"hnsw-with-levels\" height=\"full\" >}}\n\nHere we have the number of levels in our graph, *0* -> *4* as described by `max_level`. And we have `levels`, which shows the distribution of vertices on each level from *0* to *4* (ignoring the first `0` value). We can even find which vector is our entry point:\n\n{{< notebook file=\"get-entry-point\" height=\"full\" >}}\n\nThat's a high-level view of our Faiss-flavored HNSW graph, but before we test the index, let's dive a little deeper into how Faiss is building this structure.\n\n#### Graph Structure\n\nWhen we initialize our index we pass our vector dimensionality `d` and number of neighbors for each vertex `M`. This calls the method '`set_default_probas`', passing `M` and `1 / log(M)` in the place of `levelMult` (equivalent to *m_L* above). A Python equivalent of this method looks like:\n\n```python\ndef set_default_probas(M: int, m_L: float):\n    nn = 0  # set nearest neighbors count = 0\n    cum_nneighbor_per_level = []\n    level = 0  # we start at level 0\n    assign_probas = []\n    while True:\n        # calculate probability for current level\n        proba = np.exp(-level / m_L) * (1 - np.exp(-1 / m_L))\n        # once we reach low prob threshold, we've created enough levels\n        if proba < 1e-9: break\n        assign_probas.append(proba)\n        # neighbors is == M on every level except level 0 where == M*2\n        nn += M*2 if level == 0 else M\n        cum_nneighbor_per_level.append(nn)\n        level += 1\n    return assign_probas, cum_nneighbor_per_level\n```\n\nHere we are building two vectors — `assign_probas`, the *probability* of insertion at a given layer, and `cum_nneighbor_per_level`, the cumulative total of nearest neighbors assigned to a vertex at different insertion levels.\n\n{{< notebook file=\"levels\" height=\"full\" >}}\n\nFrom this, we can see the significantly higher probability of inserting a vector at *level 0* than higher levels (although, as we will explain below, the probability is not exactly as defined here). This function means higher levels are more sparse, reducing the likelihood of 'getting stuck', and ensuring we start with longer range traversals.\n\nOur `assign_probas` vector is used by another method called `random_level` — it is in this function that each vertex is assigned an insertion level.\n\n```python\ndef random_level(assign_probas: list, rng):\n    # get random float from 'r'andom 'n'umber 'g'enerator\n    f = rng.uniform() \n    for level in range(len(assign_probas)):\n        # if the random float is less than level probability...\n        if f < assign_probas[level]:\n            # ... we assert at this level\n            return level\n        # otherwise subtract level probability and try again\n        f -= assign_probas[level]\n    # below happens with very low probability\n    return len(assign_probas) - 1\n```\n\nWe generate a random float using Numpy's random number generator `rng` (initialized below) in `f`. For each `level`, we check if `f` is less than the assigned probability for that level in `assign_probas` — if so, that is our insertion layer.\n\nIf `f` is too high, we subtract the `assign_probas` value from `f` and try again for the next level. The result of this logic is that vectors are *most likely* going to be inserted at *level 0*. Still, if not, there is a decreasing probability of insertion at ease increment level.\n\nFinally, if no levels satisfy the probability condition, we insert the vector at the highest level with `return len(assign_probas) - 1`. If we compare the distribution between our Python implementation and that of Faiss, we see very similar results:\n\n{{< notebook file=\"insertion-run\" height=\"full\" >}}\n\n\n![insertion distribution](/images/hnsw-8.jpg)\n<small>Distribution of vertices across layers in both the Faiss implementation (left) and the Python implementation (right).</small>\n\nThe Faiss implementation also ensures that we *always* have at least one vertex in the highest layer to act as the entry point to our graph.\n\n### HNSW Performance\n\nNow that we've explored all there is to explore on the theory behind HNSW and how this is implemented in Faiss — let's look at the effect of different parameters on our recall, search and build times, and the memory usage of each.\n\nWe will be modifying three parameters: `M`, `efSearch`, and `efConstruction`. And we will be indexing the Sift1M dataset, which you can download and prepare using [this script](https://gist.github.com/jamescalam/a09a16c17b677f2cf9c019114711f3bf).\n\nAs we did before, we initialize our index like so:\n\n```python\nindex = faiss.IndexHNSWFlat(d, M)\n```\n\nThe two other parameters, `efConstruction` and `efSearch` can be modified *after* we have initialized our `index`.\n\n```python\nindex.hnsw.efConstruction = efConstruction\nindex.add(xb)  # build the index\nindex.hnsw.efSearch = efSearch\n# and now we can search\nindex.search(xq[:1000], k=1)\n```\n\nOur `efConstruction` value *must* be set before we construct the index via `index.add(xb)`, but `efSearch` can be set anytime before searching.\n\nLet's take a look at the recall performance first.\n\n\n![recall viz](/images/hnsw-9.jpg)\n<small>Recall@1 performance for various `M`, `efConstruction`, and `efSearch` parameters.</small>\n\nHigh `M` and `efSearch` values can make a big difference in recall performance — and it's also evident that a reasonable `efConstruction` value is needed. We can also increase `efConstruction` to achieve higher recall at lower `M` and `efSearch` values.\n\nHowever, this performance does not come for free. As always, we have a balancing act between recall and search time — let's take a look.\n\n![search time viz](/images/hnsw-10.jpg)\n<small>Search time in µs for various `M`, `efConstruction`, and `efSearch` parameters when searching for `1000` queries. Note that the y-axis is using a log scale.</small>\n\nAlthough higher parameter values provide us with better recall, the effect on search times can be dramatic. Here we search for `1000` similar vectors (`xq[:1000]`), and our recall/search-time can vary from 80%-1ms to 100%-50ms. If we're happy with a rather terrible recall, search times can even reach 0.1ms.\n\nIf you've been following our [articles on vector similarity search](/learn/), you may recall that `efConstruction` has a [negligible effect on search-time](/learn/vector-indexes/) — but that is not the case here...\n\nWhen we search using a few queries, it *is* true that `efConstruction` has little effect on search time. But with the `1000` queries used here, the small effect of `efConstruction` becomes much more significant.\n\nIf you believe your queries will mostly be low volume, `efConstruction` is a great parameter to increase. It can improve recall with little effect on *search time*, particularly when using lower `M` values.\n\n\n![efConstruction search time](/images/hnsw-11.jpg)\n<small>`efConstruction` and search time when searching for only one query. When using lower `M` values, the search time remains almost unchanged for different `efConstruction` values.</small>\n\nThat all looks great, but what about the memory usage of the HNSW index? Here things can get slightly *less* appealing.\n\n![memory viz](/images/hnsw-12.jpg)\n<small>Memory usage with increasing values of `M` using our Sift1M dataset. `efSearch` and `efConstruction` have no effect on the memory usage.</small>\n\nBoth `efConstruction` and `efSearch` do not affect index memory usage, leaving us only with `M`. Even with `M` at a low value of `2`, our index size is already above 0.5GB, reaching almost 5GB with an `M` of `512`.\n\nSo although HNSW produces incredible performance, we need to weigh that against high memory usage and the inevitable high infrastructure costs that this can produce.\n\n#### Improving Memory Usage and Search Speeds\n\nHNSW is not the best index in terms of memory utilization. However, if this is important and using [another index](/learn/vector-indexes/) isn’t an option, we can improve it by compressing our vectors using [product quantization (PQ)](/learn/product-quantization/). Using PQ will reduce recall and increase search time — but as always, much of ANN is a case of balancing these three factors.\n\nIf, instead, we'd like to improve our search speeds — we can do that too! All we do is add an IVF component to our index. There is plenty to discuss when adding IVF or PQ to our index, so we wrote an [entire article on mixing-and-matching of indexes](/learn/composite-indexes/).\n\nThat's it for this article covering the Hierarchical Navigable Small World graph for vector similarity search! Now that you’ve learned the intuition behind HNSW and how to implement it in Faiss, you’re ready to go ahead and test HNSW indexes in your own vector search applications, or use a managed solution like [Pinecone](/) or OpenSearch that has vector search ready-to-go!\n\nIf you’d like to continue learning more about vector search and how you can use it to supercharge your own applications, we have a [whole set of learning materials](/learn/) aiming to bring you up to speed with the world of vector search.\n\n{{< newsletter text=\"Subscribe for the latest in similarity search!\" inputText=\"Email address...\" buttonText=\"Submit\">}}\n\n## References\n\n[1] E. Bernhardsson, [ANN Benchmarks](https://github.com/erikbern/ann-benchmarks) (2021), GitHub\n\n[2] W. Pugh, [Skip lists: a probabilistic alternative to balanced trees](https://15721.courses.cs.cmu.edu/spring2018/papers/08-oltpindexes1/pugh-skiplists-cacm1990.pdf) (1990), Communications of the ACM, vol. 33, no.6, pp. 668-676\n\n[3] Y. Malkov, D. Yashunin, [Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs](https://arxiv.org/abs/1603.09320) (2016), IEEE Transactions on Pattern Analysis and Machine Intelligence\n\n[4] Y. Malkov et al., [Approximate Nearest Neighbor Search Small World Approach](https://www.iiis.org/CDs2011/CD2011IDI/ICTA_2011/PapersPdf/CT175ON.pdf) (2011), International Conference on Information and Communication Technologies & Applications\n\n[5] Y. Malkov et al., [Scalable Distributed Algorithm for Approximate Nearest Neighbor Search Problem in High Dimensional General Metric Spaces](https://www.researchgate.net/publication/262334462_Scalable_Distributed_Algorithm_for_Approximate_Nearest_Neighbor_Search_Problem_in_High_Dimensional_General_Metric_Spaces) (2012), Similarity Search and Applications, pp. 132-147\n\n[6] Y. Malkov et al., [Approximate nearest neighbor algorithm based on navigable small world graphs](https://publications.hse.ru/mirror/pubs/share/folder/x5p6h7thif/direct/128296059) (2014), Information Systems, vol. 45, pp. 61-68\n\n[7] M. Boguna et al., [Navigability of complex networks](https://arxiv.org/abs/0709.0303) (2009), Nature Physics, vol. 5, no. 1, pp. 74-80\n\n[8] Y. Malkov, A. Ponomarenko, [Growing homophilic networks are natural navigable small worlds](https://arxiv.org/abs/1507.06529) (2015), PloS one\n\nFacebook Research, [Faiss HNSW Implementation](https://github.com/facebookresearch/faiss/blob/main/faiss/impl/HNSW.cpp), GitHub"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbb7"
  },
  "filename": "openai-gen-qa.md",
  "title": "post",
  "category": "\"Generative Question-Answering with Long-Term Memory\"",
  "content": "---\nlayout: post\ntitle: \"Generative Question-Answering with Long-Term Memory\"\nheadline: \"Generative Question-Answering with Long-Term Memory\"\ncategories:\n - Question Answering\ntoc: >-\nweight: 2\nauthors:\n - name: James Briggs\n   position: Developer Advocate\n   src: /images/james-briggs.jpeg\n   href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: Building better generative AI models with long-term memory components.\n# Open graph\nimages: [\"/images/openai-gen-qa-0.png\"]\nthumbnail: \"https://www.pinecone.io/images/openai-gen-qa-0.jpg\"\n---\n\nGenerative AI sparked several _\"wow\"_ moments in 2022. From generative art tools like OpenAI's DALL-E 2, Midjourney, and Stable Diffusion, to the next generation of **L**arge **L**anguage **M**odels like OpenAI's GPT-3.5 generation models, BLOOM, and chatbots like LaMDA and ChatGPT.\n\nIt’s hardly surprising that Generative AI is experiencing a boom in interest and innovation [1]. Yet, this marks the *just* first year of generative AI's widespread adoption. The early days of a new field poised to disrupt how we interact with machines.\n\nOne of the most thought-provoking use cases belongs to **G**enerative **Q**uestion-**A**nswering (GQA). Using GQA, we can sculpt human-like interaction with machines for information retrieval (IR).\n\nWe all use IR systems every day. Google search indexes the web and retrieves relevant information to your search terms. Netflix uses your behavior and history on the platform to recommend new TV shows and movies, and Amazon does the same with products [2].\n\nThese applications of IR are world-changing. Yet, they may be little more than a faint echo of what we will see in the coming months and years with the combination of IR and GQA.\n\nImagine a Google that can answer your queries with an intelligent and insightful summary based on the top 20 pages — highlighting key points and information sources.\n\nThe technology available today already makes this possible and surprisingly easy. This article will look at retrieval-enhanced GQA and how to implement it with Pinecone and OpenAI.\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/dRUIGgNBvVk\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n---\n\n# Generative Question-Answering\n\nThe most straightforward GQA system requires nothing more than a user text query and a large language model (LLM).\n\n![gqa-simple](./images/openai-gen-qa-1.png)\n\n<small>Simplest GQA system.</small>\n\nWe can access one of the most advanced LLMs in the world via OpenAI. To start, we sign up for an [API key](https://beta.openai.com).\n\n![openai-api-key](./images/openai-gen-qa-2.png)\n\n<small>After signing up for an account, API keys can be created by clicking on your account (top-right) > View API keys > Create new secret key.</small>\n\nThen we switch to a Python file or notebook, install some prerequisites, and initialize our connection to OpenAI.\n\n{{< notebook file=\"openai-transcriptions\" height=\"full\" >}}\n\nFrom here, we can use the OpenAI completion endpoint to ask a question like _\"who was the 12th person on the moon and when did they land?\" _:\n\n{{< notebook file=\"openai-transcripts-complete-easy\" height=\"full\" >}}\n\nWe get an accurate answer immediately. Yet, this question is relatively easy, what happens if we ask about a lesser-known topic?\n\n{{< notebook file=\"openai-transcripts-complete-harder\" height=\"full\" >}}\n\nAlthough this answer is technically correct, it isn't an answer. It tells us to use a supervised training method and learn the relationship between sentences. Both of these facts are true but do not answer the original question.\n\nThere are two options for allowing our LLM to better understand the topic and, more precisely, answer the question.\n\n1. We fine-tune the LLM on text data covering the domain of fine-tuning sentence transformers.\n2. We use *retrieval-enhanced ML*, meaning we add an information retrieval component to our GQA process. Adding a retrieval step allows us to retrieve relevant information and feed this into the LLM as a _secondary source_ of information.\n\nIn the following sections, we will outline how to implement option **two**.\n\n---\n\n# Building a Knowledge Base\n\nWith option **two** of implementing retrieval, we need an external _\"knowledge base \"_. A knowledge base acts as the place where we store information and as the system that effectively retrieves this information.\n\nA knowledge base is a store of information that can act as an external reference for GQA models. We can think of it as the _\"long-term memory\"_ for AI systems.\n\nWe refer to knowledge bases that can enable the retrieval of semantically relevant information as *vector databases*.\n\nA vector database stores vector representations of information encoded using specific ML models. These models have an \"understanding\" of language and can encode passages with similar meanings into a similar vector space and dissimilar passages into a dissimilar vector space.\n\n![similar-v-dissimilar](./images/openai-gen-qa-3.png)\n\nWe can achieve this with OpenAI via the embed endpoint:\n\n{{< notebook file=\"openai-transcripts-embed\" height=\"full\" >}}\n\nWe'll need to repeat this embedding process over many records that will act as our pipeline's external source of information. These records still need to be downloaded and prepared for embedding.\n\n## Data Preparation\n\nThe dataset we will use in our knowledge base is the `jamescalam/youtube-transcriptions` dataset hosted on Hugging Face _Datasets_. It contains transcribed audio from several ML and tech YouTube channels. We download it with the following:\n\n{{< notebook file=\"openai-transcripts-dataset\" height=\"full\" >}}\n\nThe dataset contains many small snippets of text data. We need to merge several snippets to create more substantial chunks of text that contain more meaningful information.\n\n{{< notebook file=\"openai-transcripts-merge-snippets\" height=\"full\" >}}\n\nWith the text chunks created, we can begin initializing our knowledge base and populating it with our data.\n\n## Creating the Vector Database\n\nThe vector database is the storage and retrieval component in our pipeline. We use Pinecone as our vector database. For this, we need to sign up for a [free API key](https://app.pinecone.io) and enter it below, where we create the index for storing our data.\n\n{{< notebook file=\"openai-transcripts-create-index\" height=\"full\" >}}\n\nThen we embed and index a dataset like so:\n\n{{< notebook file=\"openai-transcripts-indexing\" height=\"full\" >}}\n\nWe're ready to combine OpenAI's `Completion` and `Embedding` endpoints with our Pinecone vector database to create a retrieval-enhanced GQA system.\n\n---\n\n# OP Stack\n\nThe OpenAI Pinecone (OP) stack is an increasingly popular choice for building high-performance AI apps, including retrieval-enhanced GQA.\n\nOur pipeline during *query time* consists of the following:\n\n1. OpenAI `Embedding` endpoint to create vector representations of each query.\n2. Pinecone vector database to search for relevant passages from the database of previously indexed contexts.\n3. OpenAI `Completion` endpoint to generate a natural language answer considering the retrieved contexts.\n\n![retrieval-enhanced-gqa-query](./images/openai-gen-qa-4.png)\n\nWe start by encoding queries using the same encoder model to create a query vector `xq`.\n\n{{< notebook file=\"openai-transcripts-xq\" height=\"full\" >}}\n\nThe query vector `xq` is used to query Pinecone via `index.query`, and previously indexed passage vectors are compared to find the most similar matches — returned in `res` above.\n\nUsing these returned contexts, we can construct a prompt instructing the generative LLM to answer the question based on the retrieved contexts. To keep things simple, we will do all this in a function called `retrieve`.\n\n{{< notebook file=\"openai-transcripts-retrieve\" height=\"full\" >}}\n\n<small>Note that the generated *expanded query* (`query_with_contexts`) has been shortened for readability.</small>\n\nFrom `retrieve`, we produce a longer prompt (`query_with_contexts`) containing some instructions, the contexts, and the original question.\n\nThe prompt is then fed into the generative LLM via OpenAI's `Completion` endpoint. As before, we use the `complete` function to handle everything.\n\n{{< notebook file=\"openai-transcripts-complete-final\" height=\"full\" >}}\n\nBecause of the additional _\"source knowledge\"_ (information fed directly into the model), we have eliminated the hallucinations of the LLM — producing accurate answers to our question.\n\nBeyond providing more factual answers, we also have the *sources* of information from Pinecone used to generate our answer. Adding this to downstream tools or apps can help improve user trust in the system. Allowing users to confirm the reliability of the information being presented to them.\n\n---\n\nThat's it for this walkthrough of retrieval-enhanced **G**enerative **Q**uestion **A**nswering (GQA) systems.\n\nAs demonstrated, LLMs alone work incredibly well but struggle with more niche or specific questions. This often leads to *hallucinations* that are rarely obvious and likely to go undetected by system users.\n\nBy adding a _\"long-term memory\"_ component to our GQA system, we benefit from an external knowledge base to improve system factuality and user trust in generated outputs.\n\nNaturally, there is vast potential for this type of technology. Despite being a new technology, we are already seeing its use in [YouChat](https://blog.you.com/introducing-youchat-the-ai-search-assistant-that-lives-in-your-search-engine-eff7badcd655), several [podcast search apps](https://huberman.rile.yt), and rumors of its upcoming use as a challenger to Google itself [3].\n\nThere is potential for disruption in any place where the need for information exists, and retrieval-enhanced GQA represents one of the best opportunities for taking advantage of the outdated information retrieval systems in use today.\n\n# References\n\n[1] E. Griffith, C. Metz, [A New Area of A.I. Booms, Even Amid the Tech Gloom](https://www.nytimes.com/2023/01/07/technology/generative-ai-chatgpt-investments.html) (2023), NYTimes\n\n[2] G. Linden, B. Smith, J. York, [Amazon.com Recommendations: Item-to-Item Collaborative Filtering](https://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf) (2003), IEEE\n\n[3] T. Warren, [Microsoft to challenge Google by integrating ChatGPT with Bing search](https://www.theverge.com/2023/1/4/23538552/microsoft-bing-chatgpt-search-google-competition) (2023), The Verge\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbb8"
  },
  "filename": "bertopic.md",
  "title": "post",
  "category": "\"BERTopic",
  "content": "---\nlayout: post\ntitle: \"BERTopic: The Future of Topic Modeling\"\nheadline: \"Advanced Topic Modeling with BERTopic\"\ncategories:\n  - Algorithms & Libraries\ntoc: >-\nweight: 1\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: Demystifying BERTopic and how it works with transformers, UMAP, HDBSCAN, and c-TF-IDF.\n# Open graph\nimages: [\"/images/bertopic-0.png\"]\nthumbnail: \"https://www.pinecone.io/images/bertopic-0.png\"\n---\n\n90% of the world's data is unstructured. It is built by humans, for humans. That's great for human consumption, but it is _very_ hard to organize when we begin dealing with the massive amounts of data abundant in today's information age.\n\nOrganization is complicated because unstructured text data is not intended to be understood by machines, and having humans process this abundance of data is wildly expensive and _very slow_.\n\nFortunately, there is light at the end of the tunnel. More and more of this unstructured text is becoming accessible and understood by machines. We can now [search text based on _meaning_](/learn/dense-vector-embeddings-nlp/), identify the sentiment of text, extract entities, and much more.\n\n[Transformers](/learn/transformers/) are behind much of this. These transformers are (unfortunately) not Michael Bay's Autobots and Decepticons and (fortunately) not buzzing electrical boxes. Our NLP transformers lie somewhere in the middle, they're not sentient Autobots (yet), but they can understand language in a way that existed only in sci-fi until a short few years ago.\n\nMachines with a human-like comprehension of language are pretty helpful for organizing masses of unstructured text data. In machine learning, we refer to this task as _topic modeling_, the automatic clustering of data into particular topics.\n\nBERTopic takes advantage of the superior language capabilities of these (not yet sentient) transformer models and uses some other ML magic like UMAP and HDBSCAN (more on these later) to produce what is one of the most advanced techniques in language topic modeling today.\n\n---\n\n## BERTopic at a Glance\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/fb7LENb9eag\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\nWe will dive into the details behind BERTopic [1], but before we do, let us see how we can use it and take a first glance at its components.\n\nTo begin, we need a dataset. We can download the dataset from HuggingFace datasets with:\n\n```python\nfrom datasets import load_dataset\n\ndata = load_dataset('jamescalam/python-reddit')\n```\n\nThe dataset contains data extracted using the Reddit API from the _/r/python_ subreddit. The code used for this (and all other examples) can [be found here](https://github.com/pinecone-io/examples/tree/master/learn/algos-and-libraries/bertopic).\n\nReddit thread contents are found in the `selftext` feature. Some are empty or short, so we remove them with:\n\n```python\ndata = data.filter(\n    lambda x: True if len(x['selftext']) > 30 else 0\n)\n```\n\nWe perform topic modeling using the `BERTopic` library. The _\"basic\"_ approach requires just a few lines of code.\n\n```python\nfrom bertopic import BERTopic\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# we add this to remove stopwords\nvectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\")\n\nmodel = BERTopic(\n    vectorizer_model=vectorizer_model,\n    language='english', calculate_probabilities=True,\n    verbose=True\n)\ntopics, probs = model.fit_transform(text)\n```\n\nFrom `model.fit_transform` we return two lists:\n\n- `topics` contains a one-to-one mapping of inputs to their modeled _topic_ (or cluster).\n- `probs` contains a list of probabilities that an input belongs to their assigned topic.\n\nWe can then view the topics using `get_topic_info`.\n\n{{< notebook file=\"bertopic-glance-topics-table\" height=\"full\" >}}\n\nThe top `-1` topic is typically assumed to be irrelevant, and it usually contains stop words like _\"the\"_, _\"a\"_, and _\"and\"_. However, we removed stop words via the `vectorizer_model` argument, and so it shows us the _\"most generic\"_ of topics like _\"Python\"_, _\"code\"_, and _\"data\"_.\n\nThe library has several built-in visualization methods like `visualize_topics`, `visualize_hierarchy`, and `visualize_barchart`.\n\n![bertopic-viz-hierarchy](/images/bertopic-1.png)\n<small>BERTopic's `visualize_hierarchy` visualization allows us to view the \"hierarchy\" of topics.</small>\n\nThese represent the surface level of the BERTopic library, which has excellent documentation, so we will not rehash that here. Instead, let's try and understand _how_ BERTopic works.\n\n## Overview\n\nThere are _four_ key components used in BERTopic [2], those are:\n\n- A transformer embedding model\n- UMAP dimensionality reduction\n- HDBSCAN clustering\n- Cluster tagging using c-TF-IDF\n\nWe already did _all_ of this in those few lines of BERTopic code; everything is just abstracted away. However, we can optimize the process by understanding the essentials of each component. This section will work through each component _without_ BERTopic, and learn how they work before returning to BERTopic at the end.\n\n### Transformer Embedding\n\nBERTopic supports several libraries for encoding our text to dense vector embeddings. If we build poor quality embeddings, nothing we do in the other steps will be able to help us, so it is _very important_ that we choose a suitable embedding model from one of the supported libraries, which include:\n\n- Sentence Transformers\n- Flair\n- SpaCy\n- Gensim\n- USE (from TF Hub)\n\nOf the above, the _Sentence Transformers_ library provides the most extensive library of high-performing [sentence embedding models](/learn/sentence-embeddings/). They can be found on HuggingFace Hub by searching for _\"sentence-transformers\"_.\n\n![hf-hub-screenshot](/images/bertopic-2.jpg)\n\n<small>We can find _official_ sentence transformer models by searching for _\"sentence-transformers\"_ on HuggingFace Hub.</small>\n\nThe first result of this search is `sentence-transformers/all-MiniLM-L6-v2`, this is a popular high-performing model that creates _384_-dimensional sentence embeddings.\n\nTo initialize the model and encode our Reddit topics data, we first `pip install sentence-transformers` and then write:\n\n{{< notebook file=\"bertopic-embedding\" height=\"full\" >}}\n\nHere we have encoded our text in batches of `16`. Each batch is added to the `embeds` array. Once we have all of the [sentence embeddings](/learn/sentence-embeddings/) in `embeds` we're ready to move on to the next step.\n\n### Dimensionality Reduction\n\nAfter building our embeddings, BERTopic compresses them into a lower-dimensional space. This means that our 384-dimensional vectors are transformed into two/three-dimensional vectors.\n\nWe can do this because 384 dimensions are _a lot_, and it is unlikely that we really need that many dimensions to represent our text [4]. Instead, we attempt to _compress_ that information into two or three dimensions.\n\nWe do this so that the following HDBSCAN clustering step can be done more efficiently. Performing the clustering step with 384-dimensions would be desperately slow [5].\n\nAnother benefit is that we can visualize our data; this is incredibly helpful when assessing whether our data can be clustered. Visualization also helps when tuning the dimensionality reduction parameters.\n\nTo help us understand dimensionality reduction, we will start with a 3D representation of the world. You can find [the code for this part here](https://github.com/pinecone-io/examples/tree/master/learn/algos-and-libraries/bertopic).\n\n{{< plotly obj=\"bertopic-1\" >}}\n\n<small>3D scatter plot of points from the [`jamescalam/world-cities-geo`](https://huggingface.co/datasets/jamescalam/world-cities-geo) dataset.</small>\n\nWe can apply many dimensionality reduction techniques to this data; two of the most popular choices are PCA and t-SNE.\n\n{{< plotly obj=\"bertopic-2\" >}}\n\n<small>Our 2D world reduced using PCA.</small>\n\nPCA works by preserving _larger distances_ (using mean squared error). The result is that the _global structure_ of data is usually preserved [6]. We can see that behavior above as each continent is grouped with its neighboring continent(s). When we have easily distinguishable clusters in datasets, this can be good, but it performs poorly for more nuanced data where _local structures_ are important.\n\n{{< plotly obj=\"bertopic-3\" >}}\n\n<small>2D Earth reduced using t-SNE.</small>\n\nt-SNE is the opposite; it preserves _local structures_ rather than _global_. This localized focus results from t-SNE building a graph, connecting all of the nearest points. These local structures can indirectly suggest the global structure, but they are not strongly captured.\n\n---\n\nPCA focuses on preserving _dissimilarity_ whereas t-SNE focuses on preserving _similarity_.\n\n---\n\nFortunately, we can capture the best of both using a lesser-known technique called **U**niform **M**anifold **A**pproximation and **P**roduction (UMAP).\n\nWe can apply UMAP in Python using the UMAP library, installed using `pip install umap-learn`. To map to a 3D or 2D space using the default UMAP parameters, all we write is:\n\n```python\nimport umap\n\nfit = umap.UMAP(n_components=3)  # by default this is 2\nu = fit.fit_transform(data)\n```\n\nThe UMAP algorithm can be fine-tuned using several parameters. Still, the simplest and most effective tuning can be achieved with just the `n_neighbors` parameter.\n\nFor each datapoint, UMAP searches through other points and identifies the **k**th nearest neighbors [3]. It is **k**, controlled by the `n_neighbors` parameter.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/bertopic-9.mp4\" type=\"video/mp4\">\n</video>\n\n<small>**k** and `n_neighbors` are synonymous here. As we increase `n_neighbors` the graph built by UMAP can consider more distant points and better represent the global structure.</small>\n\nWhere we have many points (high-density regions), the distance between our point and its **k**th nearest neighbor is usually smaller. In low-density regions with fewer points, the distance will be much greater.\n\n![bertopic-umap-density](/images/bertopic-3.png)\n\n<small>Density is measured indirectly using the distances between **k**th nearest neighbors in different regions.</small>\n\nUMAP will attempt to preserve distances to the **k**th nearest point is what UMAP attempts to preserve when shifting to a lower dimension.\n\nBy increasing `n_neighbors` we can preserve more global structures, whereas a lower `n_neighbors` better preserves local structures.\n\n![bertopic-n_neighbors-global](/images/bertopic-4.png)\n<small>Higher `n_neighbors` (**k**) means we preserve larger distances and thus maintain more of the global structure.</small>\n\nCompared to other dimensionality reduction techniques like PCA or t-SNE, finding a good `n_neighbours` value allows us to preserve _both_ local and global structures relatively well.\n\nApplying it to our 3D globe, we can see neighboring countries remain neighbors. At the same time, continents are placed correctly (with North-South inverted), and islands are separated from continents. We even have what seems to be the Spanish Peninsula in \"western Europe\".\n\n{{< plotly obj=\"bertopic-4\" >}}\n\n<small>The UMAP-reduced Earth.</small>\n\nUMAP maintains distinguishable features that are not preserved by PCA and a better global structure than t-SNE. This is a great overall example of where the benefit of UMAP lies.\n\nUMAP can also be used as a supervised dimensionality reduction method by passing labels to the `target` argument if we have labeled data. It is possible to produce even more meaningful structures using this supervised approach.\n\nWith all that in mind, let us apply UMAP to our Reddit topics data. Using `n_neighbors` of `3`-`5` seems to work best. We can add `min_dist=0.05` to allow UMAP to place points closer together (the default value is `1.0`); this helps us separate the three _similar_ topics from _r/Python_, _r/LanguageTechnology_, and _r/pytorch_.\n\n```python\nfit = umap.UMAP(n_neighbors=3, n_components=3, min_dist=0.05)\nu = fit.fit_transform(embeds)\n```\n\n{{< plotly obj=\"bertopic-5\" >}}\n\n<small>Reddit topics data reduced to 3D space using UMAP.</small>\n\nWith our data reduced to a lower-dimensional space and topics easily visually identifiable, we're in an excellent spot to move on to clustering.\n\n### HDBSCAN Clustering\n\nWe have visualized the UMAP reduced data using the existing _sub_ feature to color our clusters. It looks pretty, but we don't usually perform topic modeling to label already labeled data. If we assume that we have no existing labels, our UMAP visual will look like this:\n\n{{< plotly obj=\"bertopic-6\" >}}\n\n<small>UMAP reduced cities data, we can distinguish many clusters/continents, but it is much more difficult without label coloring.</small>\n\nNow let us look at how HDBSCAN is used to cluster the (now) low-dimensional vectors.\n\nClustering methods can be broken into flat or hierarchical _and_ centroid or density-based techniques [5]. Each of which has its own benefits and drawbacks.\n\nFlat or hierarchical focuses simply on whether there is (or is not) a hierarchy in the clustering method. For example, we may (ideally) view our graph hierarchy as moving from continents to countries to cities. These methods allow us to view a given hierarchy and try to identify a logical \"cut\" along the tree.\n\n![bertopic-hierarchy](/images/bertopic-5.png)\n<small>Hierarchical techniques begin from one large cluster and split this cluster into smaller and smaller parts and try to find the *ideal* number of clusters in the hierarchy.</small>\n\nThe other split is between centroid-based or density-based clustering. That is clustering based on proximity to a centroid or clustering based on the density of points. Centroid-based clustering is ideal for _\"spherical\"_ clusters, whereas density-based clustering can handle more irregular shapes _and_ identify outliers.\n\n![bertopic-density-centroid](/images/bertopic-6.png)\n<small>Centroid-based clustering (left) vs density-based clustering (right).</small>\n\nHDBSCAN is a hierarchical, density-based method. Meaning we can benefit from the easier tuning and visualization of hierarchical data, handle irregular cluster shapes, _and_ identify outliers.\n\nWhen we first apply HDBSCAN clustering to our data, we will return _many_ tiny clusters, identified by the red _circles_ in the _condensed tree plot_ below.\n\n{{< notebook file=\"bertopic-hdbscan-tree\" height=\"full\" >}}\n\n---\n\n_The condensed tree plot shows the drop-off of points into outliers and the splitting of clusters as the algorithm scans by increasing lambda values._\n\n_HDBSCAN chooses the final clusters based on their size and persistence over varying lambda values. The tree's thickest, most persistent \"branches\" are viewed as the most stable and, therefore, best candidates for clusters._\n\n---\n\nThese clusters are not very useful because the _default_ minimum number of points needed to \"create\" a cluster is just `5`. Given our ~3K points dataset where we aim to produce ~4 subreddit clusters, this is small. Fortunately, we can increase this threshold using the `min_cluster_size` parameter.\n\n{{< notebook file=\"bertopic-hdbscan-tree-2\" height=\"full\" >}}\n\nBetter, but not quite there, we can try to reduce the `min_cluster_size` to `60` to pull in the three clusters below the green block.\n\n{{< notebook file=\"bertopic-hdbscan-tree-3\" height=\"full\" >}}\n\nUnfortunately, this still pulls in the green block and even allows _too small_ clusters (as on the left). Another option is to keep `min_cluster_size=80` but add `min_samples=40`, to allow for more sparse core points.\n\n{{< notebook file=\"bertopic-hdbscan-tree-4\" height=\"full\" >}}\n\nNow we have four clusters, and we can visualize them using the data in `clusterer.labels_`.\n\n{{< plotly obj=\"bertopic-7\" >}}\n\n<small>HDBSCAN clustered Reddit topics, accurately identifying the different subreddit clusters. The sparse blue points are outliers and are not identified as belonging to any cluster.</small>\n\nA few outliers are marked in blue, some of which make sense (pinned daily discussion threads) and others that do not. However, overall, these clusters are very accurate. With that, we can try to identify the meaning of these clusters.\n\n### Topic Extraction with c-TF-IDF\n\nThe final step in BERTopic is extracting topics for each of our clusters. To do this, BERTopic uses a modified version of TF-IDF called c-TF-IDF.\n\nTF-IDF is a popular technique for identifying the most relevant _\"documents\"_ given a term or set of terms. c-TF-IDF turns this on its head by finding the most relevant _terms_ given all of the \"documents\" within a cluster.\n\n![bertopic-ctfidf-intuition](/images/bertopic-7.png)\n<small>c-TF-IDF looks at the most relevant terms from each class (cluster) to create topics.</small>\n\nIn our Reddit topics dataset, we have been able to identify very distinct clusters. However, we still need to determine what these clusters talk about. We start by preprocessing the `selftext` to create tokens.\n\n{{< notebook file=\"bertopic-ctfidf-tokens\" height=\"full\" >}}\n\nPart of c-TF-IDF requires calculating the frequency of term _t_ in class _c_. For that, we need to see which tokens belong in each class. We first add the cluster/class labels to `data`.\n\n```python\n# add the cluster labels to our dataset\ndata = data.add_column('class', clusterer.labels_)\n```\n\nNow create class-specific lists of tokens.\n\n```python\nclasses = {label: {'tokens': []} for label in set(clusterer.labels_)}\n# add tokenized sentences to respective class\nfor row in data:\n    classes[row['class']]['tokens'].extend(row['tokens'])\n```\n\nWe can calculate **T**erm **F**requency (TF) per class.\n\n{{< notebook file=\"bertopic-ctfidf-tf\" height=\"full\" >}}\n\nNote that this can take some time; our TF process prioritizes readability over any notion of efficiency. Once complete, we're ready to calculate **I**nverse **D**ocument **F**requency (IDF), which tells us how common a term is. Rare terms signify greater relevance than common terms (and will output a greater IDF score).\n\n{{< notebook file=\"bertopic-ctfidf-idf\" height=\"full\" >}}\n\nWe now have TF and IDF scores for every term, and we can calculate the c-TF-IDF score by simply multiplying both.\n\n{{< notebook file=\"bertopic-ctfidf-tfidf\" height=\"full\" >}}\n\nIn `tf_idf`, we have a `vocab` sized list of TF-IDF scores for each class. We can use Numpy's `argpartition` function to retrieve the index positions containing the greatest TF-IDF scores per class.\n\n{{< notebook file=\"bertopic-ctfidf-top-idx\" height=\"full\" >}}\n\nNow we map those index positions back to the original words in the `vocab`.\n\n{{< notebook file=\"bertopic-ctfidf-top-terms\" height=\"full\" >}}\n\nHere we have the top five most relevant words for each cluster, each identifying the most relevant topics in each subreddit.\n\n## Back to BERTopic\n\nWe've covered a considerable amount, but can we apply what we have learned to the BERTopic library?\n\nFortunately, all we need are a few lines of code. As before, we initialize our custom embedding, UMAP, and HDBSCAN components.\n\n```python\nfrom sentence_transformers import SentenceTransformer\nfrom umap import UMAP\nfrom hdbscan import HDBSCAN\n\nembedding_model = SentenceTransformer('all-MiniLM-L6-v2')\numap_model = UMAP(n_neighbors=3, n_components=3, min_dist=0.05)\nhdbscan_model = HDBSCAN(min_cluster_size=80, min_samples=40,\n                        gen_min_span_tree=True,\n                        prediction_data=True)\n```\n\nYou might notice that we have added `prediction_data=True` as a new parameter to `HDBSCAN`. We need this to avoid an **AttributeError** when integrating our custom HDBSCAN step with BERTopic. Adding `gen_min_span_tree` adds another step to HDBSCAN that can improve the resultant clusters.\n\nWe must also initialize a `vectorizer_model` to handle stopword removal during the c-TF-IDF step. We will use the list of stopwords from NLTK but add a few more tokens that seem to pollute the results.\n\n```python\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\n\nstopwords = list(stopwords.words('english')) + ['http', 'https', 'amp', 'com']\n\n# we add this to remove stopwords that can pollute topcs\nvectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=stopwords)\n```\n\nWe're now ready to pass all of these to a `BERTopic` instance and process our Reddit topics data.\n\n```python\nfrom bertopic import BERTopic\n\nmodel = BERTopic(\n    umap_model=umap_model,\n    hdbscan_model=hdbscan_model,\n    embedding_model=embedding_model,\n    vectorizer_model=vectorizer_model,\n    top_n_words=5,\n    language='english',\n    calculate_probabilities=True,\n    verbose=True\n)\ntopics, probs = model.fit_transform(data['selftext'])\n```\n\nWe can visualize the new topics with `model.visualize_barchart()`\n\n![bertopic-final-barchart](/images/bertopic-8.png)\n\n<small>Our final topics produced using the BERTopic library with the tuned UMAP and HDBSCAN parameters.</small>\n\nWe can see the topics align perfectly with _r/investing_, _r/pytorch_, _r/LanguageTechnology_, and _r/Python_.\n\nTransformers, UMAP, HDBSCAN, and c-TF-IDF are clearly powerful components that have huge applications when working with unstructured text data. BERTopic has abstracted away much of the complexity of this stack, allowing us to apply these technologies with nothing more than a few lines of code.\n\nAlthough BERTopic can be simple, you have seen that it is possible to dive quite deeply into the individual components. With a high-level understanding of those components, we can greatly improve our topic modeling performance.\n\nWe have covered the essentials here, but we genuinely are just scratching the surface of topic modeling in this article. There is much more to BERTopic and each component than we could ever hope to cover in a single article.\n\nSo go and apply what you have learned here, and remember that despite showing the incredible performance of BERTopic shown here, there is even more that it can do.\n\n{{< newsletter text=\"Subscribe for the latest in NLP!\" inputText=\"Email address...\" buttonText=\"Submit\">}}\n\n## Resources\n\n🔗 [All Notebook Scripts](https://github.com/pinecone-io/examples/tree/master/learn/algos-and-libraries/bertopic)\n\n[1] M. Grootendorst, [BERTopic Repo](https://github.com/MaartenGr/BERTopic), GitHub\n\n[2] M. Grootendorst, [BERTopic: Neural Topic Modeling with a Class-based TF-IDF Procedure](https://arxiv.org/abs/2203.05794) (2022)\n\n[3] L. McInnes, J. Healy, J. Melville, [UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction](https://arxiv.org/abs/1802.03426) (2018)\n\n[4] L. McInnes, [Talk on UMAP for Dimension Reduction](https://www.youtube.com/watch?v=nq6iPZVUxZU) (2018), SciPy 2018\n\n[5] J. Healy, [HDBSCAN, Fast Density Based Clustering, the How and the Why](https://www.youtube.com/watch?v=dGsxd67IFiU) (2018), PyData NYC 2018\n\n[6] L. McInnes, [A Bluffer's Guide to Dimension Reduction](https://www.youtube.com/watch?v=9iol3Lk6kyU) (2018), PyData NYC 2018\n\n[UMAP Explained](https://www.youtube.com/watch?v=6BPl81wGGP8), AI Coffee Break with Letitia, YouTube\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbb9"
  },
  "filename": "embeddings-identify-fake-news.md",
  "title": "post",
  "category": "\"Embeddings to Identify Fake News\"",
  "content": "---\nlayout: post\ntitle: \"Embeddings to Identify Fake News\"\nheadline: \"Embeddings to Identify Fake News\"\ncategories:\n  - Applications of Vector Search\ntoc: >-\nweight: 4\nauthor:\n  name: Diego Lopez Yse\n  position: Data Scientist\n  src: /images/diego-lopez-yse.jpeg\n  href: \"https://lopezyse.medium.com/\"\ndescription: Learn how Machine Learning can be used to identify fake news.\n# Open Graph\nimages: ['/images/embeddings-to-identify-fake-news-4.png']\n---\n\n\n**How to fight misinformation in the digital era.**\n\nTimes of crisis are perfect for controversial statements. Do you think COVID-19 was genetically engineered as a biological weapon? And that climate policies will hurt working families and ruin the economy? What about Bitcoin? Is it the latest economic revolution, or a massive fraudulent system? Hard to tell since we’re living in the era of fake news.\n\n**Fake news** consists of deliberate misinformation under the guise of being authentic news, spread via some communication channel, and produced with a particular objective like generating revenue, promoting or discrediting a public figure, a political movement, an organization, etc.\n\n![Total facebook engagements for top 20 election stories](/images/embeddings-to-identify-fake-news-1.png)\n<small>Analysis shows how Viral Fake Election News Stories outperformed Real News on Facebook during the US presidential elections in 2016. Source: [University of Windsor](https://scholar.uwindsor.ca/cgi/viewcontent.cgi?article=9401&context=etd)</small>\n\nFake news travels [six times faster](https://www.wbur.org/hereandnow/2018/03/09/social-media-buzz-study-false-news) on Twitter and reaches significantly more people than actual news. And from the big universe of fake news, false political news travels [farther, faster, deeper, and more broadly](https://news.mit.edu/news-clip/wbur-136) than any other type.\n\nDuring the [2018 national elections in **Brazil**](https://www.nytimes.com/2018/10/17/opinion/brazil-election-fake-news-whatsapp.html), WhatsApp was used to spread alarming amounts of misinformation, rumours and false news favouring Jair Bolsonaro. In 2019, the two main Indian political parties took these tactics to a new scale by [trying to influence India’s 900 million eligible voters](https://www.hindustantimes.com/india-news/bjp-plans-a-whatsapp-campaign-for-2019-lok-sabha-election/story-lHQBYbxwXHaChc7Akk6hcI.html) through creating content on Facebook and spreading it on WhatsApp.\n\nBut these tactics aren’t only applied in the political arena: They also involve activities from [manipulating share prices](https://www.ft.com/content/32013b6a-202f-11ea-b8a1-584213ee7b2b) to attacking commercial rivals with fake customer critics.\n\nHow can we deal with this problem? Are we supposed to just live with it? Fortunately we don’t have to, and we can use Machine Learning to identify fake news. Let’s see how.\n\n### Embeddings\n\nFake documents and articles contain attributes and linguistic signs that can be used to reveal patterns. Considering their textual components, features like [author, context, and writing style](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8053013/) can help in identifying fake news.\n\nBut before applying any Machine Learning technique to text, we need to format the input data into a numerical representation that can be understood by the model we’re building. Here’s where the problem begins: Traditional techniques like Term frequency–inverse document frequency ([TF-IDF](https://lopezyse.medium.com/nlp-text-vectorization-e472a3a9983a)) result in high-dimensional representations of linguistic information, which lead to some negative effects like the [Curse of Dimensionality](https://analyticsindiamag.com/curse-of-dimensionality-and-what-beginners-should-do-to-overcome-it/), increasing the error of our model as the number of features increases.\n\nOne way to overcome these problems is through word and [sentence embeddings](https://www.pinecone.io/learn/sentence-embeddings/), which give us low dimensional, distributed representations of the data. **Embeddings are representations of words or sentences** in multidimensional spaces such that words or sentences with similar meanings have similar embeddings. It means that each word or sentence is [mapped to the vector of real numbers](https://orangedatamining.com/blog/2020/2020-10-15-document-embedders/) that represents those words or sentences.\n\nThis is what an embedding for the word “king” looks like:\n\n![Vector embedding representation example](/images/embeddings-to-identify-fake-news-2.png)\n<small>Example of a vector embedding representation (GloVe trained on Wikipedia). Source: [Jay Alammar](https://jalammar.github.io/)</small>\n\n\n[Embeddings](https://www.pinecone.io/learn/vector-embeddings/) not only convert the word or text to a numerical representation, but also identify and incorporate their semantic and syntax information.\n\n![Visualizations of real embeddings](/images/embeddings-to-identify-fake-news-3.png)\n<small>Position (distance and direction) in the vector space can encode semantics in a good embedding. For example, the following visualizations of real embeddings show geometrical relationships that capture semantic relations like the relation between a country and its capital. This sort of meaningful space gives your machine learning system opportunities to detect patterns that may help with the learning task. Source: [Google](https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space)</small>\n\nEmbeddings are distributed representations of words and text in a continuous vector space and can facilitate tasks like [semantic search](https://www.pinecone.io/learn/semantic-search/), clustering, recommendation, sentiment analysis, [question-answering](https://www.pinecone.io/learn/question-answering/), or deduplication.\n\n### How to create embeddings\n\nThere are a number of ways to create embeddings and many techniques for capturing the important structure of a high dimensional space in a low dimensional space. [Although methods like principal component analysis (PCA) have been used to create embedding](https://developers.google.com/machine-learning/crash-course/embeddings/obtaining-embeddings), newer techniques have shown better performance. Let’s begin with word embeddings.\n\n#### Word embeddings\n\nWord embeddings are vectorized representations of words and perhaps one of the most important innovations in the [Natural Language Processing](https://www.pinecone.io/learn/dense-vector-embeddings-nlp/) (NLP) discipline. Let’s see look at of the main algorithms already in use:\n\n#### Word2vec\n\nSince its inception in 2013, Word2vec has become widely used both in research and commercial areas. The idea behind it is that it’s possible to **predict a word based on its context** (neighbouring words) under the assumption that the [meaning of a word can be inferred by the company it keeps](https://kavita-ganesan.com/comparison-between-cbow-skipgram-subword/#.YYwHZ2DMI2w). Word2vec can use two architectures to produce a distributed representation of words: continuous bag-of-words or **CBOW** (where we predict the current word from a window of surrounding context words) and **Skip-gram** (where we try to predict the context words using the main word).\n\n![Word2vec algorithm](/images/embeddings-to-identify-fake-news-4.png)\n<small>The Word2vec algorithm implements both CBOW and Skip-gram. The basic idea behind the two training models is that either a word is used to predict the context of it (Skip-gram) or the other way around, to use the context to predict a current word (CBOW). This task is iterated word by word over the corpus. Source: [ResearchGate](https://www.researchgate.net/figure/Continuous-Bag-of-words-CBOW-CB-and-Skip-gram-SG-training-model-illustrations_fig1_326588219)</small>\n\n#### GloVe\n\nDeveloped by Stanford, GloVe (Global Vectors for Word Representation) is another method to create word embeddings. Its advantage over Word2Vec is that it doesn’t rely just on local statistics (local context information of the words), but incorporates global statistics (word co-occurrence) from the whole text corpus.\n\nGloVe uses **co-occurrence** (how frequently two words appear together) statistics at a global text level to model the vector representations of words. This is an important aspect since word-word co-occurrences may carry rich semantic information. [For example](https://d2l.ai/chapter_natural-language-processing-pretraining/glove.html), in a large corpus the word “solid” is more likely to co-occur with “ice” than with “steam”, but the word “gas” probably co-occurs with “steam” more frequently than with “ice”.\n\n#### Fasttext\n\nFasttext was developed by Facebook with the idea to use the **internal structure of words** to improve vector representations (creating “sub-words” from words), which is a huge advantage over other models.\n\nBoth in Word2Vec and GloVe an embedding is created for each word, and, as such, they can’t handle any words they haven’t encountered during their training. Alternatively, Fasttext can [derive word vectors for unknown words](https://medium.com/analytics-vidhya/word-embeddings-in-nlp-word2vec-glove-fasttext-24d4d4286a73), creating embeddings for **words that weren’t seen before**.\n\n![Fasttext embeddings](/images/embeddings-to-identify-fake-news-5.png)\n<small>Fasttext generates character n-grams of length n. For example, for the word “eating”, character n-grams of length 3 can be generated by sliding a window of 3 characters from the start of the angular bracket till the ending angular bracket is reached. Source: [A Visual Guide to FastText Word Embeddings](https://amitness.com/2020/06/fasttext-embeddings/)</small>\n\n\n### Sentence embeddings\n\nWord embeddings are highly effective for tasks that don’t require comparisons between sentences or documents. But using word embeddings over large pieces of text can limit our understanding if we need to compute the semantic similarity of different texts, analyse intention, sentiment, or cluster them.\n\n![Sentence embeddings](/images/embeddings-to-identify-fake-news-6.png)\n<small>Pairwise semantic similarity comparison via outputs from TensorFlow Hub Universal Sentence Encoder. Source: [Google AI Blog](https://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html)</small>\n\nWhat if, instead of embedding individual words, we could embed sentences? Sentence embeddings are the extension of the key ideas behind word embeddings.\n\n#### Doc2vec\n\nBased on the Word2vec algorithm, Doc2vec represents pieces of texts (ranging from sentences to documents) as fixed-length, low dimensional embeddings. In this architecture, the two learning algorithms are Distributed Memory version of Paragraph Vector (**PV-DM**), and Distributed Bag of Words version of Paragraph Vector (**PV-DBOW**).\n\nIn PV-DM, [a **paragraph “id”** is inserted as another word](https://gab41.lab41.org/doc2vec-to-assess-semantic-similarity-in-source-code-667acb3e62d7) in an ordered sequence of words. PV-DM attempts to predict a word in the ordered sequence based on the other surrounding words in the sentence and the context provided by the paragraph “id”. On the other hand, PV-DBOW takes a given paragraph “id” and uses it to predict words in the window without any restriction on word order.\n\n#### SentenceBERT\n\nBERT is a word embedding algorithm ([Transformer](https://www.pinecone.io/learn/transformers/) based) that can **encode the context of words**. This means that while standard word embedding algorithms would produce the same vector for “bank” whether it was “a grassy bank” or “the bank of England”, BERT would instead [modify the encoding for “bank” based on the surrounding context](https://www.pinecone.io/learn/dense-vector-embeddings-nlp/).\n\nThis capability was extended from words to sentences through SentenceBERT (SBERT), which outputs sentence embeddings with an impressive **speed** performance. While finding the most similar sentence pair from 10K sentences took 65 hours with BERT, [SBERT embeddings](https://www.pinecone.io/learn/sentence-embeddings/) are created in around 5 seconds and compared with cosine similarity in around 0.01 seconds.\n\n#### Universal Sentence Encoder (USE)\n\nOne technique that proved superior performance is the Universal Sentence Encoder (USE) developed by Google.\n\nThe idea is to design [an embedding to solve multiple tasks](https://amitness.com/2020/06/universal-sentence-encoder/) and, based on the mistakes it makes on those, **update** the sentence embedding. Since the same embedding has to work on multiple generic tasks, it captures only the most informative features and discards noise. The intuition is that this will result in a generic embedding that transfers universally to a wide variety of NLP tasks such as relatedness, clustering, paraphrase detection and text classification.\n\n![Universal Sentence Encoder representation](/images/embeddings-to-identify-fake-news-7.png)\n<small>A representation of the Universal Sentence Encoder (USE). Source: [Universal Sentence Encoder Visually Explained](https://amitness.com/2020/06/universal-sentence-encoder/)</small>\n\nUSE comes with two variations: one trained with Transformers and the other trained with Deep Averaging Network (DAN). While the one [using Transformers has higher accuracy](https://www.analyticsinsight.net/googles-universal-sentence-encoder-would-revolutionize-the-application-of-neural-network/), it is computationally more intensive. Alternatively, the DAN variant aims at [efficient inference](https://analyticsindiamag.com/guide-to-universal-sentence-encoder-with-tensorflow/) despite a [slightly reduced accuracy](https://amitness.com/2020/06/universal-sentence-encoder/).\n\n### A classification problem\n\nFrom a Machine Learning perspective, the challenge of identifying fake news can be translated into a **classification problem**: Given a text input, we want to classify it correctly as fake or not fake. Using embeddings in a classification task, we can classify the label of an unknown object (e.g. an unseen article) based on the labels of the most similar known objects (e.g. articles already labelled as “fake” or “not fake”).\n\nEmbedding models like [Word2vec (for words) and Doc2vec (for sentences) can serve as feature extraction methods for the creation of various classifications tasks](https://www.researchgate.net/publication/351285564_Fake_news_detection_based_on_word_and_document_embedding_using_machine_learning_classifiers). After creating embeddings for our texts, we can train our model on these embeddings so it can learn to differentiate between misleading facts and real news. The classifier model is then evaluated and adjusted based on the results.\n\n![Data processing and classification](/images/embeddings-to-identify-fake-news-8.png)\n<small>Starting with pre-processing the dataset by removing unnecessary characters and words from the data, a matrix of features is formed representing the documents involved. The next step in the process is to train the classifier, so it can distinguish between fake and real news. Source: [ResearchGate](https://www.researchgate.net/publication/322128415_Detecting_opinion_spams_and_fake_news_using_text_classification)</small>\n\nThe process of converting text into embeddings results in fixed-length vector representations that attempt to encode each sentence key information. Next, we can compare the similarity (e.g. [cosine similarity](https://www.pinecone.io/learn/what-is-similarity-search/)) between sentence embeddings in our dataset to identify potential references to fake news.\n\n### The future of misinformation\n\nToday, machines are playing a key role in the fake news arena. Text-generation systems like [GPT-3](https://singularityhub.com/2020/06/18/openais-new-text-generator-writes-even-more-like-a-human/) have the ability to produce coherent text from minimal prompts: Feed it a title, and it will write a story. But can these systems also spot other model-generated outputs?\n\nMachine language models like GPT-3 produce sentences by predicting the next word in a sequence of text. So, if they can predict most of the words in a given passage, it’s likely it was written by one of their own. [This idea](https://www.technologyreview.com/2019/03/12/136668/an-ai-for-generating-fake-news-could-also-help-detect-it/) was tested using the [Giant Language model Test Room](http://gltr.io/dist/index.html) (GLTR), where human-generated text showed a higher fraction of red and purple words (meaning a decreasing ease of predictability) in contrast to machine-generated text that showed a greater share of green and yellow words (meaning it was likely written by a language model).\n\n![Passage written by a human](/images/embeddings-to-identify-fake-news-9.png)\n<small>A reading comprehension passage from a US standardized test, written by a human. Source: [MIT Technology Review](https://www.technologyreview.com/2019/03/12/136668/an-ai-for-generating-fake-news-could-also-help-detect-it/)</small>\n\n![Passage written by GPT-2](/images/embeddings-to-identify-fake-news-10.png)\n<small>A passage written by OpenAI’s downgraded GPT-2. Source: [MIT Technology Review](https://www.technologyreview.com/2019/03/12/136668/an-ai-for-generating-fake-news-could-also-help-detect-it/)</small>\n\n\nBut here’s the catch: a machine language model might be good at detecting its own output, but [not necessarily the output of others](https://www.aiweirdness.com/it-takes-a-bot-to-know-one-19-03-08/).\n\nWith machines able to mass produce content, the future of fake news is a challenging one. But technology can also be used to solve this problem. Improving the way machines represent and process content can lead the way to new solutions. Specifically, **embeddings** can serve as an effective method to represent massive amounts of texts (or other types of content) in order to improve our understanding about fake news.\n\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbba"
  },
  "filename": "retriever-models.md",
  "title": "ebook-post",
  "category": "\"Retrievers for Question-Answering\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Retrievers for Question-Answering\"\nheadline: \"Retriever Models for Open Domain Question-Answering\"\ncategories:\n  - NLP for Semantic Search\ntoc: >-\nweight: 8\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: How to fine-tune retriever models to find relevant contexts in vector databases.\n# Open graph\nimages: ['/images/train-retriever-0.jpg']\n---\n\nIt's a sci-fi staple. A vital component of the legendary Turing test. The dream of many across the world. And, until recently, impossible.\n\nWe are talking about the ability to ask a machine a question and receive a genuinely intelligent, insightful answer.\n\nUntil recently, technology like this existed only in books, Hollywood, and our collective imagination. Now, it is everywhere. Most of us use this technology every day, and we often don't even notice it.\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/w1dMEWm7jBc\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\nGoogle is just one example. Over the last few years, Google has gradually introduced an intelligent question-answering angle to search. When we now ask *how do I tie my shoelaces?\"* Google gives us the 'exact answer' alongside the *context* or video this answer came from:\n\n![image-20220104094649638](/images/train-retriever-1.png)\n<small>In response to our question, Google finds the exact (audio-to-text) answer to be *\"Start by taking the first lace. And place it behind the second one...\"*, and highlights the exact part of the video that contains this extracted answer.</small>\n\nWe can ask other questions like *\"Is Google Skynet?\"* and this time return an even more precise answer **\"Yes\"**.\n\n![image-20220104095121529](/images/train-retriever-2.png)\n<small>At least Google is honest.</small>\n\nIn this example, Google returns an exact answer and the *context* (paragraph) from where the answer is extracted.\n\nHow does Google do this? And more importantly, why should we care?\n\nThis search style emulates a human-like interaction. We're asking a question in natural language as if we were speaking to another person. This natural language Q&A creates a very different search experience to traditional search.\n\nImagine you find yourself in the world's biggest warehouse. You have no idea how the place is organized. All you know is that your task is to find some round marble-like objects.\n\nWhere do you start? Well, we need to figure out how the warehouse is organized. Maybe everything is stored alphabetically, categorized by industry, or intended use. The traditional search interface requires that we understand how the warehouse is structured before we begin searching. Often, there is a specific 'query language' such as:\n\n```sql\nSELECT * WHERE 'material' == 'marble'\n\nor\n\n(\"marble\" | \"stone\") & \"product\"\n```\n\nOur first task is to learn this query language so we can search. Once we understand how the warehouse is structured, we use that knowledge to begin our search. How do we find *\"round marble-like objects\"*? We can narrow our search down using similar *queries* to those above, but we are in the world's biggest warehouse, so this will take a *very* long time.\n\nWithout a natural Q&A-style interface, this is your search. Unless your users know the ins and outs of the warehouse and its contents, they're going to struggle.\n\nWhat happens if we add a natural Q&A-style interface to the warehouse? Imagine we now have people in the warehouse whose entire purpose is to guide us through the warehouse. These people know exactly where everything is.\n\nThose people can understand our question of *\"where can I find the round marble-like objects?\"*. It may take a few tries until we find the exact object we're looking for, but we now have a guide that understands our question. There is no longer the need to understand how the warehouse is organized nor to *know* the exact name of what it is we're trying to find.\n\nWith this natural Q&A-style interface, your users now have a guide. They just need to be able to ask a question.\n\n\n## Answering Questions\n\nHow can we design these natural, human-like Q&A interfaces? The answer is **o**pen-**d**omain **q**uestion-**a**nswering (ODQA). ODQA allows us to use natural language to query a database.\n\nThat means that, given a dataset like a set of internal company documents, online documentation, or as is the case with Google, everything on the world's internet, we can retrieve relevant information in a natural, more human way.\n\nHowever, ODQA is not a single model. It is more complex and requires three primary components.\n\n* A **[vector database](/learn/vector-database/)** to store information-rich vectors that numerically represent the *meaning* of *contexts* (paragraphs that we use to extract answers to our questions).\n* The **retriever** model encodes questions and contexts into the same vector space. It is these context vectors that we later store in the vector database. The retriever also encodes questions to be compared to the context vectors in a *vector database* to *retrieve* the most relevant contexts.\n* A **reader** model takes a question and context and attempts to identify a *span* (sub-section) from the context which answers the question.\n\nBuilding a retriever model is our focus here. Without it, there is no ODQA; it is arguably the most critical component in the whole process. We *need* our retriever model to return relevant results; otherwise, the reader model will receive and output garbage.\n\nIf we instead had a mediocre reader model, it may still return garbage to us, but it has a much smaller negative impact on the ODQA pipeline. A good retriever means we can at least retrieve relevant contexts, therefore successfully returning relevant information to the user. A paragraph-long context isn't as clean-cut as a perfectly framed two or three-word answer, but it's better than nothing.\n\nOur focus in this article is on building a *retriever* model, of which the *vector database* is a crucial component, as we will see later.\n\n### Train or Not?\n\nDo we need to fine-tune our retriever models? Or can we use pretrained models like those in the [HuggingFace model hub](https://huggingface.co/models)?\n\nThe answer is: *It depends*. An excellent concept from Nils Reimers describes the difficulty of benchmarking models where the use case is within a niché domain that very few people would understand. The idea is that most benchmarks and datasets focus on this short head of knowledge (where most people understand), whereas the most exciting use cases belong in the long-tail portion of the graph [1].\n\n![long_tailed_semantic_relatedness_nils](/images/train-retriever-3.jpg)\n\n<small>Nils Reimer's *long tail of semantic relatedness* [1]. The more people that know about something (y-axis), the easier it is to find benchmarks and labeled data (x-axis), but the most interesting use cases belong in the long-tail region.</small>\n\nWe can take the same idea and modify the x-axis to indicate whether we should be able to take a pretrained model or fine-tune our own.\n\n![long_tailed_semantic_relatedness](/images/train-retriever-4.jpg)\n\n<small>The more something is common knowledge (y-axis), the easier it is to find pretrained models that excel in the broader, more general scope. However, as before, most interesting use cases belong in the long-tail, and here is where we would need to fine-tune our own model.</small>\n\nImagine you are walking down your local high street. You pick a stranger at random and ask them the sort of question that you would expect from your use case. Do you think they would get the answer? If there's a good chance they will, you might be able to get away with a pretrained model.\n\nOn the other hand, if you ask this stranger what the difference is between RoBERTa and DeBERTa, there is a very high chance that they will have no idea what you're asking. In this case, you will probably need to fine-tune a retriever model.\n\n\n\n## Fine-Tuning a Retriever\n\nLet's assume the strangers on the street have no chance of answering our questions. Most likely, a custom retriever model is our best bet. But, how do we train/fine-tune a custom retriever model?\n\nThe very first ingredient is *data*. Our retriever consumes a question and returns relevant contexts to us. For it to do this, it must learn to encode similar question-context pairs into the same vector space.\n\n![encode_question_context](/images/train-retriever-5.jpg)<small>The retriever model must learn to encode similar question-context pairs into a similar vector space.</small>\n\nOur first task is to find and create a set of question-context pairs. One of the best-known datasets for this is the **S**tanford **Q**uestion **A**nswering **D**ataset (SQuAD).\n\n### Step One: Data\n\nSQuAD is a reading comprehension dataset built from question, context, and answers with information from Wikipedia articles. Let's take a look at an example.\n\n{{< notebook file=\"download-squad-1\" height=\"full\" >}}\n\nWe first download the `squad_v2` dataset via 🤗 *Datasets*. In the first sample, we can see:\n\n* the `title` (or topic) of *Beyoncé*\n* the `context`, a short paragraph from Wikipedia about Beyoncé\n* a `question`, *\"When did Beyonce start becoming popular?\"*\n* the answer `text`, *\"in the late 1990s\"*, which is extracted from the *context*\n* the `answer_start`, which is the starting position of the answer within the *context* string.\n\nThe SQuAD v2 dataset contains *130,319* of these samples, more than enough for us to train a good retriever model.\n\nWe will be using the *Sentence Transformers* library to train our retriever model. When using this library, we must format our training data into a list of `InputExample` objects.\n\n{{< notebook file=\"input-examples\" height=\"full\" >}}\n\nAfter creating this list of `InputExample` objects, we need to load them into a data loader. A data loader is commonly used with PyTorch, which Sentence Transformers uses under the hood. Because of this, we can often use the PyTorch `DataLoader` class.\n\nHowever, we need to do something slightly different. Our training data consists of positive question-context pairs; positive meaning that every sample in our dataset can be viewed as having a positive or *high* similarity. There are no negative or dissimilar pairs.\n\nWhen our data looks like this, one of the most effective training techniques we can use uses the **M**ultiple **N**egatives **R**anking (MNR) loss function. We will not explain MNR loss in this article, but you [can learn about it here](/learn/fine-tune-sentence-transformers-mnr/).\n\nOne crucial property of training with MNR loss is that each training batch does *not* contain duplicate questions or contexts. This is a problem, as the SQuAD data includes several questions for each context. Because of this, if we used the standard `DataLoader`, there is a high probability that we would find duplicate contexts in our batches.\n\n![image-20220104185439001](/images/train-retriever-6.png)\n<small>Screenshot from [HuggingFace's dataset viewer](https://huggingface.co/datasets/squad_v2/viewer/squad_v2/train) for the *squad_v2* dataset. Each row represents a different question, but they all map to the same *context*.</small>\n\nFortunately, there is an easy solution to this. *Sentence Transformers* provides a set of modified data loaders. One of those is the `NoDuplicatesDataLoader`, which ensures our batches contain *no* duplicates.\n\n{{< notebook file=\"no-dupes-loader\" height=\"full\" >}}\n\nWith that, our training data is fully prepared, and we can move on to initializing and training our retriever model.\n\n### Step Two: Initialize and Train\n\nBefore training our model, we need to initialize it. For this, we begin with a pretrained transformer model from the [HuggingFace model hub](https://huggingface.co/models). A popular choice for sentence transformers is Microsoft's MPNet model, which we access via `microsoft/mpnet-base`.\n\nThere is one problem with our pretrained transformer model. It outputs many word/token-level vector embeddings. We don't want token vectors; we need *sentence vectors*.\n\nWe need a way to transform the many token vectors output by the model into a *single* sentence vector.\n\n![token_to_sentence_vecs](/images/train-retriever-7.jpg)<small>Transformation of the many token vectors output by a transformer model into a single sentence vector.</small>\n\nTo perform this transformation, we add a *mean pooling layer* to process the outputs of the transformer model. There are a few different pooling techniques. The one that we will use is *mean pooling*. This approach will take the many token vectors output by the model and average the activations across each vector dimension to create a single sentence vector.\n\nWe can do this via `models` and `SentenceTransformer` utilities of the *Sentence Transformers* library.\n\n{{< notebook file=\"init-model-1\" height=\"full\" >}}\n\nWe have a `SentenceTransformer` object; a pretrained `microsoft/mpnet-base` model followed by a mean pooling layer.\n\nWith our model defined, we can initialize our MNR loss function.\n\n{{< notebook file=\"mnr-loss\" height=\"full\" >}}\n\nThat is everything we need for fine-tuning the model. We set the number of training epochs to `1`; anything more for sentence transformers often leads to overfitting. Another method to reduce the likelihood of overfitting is adding a learning rate warmup. Here, we warmup for the first 10% of our training steps (10% is the *go to* % for warmup steps; if you find the model is overfitting, try increasing the number).\n\n{{< notebook file=\"fit-model\" height=\"full\" >}}\n\nWe now have an ODQA retriever model saved to the local `./mpnet-mnr-squad2` directory. That's great, but we have no idea how well the model performs, so our next step is to evaluate model performance.\n\n## Retriever Evaluation\n\nEvaluation of retriever models is slightly different from the evaluation of most language models. Typically, we input some text and calculate the error between clearly defined predicted and true values.\n\nFor information retrieval (IR), we need a metric that measures the rate of successful vs. unsuccessful retrievals. A popular metric for this is [mAP@K](https://sdsawtelle.github.io/blog/output/mean-average-precision-MAP-for-recommender-systems.html). In short, this is an averaged precision value (fraction of retrieved contexts that are relevant) that considers the top *K* of retrieved results.\n\nThe setup for IR evaluation is a little more involved than with other evaluators in the *Sentence Transformers* library. We will be using the `InformationRetrievalEvaluator`, and this requires three inputs:\n\n* `ir_queries` is a dictionary mapping question IDs to question text\n* `ir_corpus` maps context IDs to context text\n* `ir_relevant_docs` maps question IDs to their relevant context IDs\n\nBefore we initialize the evaluator, we need to download a new set of samples that our model has not seen before and format them into the three dictionaries above. We will use the SQuAD *validation* set.\n\n{{< notebook file=\"squad-dev\" height=\"full\" >}}\n\nTo create the dictionary objects required by the `InformationRetrievalEvaluator`, we must assign unique IDs to both contexts and questions. And we need to ensure that duplicate contexts are *not* assigned different IDs. To handle these, we will first convert our dataset object into a Pandas dataframe.\n\n{{< notebook file=\"dev-to-df\" height=\"full\" >}}\n\nFrom here, we can quickly drop duplicate contexts with the `drop_duplicates` method. As we no longer have duplicates, we can append `'con'` to each context ID, giving each *unique* context a unique ID different from any question IDs.\n\n{{< notebook file=\"no-dupes\" height=\"full\" >}}\n\nWe now have unique question IDs in the `squad_df` dataframe and unique context IDs in the `no_dupe` dataframe. Next, we perform an inner join on the `context` feature to bring these two sets of IDs together and find our question ID to context ID mappings.\n\n{{< notebook file=\"context-join\" height=\"full\" >}}\n\nWe're now ready to build the three mapping dictionaries for the `InformationRetrievalEvaluator`. First, we map question/context IDs to questions/contexts.\n\n{{< notebook file=\"ir-queries-corpus\" height=\"full\" >}}\n\nAnd then map question IDs to a *set* of relevant context IDs. For the SQuAD data, we only have *many-to-one* or *one-to-one* question ID to context ID mappings, but we will write our code to *additionally* handle *one-to-many* mappings (so we can handle other, non-SQuAD datasets).\n\n{{< notebook file=\"ir-relevant-docs\" height=\"full\" >}}\n\nOur evaluator inputs are ready, so we initialize the evaluator and then evaluate our `model`.\n\n{{< notebook file=\"ir-eval-our-model\" height=\"full\" >}}\n\nWe return a mAP@K score of 0.74, where @K is *100* by default. This performance is comparable to other state-of-the-art retriever models. Performing the same evaluation with the `multi-qa-mpnet-base-cos-v1` returns a mAP@K score of 0.76, just two percentage points greater than our custom model.\n\n{{< notebook file=\"ir-eval-other-model\" height=\"full\" >}}\n\nOf course, if your target domain was SQuAD data, the pretrained `multi-qa-mpnet-base-cos-v1` model would be the better model. But if you have your own unique dataset and domain. A custom model fine-tuned on that domain will *very likely* outperform existing models like `multi-qa-mpnet-base-cos-v1` *in that domain*.\n\n## Storing the Vectors\n\nWe have our retriever model, we've evaluated it, and we're happy with its performance. But we don't know how to use it.\n\nWhen you perform a Google search, Google does *not* look at the whole internet, encode all of that information into vector embeddings, and then compare all of those vectors to your query vector. We would be waiting a *very* long time to return results if that were the case.\n\nInstead, Google has already searched for, collected, and encoded all of that data. Google then stores those encoded vectors in some sort of vector database. When you query now, the only thing Google needs to encode is your question.\n\nTaking this a step further, comparing your query vector to *all* vectors indexed by Google (which represent the entire Google-accessible internet) would still take an incredibly long time. We refer to this accurate but inefficient comparison of every single vector as an *exhaustive search*.\n\nFor big datasets, an exhaustive search is too slow. The solution to this is to perform an *approximate search*. An approximate search allows us to massively reduce our search scope to a smaller but (hopefully) more relevant sub-section of the index. Making our search times much more manageable.\n\nThe [Pinecone vector database](/) is a straightforward and robust solution that allows us to (1) store our context vectors and (2) perform an *accurate and fast* approximate search. These are the two elements we need for a promising ODQA pipeline.\n\nAgain, we need to work through a few steps to set up our vector database.\n\n![vector_db_setup](/images/train-retriever-8.jpg)<small>Steps from retriever and context preparation (top-right) that allow us to *encode contexts* into *context vectors*. After initializing a vector database index, we can populate the index with the *context vectors*.</small>\n\nAfter working through each of those steps, we will be ready to begin retrieving relevant contexts.\n\n### Encoding Contexts\n\nWe have already created our retriever model, and during the earlier evaluation step, we downloaded the SQuAD validation data. We can use this same validation data and encode all *unique* contexts.\n\n{{< notebook file=\"encode-contexts\" height=\"full\" >}}\n\nAfter removing duplicate contexts, we're left with 1,204 samples. It is a tiny dataset but large enough for our example.\n\n### Initializing the Index\n\nBefore adding the context vectors to our index, we need to initialize it. Fortunately, Pinecone makes this very easy. We start by installing the Pinecone client if required:\n\n```\n!pip install pinecone-client\n```\n\nThen we initialize a connection to Pinecone. For this, we need a [free API key](https://app.pinecone.io/).\n\n{{< notebook file=\"init-pinecone\" height=\"full\" >}}\n\nWe then create a new index with `pinecone.create_index`. Before initializing the index, we should check that the index name does not already exist (which it will not if this is your first time creating the index).\n\n{{< notebook file=\"create-index\" height=\"full\" >}}\n\nWhen creating a new index, we need to specify the index `name`, and the dimensionality of vectors to be added. We either check our encoded context vectors’ dimensions directly or find the dimension attribute within the retriever model (as shown above).\n\n### Populating the Index\n\nAfter creating both our index and the context vectors, we can go ahead and *upsert* (upload) the vectors into our index.\n\n{{< notebook file=\"upsert-vectors\" height=\"full\" >}}\n\nPinecone expects us to [upsert data](https://www.pinecone.io/docs/insert-data/) in the format:\n\n```python\nvectors =\n[\n(id_0, vector_0, metadata_0),\n(id_1, vector_1, metadata_1)\n]\n```\n\nOur IDs are the unique alphanumeric identifiers that we saw earlier in the SQuAD data. The vectors are our encoded context vectors formatted as lists; the metadata is a dictionary that allows us to store extra information in a key-value format.\n\n---\n\n*Using the metadata field, Pinecone allows us to [create complex or straightforward metadata filters](https://www.pinecone.io/learn/vector-search-filtering/) to target our search scope to specific numeric ranges, categories, and more.*\n\n---\n\nOnce the upsert is complete, the retrieval components of our ODQA pipeline are ready to go, and we can begin asking questions.\n\n## Making Queries\n\nWith everything set up, querying our retriever-vector database pipeline is pretty straightforward. We first define a question and encode it as we did for our context vectors before.\n\n{{< notebook file=\"encode-query\" height=\"full\" >}}\n\nAfter creating our query vector, we pass it to Pinecone via the `index.query` method, specify how many results we'd like to return with `top_k`, and `include_metadata` so that we can see the text associated with each returned vector.\n\n{{< notebook file=\"query1\" height=\"full\" >}}\n\nWe return the correct context as our second top result in this example. The first result is relevant in the context of Normans and Normandy, but it does not answer the specific question of *when* the Normans were in Normandy.\n\nLet's try a couple more questions.\n\n{{< notebook file=\"query2\" height=\"full\" >}}\n\nFor this question, we return the correct context as the highest result with a much higher score than the remaining samples.\n\n{{< notebook file=\"query3\" height=\"full\" >}}\n\nWe return the correct context in the first position. Again, there is a good separation between sample scores of the correct context and other contexts.\n\n\n\nThat's it for this guide to fine-tuning and implementing a custom retriever model in an ODQA pipeline. Now we can implement two of the most crucial components in ODQA: enabling a more human and natural approach to information retrieval.\n\nOne of the most incredible things about ODQA is how widely applicable it is. Organizations across almost every industry have the opportunity to benefit from more intelligent and efficient information retrieval.\n\nAny organization that handles unstructured information such as word documents, PDFs, emails, and more has a clear use case: freeing this information and enabling easy and natural access through QA systems.\n\nAlthough this is the most apparent use case, there are many more, whether it be an internal efficiency speedup or a key component in a product (as with Google search). The opportunities are both broad and highly impactful.\n\n\n## References\n\n[1] N. Reimers, [Neural Search for Low Resource Scenarios](https://www.youtube.com/watch?v=XNJThigyvos) (2021), YouTube\n\nS. Sawtelle, [Mean Average Precision (MAP) For Recommender Systems](https://sdsawtelle.github.io/blog/output/mean-average-precision-MAP-for-recommender-systems.html) (2016), GitHub\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbbb"
  },
  "filename": "sentence-embeddings.md",
  "title": "ebook-post",
  "category": "\"Sentence Transformers and Embeddings\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Sentence Transformers and Embeddings\"\nheadline: \"Sentence Transformers: Meanings in Disguise\"\ncategories:\n  - NLP for Semantic Search\ntoc: >-\nweight: 2\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: How sentence transformers and embeddings can be used for a range of semantic similarity applications.\n# Open Graph\nimages: ['/images/sentence-embeddings-1.jpg']\n---\n\n**Once you learn about and generate sentence embeddings, combine them with the [Pinecone vector database](/) to easily build applications like semantic search, deduplication, and multi-modal search. [Try it now for free.](https://app.pinecone.io)**\n\nTransformers have wholly rebuilt the landscape of natural language processing (NLP). Before transformers, we had *okay* translation and language classification thanks to recurrent neural nets (RNNs) — their language comprehension was limited and led to many minor mistakes, and coherence over larger chunks of text was practically impossible.\n\nSince the introduction of the first transformer model in the 2017 paper *'Attention is all you need'* [1], NLP has moved from RNNs to models like BERT and GPT. These new models can answer questions, write articles *(maybe GPT-3 wrote this)*, enable incredibly intuitive semantic search — and much more.\n\nThe funny thing is, for many tasks, the latter parts of these models are the same as those in RNNs — often a couple of feedforward NNs that output model predictions.\n\nIt's the *input* to these layers that changed. The [dense embeddings](https://www.pinecone.io/learn/dense-vector-embeddings-nlp/) created by transformer models are so much richer in information that we get massive performance benefits despite using the same final outward layers.\n\nThese increasingly rich sentence embeddings can be used to quickly compare sentence similarity for various use cases. Such as:\n\n* **Semantic textual similarity (STS)** — comparison of sentence pairs. We may want to identify patterns in datasets, but this is most often used for benchmarking.\n* **Semantic search** — information retrieval (IR) using semantic meaning. Given a set of sentences, we can search using a *'query'* sentence and identify the most similar records. Enables search to be performed on concepts (rather than specific words).\n* **Clustering** — we can cluster our sentences, useful for topic modeling.\n\nIn this article, we will explore how these embeddings have been adapted and applied to a range of semantic similarity applications by using a new breed of transformers called *'sentence transformers'*.\n\n<p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/WS1uVMGhlWQ\" title=\"Sentence Embeddings\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>\n\n## Some “Context”\n\nBefore we dive into sentence transformers, it might help to piece together why transformer embeddings are so much richer — and where the difference lies between a vanilla *transformer* and a *sentence transformer*.\n\nTransformers are indirect descendants of the previous RNN models. These old recurrent models were typically built from many recurrent *units* like [LSTMs or GRUs](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21).\n\nIn *machine translation*, we would find [encoder-decoder networks](https://machinelearningmastery.com/encoder-decoder-recurrent-neural-network-models-neural-machine-translation/). The first model for *encoding* the original language to a *context vector*, and a second model for *decoding* this into the target language.\n\n![Encoder decoder bottleneck](/images/sentence-embeddings-2.jpg)\n<small>Encoder-decoder architecture with the single context vector shared between the two models, this acts as an information bottleneck as *all* information must be passed through this point.</small>\n\nThe problem here is that we create an *information bottleneck* between the two models. We're creating a massive amount of information over multiple time steps and trying to squeeze it all through a single connection. This limits the encoder-decoder performance because much of the information produced by the encoder is lost before reaching the decoder.\n\nThe *attention mechanism* provided a solution to the bottleneck issue. It offered another route for information to pass through. Still, it didn't overwhelm the process because it focused *attention* only on the most relevant information.\n\nBy passing a *context vector* from each timestep into the attention mechanism (producing *annotation* vectors), the information bottleneck is removed, and there is better information retention across longer sequences.\n\n![Encoder decoder attention](/images/sentence-embeddings-3.jpg)\n<small>Encoder-decoder with the attention mechanism. The attention mechanism considered all encoder output activations and each timestep's activation in the decoder, which modifies the decoder outputs.</small>\n\nDuring decoding, the model decodes one word/timestep at a time. An alignment (e.g., similarity) between the word and all encoder annotations is calculated for each step.\n\nHigher alignment resulted in greater weighting to the encoder annotation on the output of the decoder step. Meaning the mechanism calculated which encoder words to pay *attention* to.\n\n![Attention example](/images/sentence-embeddings-4.png)\n<small>Attention between an English-French encoder and decoder, source [2].</small>\n\nThe best-performing RNN encoder-decoders all used this attention mechanism.\n\n### Attention is All You Need\n\nIn 2017, a paper titled *Attention Is All You Need* was published. This marked a turning point in NLP. The authors demonstrated that we could remove the RNN networks and get superior performance using *just* the attention mechanism — with a few changes.\n\nThis new attention-based model was named a *'transformer'*. Since then, the NLP ecosystem has entirely shifted from RNNs to transformers thanks to their vastly superior performance and incredible capability for generalization.\n\nThe first transformer removed the need for RNNs through the use of *three* key components:\n\n* Positional Encoding\n* Self-attention\n* Multi-head attention\n\n**Positional encoding** replaced the key advantage of RNNs in NLP — the ability to consider the order of a sequence (they were *recurrent*). It worked by adding a set of varying sine wave activations to each input embedding based on position.\n\n**Self-attention** is where the attention mechanism is applied between a word and all of the other words in its own context (sentence/paragraph). This is different from vanilla attention which specifically focused on attention between encoders and decoders.\n\n**Multi-head attention** can be seen as several *parallel* attention mechanisms working together. Using several attention *heads* allowed the representation of several sets of relationships (rather than a single set).\n\n### Pretrained Models\n\nThe new transformer models generalized much better than previous RNNs, which were often built specifically for each use-case.\n\nWith transformer models, it is possible to use the same *'core'* of a model and simply swap the last few layers for different use cases (without retraining the *core*).\n\nThis new property resulted in the rise of *pretrained* models for NLP. Pretrained transformer models are trained on vast amounts of training data — often at high costs by the likes of Google or OpenAI, then released for the public to use for free.\n\nOne of the most widely used of these pretrained models is BERT, or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers by Google AI.\n\nBERT spawned a whole host of further models and derivations such as distilBERT, RoBERTa, and ALBERT, covering tasks such as classification, Q&A, POS-tagging, and more.\n\n### BERT for Sentence Similarity\n\nSo far, so good, but these transformer models had one issue when building sentence vectors: Transformers work using word or *token*-level embeddings, *not* sentence-level embeddings.\n\nBefore sentence transformers, the approach to calculating *accurate* sentence similarity with BERT was to use a cross-encoder structure. This meant that we would pass two sentences to BERT, add a classification head to the top of BERT — and use this to output a similarity score.\n\n![Cross encoder](/images/sentence-embeddings-5.jpg)\n<small>The BERT cross-encoder architecture consists of a BERT model which consumes sentences A and B. Both are processed in the same sequence, separated by a `[SEP]` token. All of this is followed by a feedforward NN classifier that outputs a similarity score.</small>\n\nThe cross-encoder network does produce very accurate similarity scores (better than SBERT), but it's *not scalable*. If we wanted to perform a similarity search through a small 100K sentence dataset, we would need to complete the cross-encoder inference computation 100K times.\n\nTo cluster sentences, we would need to compare all sentences in our 100K dataset, resulting in just under 500M comparisons — this is simply not realistic.\n\nIdeally, we need to pre-compute sentence vectors that can be stored and then used whenever required. If these vector representations are good, all we need to do is calculate the cosine similarity between each.\n\nWith the original BERT (and other transformers), we can build a sentence embedding by averaging the values across all token embeddings output by BERT (if we input 512 tokens, we output 512 embeddings). Alternatively, we can use the output of the first `[CLS]` token (a BERT-specific token whose output embedding is used in classification tasks).\n\nUsing one of these two approaches gives us our sentence embeddings that can be stored and compared much faster, shifting search times from 65 hours to around 5 seconds (see below). However, the accuracy is not good, and is worse than using averaged GloVe embeddings (which were developed in 2014).\n\n**The solution** to this lack of an accurate model *with* reasonable latency was designed by Nils Reimers and Iryna Gurevych in 2019 with the introduction of sentence-BERT (SBERT) and the `sentence-transformers` library.\n\nSBERT outperformed the previous state-of-the-art (SOTA) models for all common semantic textual similarity (STS) tasks — more on these later — except a single dataset (SICK-R).\n\nThankfully for scalability, SBERT produces sentence embeddings — so we do *not* need to perform a whole inference computation for every sentence-pair comparison.\n\nReimers and Gurevych demonstrated the dramatic speed increase in 2019. Finding the most similar sentence pair from 10K sentences took 65 hours with BERT. With SBERT, embeddings are created in ~5 seconds and compared with cosine similarity in ~0.01 seconds.\n\nSince the SBERT paper, many more sentence transformer models have been built using similar concepts that went into training the original SBERT. They’re all trained on many similar and dissimilar sentence pairs.\n\nUsing a loss function such as softmax loss, multiple negatives ranking loss, or MSE margin loss, these models are optimized to produce similar embeddings for similar sentences, and dissimilar embeddings otherwise.\n\nNow you have some context behind sentence transformers, where they come from, and why they're needed. Let's dive into how they work.\n\n*[3] The SBERT paper covers many of the statements, techniques, and numbers from this section.*\n\n## Sentence Transformers\n\nWe explained the cross-encoder architecture for sentence similarity with BERT. SBERT is similar but drops the final classification head, and processes one sentence at a time. SBERT then uses mean pooling on the final output layer to produce a sentence embedding.\n\nUnlike BERT, SBERT is fine-tuned on sentence pairs using a *siamese* architecture. We can think of this as having two identical BERTs in parallel that share the exact same network weights.\n\n![Start sbert](/images/sentence-embeddings-6.jpg)\n<small>An SBERT model applied to a sentence pair *sentence A* and *sentence B*. Note that the BERT model outputs token embeddings (consisting of 512 768-dimensional vectors). We then compress that data into a single 768-dimensional sentence vector using a pooling function.</small>\n\nIn reality, we are using a single BERT model. However, because we process sentence A followed by sentence B as *pairs* during training, it is easier to think of this as two models with tied weights.\n\n### Siamese BERT Pre-Training\n\nThere are different approaches to training sentence transformers. We will describe the original process featured most prominently in the original SBERT that optimizes on *softmax-loss*. Note that this is a high-level explanation, we will save the in-depth walkthrough for another article.\n\nThe softmax-loss approach used the *'siamese'* architecture fine-tuned on the Stanford Natural Language Inference (SNLI) and Multi-Genre NLI (MNLI) corpora.\n\nSNLI contains 570K sentence pairs, and MNLI contains 430K. The pairs in both corpora include a `premise` and a `hypothesis`. Each pair is assigned one of three labels:\n\n* **0** — *entailment*, e.g. the `premise` suggests the `hypothesis`.\n* **1** — *neutral*, the `premise` and `hypothesis` could both be true, but they are not necessarily related.\n* **2** — *contradiction*, the `premise` and `hypothesis` contradict each other.\n\nGiven this data, we feed sentence A (let's say the `premise`) into siamese BERT A and sentence B (`hypothesis`) into siamese BERT B.\n\nThe siamese BERT outputs our pooled sentence embeddings. There were the results of *three* different pooling methods in the SBERT paper. Those are *mean*, *max*, and *[CLS]*-pooling. The *mean*-pooling approach was best performing for both NLI and STSb datasets.\n\nThere are now two sentence embeddings. We will call embeddings A `u` and embeddings B `v`. The next step is to concatenate `u` and `v`. Again, several concatenation approaches were tested, but the highest performing was a `(u, v, |u-v|)` operation:\n\n![UV Vectors](/images/sentence-embeddings-7.jpg)\n<small>We concatenate the embeddings **u**, **v**, and **|u - v|**.</small>\n\n`|u-v|` is calculated to give us the element-wise difference between the two vectors. Alongside the original two embeddings (`u` and `v`), these are all fed into a feedforward neural net (FFNN) that has *three* outputs.\n\nThese three outputs align to our NLI similarity labels **0**, **1**, and **2**. We need to calculate the softmax from our FFNN, which is done within the [cross-entropy loss function](/learn/cross-entropy-loss/). The softmax and labels are used to optimize on this *'softmax-loss'*.\n\n![SBERT Training](/images/sentence-embeddings-8.jpg)\n<small>The operations were performed during training on two sentence embeddings, `u` and `v`. Note that *softmax-loss* refers cross-entropy loss (which contains a softmax function by default).</small>\n\nThis results in our pooled sentence embeddings for similar sentences (label **0**) becoming *more similar*, and embeddings for dissimilar sentences (label **2**) becoming *less similar*.\n\nRemember we are using *siamese* BERTs **not** *dual* BERTs. Meaning we don't use two independent BERT models but a single BERT that processes sentence A followed by sentence B.\n\nThis means that when we optimize the model weights, they are pushed in a direction that allows the model to output more similar vectors where we see an *entailment* label and more dissimilar vectors where we see a *contradiction* label.\n\n---\n\n*We are working on a step-by-step guide to training a siamese BERT model with the SNLI and MNLI corpora described above using both the softmax-loss and multiple-negatives-ranking-loss approaches. You can get an email as soon as we release the article by [clicking here](https://www.pinecone.io/learn/) (the form is at the bottom of the page).*\n\n---\n\nThe fact that this training approach works is not particularly intuitive and indeed has been described by Reimers as *coincidentally* producing good sentence embeddings [5].\n\nSince the original paper, further work has been done in this area. Many more models such as the [latest MPNet and RoBERTa models trained on 1B+ samples](https://huggingface.co/spaces/flax-sentence-embeddings/sentence-embeddings) (producing much better performance) have been built. We will be exploring some of these in future articles, and the superior training approaches they use.\n\nFor now, let's look at how we can initialize and use some of these sentence-transformer models.\n\n### Getting Started with Sentence Transformers\n\nThe fastest and easiest way to begin working with sentence transformers is through the `sentence-transformers` library created by the creators of SBERT. We can install it with `pip`.\n\n```bash\n!pip install sentence-transformers\n```\n\nWe will start with the original SBERT model `bert-base-nli-mean-tokens`. First, we download and initialize the model.\n\n{{< notebook file=\"sbert-init\" height=\"full\" >}}\n\nThe output we can see here is the `SentenceTransformer` object which contains *three* components:\n\n* The **transformer** itself, here we can see the max sequence length of `128` tokens and whether to lowercase any input (in this case, the model does *not*). We can also see the model class, `BertModel`.\n\n* The **pooling** operation, here we can see that we are producing a `768`-dimensional sentence embedding. We are doing this using the *mean pooling* method.\n\nOnce we have the model, building sentence embeddings is quickly done using the `encode` method.\n\n{{< notebook file=\"encoding-sentences\" height=\"full\" >}}\n\nWe now have sentence embeddings that we can use to quickly compare sentence similarity for the use cases introduced at the start of the article; STS, semantic search, and clustering.\n\nWe can put together a fast STS example using nothing more than a cosine similarity function and Numpy.\n\n{{< notebook file=\"cos-sim\" height=\"full\" >}}\n\n![SBERT heatmap](/images/sentence-embeddings-9.jpg)\n<small>Heatmap showing cosine similarity values between all sentence-pairs.</small>\n\nHere we have calculated the cosine similarity between every combination of our five sentence embeddings. Which are:\n\n| Index | Sentence |\n| ----- | ------------------------------------------------------------ |\n| 0 | the fifty mannequin heads floating in the pool kind of freaked them out |\n| 1 | she swore she just saw her sushi move |\n| 2 | he embraced his new life as an eggplant |\n| 3 | my dentist tells me that chewing bricks is very bad for your teeth |\n| 4 | the dental specialist recommended an immediate stop to flossing with construction materials |\n\nWe can see the highest similarity score in the bottom-right corner with `0.64`. As we would hope, this is for sentences `4` and `3`, which both describe poor dental practices using construction materials.\n\n## Other sentence-transformers\n\nAlthough we returned good results from the SBERT model, many more sentence transformer models have since been built. Many of which we can find in the `sentence-transformers` library.\n\nThese newer models can significantly outperform the original SBERT. In fact, SBERT is no longer listed as an available model on the [SBERT.net models page](https://www.sbert.net/docs/pretrained_models.html).\n\n| Model | Avg. Performance | Speed | Size (MB) |\n| ---------------------- | ---------------- | ----- | --------- |\n| `all-mpnet-base-v2` | 63.30 | 2800 | 418 |\n| `all-roberta-large-v1` | 53.05 | 800 | 1355 |\n| `all-MiniLM-L12-v1` | 59.80 | 7500 | 118 |\n\n<small>A few of the top-performing models on the sentence transformers model page.</small>\n\nWe will cover some of these later models in more detail in future articles. For now, let's compare one of the highest performers and run through our STS task.\n\n{{< notebook file=\"sentence-transformer-init\" height=\"full\" >}}\n\nHere we have the `SentenceTransformer` model for `all-mpnet-base-v2`. The components are very similar to the `bert-base-nli-mean-tokens` model, with some small differences:\n\n* `max_seq_length` has increased from `128` to `384`. Meaning we can process sequences that are *three* times longer than we could with SBERT.\n* The base model is now `MPNetModel` [4] not `BertModel`.\n* There is an additional normalization layer applied to sentence embeddings.\n\nLet's compare the STS results of `all-mpnet-base-v2` against SBERT.\n\n{{< notebook file=\"mpnet-sts\" height=\"full\" >}}\n\n![SBERT and MPNet heatmaps](/images/sentence-embeddings-10.jpg)\n<small>Heatmaps for both SBERT and the MPNet sentence transformer.</small>\n\nThe semantic representation of later models is apparent. Although SBERT correctly identifies `4` and `3` as the most similar pair, it also assigns reasonably high similarity to other sentence pairs.\n\nOn the other hand, the MPNet model makes a *very* clear distinction between similar and dissimilar pairs, with most pairs scoring less than 0.1 and the `4`-`3` pair scored at *0.52*.\n\nBy increasing the separation between dissimilar and similar pairs, we’re:\n\n1. Making it easier to automatically identify relevant pairs.\n2. Pushing predictions closer to the *0* and *1* target scores for *dissimilar* and *similar* pairs used during training. This is something we will see more of in our future articles on fine-tuning these models.\n\n---\n\nThat's it for this article introducing sentence embeddings and the current SOTA sentence transformer models for building these incredibly useful embeddings.\n\nSentence embeddings, although only recently popularized, were produced from a long range of fantastic innovations. We described some of the mechanics applied to create the first sentence transformer, SBERT.\n\nWe also demonstrated that despite SBERT's very recent introduction in 2019, other sentence transformers already outperform the model. Fortunately for us, it's easy to switch out SBERT for one of these newer models with the `sentence-transformers` library.\n\nIn future articles, we will dive deeper into some of these newer models and how to train our own sentence transformers.\n\n{{< newsletter text=\"Subscribe for more semantic search material!\" inputText=\"Email address...\" buttonText=\"Submit\">}}\n\n## References\n\n[1] A. Vashwani, et al., [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (2017), NeurIPS\n\n[2] D. Bahdanau, et al., [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) (2015), ICLR\n\n[3] N. Reimers, I. Gurevych, [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084) (2019), ACL\n\n[4] [MPNet Model](https://huggingface.co/transformers/model_doc/mpnet.html), Hugging Face Docs\n\n[5] N. Reimers, [Natural Language Inference](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/nli/README.md), sentence-transformers on GitHub"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbbc"
  },
  "filename": "welcome-ram.md",
  "title": "post",
  "category": "\"Meet Pinecone’s New VP of Engineering, Dr. Ram Sriharsha\"",
  "content": "---\nlayout: post\ntitle: \"Meet Pinecone’s New VP of Engineering, Dr. Ram Sriharsha\"\nheadline: \"Meet Pinecone’s New VP of Engineering, Dr. Ram Sriharsha\"\ncategories:\n  - Company News\ntoc: >-\nauthor:\n  name: Greg Kogan\n  position: VP Marketing\n  src: /images/company-greg.png\n  href: https://www.linkedin.com/in/gkogan/\ndescription: Our new VP of Engineering, Dr. Ram Sriharsha, brings a wealth of knowledge to our team and is uniquely positioned to help execute on our lofty vision.\nimages: ['/images/team-ram-sriharsha.jpg']\ndate: \"2021-11-22\"\nthumbnail: \"/images/team-ram-sriharsha-resized.jpg\"\n# Date: November 22, 2021\n---\n\nPinecone is rapidly growing, which means we need people driving our teams that know how to not only lead at the helm but support us from the bottom up, so we can grow from a strong foundation. With experience in engineering, product management, and VP roles at the likes of Yahoo, Databricks, and Splunk, our new VP of Engineering, [Dr. Ram Sriharsha](https://www.linkedin.com/in/harsha340/), brings a wealth of knowledge to our team and is uniquely positioned to help execute on our lofty vision. \n\nAt Yahoo, he was both a principal software engineer and then research scientist; at Databricks, he was the product and engineering lead for the unified analytics platform for genomics; and, in his three years at Splunk, he played multiple roles including Sr Principal Scientist, VP Engineering and Distinguished Engineer.\n\nWith this wealth of experience, he could have done anything. So, why did he choose to join us? The decision was easy, Ram said. \n\n“I tend to choose companies and technologies that are very cutting edge,” he said. “I try to place my bet on technologies that are going to be the way things are done. ... From my own personal experience, I knew that semantic search over [vector embeddings](/learn/vector-embeddings/) wasn’t a solved problem, and I recognized the significance of such a technology to unlocking new use cases around unstructured data. ... Pinecone is working on exactly this.”\n\nJumping onto the Databricks and Splunk teams after seeing their potential, Ram’s intuition is keen and impressive. But, again, we were curious as to why a seemingly small, scrappy company was so attractive to him. \n\nYou can make more moves working on cutting edge technologies at a company of this size, where everyone’s contributions are ground-shaking, than you could even as VP at a big company, he said. Pinecone is the place to be, in his eyes. \n\n“I really want to build systems and interface with users,” he said. “At Pinecone, you’re continually iterating and learning. In a big company, Version 0 can take many years to get out, and by then you’re not connected to the product anymore. At Pinecone, that’s not a problem. For example, one of our scientists fixed something and got the fix into production the very same night. This rarely happens at a big company.”\n\nIn addition, the technology Pinecone is creating was a no brainer for Ram, someone whose finger is always on the pulse. He doesn’t just want to work at a company that is pursuing greatness; he wants to get his hands dirty too. He watched as CEO Edo Liberty was on the forefront of Machine Learning research and knew he had to be involved. He wasn’t making the old stuff better; he was building a different type of database in a new domain. \n\nThe team is innovating on science, as well, Ram said. \n\n“We are focused on building the world’s best [vector database](/learn/vector-database/),” Ram said. “The use cases you can build on top of it are fascinating to me. We are writing our entire database in Rust, which is interesting — if we can shave off 20% memory usage, that's savings in cost for customers. It's very valuable for us to do that. What we are building is pushing the limits of databases themselves. How do we build algorithms that are accurate but allow us to scale? These are really amazing challenges in redesigning databases.”\n\nAfter working on teams as the first employee, as well as stepping into high-level leadership roles right away, Ram knows what it takes to create a strong team and maintain that team for years to come. From the outset of his role, he wants to ensure he builds the best team of scientists possible and is looking for people who are just as excited as he is about transforming search using machine learning and vector databases. \n\nWhen thinking about hiring, it’s not just about coding skill, either. Having an open culture and strong teamwork is vital, and that’s why he chose Pinecone, he said. Everyone has the opportunity to build and contribute, and he wants all engineers to feel like they’re getting their hands dirty and owning their own projects. \n\nEven with a strong culture coming in, though, the team cannot remain complacent, especially as it begins to think about expansion and reaching out to different audiences for hiring. This includes working with a recruiting company focused on diversity, building relationships with organizations that support women in coding, like Grace Hopper, and constantly evolving to be welcoming and inclusive. The team has a long way to go with this, he said, but he’s excited to work on it.  \n\nRam is set to take Pinecone into its next phase of greatness, as we work to build out and up. He hopes everyone is as excited about this journey as he is, and he invites anyone who resonates with it to come along. \n\n“We are truly trying to build something fundamentally new here,” Ram said. “I really want that to resonate with people reading this. If you resonate with this, this is the place to be.”\n\n[We are hiring. Come work with Ram!](https://www.pinecone.io/careers/)\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbbd"
  },
  "filename": "k-means-clustering.md",
  "title": "post",
  "category": "\"Introduction to K-Means Clustering\"",
  "content": "---\nlayout: post\ntitle: \"Introduction to K-Means Clustering\"\nheadline: \"Introduction to K-Means Clustering\"\ncategories:\n  - Algorithms & Libraries\ntoc: >-\nweight: 4\nauthor:\n  name: Diego Lopez Yse\n  position: Data Scientist\n  src: /images/diego-lopez-yse.jpeg\n  href: \"https://lopezyse.medium.com/\"\ndescription: An introduction to K-Means Clustering.\n# Open Graph\nimages: ['/images/global-data-creation.png']\n---\n\nWith massive data volumes growing at exponential rates, we need to find scalable methods to process them and find insights. The world of data entered the Zettabyte era several years ago. What’s a Zettabyte? Well, it is [enough storage](https://www.pcmag.com/news/seagate-is-the-first-company-to-ship-3-zettabytes-of-hard-drive-storage) for 30 billion 4K movies, or 60 billion video games, or 7.5 trillion MP3 songs.\n\nToday, the total amount of data created, captured, copied, and consumed globally is in the order of 100 Zettabytes and just keeps growing.\n\n![Global data creation](/images/global-data-creation.png)\n<small>Through 2035, global data creation is projected to grow to more than 2,142 Zettabytes. From 2020, the growth was higher than previously expected caused by increased demand due to the COVID-19 pandemic, as more people worked and learned from home and used more home entertainment options. Source: [Statista](https://www.statista.com)</small>\n\nAlthough this might seem overwhelming, the good news is that we can turn to machines for help: There are many different Machine Learning algorithms to discover patterns in big data that lead to actionable insights.\n\nDepending on the way the algorithm “learns” about data to make predictions, we classify them into two groups, each proving one different type of learning:\n\n- **Supervised learning:** existing data is already labeled and you know which behavior you want to predict in the new data you obtain.\n\n- **Unsupervised learning:**  there is no output variable to guide the learning process,and data is explored by algorithms to find patterns. Since the data has no labels, the algorithm identifies similarities on the data points and groups them into clusters.\n\nUnder unsupervised learning, all the objects in the same group (cluster) should be more similar to each other than to those in other clusters; data points from different clusters should be as different as possible. Clustering allows you to find and organize data into groups that have been formed organically, rather than defining groups before looking at the data.\n\nWhile this article will focus most closely on K-means, there are other powerful types of clustering that can be used as well. Let’s take a look at the main ones like hierarchical, density-based, and partitional clustering.\n\n{{< newsletter text=\"Subscribe for more ML articles!\" inputText=\"Email address...\" buttonText=\"Submit\">}}\n\n## Hierarchical Clustering\n\nCluster assignments are determined by building a hierarchy. This is implemented by either a bottom-up or a top-down approach:\n\n- The bottom-up approach is called **agglomerative** clustering and merges the two points that are the most similar until all points have been merged into a single cluster.\n\n- The top-down approach is **divisive** clustering and starts with all points as one cluster and splits the least similar clusters at each step until only single data points remain.\n\n![Hierarchical clustering](/images/hierarchical-clustering.png)\n<small>The agglomerative case starts with every object being a cluster and, in the next steps, merges them with the two closest clusters. The process finishes with every object in one cluster. The divisive algorithm, contrastingly, starts with every object in one cluster and ends with every object in individual clusters. Source: [QuantDare](https://quantdare.com/hierarchical-clustering/)</small>\n\n\nThese methods produce a tree-based hierarchy of points called a dendrogram. The number of clusters “k” is often predetermined by the user, and clusters are assigned by cutting the dendrogram at a specified depth that results in “k” groups of smaller dendrograms.\n\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/hierarchical-clustering-dendrogram.mp4\" type=\"video/mp4\">\n</video>\n\n<small class=\"video\">Hierarchical Clustering returns an output (typically as a dendrogram like the right figure) from which the user can decide the appropriate number of clusters (either manually or algorithmically). If done manually, the user may cut the dendrogram where the merged clusters are too far apart (represented by long lines in the dendrogram). Alternatively, the user can just return a specific number of clusters. Source: [Dashee87](https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/)</small>\n\nUnlike other clustering techniques, hierarchical clustering is a **deterministic process**, which means that assignments won’t change when you run an algorithm multiple times on the same input data. Hierarchical clustering methods often reveal the finer details about the relationships between data objects and provide interpretable dendrograms. On the other hand, they’re computationally expensive with respect to algorithm complexity and sensitive to noise and outliers.\n\n## Density-Based Clustering\n\nUnder this category, cluster assignments are determined based on the density of data points in a region and assigned where there are high densities of data points separated by low-density regions.\n\nUnlike other clustering categories, this approach doesn’t require the user to specify the number of clusters. Instead, there is a distance-based parameter that acts as a tunable threshold and determines how close points must be to be considered a cluster member.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/density-based-clustering-algorithm.mp4\" type=\"video/mp4\">\n</video>\n\n<small class=\"video\">Density-Based Clustering algorithms like DBSCAN don’t require a preset number of clusters. It also identifies outliers as noises unlike others that simply throws them into a cluster even if the data point is very different. Additionally, it is able to find arbitrarily sized and arbitrarily shaped clusters quite well. Source: [Primo.ai](http://primo.ai/index.php?title=Density-Based_Spatial_Clustering_of_Applications_with_Noise_(DBSCAN))</small>\n\nDensity-based clustering methods excel at identifying clusters of nonspherical shapes, and they are resistant to outliers. Nevertheless, they aren’t well suited for clustering in high-dimensional spaces (since [density of data points is very low](https://scialert.net/fulltext/?doi=itj.2011.1092.1105) in those spaces), and they are not able to produce [clusters of differing density](http://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/IEEE_SSCI_2016/pdf/SSCI16_paper_256.pdf).\n\n## Partitional clustering\n\nWith this method, data objects are divided into non-overlapping groups: No object can be a member of more than one cluster, and every cluster must have at least one object.\n\nLike in hierarchical clustering, the user needs to define the number of clusters “k”, which ultimately produces **non-deterministic** results: Partitional clustering produces different results from two or more separate runs even if the runs were based on the same input.\n\nThis clustering method works very well when clusters have a spherical shape (due to its [fixed distance norm](https://academic.oup.com/bioinformatics/article/21/9/1927/408943)), and they’re scalable with respect to algorithm complexity. However, they’re not well suited for clusters with complex shapes and different sizes, and they break down when used with clusters of different densities, since it doesn’t employ density parameters.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/partitional-clustering-algorithm.mp4\" type=\"video/mp4\">\n</video>\n\n<small class=\"video\">Partitional clustering algorithms deal with the data space and focus on creating a certain number of divisions of the space. Source: [What Matrix](https://www.whatmatrix.com/portal/clustering-algorithms-from-start-to-state-of-the-art-2/)</small>\n\n**K-means** is an example of a partitional clustering algorithm. Once the algorithm has been run and the groups are defined, any new data can be easily assigned to the existing groups. K-means is an extremely popular clustering algorithm, widely used in tasks like behavioral segmentation, inventory categorization, sorting sensor measurements, and detecting bots or anomalies.\n\n## K-means clustering\n\nFrom the universe of unsupervised learning algorithms, K-means is probably the most recognized one. This algorithm has a clear objective: partition the data space in such a way so that data points within the same cluster are as similar as possible (intra-class similarity), while data points from different clusters are as dissimilar as possible (inter-class similarity).\n\n![Intercluster and intracluster distance](/images/intercluster-intracluster-distance.png)\n<small>An illustration of inter-cluster and intra-cluster distance. Source: [dinhanhthi.com](https://dinhanhthi.com/)\n</small>\n\nIn K-means, each cluster is represented by its center (called a “centroid”), which corresponds to the arithmetic mean of data points assigned to the cluster. A **centroid** is a data point that represents the center of the cluster (the mean), and it might not necessarily be a member of the dataset. This way, the algorithm works through an iterative process until each data point is closer to its own cluster’s centroid than to other clusters’ centroids, minimizing intra-cluster distance at each step. But how?\n\nK-means searches for a predetermined number of clusters within an unlabelled dataset by using an iterative method to produce a final clustering based on the number of clusters defined by the user (represented by the variable K). For example, by setting “k” equal to 2, your dataset will be grouped in 2 clusters, while if you set “k” equal to 4 you will group the data in 4 clusters.\n\nK-means triggers its process with arbitrarily chosen data points as proposed centroids of the groups and iteratively recalculates new centroids in order to converge to a final clustering of the data points. Specifically, the process works as follows:\n\n1. The algorithm randomly chooses a centroid for each cluster. For example, if we choose a “k” of 3, the algorithm randomly picks 3 centroids.\n\n2. K-means assigns every data point in the dataset to the nearest centroid, meaning that a data point is considered to be in a particular cluster if it is closer to that cluster’s centroid than any other centroid.\n\n3. For every cluster, the algorithm recomputes the centroid by taking the average of all points in the cluster, reducing the total intra-cluster variance in relation to the previous step. Since the centroids change, the algorithm re-assigns the points to the closest centroid.\n\n4. The algorithm repeats the calculation of centroids and assignment of points until the sum of distances between the data points and their corresponding centroid is minimized, a maximum number of iterations is reached, or no changes in centroids value are produced.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/k-means-algorithm.mp4\" type=\"video/mp4\">\n</video>\n\n<small class=\"video\">The figure shows the centroids updating through the first five iterations from two different runs of the K-means algorithm on the same dataset. The purpose of this figure is to show that the initialization of the centroids is an important step. Source: [Real Python](https://realpython.com/k-means-clustering-python/)</small>\n\n### Finding the value of K\n\nHow do you choose the right value of “k”? When you define “k” you are telling the algorithm how many centroids you want, but how do you know how many clusters to produce?\n\nOne popular approach is testing different numbers of clusters and measuring the resulting Sum of Squared Errors (SSE), choosing the “k” value at which an increase will cause a very small decrease in the error sum, while a decrease will sharply increase the error sum. This point that defines the optimal number of clusters is known as the “elbow point”.\n\n![Sum of squared errors](/images/sum-squared-errors.png)\n<small>As the number of clusters increases, the Sum of Squared Errors within clusters will start to decrease. The error value is largest when “k” = 1. We can see that the graph will rapidly change at a point,thus creating an elbow shape. From this point, the graph starts to move almost parallel to the X-axis. The “k”  value corresponding to this point is the optimal number of clusters. Source: [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/01/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning/)</small>\n\nAnother alternative is to use the Silhouette Coefficient metric. This coefficient is a measure of cluster cohesion and separation, frequently used in unsupervised learning problems. It quantifies how well a data point fits into its assigned cluster based on two factors:\n\n- How close the data point is to other points in the cluster\n- How far away the data point is from points in other clusters\n\nSilhouette coefficient values range between -1 and 1, meaning that well-defined clusters result in positive values of this coefficient, while incorrect clusters will result in negative values.\n\nWe can use a Silhouette plot to display a [measure of how close each point in one cluster is to a point in the neighboring clusters](https://neptune.ai/blog/k-means-clustering) and thus provide a way to assess parameters like the number of clusters visually.\n\n![Silhouette analysis](/images/silhouette-analysis.png)\n<small>Using the above Silhouette analysis, we can choose an optimal k value as 3 because the average silhouette score is higher and indicates that the data points are optimally positioned. Source: [Neptune Blog](https://neptune.ai/blog/k-means-clustering)</small>\n\n## When to Use K-Means Clustering\n\nK-means presents huge [advantages](https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages), since it scales to large data sets, is relatively simple to implement, guarantees convergence, can warm-start the positions of centroids, it easily adapts to new examples, and generalizes to clusters of different shapes and sizes, such as elliptical clusters.\n\nBut as any other Machine Learning method, it also presents downsides. The most obvious one is that you need to define the number of clusters manually, and, although we showed some ways to find the optimal “k”, this is a decision that will deeply affect the results.\n\nAlso, K-means is highly dependent on initial values. For low values of “k”, you can mitigate this dependence by running K-means several times with different initial values and picking the best result. As “k” increases, you need [advanced versions of K-means](https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages) to pick better values of the initial centroids (called K-means seeding). K-means produces clusters with uniform sizes (in terms of density and quantity of observations), even though the underlying data might behave in a very different way. Finally, K-means is very sensitive to outliers, since centroids can be dragged in the presence of noisy data.\n\nK-means is highly flexible and can be used to cluster data in lots of different domains. It also can be modified to adapt it to specific challenges, making it extremely powerful. Whether you’re dealing with structured data, [embeddings](/learn/vector-embeddings/), or any other data type, you should definitely consider using K-means.\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbbe"
  },
  "filename": "dense-vector-embeddings-nlp.md",
  "title": "ebook-post",
  "category": "\"Dense Vectors\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Dense Vectors\"\nheadline: \"Dense Vectors: Capturing Meaning with Code\"\ncategories:\n  - NLP for Semantic Search\ntoc: >-\nweight: 1\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: An overview of dense vector embeddings with NLP.\n# Open Graph\nimages: ['/images/nlp-embedding-methods-8.jpg']\n---\n\n**[Pinecone](/) is a vector database for storing and searching through dense vectors. Why would you ever want to do that? Keep reading to find out, then [try Pinecone for free](https://app.pinecone.io).**\n\nThere is perhaps no greater contributor to the success of modern Natural Language Processing (NLP) technology than vector representations of language. The meteoric rise of NLP was ignited with the introduction of word2vec in 2013 [1].\n\nWord2vec is one of the most iconic and earliest examples of dense vectors representing text. But since the days of word2vec, developments in representing language have advanced at ludicrous speeds.\n\nThis article will explore *why* we use dense vectors — and some of the best approaches to building dense vectors available today.\n\n<p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/bVZJ_O_-0RE\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>\n\n## Dense vs Sparse Vectors\n\nThe first question we should ask is *why should we represent text using vectors?* The straightforward answer is that for a computer to understand human-readable text, we need to convert our text into a machine-readable format.\n\nLanguage is inherently full of information, so we need a reasonably large amount of data to represent even small amounts of text. [Vectors](/learn/vector-embeddings/) are naturally good candidates for this format.\n\nWe also have two options for vector representation; *sparse* vectors or *dense* vectors.\n\nSparse vectors can be stored more efficiently and allow us to perform syntax-based comparisons of two sequences. For example, given two sentences; `\"Bill ran from the giraffe toward the dolphin\"`, and `\"Bill ran from the dolphin toward the giraffe\"` we would get a perfect (or near-perfect) match.\n\nWhy? Because despite the meaning of the sentences being different, they are composed of the same syntax (e.g., words). And so, sparse vectors would be closely or even perfectly matched (depending on the construction approach).\n\nWhere sparse vectors represent text syntax, we could view dense vectors as *numerical representations of semantic meaning*. Typically, we are taking words and encoding them into very dense, high-dimensional vectors. The abstract meaning and relationship of words are numerically encoded.\n\n---\n\n*Sparse vectors are called sparse because vectors are sparsely populated with information. Typically we would be looking at thousands of zeros to find a few ones (our relevant information). Consequently, these vectors can contain many dimensions, often in the tens of thousands.*\n\n![clustered](/images/nlp-embedding-methods-2.jpg)\n<small>Sparse and dense vector comparison. Sparse vectors contain *sparsely* distributed bits of information, whereas dense vectors are much more information-rich with densely-packed information in every dimension.</small>\n\n---\n\n*Dense vectors are still highly dimensional (784-dimensions are common, but it can be more or less). However, each dimension contains relevant information, determined by a neural net — compressing these vectors is more complex, so they typically use more memory.*\n\n---\n\nImagine we create dense vectors for every word in a book, reduce the dimensionality of those vectors and then visualize them in 3D — we will be able to identify relationships. For example, days of the week may be clustered together:\n\n![clustered](/images/nlp-embedding-methods-3.jpg)\n\n<small>Example of the clustering of related keywords as is typical with word embeddings such as *word2vec* or *GLoVe*.</small>\n\nOr we could perform 'word-based' arithmetic:\n\n![vector_arithmetic](/images/nlp-embedding-methods-4.jpg)\n\n<small>A classic example of arithmetic performed on word vectors from another Mikolov paper [2].</small>\n\nAnd all of this is achieved using equally complex neural nets, which identify patterns from *massive* amounts of text data and translate them into dense vectors.\n\nTherefore we can view the difference between sparse and dense vectors as representing *syntax* in language versus representing *semantics* in language.\n\n## Generating Dense Vectors\n\nMany technologies exist for building dense vectors, ranging from vector representations of words or sentences, Major League Baseball players [3], or even cross-media text and images.\n\n---\n\n*We usually take an existing public model to generate vectors. For almost every scenario there is a high-performance model out there and it is easier, faster, and *often* much more accurate to use them. There are cases, for example for industry or language-specific embeddings where you sometimes need to fine-tune or even train a new model from scratch, but it isn’t common.*\n\n---\n\nWe will explore a few of the most exciting and valuable of these technologies, including:\n\n* The '2vec' methods\n* Sentence Transformers\n* Dense Passage Retrievers (DPR)\n* Vision Transformers (ViT)\n\n### Word2Vec\n\nAlthough we now have superior technologies for building embeddings, no overview on dense vectors would be complete without word2vec. Although *not* the first, it was the first widely used dense embedding model thanks to (1) being *very good*, and (2) the release of the [word2vec toolkit](https://code.google.com/archive/p/word2vec/) — allowing easy training or usage of pre-trained word2vec embeddings.\n\nGiven a sentence, word embeddings are created by taking a specific word (translated to a one-hot encoded vector) and mapping it to surrounding words through an encoder-decoder neural net.\n\n![skip_gram](/images/nlp-embedding-methods-5.jpg)\n\n<small>The skip-gram approach to building dense vectors embeddings in word2vec.</small>\n\nThis is the *skip-gram* version of word2vec which, given a word `fox`, attempts to predict surrounding words (its context). After training we discard the left and right blocks, keeping only the middle dense vector. This vector represents the word to the left of the diagram and can be used to embed this word for downstream language models.\n\nWe also have the *continuous bag of words (CBOW)*, which switches the direction and aims to predict a word based on its context. This time we produce an embedding for the word on the right (in this case, still `fox`).\n\n![cbow](/images/nlp-embedding-methods-6.jpg)\n\n<small>The continuous bag of words (CBOW) approach to building dense vector embeddings in word2vec.</small>\n\nBoth skip-gram and CBOW are alike in that they produce a dense embedding vector from the middle *hidden layer* of the encoder-decoder network.\n\nFrom this, Mikolov et al. produced the infamous `King - Man + Woman == Queen` example of vector arithmetic applied to language we saw earlier [2].\n\nWord2vec spurred a flurry of advances in NLP. Still, when it came to representing longer chunks of text using single vectors — word2vec was useless. It allowed us to encode single words (or n-grams) but nothing more, meaning long chunks of text could only be represented by *many* vectors.\n\nTo compare longer chunks of text effectively we need it to be represented by a single vector. Because of this limitation, several *extended* embedding methods quickly cropped up, such as sentence2vec and doc2vec.\n\nWhether word2vec, sentence2vec, or even (batter|pitcher)2vec (representations of Major League Baseball players [3]), we now have vastly superior technologies for building these dense vectors. So although *'2vec'* is where it started, we don't often see them in use today.\n\n### Sentence Similarity\n\nWe've explored the beginnings of word-based embedding with word2vec and briefly touched on the other *2vecs* that popped up, aiming to apply this vector embedding approach to longer chunks of text.\n\nWe see this same evolution with transformer models. These models produce incredibly information-rich dense vectors, which can be used for a variety of applications from sentiment analysis to question-answering. Thanks to these rich embeddings, transformers have become the dominant modern-day language models.\n\n[BERT](/learn/semantic-search/) is perhaps the most famous of these transformer architectures (although the following applies to *most* transformer models).\n\nWithin BERT, we produce vector embeddings for each word (or *token*) similar to word2vec. However, embeddings are much richer thanks to much deeper networks — and we can even encode the *context* of words thanks to the attention mechanism.\n\nThe attention mechanism allows BERT to prioritize which context words should have the biggest impact on a specific embedding by considering the *alignment* of said context words (we can imagine it as BERT literally *paying attention* to specific words depending on the context).\n\nWhat we mean by 'context' is, where word2vec would produce the same vector for 'bank' whether it was *\"a grassy bank\"* or *\"the bank of England\"* — BERT would instead modify the encoding for *bank* based on the surrounding context, thanks to the attention mechanism.\n\nHowever, there is a problem here. We want to focus on comparing *sentences*, not words. And BERT embeddings are produced for each token. So this doesn't help us in sentence-pair comparisons. What we need is a single vector that represents our sentences or paragraphs like sentence2vec.\n\nThe first transformer explicitly built for this was *Sentence-BERT (SBERT)*, a modified version of BERT [4].\n\nBERT (and SBERT) use a *WordPiece* tokenizer — meaning that every word is equal to one *or more* tokens. SBERT allows us to create a single vector embedding for sequences containing no more than 128 tokens. Anything beyond this limit is cut.\n\nThis limit isn't ideal for *long* pieces of text, but more than enough when comparing sentences or small-average length paragraphs. And many of the latest models allow for longer sequence lengths too!\n\n#### Embedding With Sentence Transformers\n\nLet's look at how we can quickly pull together some sentence embeddings using the `sentence-transformers` library [5]. First, we import the library and initialize a sentence transformer model from Microsoft called `all-mpnet-base-v2` (maximum sequence length of `384`).\n\n{{< notebook file=\"sentence-transformers-init\" height=\"full\" >}}\n\n\nThen we can go ahead and encode a few sentences, some more similar than others — while sharing *very few* matching words.\n\n{{< notebook file=\"encoding-sentences-1\" height=\"full\" >}}\n\n\nAnd what does our sentence transformer produce from these sentences? A 768-dimensional dense representation of our sentence. The performance of these embeddings when compared using a similarity metric such as cosine similarity is, in most cases — excellent.\n\n{{< notebook file=\"sentence-cos-sim\" height=\"full\" >}}\n\n\nDespite our most semantically similar sentences about bees and their queen sharing *zero* descriptive words, our model correctly embeds these sentences in the closest vector space when measured with cosine similarity!\n\n### Question-Answering\n\nAnother widespread use of transformer models is for questions and answers (Q&A). Within Q&A, there are several different *architectures* we can use. One of the most common is *open domain Q&A (ODQA)*.\n\nODQA allows us to take a big set of sentences/paragraphs that contain answers to our questions (such as paragraphs from Wikipedia pages). We then ask a question to return a small chunk of one (or more) of those paragraphs which best answers our question.\n\nWhen doing this, we are making use of three components or models:\n\n* Some sort of **database** to store our sentence/paragraphs (called *contexts*).\n* A **retriever** retrieves contexts that it sees as similar to our question.\n* A **reader** model which extracts the *answer* from our related context(s).\n\n![odqa](/images/nlp-embedding-methods-7.jpg)\n\n<small>An example open domain question-answering (ODQA) architecture.</small>\n\nThe *retriever* portion of this architecture is our focus here. Imagine we use a sentence-transformer model. Given a question, the retriever would return sentences most similar to our question — but we want answers *not* questions.\n\nInstead, we want a model that can map question-answers pairs to the same point in vector space. So given the two sentences:\n\n```\n\"What is the capital of France?\"\nAND\n\"The capital of France is Paris.\"\n```\n\nWe want a model that maps these two sentences to the same (or *very close*) vectors. And so when we receive a question `\"What is the capital of France?\"`, we want the output vector to have very high similarity to the vector representation of `\"The capital of France is Paris.\"` in our [vector database](/learn/vector-database/).\n\nThe most popular model for this is Facebook AI's *Dense Passage Retriever (DPR)*.\n\nDPR consists of two smaller models — a *context* encoder and a *query* encoder. Again they're both using the BERT architecture and are trained in parallel on question-answer pairs. We use a contrastive loss function, calculated as the difference between the two vectors output by each encoder [6].\n\n![dpr](/images/nlp-embedding-methods-8.jpg)\n\n<small>Bi-encoder structure of DPR, we have both a *question encoder* and a *context encoder* — both are optimized to output the same (or close) embeddings for each question-context pair.</small>\n\nSo when we give our question encoder `\"What is the capital of France?\"`, we would hope that the output vector would be similar to the vector output by our context encoder for `\"The capital of France is Paris.\"`.\n\nWe can't rely on all of the question-answer relationships on having been seen during training. So when we input a new question such as `\"What is the capital of Australia?\"` our model might output a vector that we could think of as similar to `\"The capital of Australia is ___\"`. When we compare that to context embeddings in our database, this *should* be similar to `\"The capital of Australia is Canberra\"` (or so we hope).\n\n#### Fast DPR Setup\n\nLet's take a quick look at building some context and query embeddings with DPR. We'll be using the `transformers` library from Hugging Face.\n\nFirst, we initialize tokenizers and models for both our context (`ctx`) model and `question` model.\n\n{{< notebook file=\"init-qa\" height=\"full\" >}}\n\n\nGiven a question and several contexts we tokenize and encode like so:\n\n{{< notebook file=\"qa-encode\" height=\"full\" >}}\n\n\n*Note that we have included the questions within our contexts to confirm that the bi-encoder architecture is not just producing a straightforward semantic similarity operation as with sentence-transformers.*\n\nNow we can compare our query embeddings `xq` against all of our context embeddings `xb` to see which are the most similar with *cosine similarity*.\n\n{{< notebook file=\"qa-cos-sim\" height=\"full\" >}}\n\n\nOut of our three questions, we returned two correct answers as the *very top* answer. It’s clear that DPR is not the *perfect* model, particularly when considering the simple nature of our questions and small dataset for DPR to retrieve from.\n\nOn the positive side however, in ODQA we would return many more contexts and allow a *reader* model to identify the best answers. Reader models can ‘re-rank’ contexts, so retrieving the top context immediately is not required to return the correct answer. If we were to retrieve the most relevant result 66% of the time, it would likely be a good result.\n\nWe can also see that despite hiding *exact matches* to our questions in the contexts, they interfered with only our last question, being correctly ignored by the first two questions.\n\n### Vision Transformers\n\nComputer vision (CV) has become the stage for some exciting advances from transformer models — which have historically been restricted to NLP.\n\nThese advances look to make transformers the first widely adopted ML models that excel in both NLP *and* CV. And in the same way that we've been creating dense vectors representing language. We can do the same for images — and even encode images and text into the same vector space.\n\n![same_vector_space](/images/nlp-embedding-methods-9.jpg)\n\n<small>Using specific text and image encoders, we can encode text and images to the same vector space. Photo credit [Alvan Nee](https://unsplash.com/photos/T-0EW-SEbsE).</small>\n\nThe *Vision Transformer (ViT)* was the first transformer applied to CV without the assistance of any upstream CNNs (as with VisualBERT [7]). The authors found that ViT can *sometimes* outperform state-of-the-art (SOTA) CNNs (the long-reigning masters of CV) [8].\n\nThese ViT transformers have been used alongside the more traditional language transformers to produce fascinating image and text encoders, as with OpenAI's CLIP model [9].\n\nThe CLIP model uses two encoders like DPR, but this time we use a ViT model as our image encoder and a *masked self-attention* transformer like BERT for text [10]. As with DPR, these two models are trained in parallel and optimized via a contrastive loss function — producing *high similarity* vectors for image-text pairs.\n\nThat means that we can encode a set of images and then match those images to a caption of our choosing. And we can use the same encoding and cosine similarity logic we have used throughout the article. Let's go ahead and try.\n\n#### Image-Text Embedding\n\nLet's first get a few images to test. We will be using three images of dogs doing different things from Unsplash (links in the caption below).\n\n{{< notebook file=\"get-images\" height=\"full\" >}}\n\n\n![dog_pics](/images/nlp-embedding-methods-10.jpg)\n\n<small>Images downloaded from Unsplash (captions have been manually added — they are not included with the images), photo credits to Cristian Castillo \\[[1](https://unsplash.com/photos/73pyV0JJOmE), [2](https://unsplash.com/photos/qA9wk6SDuVw)\\] and [Alvan Nee](https://unsplash.com/photos/T-0EW-SEbsE).</small>\n\nWe can initialize the CLIP `model` and `processor` using `transformers` from Hugging Face.\n\n{{< notebook file=\"CLIP-init\" height=\"full\" >}}\n\n\nNow let's create three true captions (plus some random) to describe our images and preprocess them through our `processor` before passing them on to our `model`. We will get output logits and use an `argmax` function to get our predictions.\n\n{{< notebook file=\"CLIP-predictions\" height=\"full\" >}}\n\n\nAnd there, we have flawless image-to-text matching with CLIP! Of course, it is not perfect (our examples here are reasonably straightforward), but it produces some awe-inspiring results in no time at all.\n\nOur model has dealt with comparing text and image embeddings. Still, if we wanted to extract those same embeddings used in the comparison, we access `outputs.text_embeds` and `outputs.image_embeds`.\n\n{{< notebook file=\"CLIP-embeddings\" height=\"full\" >}}\n\n\nAnd again, we can follow the same logic as we previously used with cosine similarity to find the closest matches. Let's compare the embedding for `'a dog hiding behind a tree'` with our three images with this alternative approach.\n\n{{< notebook file=\"CLIP-embeds-cos-sim\" height=\"full\" >}}\n\n\nAs expected, we return the dog hiding behind a tree!\n\n---\n\nThat's it for this overview of both the early days of dense vector embeddings in NLP and the current SOTA. We've covered some of the most exciting applications of both text and image embeddings, such as:\n\n* Semantic Similarity with `sentence-transformers`.\n* Q&A retrieval using Facebook AI's DPR model.\n* Image-text matching with OpenAI's CLIP.\n\nWe hope you learned something from this article.\n\n{{< newsletter text=\"Subscribe for more vector search tutorials!\" inputText=\"Email address...\" buttonText=\"Submit\">}}\n\n## References\n\n[1] T. Mikolov, et al., [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781) (2013)\n\n[2] T. Mikolov, et al., [Linguistic Regularities in Continuous Space Word Representations](https://aclanthology.org/N13-1090/) (2013), NAACL HLT\n\n[3] M. Alcorn, [(batter|pitcher)2vec: Statistic-Free Talent Modeling With Neural Player Embeddings](https://www.sloansportsconference.com/research-papers/batter-pitcher-2vec-statistic-free-talent-modeling-with-neural-player-embeddings) (2017), MIT Sloan: Sports Analytics Conference\n\n[4] N. Reimers, I. Girevych, [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084) (2019), EMNLP\n\n[5] N. Reimers, [SentenceTransformers Documentation](https://www.sbert.net/index.html), sbert.net\n\n[6] V. Karpukhin, et al., [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) (2020), EMNLP\n\n[7] L. H. Li, et al., [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/abs/1908.03557) (2019), arXiv\n\n[8] A. Dosovitskiy, et al., [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) (2020), arXiv\n\n[9] A. Radford, et al., [CLIP: Connecting Text and Images](https://openai.com/blog/clip/) (2021), OpenAI Blog\n\n[10] [CLIP Model Card](https://huggingface.co/openai/clip-vit-base-patch32), Hugging Face"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbbf"
  },
  "filename": "product-quantization.md",
  "title": "ebook-post",
  "category": "\"Product Quantization\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Product Quantization\"\nheadline: \"Product Quantization: Compressing high-dimensional vectors by 97% \"\ncategories:\n  - \"Faiss: The Missing Manual\"\ntoc: >-\nweight: 5\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: Breaking down one of the most important developments in vector similarity search\n# Open Graph\nimages: ['/images/product-quantization-5.jpg']\n---\n\n![Product Quantization 101](/images/product-quantization-1.gif)\n\n---\n\n**[Pinecone](/) lets you add vector search to applications without knowing anything about algorithm optimizations, and [it's free to try](https://app.pinecone.io). However, we know you like seeing how things work, so enjoy learning about memory-efficient search with product quantization!**\n\n---\n\nVector similarity search can require huge amounts of memory. Indexes containing 1M dense vectors (a small dataset in today’s world) will often require several GBs of memory to store.\n\nThe problem of excessive memory usage is exasperated by high-dimensional data, and with ever-increasing dataset sizes, this can *very* quickly become unmanageable.\n\nProduct quantization (PQ) is a popular method for dramatically compressing high-dimensional vectors to use 97% less memory, and for making nearest-neighbor search speeds 5.5x faster in our tests.\n\nA composite IVF+PQ index speeds up the search by another 16.5x without affecting accuracy, for a whopping total speed increase of 92x compared to non-quantized indexes.\n\nIn this article, we will cover all you need to know about PQ: How it works, pros and cons, implementation in Faiss, composite IVFPQ indexes, and how to achieve the speed and memory optimizations mentioned above.\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/t9mRf2S5vDI\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n---\n\n## What is Quantization\n\nQuantization is a generic method that refers to the compression of data into a smaller space. I know that might not make much sense — let me explain.\n\nFirst, let's talk about dimensionality reduction — which is *not* the same as quantization.\n\nLet’s say we have a high-dimensional vector, it has a dimensionality of `128`. These values are 32-bit floats in the range of *0.0 -> 157.0* (our scope `S`). Through dimensionality reduction, we aim to produce another, lower-dimensionality vector.\n\n![Product Quantization](/images/product-quantization-2.jpg)\n<small>Dimensionality reduction reduces the dimensionality `D` of vectors, but not the scope S.</small>\n\nOn the other hand, we have quantization. Quantization does not care about dimensionality `D`. Instead, it targets the potential scope of values. Rather than reducing `D`, we reduce `S`.\n\n![Product Quantization](/images/product-quantization-4.jpg)\n<small>Quantization reduces the scope S of possible vectors. Note that with pre-quantization the scope is typically infinite.</small>\n\nThere are many ways of doing this. For example, we have *clustering*. When we cluster a set of vectors we replace the larger scope of potential values (all possible vectors), with a smaller *discrete and symbolic* set of centroids.\n\nAnd this is really how we can define a quantization operation. The transformation of a vector into a space with a finite number of possible values, where those values are *symbolic* representations of the original vector.\n\nJust to make it very clear, these symbolic representations vary in form. They can be centroids as is the case for PQ, or [binary codes like those produced by LSH](/learn/locality-sensitive-hashing-random-projection/).\n\n### Why Product Quantization?\n\nQuantization is primarily used to reduce the memory footprint of indexes — an important task when comparing large arrays of vectors as they must all be loaded in memory to be compared.\n\nPQ is not the only quantization method that does this, however — but other methods do not manage to reduce memory size as effectively as PQ. We can actually calculate memory usage and quantization operation complexity for PQ and other methods like so:\n\n```\nkmeans = kD\nPQ = mk^*D^* = k^{1/m}D\n```\n\nWe know that `D` represents the dimensionality of our input vectors, but `k` and `m` may be new. `k` represents the *total* number of centroids (or *codes*) that will be used to represent our vectors. And `m` represents the number of *subvectors* that we will split our vectors into (more on that later).\n\n*(A 'code' refers to the quantized representation of our vectors)*\n\n![Product Quantization](/images/product-quantization-3.jpg)\n<small>Memory usage (and complexity) vs dimensionality using k=2048 and m=8.</small>\n\nThe problem here is that for good results, the recommended `k` value is of `2048` (2<sup>11</sup>) or more[1]. Given a vector dimensionality of `D`, clustering without PQ leaves us with *very high* memory requirements and complexity:\n\n<script src=\"https://gist.github.com/jamescalam/2fc852bd28aa6ae140ce3f9ce0326936.js\"></script>\n\nGiven an m value of `8`, the equivalent memory usage and *assignment* complexity for PQ is significantly lower — thanks to the *chunking* of vectors into subvectors and the subquantization process being applied to those smaller dimensionalities `k*` and `D*`, equal to `k/m` and `D/m` respectively.\n\nA second important factor is quantizer training. Quantizers require datasets that are several times larger than k for effective training, that is *without product subquantization*.\n\nUsing subquantizers, we only need several multiples of `k* (which is k/m)` — this can still be a large number — but it can be significantly reduced.\n\n## How Product Quantization Works\n\nLet's work through the logic of PQ. We would usually have many vectors (all of equal length) — but for the sake of simplicity, we will use a single vector in our examples.\n\nIn short, PQ is the process of:\n\n* Taking a big, high-dimensional vector,\n* Splitting it into equally sized chunks — our subvectors,\n* Assigning each of these subvectors to its nearest *centroid* (also called reproduction/reconstruction values),\n* Replacing these centroid values with unique IDs — each ID represents a centroid\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/product-quantization-6.mp4\" type=\"video/mp4\">\n</video>\n\n<small class=\"video\">High-level view of PQ: Take a big vector, split it into subvectors, assign each to its nearest centroid value, and replace the centroid value with its unique ID — producing a tiny vector of IDs.</small>\n\nAt the end of the process, we've reduced our highly dimensional vector — that requires *a lot of memory* — to a tiny vector of IDs that require *very little memory*.\n\nOur vector is of length D 12. We start by splitting this vector into `m` subvectors like so:\n\n![Product Quantization](/images/product-quantization-5.jpg)\n<small>Here we are splitting our high-dimensional vector x into several subvectors u_j.</small>\n\n{{< notebook file=\"make-subvec\" height=\"full\" >}}\n\nWe can refer to each subvector by its position `j`.\n\nFor the next part, think about clustering. As a random example, given a large number of vectors we can say *\"I want three clusters\"* — and we then optimize these cluster centroids to split our vectors into *three* categories based on each vector’s nearest centroid.\n\nFor PQ we do the same thing with one minor difference. Each subvector space (subspace) is assigned its own set of clusters — and so what we produce is a set of clustering algorithms across multiple subspaces.\n\n{{< notebook file=\"build-clusters\" height=\"full\" >}}\n\nEach of our subvectors will be assigned to one of these centroids. In PQ terminology these centroids are called *reproduction values* and are represented by `cⱼ,ᵢ` where `j` is our subvector identifier, and `i` identifies the chosen centroid *(there are `k*` centroids for each subvector space j)*.\n\n![Product Quantization](/images/product-quantization-8.jpg)\n<small>Our subvectors are replaced with a specific centroid vector — which can then be replaced with a unique ID specific to that centroid vector.</small>\n\n{{< notebook file=\"clustering\" height=\"full\" >}}\n\nWhen we process a vector with PQ, it is split into our subvectors, those subvectors are then processed and assigned to their nearest (sub)cluster centroids (reproduction values).\n\nRather than storing our quantized vector to be represented by the `D*`-dimensional centroids, we replace it with a centroid ID. Every centroid `cⱼ,ᵢ` has its own ID, which can later be used to map those ID values back to the full centroids via our codebook `c`.\n\n{{< notebook file=\"ids-to-centroid\" height=\"full\" >}}\n\nWith that, we have compressed a 12-dimensional vector into a 4-dimensional vector of IDs. We have used a small dimensionality here for the sake of simplicity, and so the benefits of such a technique may not be inherently clear.\n\nLet’s switch from our original 12-dimensional vector of 8-bit integers to a more realistic 128-dimensional vector of 32-bit floats (as we will be using throughout the next section). We can find a good balance in performance after compression to an 8-bit integer vector containing just *eight* dimensions.\n\n```\nOriginal: 128×32 = 4096\nQuantized: 8×8 = 64\n```\nThat's a big difference — 64x!\n\n---\n\n## PQ Implementation in Faiss\n\nSo far we've worked through the logic behind a simple, readable implementation of PQ in Python. Realistically we wouldn't use this because it is not optimized and we already have excellent implementations elsewhere. Instead, we would use a library like [Faiss](/learn/faiss/) — or a production-ready service like [Pinecone](/).\n\nWe'll take a look at how we can build a PQ index in Faiss, and we'll even take a look at combining PQ with an Inverted File (IVF) step to improve search speed.\n\n<p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/BMYBwbkbVec\" title=\"Product Quantization Implementation in Faiss\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>\n\nBefore we start, we need to get data. We will be using the Sift1M dataset. It can be downloaded and opened using this script:\n\n{{< notebook file=\"get-sift1m\" height=\"full\" >}}\n\nNow let's move onto our first index: IndexPQ.\n\n### IndexPQ\n\nOur first index is a pure PQ implementation using IndexPQ. To initialize the index we need to define three parameters.\n\n{{< notebook file=\"index-pq\" height=\"full\" >}}\n\nWe have our vector dimensionality `D`, the number of subvectors we'd like to split our full vectors into (we must `assert` that `D` is divisible by `m`).\n\nFinally, we include the nbits parameter. This defines the number of bits that each subquantizer can use, we can translate this into the number of centroids assigned to each subspace as `k_ = 2**nbits`. An `nbits` of 11 leaves us with `2048` centroids per subspace.\n\nBecause we are using PQ, which uses clustering — we must train our index using our xb dataset.\n\n{{< notebook file=\"pq-train\" height=\"full\" >}}\n\nAnd with that, we can go ahead and add our vectors to the index and `search`.\n\n{{< notebook file=\"pq-search\" height=\"full\" >}}\n\nFrom our search, we will return the top `k` closest matches (not the same `k` used in earlier notation). Returning our distances in `dist`, and the indices in `I`.\n\nWe can compare the *recall* performance of our `IndexPQ` against that of a flat index — which has 'perfect' recall (thanks to not compressing vectors and performing an exhaustive search).\n\n{{< notebook file=\"l2-pq-recall\" height=\"full\" >}}\n\nWe're getting 50% which is a reasonable recall *if* we are happy to sacrifice the perfect results for the reduced memory usage of PQ. There's also a reduction to just 18% of the flat search time — something that we can improve *even further* using IVF later.\n\nLower recall rates are a major drawback of PQ. This can be counteracted *somewhat* by using larger `nbits` values at the cost of slower search times and *very* slow index construction times. However, very high recall is out of reach for both PQ and IVFPQ indexes. If higher recall is required another index should be considered.\n\nHow does IndexPQ compare to our flat index in terms of memory usage?\n\n{{< notebook file=\"flat-pq-memory\" height=\"full\" >}}\n\nMemory usage using IndexPQ is — put simply — fantastic, with a memory reduction of 98.4%. It is possible to translate some of these preposterous performance benefits into search speeds too by using an IVF+PQ index.\n\n### IndexIVFPQ\n\nTo speed up our search time we can add another step, using an IVF index, which will act as the initial broad stroke in reducing the scope of vectors in our search.\n\nAfter this, we continue our PQ search as we did before — but with a significantly reduced number of vectors. Thanks to minimizing our search scope, we should find we get vastly improved search speeds.\n\nLet's see how that works. First, we initialize our IVF+PQ index like so:\n\n{{< notebook file=\"index-ivfpq\" height=\"full\" >}}\n\nWe have a new parameter here, `nlist` defines how many Voronoi cells we use to cluster our *already* quantized PQ vectors ([learn more about IndexIVF here](/learn/vector-indexes/)).\nYou may be asking, what on earth is a Voronoi cell — what does any of this even mean? Let's visualize some 2D 'PQ vectors':\n\n![PQ Vectors in 2D](/images/product-quantization-7.jpg)\n<small>2D chart showing our reconstructed 'PQ' vectors. However, in reality, we would never use PQ for 2D vectors as there is simply not enough dimensionality for us to split into subvectors and subquantization.</small>\n\nLet's add some Voronoi cells:\n\n![Product Quantization: Voroni Cells](/images/product-quantization-10.jpg)\n<small>2D chart showing our quantized 'PQ' vectors that have now been assigned to different Voronoi cells via IVF.</small>\n\nAt a high level, they're simply a set of partitions. Similar vectors are assigned to different partitions (or *cells*), and when it comes to search — we introduce our query vector xq and restrict our search to the nearest cell:\n\n![Product Quantization: Voroni Cells](/images/product-quantization-9.jpg)\n<small>IVF allows us to restrict our search to only vectors that have been assigned nearby cells. The magenta point is our query vector xq.\nNow let's go ahead with our `train` and `search` — and see how our search speed and recall are doing.</small>\n\n{{< notebook file=\"ivfpq-train-search\" height=\"full\" >}}\n\nA lightning-fast search time of 86.3μs, but the recall has decreased from our IndexPQ significantly (50% to 34%). Given equivalent parameters, both IndexPQ and IndexIVFPQ *should* be able to attain equal recall performance.\n\nThe secret to improving our recall, in this case, is bumping up the `nprobe` parameter — which tells us *how many* of the nearest Voronoi cells to include in our search scope.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/product-quantization-11-ivf-nprobe.mp4\" type=\"video/mp4\">\n</video>\n\n<small class=\"video\">2D chart showing our quantized 'PQ' vectors that have now been assigned to different Voronoi cells via IVF.</small>\n\nAt one extreme, we can include all cells by setting nprobe to our `nlist` value — this will return the maximum possible recall.\n\nOf course, we don't want to include all cells. It would make our IVF index pointless as we would then not be restricting our search scope (making this equivalent to a flat index). Instead, we find the lowest `nprobe` value that achieves this recall performance.\n\n{{< notebook file=\"nprobe-ivfpq\" height=\"full\" >}}\n\nWith a nprobe of 48, we achieve the best possible recall score of 52% (as demonstrated with `nprobe == 2048`), while minimizing our search scope (and therefore maximizing search speeds).\n\nBy adding our IVF step, we've dramatically reduced our search time from 1.49ms for IndexPQ to 0.09ms for IndexIVFPQ. And thanks to our PQ vectors we've then paired that with minuscule memory usage that’s 96% lower than the Flat index.\n\nAll in all, IndexIVFPQ gave us a *huge reduction* in memory usage — albeit slightly larger than IndexPQ at 9.2MB vs 6.5MB — and lightning-fast search speeds, all while maintaining a reasonable recall of around 50%.\n\n---\n\nThat's it for this article! We've covered the intuition behind product quantization (PQ), and how it manages to compress our index and enable incredibly efficient memory usage.\n\nWe put together the Faiss IndexPQ implementation and tested search times, recall, and memory usage — then optimized the index even further by pairing it with an IVF index using IndexIVFPQ.\n\n<table class=\"table table-responsive\">\n<thead>\n  <tr>\n    <th></th>\n    <th>FlatL2</th>\n    <th>PQ</th>\n    <th>IVFPQ</th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td><strong>Recall (%)</strong></td>\n    <td>100</td>\n    <td>50</td>\n    <td>52</td>\n  </tr>\n  <tr>\n    <td><strong>Speed (ms)</strong></td>\n    <td>8.26</td>\n    <td>1.49</td>\n    <td>0.09</td>\n  </tr>\n  <tr>\n    <td><strong>Memory (MB)</strong></td>\n    <td>256</td>\n    <td>6.5</td>\n    <td>9.2</td>\n  </tr>\n</tbody>\n</table>\n\nThe results of our tests show impressive memory compression and search speeds, with reasonable recall scores.\n\nIf you're interested in learning more about the variety of indexes available in search, including more detail on the IVF index, read our [article about the best indexes for similarity search](/learn/vector-indexes/).\n\n{{< newsletter text=\"Subscribe for the latest in similarity search!\" inputText=\"Email address...\" buttonText=\"Submit\">}}\n\n---\n\n## References\n\n* [1] H Jégou, et al., [Product quantization for nearest neighbor search](https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf) (2010)\n* [Jupyter Notebooks](https://github.com/pinecone-io/examples/tree/master/product_quantization)\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbc0"
  },
  "filename": "gpl.md",
  "title": "ebook-post",
  "category": "\"Domain Adaptation with Generative Pseudo-Labeling (GPL)\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Domain Adaptation with Generative Pseudo-Labeling (GPL)\"\nheadline: \"Domain Adaptation with Generative Pseudo-Labeling (GPL)\"\ncategories:\n  - NLP for Semantic Search\ntoc: >-\nweight: 13\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: Unsupervised Domain adaptation with Generative Pseudo-Labeling\n# Open graph\nimages: ['https://www.pinecone.io/images/gpl-0.jpg']\n---\n\nIn 1999, a concept known as the *semantic web* was described by the creator of the *World Wide Web*, Tim Berners-Lee. This dream of Berners-Lee was the internet of today that we know and love but deeply understood by machines [1].\n\nThis futuristic vision had seemed to be utterly infeasible, but, in recent years, has become much more than a dream. Thanks to techniques like **G**enerative **P**seudo-**L**abeling (GPL) that allow us to fine-tune new or existing models in previously inaccessible domains, machines are ever closer to understanding the meaning behind the content on the web.\n\n<div style=\"padding: 1rem;\">\n    <em>\n    I have a dream for the Web [in which computers] become capable of analyzing all the data on the Web – the content, links, and transactions between people and computers. A \"Semantic Web\", which makes this possible, has yet to emerge, but when it does, the day-to-day mechanisms of trade, bureaucracy and our daily lives will be handled by machines talking to machines. The \"intelligent agents\" people have touted for ages will finally materialize.\n    </em>\n    <div style=\"padding-top: 1rem; text-align: right;\">\n        <b><em>- Tim Berners-Lee, 1999</em></b>\n    </div>\n</div>\n\nBerners-Lee's vision is fascinating, in particular the *\"intelligent agents\"* referred to in his quote above. Generally speaking, these intelligent agents (IAs) perceive  their environment, take actions based on that environment to achieve a particular goal, and improve their performance with self-learning.\n\nThese IAs sound very much like the ML models we see today. Some models can look at a web page's content (its environment), scrape and classify the meaning of this content (takes actions to achieve goals), and do this through an iterative learning process.\n\nA model that can read and comprehend the *meaning* of language from the internet is a vital component of the semantic web. There are already models that can do this within a limited scope.\n\nHowever, there is a problem. These **L**anguage **M**odels (LMs) need to learn before becoming these autonomous, language-comprehending IAs. They must be *trained*.\n\nTraining LMs is hard; they need vast amounts of data. In particular, bi-encoder models (as explored in earlier chapters on [AugSBERT](/learn/data-augmentation/) and [GenQ](/learn/genq/)) that can enable a large chunk of this semantic web are notoriously data-hungry.\n\nSometimes this is okay. We can fine-tune a model easily in places where we have massive amounts of relevant and labeled data. We can use a simple [supervised fine-tuning approach](/learn/fine-tune-sentence-transformers-mnr/). Unfortunately, these scenarios are few and far between. It is for this reason that existing models have this limited scope. So, what can we do?\n\nWe should first consider why it is hard to get data to train these models. On one hand, the internet is full of data, and, on the other, this data is *not* in the format we need. We usually need to use a supervised training method to train a high-performance bi-encoder model.\n\nSupervised training methods require labeled data. The problem with labeled data is that a human must (almost always) manually create it.\n\nCurrently, we have data-hungry models that require supervised training methods. We must find a way to train a model with *little* labeled data or use *unsupervised* methods that need nothing more than unstructured text data.\n\nFortunately, there are *some* unsupervised (or supervised using very little data) approaches, such as:\n\n* [Multilingual Knowledge Distillation](/learn/multilingual-transformers/#training-approaches) for low-resource languages.\n* [TSDAE](/learn/unsupervised-training-sentence-transformers/) for building simple similarity models without labeled data.\n* Data augmentation with AugSBERT for [in-domain](/learn/data-augmentation/) and [out-of-domain](/learn/domain-transfer/) tasks.\n* [GenQ](/learn/genq/) for asymmetric semantic search without labeled data.\n\nWe can apply these approaches in different scenarios with varying degrees of success. As we've seen, there is a lot of potential for models being trained using unsupervised techniques. These no (or low) resource scenarios cover the vast majority of use-cases, many of which are the most unique and interesting.\n\n![datasets_and_uniqueness](/images/gpl-1.png)\n<small>As the domain (eg topic, language) becomes more niche, the number of available labeled datasets decreases. The vast majority of domains have no labeled datasets.</small>\n\nFor example, we may identify an opportunity to introduce semantic search on internal financial documents with highly technical language specific to our organization. Or a specific use-case using a less common language such as Swahili or Dhivehi.\n\nIt is infeasible for the semantic web to find labeled data for every topic, language, or format of information found on the internet. Because of this, the dream only becomes a reality once there are techniques that can train or adapt *high-performance* IAs with nothing more than the text found on the internet, without human-made labels or curation.\n\nThere is research producing techniques placing us ever closer to this reality. One of the most promising is GPL [2]. GPL is almost a culmination of the techniques listed above. At its core, it allows us to take unstructured text data and use it to build models that can understand this text. These models can then intelligently respond to natural language queries regarding this same text data.\n\nIt is a fascinating approach, with massive potential across innumerous use cases spanning all industries and borders. With that in mind, let's dive into the details of GPL and how we can implement it to build high-performance LMs with nothing more than plain text.\n\n---\n\nWatch our webinar [Searching Freely: Using GPL for Semantic Search](https://www.youtube.com/watch?v=OQhoi1CabWw) for a rundown of GPL presented by Nils Reimers, the creator of *sentence-transformers*.\n\n---\n\n## GPL Overview\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/uEbCXwInnPs\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\nGPL can be used in two ways, as a technique used in fine-tuning a pretrained model (such as the BERT base model), or as a technique for domain adaptation of an already fine-tuned bi-encoder model (such as SBERT).\n\nBy *domain adaptation* we mean the adaptation of an existing [sentence transformer](/learn/sentence-embeddings/) to new topics (domains). Effective domain adaptation is incredibly helpful in taking models pretrained on large, existing datasets, and helping them understand a new domain that lacks any labeled datasets.\n\nFor example, any model trained on data from before 2019 will be blissfully unaware of COVID-19 and everything that comes with it. If we query any of these pre-2019 models about COVID, they will struggle to return relevant information as they simply do not know what it is, and what is relevant.\n\n{{< notebook file=\"gpl-old-model-examples\" height=\"full\" >}}\n\nGPL hopes to solve this problem by allowing us to take existing models and *adapt* them to new domains using nothing more than unlabeled data. By using unlabeled data we greatly enhance the ease of finding relevant data, all we need is unstructured text.\n\n---\n\nJust looking for a fast implementation of GPL? Skip ahead to *[\"A Simpler Approach\"](/learn/gpl/#a-simpler-approach)*.\n\n---\n\nAs you may have guessed, the same applies to the first scenario of *fine-tuning a pretrained model*. It can be hard to find relevant, labeled data. With GPL we don't need to. Unstructured text is all you need.\n\n### How it Works\n\nAt a high level, GPL consists of *three* data preparation steps and *one* fine-tuning step. We will begin by looking at the data preparation portion of GPL. The three steps are:\n\n* Query generation, creating queries from passages.\n* Negative mining, retrieving similar passages that do not match (negatives).\n* Pseudo labeling, using a cross-encoder model to assign similarity scores to pairs.\n\n![gpl_overview](/images/gpl-2.jpg)\n<small>Overview of the GPL process. Beginning with passages *P<sup>+</sup>*, we generate queries *Q*. The passages are indexed and a dense retrieval step is used to find high similarity *'negative'* passages *P<sup>-</sup>*. We then use a cross encoder to produce margin scores.</small>\n\nEach of these steps requires the use of a pre-existing model fine-tuned for each task. The team that introduced GPL also provided models that handle each task. We will discuss these models as we introduce each step and note alternative models where relevant.\n\n#### 1. Query Generation\n\nGPL is perfect for scenarios where we have no labeled data. However, it does require a *large amount* of unstructured text. That could be text data scraped from web pages, PDF documents, etc. The only requirement is that this text data is *in-domain*, meaning it is relevant to our particular use case.\n\n![in_and_out_domain](/images/gpl-3.jpg)\n<small>For a target domain of *German financial documents*, any data that fits the topic and we would expect our model to encounter is *in-domain*. Anything else is *out-of-domain*.</small>\n\nIn our examples, we will use the *CORD-19* dataset. CORD-19 can be downloaded using the script [found here](https://gist.github.com/jamescalam/06882ea05a307420c1354481325f08ad#file-00_download_cord_19-ipynb). The script will leave many JSON files in a directory called *document_parses/pdf_json* that we will be using in our query generation step. We will use a generator function called `get_text` to read in those files.\n\n{{< notebook file=\"gpl-passage-generator\" height=\"full\" >}}\n\nIf you've read our previous [chapter on GenQ](/learn/genq/), this step follows the same query generation process. However, we will outline the process for new readers.\n\nWe're starting with passages of data (the unstructured text data). Generally, these are reasonably long chunks of text, but not always.\n\n{{< notebook file=\"gpl-passage-gen-test\" height=\"full\" >}}\n\nGiven a passage, we pass it through a *query generation* T5 model. We can initialize this T5 model using *HuggingFace Transformers*.\n\n---\n\n*T5 refers to Google’s Text-to-Text Transfer Transformer. We discuss it in more detail in [the chapter on GenQ](/learn/genq/).*\n\n---\n\n\n{{< notebook file=\"gpl-gen-model-init\" height=\"full\" >}}\n\nWe are using the `doc2query/msmarco-t5-base-v1` model that was trained on a pre-COVID dataset. Nonetheless, when generating queries for COVID-related text the model can produce sensible questions by copying words from the passage text.\n\nWith this T5 model, we can begin generating queries that we use to produce synthetic *(query, passage) pairs*.\n\n{{< notebook file=\"gpl-query-gen-example\" height=\"full\" >}}\n\nQuery generation is *not perfect*. It can generate noisy, sometimes nonsensical queries. And this is where GPL improved upon GenQ. GenQ relies heavily on these synthetic queries being high-quality with little noise. With GPL, this is not the case as the final cross-encoder step labels the similarity of pairs. Meaning dissimilar pairs are likely to be labeled as such. GenQ does not have any such labeling step.\n\nWe now have *(query, passage) pairs* and can move onto the next step of identifying *negative* passages.\n\n*[Full script](https://gist.github.com/jamescalam/8498a03b66b0317d21575c0a0ac50f66#file-01_query_gen-ipynb)*\n\n### 2. Negative Mining\n\nThe (query, passage) pairs we have now are assumed to be positively similar, written as *(Q, P<sup>+</sup>)* where the query is *Q*, and the positive passage is *P<sup>+</sup>*.\n\nSuppose we fine-tune our bi-encoder on only these positive matches. In that case, our model will struggle to learn more nuanced differences. A good model must learn to distinguish between similar and dissimilar pairs even where the content of these different pairs is very similar.\n\nTo fix this, we perform a *negative mining* step to find highly similar passages to existing *P<sup>+</sup>* passages. As these new passages will be highly similar but *not* matches to our query *Q*, our model will need to learn how to distinguish them from genuine matches *P<sup>+</sup>*. We refer to these non-matches as *negative passages* and are written as *P<sup>-</sup>*.\n\nThe negative mining process is a retrieval step where, given a query, we return the *top_k* most similar results. Excluding the positive passage (if returned), we assume all other returned passages are negatives. We then select one of these *negative passages* at random to become the negative pair for our query.\n\nIt may seem counterintuitive at first. Why would we return the most similar passages and train a model to view these as dissimilar?\n\nYes, those returned results are the most similar passages to our query, but they are *not* the correct passage for our query. We are, in essence, increasing the similarity gap between *the* correct passage and all other passages, no matter how similar they may be.\n\nAdding these *'negative'* training examples *(Q, P<sup>-</sup>)* is a common approach used in many bi-encoder fine-tuning methods, including multiple negatives ranking *and* margin MSE loss (the latter of which we will be using). Using hard negatives in-particular can significantly improve the performance of our models [3].\n\n![gpl_random_negatives](/images/gpl-4.png)\n<small>The impact on model performance trained on MSMARCO with and without hard negatives. Model training used margin MSE loss. Adapted from [3].</small>\n\nWhen we later tune our model to identify the difference between these positive and negative passages, we are teaching it to determine what are often very nuanced differences.\n\nWith all of that in mind, we do need to understand that only some of the returned passages will be relevant. We will explain how that is handled in the Pseudo-labeling step later.\n\nMoving on to the implementation of negative mining. As before, we need an existing model to embed our passages and create searchable [dense vectors](/learn/vector-database/). We use the `msmarco-distilbert-base-tas-b` bi-encoder which was fine-tuned on pre-COVID datasets.\n\n{{< notebook file=\"gpl-neg-mine-model\" height=\"full\" >}}\n\nIn the GPL paper, two retrieval models are used and their results compared. To keep things simple, we will stick with a single model.\n\nWe need a [vector database](/learn/vector-database/) to store the passage embeddings. We will use Pinecone as an incredibly easy-to-use service that can scale to the millions of passage embeddings we'd like to search.\n\n{{< notebook file=\"gpl-neg-mine-pinecone\" height=\"full\" >}}\n\nWe encode our passages, assign unique IDs to each, and then upload the record to Pinecone. As we later need to match these returned vectors back to their original plaintext format, we will create an ID-to-passage mapping to be stored locally.\n\n{{< notebook file=\"gpl-neg-mine-encode-store\" height=\"full\" >}}\n<small>[Full version here](https://gist.github.com/jamescalam/9b84408d6c7f1fe4bf7eda2ab410c086#file-02_negative_mining-ipynb).</small>\n\nThe vector database is set up for us to begin *negative mining*. We loop through each query, returning *10* of the most similar passages by setting `top_k=10`.\n\n{{< notebook file=\"gpl-neg-mine-query\" height=\"full\" >}}\n\nWe then loop through each set of queries, *P<sup>+</sup>* passages, and their negatively mined results. Next, we shuffle those results and return the first that does *not* match to our *P<sup>+</sup>* passage, this becomes the *P<sup>-</sup>* passage. We write each record to file in the format *(Q, P<sup>+</sup>, P<sup>-</sup>)*, ready for the next step.\n\n*[Full script](https://gist.github.com/jamescalam/9b84408d6c7f1fe4bf7eda2ab410c086#file-02_negative_mining-ipynb)*\n\n#### 3. Pseudo-labeling\n\nPseudo-labeling is the final step in *preparing* our training data. In this step, we use a cross encoder model to generate similarity scores for both positive and negative pairs.\n\n![sim(Q, P+)](/images/gpl-8.png)\n\n<center>and</center>\n\n![sim(Q, P-)](/images/gpl-9.png)\n\n<br>\n\n{{< notebook file=\"gpl-ce-scoring\" height=\"full\" >}}\n\nGiven a *positive* and *negative* query-passage similarity score (GPL uses dot-product similarity), we then take the difference between both scores to give the *margin* between both.\n\n![margin = sim(Q, P+) - sim(Q, P-)](/images/gpl-10.png)\n\nWe calculate the margin between the two similarity scores to train our bi-encoder model using *margin MSE loss*, which requires the margin score. After generating these scores, our final data format contains the query, both passages, and the margin score.\n\n![(Q, P+, P-)](/images/gpl-11.png)\n\nThis final Pseudo-labeling step is very important in ensuring we have high quality training data. Without it, we would need to assume that all passages returned in the negative mining step are irrelevant to our query and must share the same dissimilarity when contrasted against our positive passages.\n\nIn reality this is never the case. Some negative passages are more relevant than others. The authors of GPL split these negative passages into three categories [2].\n\n![why_pseudo_label](/images/gpl-5.png)\n<small>Three categories of negative passages. Whereas previous methods like GenQ that lack the pseudo-labeling step would view passages as either positive `1` or negative `0`, GPL can score passages on a more meaningful scale.</small>\n\nWe are likely to return a mix of negative passages, from highly relevant to completely irrelevant. Pseudo-labeling allows us to score passages accordingly. Above we can see three negative categories:\n\n* **False negatives**: we haven't returned the *exact* match to our positive passage, but that does not mean we will not return relevant passages (that are in fact *not* negatives). In this case our cross-encoder will label the passage as relevant, without a cross-encoder this would be marked as irrelevant.\n* **Easy negatives**: these are passages that are loosely connected to the query (such as containing matching keywords) but are *not* relevant. The cross-encoder should mark these as having low relevance.\n* **Hard negatives**: in this case the passages may be tightly connected to the query, or even contain a partial answer, but still not answer the query. Our cross-encoder should mark these as being more relevant than *easy negatives* but less so than any positive or false negative passages.\n\nNow that we have our fully prepared data, we can move on to the training portion of GPL.\n\n*[Full script](https://gist.github.com/jamescalam/f9609f0e47937c3545364cfbef3ea1b8#file-03_ce_scoring-ipynb)*\n\n## Training with Margin MSE\n\nThe fine-tuning/training portion of GPL is not anything unique or new. It is a tried and tested bi-encoder training process that optimizes with *margin MSE loss*.\n\n![margin MSE loss function](/images/gpl-12.png)\n\nWe are looking at the sum of squared errors between the predicted margin *𝛿<sup>^</sup><sub>i</sub>* and the true margin *𝛿<sub>i</sub>* for *all samples* in the training set (from *i=0* to *i=M-1*). We make it a *mean* squared error by dividing the summed error by the number of samples in the training set *M*.\n\nLooking back at the generated training data, we have the format *(Q, P<sup>+</sup>, P<sup>-</sup>, margin)*. How do these fit into the *margin MSE loss* function above?\n\n![gpl_margin_mse_loss](/images/gpl-6.png)\n<small>High-level view of *(Q, P<sup>+</sup>, P<sup>-</sup>)* triplets and how they fit into the margin MSE loss function.</small>\n\nThe bi-encoder model creates embeddings for the query *Q*, positive passage *P<sup>+</sup>, and negative passage *P<sup>-</sup>*. We then calculate the dot-product similarity between embeddings for both *sim(Q, P<sup>+</sup>)* and *sim(Q, P<sup>-</sup>)*. These give us the predicted margin:\n\n![delta hat = sim(Q, P+) - sim(Q, P-)](/images/gpl-13.png)\n\nThe true margin *𝛿<sub>i</sub>* has already been calculated by our cross-encoder, it is simply *𝛿<sub>i</sub> = margin*.\n\nWe can use the default *sentence-transformers* methods for fine-tuning models with margin MSE loss. We begin by loading our pairs into a list of `InputExample` objects.\n\n{{< notebook file=\"gpl-train-input-examples\" height=\"full\" >}}\n\nWe can see the contents of one of our `InputExample` objects:\n\n{{< notebook file=\"gpl-train-example\" height=\"full\" >}}\n\nWe use a generic PyTorch `DataLoader` to load the data into the model during training. One crucial detail is that margin MSE loss works best with large batch sizes. A batch size of *32* or even *64* is a good target, but this does require significant GPU memory and may not be feasible. If that is the case, reduce the batch size until it fits within your hardware restraints.\n\n{{< notebook file=\"gpl-train-dataloader\" height=\"full\" >}}\n\nNext, we initialize a bi-encoder model using the pre-COVID DistilBERT bi-encoder that we used in the negative mining step. It is this model that we are adapting to better understand COVID-19 related language.\n\n{{< notebook file=\"gpl-train-model-init\" height=\"full\" >}}\n\nWe're ready to initialize the margin MSE loss that will optimize the model later.\n\n{{< notebook file=\"gpl-train-loss\" height=\"full\" >}}\n\nWith that, we're finally ready to begin fine-tuning our model. We used a single epoch, with a training set of 600K samples this is a large number of steps. It was found that GPL performance tends to stop improving after around 100K steps [2]. However, this will vary by dataset.\n\n![gpl_perf_by_steps](/images/gpl-7.png)\n<small>NDCG@10% performance for zero-shot (not adapted), GPL fine-tuned, and GPL fine-tuned + [TSDAE pre-trained models](/learn/unsupervised-training-sentence-transformers/). GPL fine-tuning using a model that had previously been pretrained using TSDAE demonstrates consistently better performance. Model performance seems to level-out after 100K training steps. Visual adapted from [2].</small>\n\n<br>\n\n{{< notebook file=\"gpl-train-fit-1\" height=\"full\" >}}\n\nOnce training is complete, we will find all of our model files in the *msmarco-distilbert-base-tas-b-covid* directory. To use our model in the future, we simply load it from the same directory using *sentence-transformers*.\n\n{{< notebook file=\"gpl-train-fit-1\" height=\"full\" >}}\n\nIf you'd like to use the model trained in this article, you can specify the model name `pinecone/msmarco-distilbert-base-tas-b-covid`.\n\nNow let's return to the COVID-19 queries we asked the initial model (without GPL adaptation).\n\n{{< notebook file=\"gpl-new-model-1epoch\" height=\"full\" >}}\n\nAs before we are asking four questions, each of which has three possible passages. Our model is tasked with scoring the similarity of each passage, the goal is to return COVID-19 related sentences higher than any other sentences.\n\nWe can see that this has worked for two of our queries. For the two queries it performs worse on, it looks like our GPL trained model is confusing the drink Corona with *\"corona\"* in the context of COVID-19.\n\nWhat we can do is try and fine-tune our model for more epochs, if we try again with a model trained for 10 epochs we get more promising results.\n\n{{< notebook file=\"gpl-new-model10\" height=\"full\" >}}\n\nNow we see much better results and our model is more easily differentiating between the Corona beer, and COVID-19.\n\n*[Full script](https://gist.github.com/jamescalam/15d968bed79b884bf50090a34093f508#file-04_finetune-ipynb)*\n\n## A Simpler Approach\n\nWe've worked through a lot of theory and code to understand GPL, and hopefully, it is now much clearer. However, we don't need to work through all of that to apply GPL. It is much easier when using the [official GPL library](https://github.com/UKPLab/gpl).\n\nDoing the same as we did before requires little more than a few lines of code. To start, we first `pip install gpl`. Our input data must use the BeIR data format, a single JSON lines (`.jsonl`) file called *corpus.jsonl*. Each sample in the file will look like this:\n\n<script src=\"https://gist.github.com/jamescalam/99a8f865f2fc75db2e834507572d169b.js\"></script>\n\nOur CORD-19 dataset is not initially in the correct format, so we must first reformat it.\n\n```python\nfrom tqdm.auto import tqdm\nimport json\nimport os\n\n# create directory if needed\nif not os.path.exists('./cord_data'):\nos.mkdir('./cord_data')\n\nid_count = 0\n\nwith open('./cord_data/corpus.jsonl', 'w') as jsonl:\n    for path in tqdm(paths):\n        # read each json file in the CORD-19 pdf_json directory\n        with open(path, 'r') as fp:\n            doc = json.load(fp)\n        # extract the passages of text from each document\n        for line in doc['body_text']:\n            line = {\n                '_id': str(id_count),\n                'title': \"\",\n                'text': line['text'].replace('\\n', ' '),\n                'metadata': doc['metadata']\n            }\n            id_count += 1\n    # iteratively write lines to the JSON lines corpus.jsonl file\n    jsonl.write(json.dumps(line)+'\\n')\n```\n\nNow we will have a new *corpus.jsonl* file in the *cord_data* directory. The first sample from the newly formatted CORD-19 dataset looks similar to this:\n\n<script src=\"https://gist.github.com/jamescalam/77b050c3f9c3029ad5e3ed813b2273a3.js\"></script>\n\nWith that newly formatted dataset, we can run the whole GPL data generation and fine-tuning process with a single, slightly lengthy function call.\n\n```python\nimport gpl\n\ngpl.train(\n    path_to_generated_data='./cord_data',\n    base_ckpt='distilbert-base-uncased',\n    batch_size_gpl=16,\n    gpl_steps=140_000,\n    output_dir='./output/cord_model',\n    generator='BeIR/query-gen-msmarco-t5-base-v1',\n    retrievers=[\n        'msmarco-distilbert-base-v3',\n        'msmarco-MiniLM-L-6-v3'\n    ],\n    cross_encoder='cross-encoder/ms-marco-MiniLM-L-6-v2',\n    qgen_prefix='qgen',\n    do_evaluation=False\n)\n```\n\nLet's break all of this down. We have:\n\n* `path_to_generated_data` - the directory containing *corpus.jsonl*.\n* `base_ckpt` - the starting point of the bi-encoder model that we will be fine-tuning.\n* `batch_size_gpl` - batch size for the margin MSE loss fine-tuning step.\n* `gpl_steps` - number of training steps to run for the MSE margin loss fine-tuning.\n* `output_dir` - where to save the fine-tuned bi-encoder model.\n* `generator` - the query generation model.\n* `retrievers` - a list of retriever models to use in the negative mining step.\n* `cross_encoder` - the cross encoder model used for pseudo-labeling.\n* `qgen_prefix` - the query generation data files prefix.\n* `do_evaluation` - whether to evaluate the model on an evaluation dataset requires an evaluation set to be provided.\n\nAfter running this, which can take some time, we have a bi-encoder fine-tuned using GPL on nothing more than the passages of text passed from the *./cord_data/corpus.jsonl* file.\n\nUsing the GPL library is a great way to apply unsupervised learning. When compared to our more in-depth process, it is *much* simpler. The one downside is that the negative mining step uses [exhaustive search](/learn/faiss-tutorial/). This type of search is no problem for smaller corpora but becomes slow for larger datasets (100K–1M+) and, depending on your hardware, impossible for anything too large to be stored in memory.\n\nThat's it for this chapter on **G**enerative **P**seudo-**L**abeling (GPL). Using this impressive approach, we can fine-tune new or existing models in domains that were previously inaccessible due to little or no labeled data.\n\nThe research on unsupervised training methods for bi-encoder models continues to progress. GPL is the latest in a series of techniques that extends the performance of these exciting models trained without labeled data.\n\nWhat is possible with GPL is impressive. Perhaps even more exciting is the possibility of further improvements to GPL or completely new methods that take the performance of these unsupervised training methods to even greater heights.\n\n\n## References\n\n[Code Notebooks](https://github.com/pinecone-io/examples/tree/master/analytics-and-ml/model-training/gpl)\n\n[1] T. Berners-Lee, M. Fischetti, [Weaving the Web, The Original Design and Ultimate Destiny of the World Wide Web by Its Inventor](https://www.w3.org/People/Berners-Lee/Weaving/) (1999)\n\n[2] K. Wang, et al., [GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval](https://arxiv.org/abs/2112.07577)\n\n[3] Y. Qu, et al., [RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2010.08191) (2021), NAACL"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbc1"
  },
  "filename": "ml-life-sciences.md",
  "title": "post",
  "category": "\"How Machine Learning is Accelerating Life Sciences\"",
  "content": "---\nlayout: post\ntitle: \"How Machine Learning is Accelerating Life Sciences\"\nheadline: \"How Machine Learning is Accelerating Life Sciences\"\ncategories:\n  - Applications of Vector Search\ntoc: >-\nweight: 6\nauthor:\n  name: Diego Lopez Yse\n  position: Data Scientist\n  src: /images/diego-lopez-yse.jpeg\n  href: \"https://lopezyse.medium.com/\"\ndescription: Machine learning (ML) is rapidly transforming the life sciences. Applied ML is improving diagnostics with computer vision (CV) and accelerating drug discovery.\n# Open Graph\nimages: ['/images/ml-life-sciences-0.jpg']\n---\n\nMoore’s Law predicts that computing will dramatically increase in power and decrease in relative cost at an [exponential pace](https://www.intel.com/content/www/us/en/silicon-innovations/moores-law-technology.html). Although this principle mainly applies to computing hardware, DNA sequencing cost has followed a similar pattern for many years, approximately halving every two years. But since January 2008 there has been a break in that trend, with sequencing costs dropping much faster than the cost of processing data on computers. The cost of getting DNA data has never been cheaper, and it will continue to decrease.\n\n![Cost per human genome](/images/ml-life-sciences-0.jpg)\n<small>The sudden and profound out-pacing of Moore's Law beginning in January 2008. Source: [National Human Genome Research Institute](https://www.genome.gov/about-genomics/fact-sheets/DNA-Sequencing-Costs-Data)</small>\n\nThe result of this is that a data tsunami is coming into Life Sciences, with estimations that over [60 million patients](https://www.biorxiv.org/content/10.1101/203554v1.full) will have their genome sequenced in a healthcare context by 2025, and that [genomics research will generate](https://www.genome.gov/about-genomics/educational-resources/fact-sheets/artificial-intelligence-machine-learning-and-genomics) between 2 and 40 **exabytes** of data within the next decade.\n\n![Number of datasets](/images/ml-life-sciences-1.png)\n<small>DNA sequencing and other biological techniques will continue to increase the number and complexity of genomic data sets. Source: [National Human Genome Research Institute](https://www.genome.gov/about-genomics/fact-sheets/DNA-Sequencing-Costs-Data)</small>\n\nBut it’s not just about data volume. Massive computing power has enabled researchers from the University of Illinois to develop a [software to simulate a 2-billion-atom cell](https://blogs.nvidia.com/blog/2022/01/20/living-cell-simulation/) that metabolizes and grows like a living cell. Cell simulation provides insights into the physical and chemical processes that form the foundation of living cells, where fundamental behaviors emerge not because they were programmed in, but because the model contained the correct parameters and mechanisms.\n\nLife Sciences is going through an accelerated transformation, and Machine Learning (ML) is responsible for it. On top of that, during the Covid-19 pandemic Life Sciences companies were forced to mobilize their resources to respond quickly to public health demands, which caused a spike of new computational methods and ways of thinking.\n\n## Applied Machine Learning in Life Sciences\n\nMassive data volumes plus improved computation have eased the path to ML models that can solve new challenges in Life Sciences. From the big universe of potential applications, some that deserve special attention are:\n\n- **Improved diagnostics**, since diagnosis is the most fundamental step in the treatment of any patient.\n- **Drug discovery**, as the biggest goal of the Life Sciences industry is to advance the research and innovation for new products and treatments.\n\n### Improving diagnostics with Computer Vision\n\nComputer Vision (CV) focuses on image and video understanding, involving [tasks such as](https://viso.ai/applications/computer-vision-in-healthcare/) object detection, image classification, and segmentation. In Life Sciences, CV models fed with medical imaging (e.g. MRI, X-rays, etc) can assist in the visualization of cells, tissues and organs to enable a more accurate diagnosis, helping to identify any issues or abnormalities.\n\nThere’s huge [versatility within imaging sources](https://bmcmedimaging.biomedcentral.com/articles/10.1186/s12880-022-00793-7), since computed tomography (CT) scans and magnetic resonance imaging (MRI) are capable of generating 3D image data, while digital microscopy can generate terabytes of whole slide image (WSI) of tissue specimens.\n\nOne example is the UC San Diego Health, which applies CV models to quickly [detect pneumonia](https://health.ucsd.edu/news/releases/Pages/2020-04-07-artificial-intelligence-enables-rapid-covid-19-lung-imaging-analysis.aspx) through X-rays imagery, which are cheaper and faster than other methods. But adding [Transfer Learning (TL)](/learn/transfer-learning/) to these types of problems can increase the performance and [accuracy](https://www.mdpi.com/2076-3417/11/3/1242) of a diagnostic approach that medical professionals can easily use as an auxiliary tool. In CV, [embeddings](https://www.pinecone.io/learn/vector-embeddings/) are often used as [a way to translate knowledge between different contexts](https://www.featureform.com/post/the-definitive-guide-to-embeddings), allowing us to exploit pre-trained models like VGG, AlexNet or ResNet.\n\n![Pretrained models](/images/ml-life-sciences-2.jpg)\n<small>In this example, three-types of pre-trained models (ResNet152, DenseNet121 and ResNet18) act as feature extractors. Redefining a classifier for a new task and applying an attention mechanism as a feature selector can improve accuracy over other models. Source: [Attention-Based Transfer Learning for Efficient Pneumonia Detection in Chest X-ray Images](https://www.mdpi.com/2076-3417/11/3/1242)</small>\n\nDue to the need for large datasets to train and tune Deep Learning architectures for CV which are not available for medical images, TL coupled with embeddings can be used to achieve tasks that go from [ocular disease recognition](https://opg.optica.org/boe/fulltext.cfm?uri=boe-8-2-579&id=357053) to [cancer detection](https://www.aimspress.com/article/doi/10.3934/mbe.2021256).\n\n![Convolutional Neural Network](/images/ml-life-sciences-3.png)\n<small>A Convolutional Neural Network (CNN) component acts as a feature extractor that takes a grid of patches as input, and encodes each patch as a fixed-length vector representation (i.e. embedding). Source: [Cancer Metastasis Detection With Neural Conditional Random Field](https://openreview.net/pdf?id=S1aY66iiM)</small>\n\nTo solve CV challenges, the [classic approach](https://link.springer.com/article/10.1007/s10278-022-00666-z) has been to use TL with pre-trained convolutional neural networks (CNNs) on natural images (e.g., ResNet), tuned on medical images. Today, due to their powerful TL abilities, pre-trained [Transformers](/learn/transformers/) (which are self-attention-based models) are becoming standard models to improve results on CV tasks.\n\n### Drug discovery\n\nDrug discovery is the process of finding new or existing molecules with specific chemical properties for the treatment of diseases. Since this has traditionally been an extremely long and expensive process, modern predictive models based on ML have gained popularity for their potential to drastically reduce costs and research times.\n\nML can be used the in the [drug discovery processes to](https://www.neuraldesigner.com/solutions/drug-design):\n\n- **Discover structure patterns**: to study the surface properties, molecular volumes or molecular interactions.\n- **Identify behavior influences**: to relate the orientation of the molecule to its characteristics.\n- **Anticipate characteristics**: to develop models capable of predicting the behavior of a molecule in accordance with its design.\n- **Improve drug designs**: to get better results and design better medicines while reducing costs.\n\nThis is what companies like Sanofi are doing in order to [reduce the sheer number of compounds they need to synthesize](https://www.fiercebiotech.com/biotech/next-gen-sanofis-platforms-chief-sets-out-ambition-cut-drug-discovery-years-ai) in the real world by doing much of the analysis on a computer.\n\nHow do you represent molecules in the data space? From the [several representation methods available](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00460-5), **embedding** methods like [Mol2Vec](https://chemrxiv.org/engage/chemrxiv/article-details/60c73d3bbdbb89110aa37c15) have emerged as novel approaches to learn high-dimensional representations of molecular substructures. Inspired by word embedding techniques known as Word2Vec, Mol2Vec encodes molecules into a set of vectors that represent similar substructures in proximity to one another in the vector space. In a [Natural Language Processing](https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1) (NLP) analogous fashion, molecules are considered as sentences and substructures as words.\n\n![Mol2vec vectors](/images/ml-life-sciences-4.jpg)\n<small>Mol2vec vectors of amino acids (bold arrows). These vectors were obtained by summing the vectors of the Morgan substructures (small arrows) present in the respective molecules (amino acids in the present example). The directions of the vectors provide a visual representation of similarities. Magnitudes reflect importance, i.e. more meaningful words. Source: [Oxford Protein Informatics Group](https://pubs.acs.org/doi/10.1021/acs.jcim.7b00616)</small>\n\nBut molecules can also be represented as graphs. **Graphs** are a ubiquitous data structure, employed extensively within computer science and related fields. Social networks, molecular graph structures, biological protein-protein networks, recommender systems — all of these domains and many more can be readily modeled as graphs, which capture interactions (i.e., edges) between individual units (i.e., nodes).\n\n![Nodes and edges](/images/ml-life-sciences-5.png)\n<small>The **nodes** can be described as the vertices that correspond to objects. The **edges** can be referred to as the connections between objects. Source: [Java T Point](https://www.javatpoint.com/directed-and-undirected-graph-in-discrete-mathematics)</small>\n\nIntuitively, one could imagine treating the atoms in a molecule as _nodes_ and the bonds as _edges_. Nodes, edges, subgraphs or entire graphs can be embedded into [low-dimensional vectors that summarize the graph position and the structure of their local graph neighborhood](https://www-cs.stanford.edu/people/jure/pubs/graphrepresentation-ieee17.pdf). These low-dimensional embeddings can be viewed as encoding or projecting graph information into a latent space, where geometric relations in this latent space correspond to interactions in the original graph.\n\n![Molecular graphs](/images/ml-life-sciences-6.png)\n<small>Embedding molecular graphs into low dimensional space to determine if they are benign or toxic. Source: [A Large-Scale Database for Graph Representation Learning](https://scottfreitas.medium.com/a-large-scale-database-for-graph-representation-learning-c096ab19aa4d)</small>\n\nThe idea behind using graph embeddings is to create insights that are not directly evident by looking at the explicit relationships between nodes.\n\n## The future\n\nThe data explosion and initiatives in Life Sciences have the potential to reshape the future of the industry and of patient care, as we witness how ML methods can do amazing things if you give them enough data. Just look at what DeepMind announced only some weeks ago, [releasing the predicted structures for almost every protein](https://www.chemistryworld.com/news/alphafold-has-predicted-the-structures-of-almost-every-known-protein/4016033.article) known to science (over 200 million structures in total), using its AI AlphaFold 2.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/ml-life-sciences.mp4\" type=\"video/mp4\">\n</video>\n\n<small class=\"video\">You can see one prediction Alpha Fold’s model created. In comparison to the time it takes in the lab, this model is able to make a prediction in a mere half an hour with 90% accuracy according to their [statement](https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology). Source: [Communicating Science](https://blogs.ubc.ca/communicatingscience20t2w212/2021/02/26/protein-folding-solved-2/)</small>\n\nBut it’s both the volume and diversity of data that force us to rethink how to solve problems in Life Sciences with ML. Life Sciences is demanding us to integrate all sorts of different data types to reach better results, while giving us a glimpse of what’s coming next for all industries: **a multimodal future**.\n\nConsider Electronic Health Records (EHRs), which offer an [efficient way to maintain patient information](https://arxiv.org/ftp/arxiv/papers/2111/2111.04898.pdf) and are becoming more and more widely used by healthcare providers around the world. EHRs can include data that go from images, clinical notes, medication lists, vital signs, to demographic information, which can provide deep insights of a patient’s condition if integrated in an effective manner.\n\nEfforts to integrate these data types are already ongoing, and multimodal ML models trained on numerous types of data could help health professionals to screen patients at risk of developing diseases like cancer more accurately. This is what [Harvard University](https://www.theregister.com/2022/08/09/ai_cancer_multimodal/) is researching, by training ML models with microscopic views of cell tissues from whole-slide images (WSIs) and text-based genomics data. Just imagine what other challenges can be faced when integrating image, sequential, text, 3D, graph and other types of data into the same information space.\n\n![Combining data](/images/ml-life-sciences-7.jpg)\n<small>Combining data collected from both home (left) and clinical settings (right), or combining predictive models built at home and in the clinic, has the potential to lead to comprehensive and integrated models that support personalized health management. Comprehensive models are more likely to perform well as they incorporate more information about an individual, and these models have the potential to be applied in the home, clinic, or wherever an individual may be. Source: [NCBI](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7141410/)</small>\n\nML will transform Life Sciences, and allow us to dream big in solving some of the most transcendental challenges for humanity, like eliminating aging or hyper-personalized healthcare. This is such a powerful Artificial Intelligence (AI) application, that the UK government has established it as a [Strategic Grand Challenge](https://www.gov.uk/government/publications/industrial-strategy-the-grand-challenges/industrial-strategy-the-grand-challenges), and people like [Vitalik Buterin](https://www.lifespan.io/news/vitalik-buterin-the-best-thing-to-donate-money-to-is-the-fight-against-aging/) (the creator of the cryptocurrency Ethereum) and [Jeff Bezos](https://www.pharmaceutical-technology.com/analysis/billionaires-anti-ageing-research/) (founder of Amazon) invested part of their fortune in this idea. From [personalized medicine](https://www.nesfircroft.com/blog/2022/01/how-is-ai-pushing-the-life-science-industry-forward?source=google.com) to [democratizing healthcare](https://www.enago.com/academy/big-10-ways-artificial-intelligence-transforming-life-sciences/) in developing regions, applied ML in Life Sciences can deeply change our lives.\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbc2"
  },
  "filename": "clip-image-search.md",
  "title": "post",
  "category": "Text-to-Image and Image-to-Image Search Using CLIP",
  "content": "---\nlayout: post\ntitle: Text-to-Image and Image-to-Image Search Using CLIP\nheadline: Text-to-Image and Image-to-Image Search Using CLIP\ncategories:\n  - Projects\ntoc: >-\nweight: 4\nauthor:\n  name: Zoumana Keita\n  position: Data Scientist\n  src: /images/zoumana-keita.jpg\n  href: \"https://www.linkedin.com/in/zoumana-keita/\"\ndescription: A complete overview of using the OpenAI's CLIP for multi-modal search\n# Open graph\nimages: [\"/images/clip-image-search-6.png\"]\n---\n\n## Introduction\n\nIndustries today deal with ever increasing amounts of data. Especially in retail, fashion, and other industries where the image representation of products plays an important role.\n\nIn such a situation, we can often describe one product in many ways, making it challenging to perform accurate and least time-consuming searches.\n\n_Could I take advantage of state-of-the-art artificial intelligence solutions to tackle such a challenge?_\n\nThis is where [OpenAI’s CLIP](/learn/clip) comes in handy. A deep learning algorithm that makes it easy to connect text and images.\n\nAfter completing this conceptual blog, you will understand: (1) what CLIP is, (2) how it works and why you should adopt it, and finally, (3) how to implement it for your own use case using both local and cloud-based vector indexes.\n\n## What is CLIP? \n\nContrastive Language-Image Pre-training (CLIP for short) is a state-of-the-art model introduced by OpenAI in February 2021 [1].\n\nCLIP is a neural network trained on about 400 million (text and image) pairs. Training uses a contrastive learning approach that aims to unify text and images, allowing tasks like image classification to be done with text-image similarity.\n\nThis means that CLIP can find whether a given image and textual description match without being trained for a specific domain. Making CLIP powerful for out-of-the-box text and image search, which is the main focus of this article. \n\nBesides text and image search, we can apply CLIP to image classification, image generation, image similarity search, image ranking, object tracking, robotics control, image captioning, and more.\n\n## Why should you adopt the CLIP models?\n\nBelow are some reasons that increased the adoption of the CLIP models by the AI community\n\n### Efficiency\n\nThe use of the contrastive objective increased the efficiency of the CLIP model by 4-to-10x more at zero-shot ImageNet classification.\n\nAlso, the adoption of the Vision Transformer created an additional 3x gain in compute efficiency compared to the standard ResNet.\n\n![The efficiency of CLIP at zero-shot transfer](/images/clip-image-search-1.png)\n<small>The efficiency of CLIP at zero-shot transfer ([source](https://arxiv.org/pdf/2103.00020v1.pdf))</small>\n\n### More general & flexible\n\nCLIP outperforms existing ImageNet models in new domains because of its ability to learn a wide range of visual representations directly from natural language.\n\nThe following graphic highlights CLIP zero-shot performance compared to ResNet models few-shot linear probe performance on fine-grained object detection, geo-localization, action recognition, and optical character recognition tasks. \n\n![Average linear probe score across 27 datasets](/images/clip-image-search-2.png)\n<small>Average linear probe score across 27 datasets ([source](https://openai.com/blog/clip/))</small>\n\n## CLIP Architecture\n\nCLIP architecture consists of two main components: (1) a text encoder, and (2) an Image encoder. These two encoders are jointly trained to predict the correct pairings of a batch of training (image, text) examples.\n\n- The *text encoder’s* backbone is a [transformer](https://arxiv.org/abs/1706.03762) model [2], and the base size uses 63 millions-parameters, 12 layers, and a 512-wide model containing 8 attention heads. \n- The *image encoder*, on the other hand, uses both a Vision Transformer (ViT) and a ResNet50 as its backbone, responsible for generating the feature representation of the image. \n\n### How does the CLIP algorithm work?\n\nWe can answer this question by understanding these three approaches: (1) contrastive pre-training, (2) dataset classifier creation from labeled text, and finally, (3) application of the zero-shot technique for classification.\n\nLet’s explain each of these three concepts. \n\n![Contrastive pre-training](/images/clip-image-search-3.png)\n<small>Contrastive pre-training ([source](https://openai.com/blog/clip/))</small>\n\n#### 1. Contrastive pre-training\n\nDuring this phase, a batch of 32,768 pairs of image and text is passed through the text and image encoders simultaneously to generate the vector representations of the text and the associated image, respectively.\n\nThe training is done by searching for each image, the closest text representation across the entire batch, which corresponds to maximizing cosine similarity between the actual N pairs that are maximally close. \n\nAlso, it makes the actual images far away from all the other texts by minimizing their cosine similarity.\n\nFinally, a symmetric cross-entropy loss is optimized over the previously computed similarity scores.\n\n![Classification dataset creation and zero-shot prediction](/images/clip-image-search-4.png)\n<small>Classification dataset creation and zero-shot prediction ([source](https://openai.com/blog/clip/))</small>\n\n#### 2. Create dataset classifier from label text\n\nThis second step section encodes all the labels/objects in the following context format: “**_a photo of a {object}_**. The vector representation of each context is generated from the text encoder. \n\nIf we have *dog, car*, and *plane* as the classes of the dataset, we will output the following context representations: \n\n- a photo of a dog\n- a photo of a car\n- a photo of a plane\n\n![Image illustration of the context representations](/images/clip-image-search-5.png)\n<small>Image illustration of the context representations</small>\n\n#### 3. Use of zero-shot prediction\n\nWe use the output of section 2 to predict which image vector corresponds to which context vector. The benefit of applying the zero-shot prediction approach is to make CLIP models generalize better on unseen data. \n\n## Implementation of CLIP With Python\n\nNow that we know the architecture of CLIP and how it works, this section will walk you through all the steps to successfully implement two real-world scenarios. First, you will understand how to perform an image search in natural language. Also, you will be able to perform an image-to-image search using. \n\nAt the end of the process, you will understand the benefits of using a vector database for such a use case. \n\n### General workflow of the use case\n\n*(Follow along with [the Colab notebook](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/projects/clip-search/CLIP_Text_to_Image_Search.ipynb)!)*\n\nThe end-to-end process is explained through the workflow below. We start by collecting data from the Hugging Face dataset, which is then processed to further generate vector index vectors through the Image and Text Encoders. Finally, the Pinecone client is used to insert them to a vector index. \n\nThe user will then be able to search images based on either text or another image. \n\n![General workflow for image search](/images/clip-image-search-6.png)\n<small>General workflow for image search</small>\n\n### Prerequisites\n\nThe following libraries are required to create the implementation. \n\n#### Install the libraries\n\n```python\n%%bash\n# Uncomment this if using it for the first time. -qqq for ZERO-OUT\npip3 -qqq install transformers torch datasets\n \n# The following two libraries avoid the UnidentifiedImageError\npip3 -qqq install gdcm\npip3 -qqq install pydicom\npip -qqq install faiss-gpu\npip -qqq install pinecone-client\n```\n\n#### Import the libraries\n\n```python\nimport os\nimport faiss\nimport torch\nimport skimage\nimport requests\nimport pinecone\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom io import BytesIO\nimport IPython.display\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset\nfrom collections import OrderedDict\nfrom transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n```\n\n### Data acquisition and exploration\n\nThe conceptual captions dataset consists of around 3.3M images with two main columns: the image URL and its caption. You can find more details from the corresponding [huggingface link](https://huggingface.co/datasets/conceptual_captions).\n\n```python\n# Get the dataset\nimage_data = load_dataset(\"conceptual_captions\", split=\"train\")\n```\n\n#### Data preprocessing\n\nNot all URLs in the dataset are valid. We fix that by testing and removing all erroneous URL entries. \n\n```python\ndef check_valid_URLs(image_URL):\n   try:\n     response = requests.get(image_URL)\n     Image.open(BytesIO(response.content))\n     return True\n   except:\n     return False\ndef get_image(image_URL):\n   response = requests.get(image_URL)\n   image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n   return image\n```\n\nThe following expression creates a new dataframe with a new column “is_valid” which is True when the URL is valid or False otherwise. \n\n```python\n# Transform dataframe\nimage_data_df[\"is_valid\"] = image_data_df[\"image_url\"].apply(check_valid_URLs)\n# Get valid URLs\nimage_data_df = image_data_df[image_data_df[\"is_valid\"]==True]\n# Get image from URL\nimage_data_df[\"image\"] = image_data_df[\"image_url\"].apply(get_image)\n```\n\nThe second step is to download the images from the URLs. This helps us avoid constant web requests.\n\n#### Image and text embeddings implementation\n\nThe prerequisites to successfully implement the encoders are the model, the processor, and the tokenizer. \n\nThe following function fulfills those requirements from the model ID and the device used for the computation, either CPU or GPU. \n\n```python\ndef get_model_info(model_ID, device):\n# Save the model to device\n\tmodel = CLIPModel.from_pretrained(model_ID).to(device)\n \t# Get the processor\n\tprocessor = CLIPProcessor.from_pretrained(model_ID)\n# Get the tokenizer\n\ttokenizer = CLIPTokenizer.from_pretrained(model_ID)\n       # Return model, processor & tokenizer\n\treturn model, processor, tokenizer\n# Set the device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# Define the model ID\nmodel_ID = \"openai/clip-vit-base-patch32\"\n# Get model, processor & tokenizer\nmodel, processor, tokenizer = get_model_info(model_ID, device)\n```\n\n##### Text embeddings\n\nWe start by generating the embedding of a single text before applying the same function across the entire dataset. \n\n```python\ndef get_single_text_embedding(text): \ninputs = tokenizer(text, return_tensors = \"pt\")\n\ttext_embeddings = model.get_text_features(**inputs)\n \t# convert the embeddings to numpy array\n\tembedding_as_np = text_embeddings.cpu().detach().numpy()\nreturn embedding_as_np\ndef get_all_text_embeddings(df, text_col):\ndf[\"text_embeddings\"] = df[str(text_col)].apply(get_single_text_embedding)\nreturn df\n# Apply the functions to the dataset\nimage_data_df = get_all_text_embeddings(image_data_df, \"caption\")\n```\n\nThe first five rows look like this:\n\n![Format of the vector index containing the captions/text embeddings](/images/clip-image-search-7.png)\n<small>Format of the vector index containing the captions/text embeddings</small>\n\n##### Image embeddings\n\nThe same process is used for image embeddings but with different functions. \n\n```python\ndef get_single_image_embedding(my_image):\nimage = processor(\n\t\ttext = None,\n\t\timages = my_image,\n\t\treturn_tensors=\"pt\"\n\t\t)[\"pixel_values\"].to(device)\nembedding = model.get_image_features(image)\n# convert the embeddings to numpy array\n\tembedding_as_np = embedding.cpu().detach().numpy()\n\treturn embedding_as_np\n def get_all_images_embedding(df, img_column):\n\tdf[\"img_embeddings\"] = df[str(img_column)].apply(get_single_image_embedding)\n\treturn df\nimage_data_df = get_all_images_embedding(image_data_df, \"image\")\n```\n\nThe final format of the text and image vector index looks like this:\n\n![Vector index with image and captions embeddings](/images/clip-image-search-8.png)\n<small>Vector index with image and captions embeddings (Image by Author)</small>\n\n### Vector storage approach — Local vector index Vs. A cloud-based vector index\n\nIn this section, we will explore two different approaches to storing the embeddings and metadata for performing the searches: The first is using the previous dataframe, and the second is using Pinecone. Both approaches use the cosine similarity metric.\n\n#### Using local dataframe as vector index\n\nThe helper function ***get_top_N_images*** generates similar images for the two scenarios illustrated in the workflow above: text-to-image search or image-to-image search.\n\n```python\nfrom sklearn.metrics.pairwise import cosine_similarity\ndef get_top_N_images(query, data, top_K=4, search_criterion=\"text\"):\n   # Text to image Search\n   if(search_criterion.lower() == \"text\"):\n     query_vect = get_single_text_embedding(query)\n   # Image to image Search\n   else:\n     query_vect = get_single_image_embedding(query)\n   # Relevant columns\n   revevant_cols = [\"caption\", \"image\", \"cos_sim\"]\n   # Run similarity Search\n   data[\"cos_sim\"] = data[\"img_embeddings\"].apply(lambda x: cosine_similarity(query_vect, x))# line 17\n   data[\"cos_sim\"] = data[\"cos_sim\"].apply(lambda x: x[0][0])\n   \"\"\"\n   Retrieve top_K (4 is default value) articles similar to the query\n   \"\"\"\n  most_similar_articles = data.sort_values(by='cos_sim',  ascending=False)[1:top_K+1] # line 24\n   return most_similar_articles[revevant_cols].reset_index()\n```\n\nLet’s understand how we perform the recommendation.\n\n→ The user provides either a text or an image as a search criterion, but the model performs a text-to-image search by default.\n\n→ In line 17, a cosine similarity is performed between each image vector and the user’s input vector.\n\n→ Finally, in line 24, sort the result based on the similarity score in descending order, and we return the most similar images by excluding the first one corresponding to the query itself. \n\n#### Example of searches \n\nThis helper function makes it easy to have a side-by-side visualization of the recommended images. Each image will have the corresponding caption and similarity score.\n\n```python\ndef plot_images_by_side(top_images):\n index_values = list(top_images.index.values)\n list_images = [top_images.iloc[idx].image for idx in index_values]\n list_captions = [top_images.iloc[idx].caption for idx in index_values]\n similarity_score = [top_images.iloc[idx].cos_sim for idx in index_values]\n n_row = n_col = 2\n _, axs = plt.subplots(n_row, n_col, figsize=(12, 12))\n axs = axs.flatten()\n for img, ax, caption, sim_score in zip(list_images, axs, list_captions, similarity_score):\n     ax.imshow(img)\n     sim_score = 100*float(\"{:.2f}\".format(sim_score))\n     ax.title.set_text(f\"Caption: {caption}\\nSimilarity: {sim_score}%\")\n plt.show()\n```\n\n##### Text-to-image \n\n→ First, the user provides the text that is used for the search. \n\n→ Second, we run a similarity search. \n\n→ Third, we plot the images recommended by the algorithm. \n\n\n```python\nquery_caption = image_data_df.iloc[10].caption\n# Print the original query text\nprint(\"Query: {}\".format(query_caption))\n# Run the similarity search\ntop_images = get_top_N_images(query_caption, image_data_df)\n# Plot the recommended images\nplot_images_by_side(top_images)\n```\n\nLine 3 generates the following text: \n\n_Query: actor arrives for the premiere of the film_\n\nLine 9 produces the plot below.\n\n![Images corresponding to the text](/images/clip-image-search-9.png)\n<small>Images corresponding to the text: “actor arrives for the premiere of the film”</small>\n\n##### Image-to-image\n\nThe same process applies. The only difference this time is that the user provides an image instead of a caption. \n\n```python\n# Get the query image and show it\nquery_image = image_data_df.iloc[55].image\nquery_image\n```\n\n![Original image of search (image at the index)](/images/clip-image-search-10.png)\n<small>Original image of search (image at the index)</small>\n\n```python\n# Run the similarity search and plot the result\ntop_images = get_top_N_images(query_image, image_data_df, search_criterion=\"image\")\n# Plot the result\nplot_images_by_side(top_images)\n```\n\nWe run the search by specifying the search_criterion which is “image” in line 2. \n\nThe final result is shown below. \n\n![Images corresponding to the image-to-image search](/images/clip-image-search-11.png)\n<small>Images corresponding to the image-to-image search (Image by Author)</small>\n\nWe can observe that some of the images are less similar which introduces noise in the recommendation. We can reduce that noise by specifying a threshold level of similarity. For instance, consider all the images with at least 60% similarity. \n\n### Leveraging the power of a managed vector index using Pinecone\n\n[Pinecone](/) provides a fully-managed, easily scalable vector database that makes it easy to build high-performance vector search applications.\n\nThis section will walk you through the steps from acquiring your API credentials to implementing the search engine. \n\n#### Acquire your Pinecone API \n\nBelow are the eight steps to acquire your API credentials, starting from the [Pinecone website](/). \n\n![Eight main steps to acquire your Pinecone Client API](/images/clip-image-search-12.png)\n<small>Eight main steps to acquire your Pinecone Client API</small>\n\n#### Configure the vector index\n\nFrom the API, we can create the index that allows us to perform all the create, update, delete, and insert actions. \n\n```python\npinecone.init(\n   api_key = \"YOUR_API_KEY\",\n   environment=\"YOUR_ENV\"  # find next to API key in console\n)\nmy_index_name = \"clip-image-search\"\nvector_dim = image_data_df.img_embeddings[0].shape[1]\n \nif my_index_name not in pinecone.list_indexes():\n # Create the vectors dimension\n pinecone.create_index(name = my_index_name,\n                       dimension=vector_dim,\n                       metric=\"cosine\", shards=1,\n                       pod_type='s1.x1')\n# Connect to the index\nmy_index = pinecone.Index(index_name = my_index_name)\n```\n\n- ***pinecone.init*** section initializes the pinecone workspace to allow future interactions. \n- from lines 8 to 9 we specify the name we want for the vector index, and also the dimension of the vectors, which is 512 in our scenario. \n- from lines 11 to 16 we create the index if it does not already exist. \n\nThe result of the following instruction shows that we have no data in the index. \n\n```python\nmy_index.describe_index_stats()\n```\n\nThe only information we have is the dimension, which is 512.\n\n```python\n{'dimension': 512, \n           'index_fullness': 0.0, \n            'namespaces': {}, \n'total_vector_count': 0}\n```\n\n#### Populate the database \n\nNow that we have configured the Pinecone database, the next step is to populate it with the following code. \n\n```python\nimage_data_df[\"vector_id\"] = image_data_df.index\nimage_data_df[\"vector_id\"] = image_data_df[\"vector_id\"].apply(str)\n# Get all the metadata\nfinal_metadata = []\nfor index in range(len(image_data_df)):\n final_metadata.append({\n     'ID':  index,\n     'caption': image_data_df.iloc[index].caption,\n     'image': image_data_df.iloc[index].image_url\n })\nimage_IDs = image_data_df.vector_id.tolist()\nimage_embeddings = [arr.tolist() for arr in image_data_df.img_embeddings.tolist()]\n# Create the single list of dictionary format to insert\ndata_to_upsert = list(zip(image_IDs, image_embeddings, final_metadata))\n# Upload the final data\nmy_index.upsert(vectors = data_to_upsert)\n# Check index size for each namespace\nmy_index.describe_index_stats()\n```\n\n_Let’s understand what is going on here._\n\nThe data to upsert requires three components: the unique identifiers (IDs) of each observation, the list of embeddings being stored, and the metadata containing additional information about the data to store. \n\n→ From lines 5 to 12, the metadata is created by storing the “ID”, “caption” and “URL” of each observation. \n\n→ On lines 14 and 15, we generate a list of IDs, and convert the embeddings into a list of lists. \n\n→ Then, we create a list of dictionaries mapping the IDs, embeddings, and metadata. \n\n→ The final data is upserted to the index with the _.upsert()_ function.\n\nSimilarly to the previous scenario, we can check that all vectors have been upserted via `my_index.describe_index_stats()`. \n\n#### Start the query\n\nAll that remains is to query our index using the text-to-image and image-to-image searches. Both will use the following syntax: \n\n```python\nmy_index.query(my_query_embedding, top_k=N, include_metadata=True)\n```\n\n→ *my_query_embedding* is the embedding (as a list) of the query (caption or image) provided by the user. \n\n→ *N* corresponds to the top number of results to return. \n\n→ *include_metadata=True* means that we want the query result to include metadata. \n\n##### Text to image\n\n```python\n# Get the query text\ntext_query = image_data_df.iloc[10].caption\n \n# Get the caption embedding\nquery_embedding = get_single_text_embedding(text_query).tolist()\n \n# Run the query\nmy_index.query(query_embedding, top_k=4, include_metadata=True)\n```\n\nBelow is the JSON response returned from the query\n\n![text-to-image query result](/images/clip-image-search-13.png)\n<small>text-to-image query result (Image by Author)</small>\n\nFrom the “matches” attribute, we can observe the top four most similar images returned by the query. \n\n##### Image-to-image\n\nThe same approach applies to image-to-image search. \n\n```python\nimage_query = image_data_df.iloc[43].image\n```\n\nThis is the image provided by the user as the search criteria.\n\n![Query image](/images/clip-image-search-14.png)\n<small>Query image</small>\n\n```python\n# Get the text embedding\nquery_embedding = get_single_image_embedding(image_query).tolist()\n\n# Run the query\nmy_index.query(query_embedding, top_k=4, include_metadata=True)\n```\n\n![image-to-image query result](/images/clip-image-search-15.png)\n<small>image-to-image query result (Image by Author)</small>\n\nOnce you’ve finished don't forget to delete your index to free up your resources with:\n\n```python\npinecone.delete_index(my_index)\n```\n\n## What are the advantages to using a Pinecone over a local pandas dataframe?\n\nThis approach using Pinecone has several advantages:\n\n→ ***Simplicity***: the querying approach is much simpler than the first approach, where the user has the full responsibility of managing the vector index. \n\n→ ***Speed***: Pinecone approach is faster, which corresponds to most industry requirements. \n\n→ ***Scalability***: vector index hosted on Pinecone is scalable with little-to-no user effort from us. The first approach would become increasingly complex and slow as we scale.\n\n→ ***Lower chance of information loss***: the vector index based on Pinecone is hosted in the cloud with backups and high information security. The first approach is too high risk for production use-cases. \n\n→ ***Web-service friendly***: the result provided by the query is in JSON format and can be consumed by other applications, making it a better fit for web-based applications. \n\n## Conclusion \n\nCongratulations, you have just learned how to fully implement an image search application using both image and natural language. I hope the benefits highlighted are valid enough to take your project to the next level using vector databases. \n\nMultiple resources are available at our [Learning Center](/learn/) to further your learning. \n\nThe source code for the article is [available on Colab](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/projects/clip-search/CLIP_Text_to_Image_Search.ipynb). \n\n## References\n\n[Code Notebook](https://github.com/pinecone-io/examples/blob/update-examples/search/multi-modal/clip-search/clip-text-image-search.ipynb)\n\n[1] A. Radford, J. W. Kim, et al., [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020v1.pdf) (2021)\n\n[2] A. Vaswani, et al., [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (2017), NeurIPS\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbc3"
  },
  "filename": "offline-evaluation.md",
  "title": "post",
  "category": "\"Evaluation Measures in Information Retrieval\"",
  "content": "---\nlayout: post\ntitle: \"Evaluation Measures in Information Retrieval\"\nheadline: \"Evaluation Measures in Information Retrieval\"\ncategories:\n  - Algorithms & Libraries\ntoc: >-\nweight: 9\nauthors:\n  - name: Laura Carnevali\n    position: Developer\n    src: /images/laura-carnevali.jpeg\n    href: \"https://www.linkedin.com/in/laura-carnevali-071a01b7/\"\n  - name: James Briggs\n    position: Developer Advocate\n    src: /images/james-briggs.jpeg\n    href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: How to measure retrieval performance with offline metrics like recall@K, MRR, MAP@K, and NDCG@K.\n# Open graph\nimages: [\"/images/offline-evaluation-0.jpg\"]\nthumbnail: \"https://www.pinecone.io/images/offline-evaluation-0.jpg\"\n---\n\nEvaluation of information retrieval (IR) systems is critical to making well-informed design decisions. From search to recommendations, evaluation measures are paramount to understanding what does and *does not* work in retrieval.\n\nMany big tech companies contribute much of their success to well-built IR systems. One of Amazon's earliest iterations of the technology was reportedly driving more than 35% of their sales<sup>[1]</sup>. Google attributes 70% of YouTube views to their IR recommender systems<sup>[2][3]</sup>.\n\nIR systems power some of the greatest companies in the world, and behind every successful IR system is a set of evaluation measures.\n\n---\n\n## Metrics in Information Retrieval\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/BD9TkvEsKwM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\nEvaluation measures for IR systems can be split into *two* categories: *online* or *offline* metrics.\n\n**Online metrics** are captured during actual usage of the IR system when it is *online*. These consider user interactions like whether a user clicked on a recommended show from Netflix or if a particular link was clicked from an email advertisement (the click-through rate or CTR). There are many online metrics, but they all relate to some form of user interaction.\n\n**Offline metrics** are measured in an isolated environment before deploying a new IR system. These look at whether a particular set of *relevant* results are returned when retrieving items with the system.\n\n![metrics_diagram](/images/offline-evaluation-1.png)\n<small>Evaluation measures can be categorized as either offline or online metrics. Offline metrics can be further divided into *order-unaware* or *order-aware*, which we will explain soon.</small>\n\nOrganizations often use *both* offline and online metrics to measure the performance of their IR systems. It begins, however, with offline metrics to predict the system's performance *before deployment*.\n\nWe will focus on the most useful and popular offline metrics:\n\n* Recall@K\n* **M**ean **R**eciprocal **R**ank (MRR)\n* **M**ean **A**verage **P**recision@K (MAP@K)\n* **N**ormalized **D**iscounted **C**umulative **G**ain (NDCG@K)\n\nThese metrics are deceptively simple yet provide invaluable insight into the performance of IR systems.\n\nWe can use one or more of these metrics in different evaluation stages. During the development of Spotify's podcast search; *Recall@K* (using $K=1$) was used during training on \"evaluation batches\", and after training, [both *Recall@K* and *MRR*](https://www.pinecone.io/learn/spotify-podcast-search/#:~:text=Spotify%20details%20their%20full%2Dretrieval%20setting%20metrics%20as%20using%20Recall%4030%20and%20MRR%4030%2C%20performed%20both%20on%20queries%20from%20the%20eval%20set%20and%20on%20their%20curated%20dataset.) (using $K=30$) were used with a much larger evaluation set.\n\nThe last paragraph will make sense by the end of this article. For now, understand that Spotify was able to predict system performance *before* deploying anything to customers. This allowed them to deploy successful A/B tests and significantly increase podcast engagement<sup>[4]</sup>.\n\nWe have two more subdivisions for these metrics; *order-aware* and *order-unaware*. This refers to whether the order of results impacts the metric score. If so, the metric is *order-aware*. Otherwise, it is *order-unaware*.\n\n## Cats in Boxes\n\nThroughout the article, we will be using a *very* small dataset of eight images. In reality, this number is likely to be millions or more.\n\n![example](/images/offline-evaluation-2.png)\n<small>Example query and ranking of the eight possible results.</small>\n\nIf we were to search for *\"cat in a box\"*, we may return something like the above. The numbers represent the relevance *rank* of each image as predicted by the IR system. Other queries would yield a different order of results.\n\n![example_highlighted](/images/offline-evaluation-3.png)\n<small>Example query and ranking with *actual relevant* results highlighted.</small>\n\nWe can see that results *2*, *4*, *5*, and *7* are *actual relevant* results. The other results are *not* relevant as they show cats *without* boxes, boxes *without* cats, or a dog.\n\n### Actual vs. Predicted\n\nWhen evaluating the performance of the IR system, we will be comparing *actual* vs. *predicted* conditions, where:\n\n* **Actual condition** refers to the true label of every item in the dataset. These are *positive* ($p$) if an item is relevant to a query or *negative* ($n$) if an item is *ir*relevant to a query.\n* **Predicted condition** is the *predicted* label returned by the IR system. If an item is returned, it is predicted as being *positive* ($\\hat{p}$) and, if it is not returned, is predicted as a *negative* ($\\hat{n}$).\n\nFrom these actual and predicted conditions, we create a set of outputs from which we calculate all of our offline metrics. Those are the true/false positives and true/false negatives.\n\nThe *positive* results focus on what the IR system returns. Given our dataset, we ask the IR system to return *two* items using the *\"cat in a box\"* query. If it returns an *actual relevant* result this is a <span onMouseOver=\"this.style.backgroundColor='#8CF1FF'\" onMouseOut=\"this.style.backgroundColor='transparent'\">*true positive* ($p\\hat{p}$)</span>; if it returns an irrelevant result, we have a <span onMouseOver=\"this.style.backgroundColor='#FAFF00'\" onMouseOut=\"this.style.backgroundColor='transparent'\">*false positive* ($n\\hat{p}$)</span>.\n\n![example_condition](/images/offline-evaluation-4.png)\n<small>On returning the top two results, we get <span onMouseOver=\"this.style.backgroundColor='#FAFF00'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$n\\hat{p}$</span> and <span onMouseOver=\"this.style.backgroundColor='#8CF1FF'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$p\\hat{p}$</span> results for the *positives*. For the unreturned *negatives* we have a mix of <span onMouseOver=\"this.style.backgroundColor='#738FAB'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$n\\hat{n}$</span> and <span onMouseOver=\"this.style.backgroundColor='#DFECF9'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$p\\hat{n}$</span>.</small>\n\nFor *negative* results, we must look at what the IR system *does not* return. Let's query for two results. Anything that *is relevant* but is *not* returned is a <span onMouseOver=\"this.style.backgroundColor='#DFECF9'\" onMouseOut=\"this.style.backgroundColor='transparent'\">*false negative* ($p\\hat{n}$)</span>. Irrelevant items that were *not* returned are <span onMouseOver=\"this.style.backgroundColor='#738FAB'\" onMouseOut=\"this.style.backgroundColor='transparent'\">*true negatives* ($n\\hat{n}$)</span>.\n\nWith all of this in mind, we can begin with the first metric.\n\n## Recall@K\n\n*Recall@K* is one of the most interpretable and popular offline evaluation metrics. It measures how many relevant items were returned (<span onMouseOver=\"this.style.backgroundColor='#8CF1FF'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$p\\hat{p}$</span>) against how many relevant items exist in the entire dataset (<span onMouseOver=\"this.style.backgroundColor='#8CF1FF'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$p\\hat{p}$</span>$+$<span onMouseOver=\"this.style.backgroundColor='#DFECF9'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$p\\hat{n}$</span>).\n$$\nRecall@K = \\frac{true Positives}{true Positives + false Negatives} = \\frac{p\\hat{p}}{p\\hat{p}+p\\hat{n}}\n$$\nThe *K* in this and all other offline metrics refers to the number of items returned by the IR system. In our example, we have a total number of *N = 8* items in the entire dataset, so *K* can be any value between $[1, ..., N]$.\n\n![recall-at-two](/images/offline-evaluation-5.png)\n<small>With *recall@2* we return the predicted top *K = 2* most relevant results.</small>\n\nWhen *K = 2*, our *recall@2* score is calculated as the number of *returned* relevant results over the total number of relevant results in the *entire dataset*. That is:\n$$\nRecall@2 = \\frac{p\\hat{p}}{p\\hat{p}+p\\hat{n}} = \\frac{1}{1+3} = 0.25\n$$\nWith recall@K, the score improves as *K* increases and the scope of returned items increases.\n\n![recall-at-k graph](/images/offline-evaluation-14.png)\n<small>With *recall@K* we will see the score increase as K increases and more positives (whether true or false) are returned.</small>\n\nWe can calculate the same recall@K score easily in Python. For this, we will define a function named `recall` that takes lists of *actual conditions* and *predicted conditions*, a *K* value, and returns a recall@K score.\n\n```python\n# recall@k function\ndef recall(actual, predicted, k):\n    act_set = set(actual)\n    pred_set = set(predicted[:k])\n    result = round(len(act_set & pred_set) / float(len(act_set)), 2)\n    return result\n```\n\nUsing this, we will replicate our eight-image dataset with *actual relevant* results in rank positions *2*, *4*, *5*, and *7*.\n\n{{< notebook file=\"metrics-recall-calc\" height=\"full\" >}}\n\n### Pros and Cons\n\nRecall@K is undoubtedly one of the most easily interpretable evaluation metrics. We know that a perfect score indicates that all relevant items are being returned. We also know that a smaller *k* value makes it harder for the IR system to score well with recall@K.\n\nStill, there are disadvantages to using *recall@K*. By increasing *K* to *N* or near *N*, we can return a perfect score every time, so relying solely on recall@K can be deceptive.\n\nAnother problem is that it is an *order-unaware metric*. That means if we used recall@4 and returned one relevant result at rank *one*, we would score the same as if we returned the same result at rank *four*. Clearly, it is better to return the actual relevant result at a higher rank, but recall@K *cannot* account for this.\n\n## Mean Reciprocal Rank (MRR)\n\nThe **M**ean **R**eciprocal **R**ank (MRR) is an *order-aware metric*, which means that, unlike recall@K, returning an actual relevant result at rank *one* scores better than at rank *four*.\n\nAnother differentiator for MRR is that it is calculated based on multiple queries. It is calculated as:\n$$\nMRR = \\frac{1}{Q} \\sum_{q=1}^{Q} \\frac{1}{rank_q}\n$$\n$Q$ is the number of queries, $q$ a specific query, and $rank_q$ the rank of the first *actual relevant* result for query $q$. We will explain the formula step-by-step.\n\nUsing our last example where a user searches for *\"cat in a box\"*. We add two more queries, giving us $Q = 3$.\n\n![mrr](/images/offline-evaluation-6.png)\n<small>We perform three queries while calculating the MRR score.</small>\n\nWe calculate the rank reciprocal $\\frac{1}{rank_q}$ for each query $q$. For the first query, the first actual relevant image is returned at position *two*, so the rank reciprocal is $\\frac{1}{2}$. Let's calculate the reciprocal rank for all queries:\n\n$$\nquery \\space 1: \\frac{1}{rank_1} = \\frac{1}{2} = 0.5\n$$\n\n$$\nquery \\space 2: \\frac{1}{rank_2} = \\frac{1}{1} = 1.0\n$$\n\n$$\nquery \\space 3: \\frac{1}{rank_3} = \\frac{1}{5} = 0.2\n$$\n\nNext, we sum all of these reciprocal ranks for queries $q=[1, ..., Q]$ (e.g., all three of our queries):\n\n$$\n\\sum_{q=1}^{Q} \\frac{1}{rank_q} = 0.5 + 1.0 + 0.2 = 1.7\n$$\n\nAs we are calculating the **mean** reciprocal rank (**M**RR), we must take the average value by dividing our total reciprocal ranks by the number of queries $Q$:\n\n$$\nMRR = \\frac{1}{Q} \\sum_{q=1}^{Q} \\frac{1}{rank_q} = \\frac{1}{3}1.7 ≅ 0.57\n$$\n\nNow let's translate this into Python. We will replicate the same scenario where $Q = 3$ using the same *actual relevant* results.\n\n{{< notebook file=\"metrics-mrr\" height=\"full\" >}}\n\nAnd as expected, we calculate the same MRR score of *0.57*.\n\n### Pros and Cons\n\nMRR has its own unique set of advantages and disadvantages. It is *order-aware*, a massive advantage for use cases where the rank of the first relevant result is important, like chatbots or [question-answering](/learn/question-answering).\n\nOn the other hand, we consider the rank of the *first* relevant item, but no others. That means for use cases where we'd like to return multiple items like recommendation or search engines, MRR is not a good metric. For example, if we'd like to recommend ~10 products to a user, we ask the IR system to retrieve 10 items. We could return just one *actual relevant* item in rank one and no other relevant items. Nine of ten irrelevant items is a terrible result, but MRR would score a perfect *1.0*.\n\nAnother *minor* disadvantage is that MRR is less readily interpretable compared to a simpler metric like recall@K. However, it is still more interpretable than many other evaluation metrics.\n\n## Mean Average Precision (MAP)\n\n**M**ean **A**verage **P**recision@K (*MAP@K*) is another popular *order-aware* metric. At first, it seems to have an odd name, a *mean* of an *average*? It makes sense; we promise.\n\nThere are a few steps to calculating *MAP@K*. We start with another metric called *precision@K*:\n$$\nPrecision@K = \\frac{true Positives}{true Positives + false Positives} = \\frac{p\\hat{p}}{p\\hat{p}+n\\hat{p}}\n$$\nYou may be think this looks very similar to *recall@K*, and it is! The only difference is that we've swapped <span onMouseOver=\"this.style.backgroundColor='#DFECF9'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$p\\hat{n}$</span> in recall for <span onMouseOver=\"this.style.backgroundColor='#FAFF00'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$n\\hat{p}$</span> here. That means we now consider both actual relevant and non-relevant results *only* from the returned items. We *do not* consider non-returned items with *precision@K*.\n\n![precision-and-recall](/images/offline-evaluation-18.png)\n<small>The difference between Recall@K and Precision@K calculation where *K = 2*.</small>\n\nLet's return to the *\"cat in a box\"* example. We will return $K = 2$ items and calculate *precision@2*.\n\n![precision-at-two](/images/offline-evaluation-7.png)\n<small>Precision@K calculation where *K = 2*.</small>\n\n$$\nPrecision@2 = \\frac{p\\hat{p}}{p\\hat{p}+n\\hat{p}} = \\frac{p\\hat{p}}{K} = \\frac{1}{2} = 0.5\n$$\n\nNote that the denominator in *precision@K* always equals $K$. Now that we have the *precision@K* value, we move on to the next step of calculating the **A**verage **P**recision@K (*AP@K*):\n\n$$\nAP@K = \\frac{\\sum_{k = 1}^{K} (Precision@k * rel_k)}{ number \\space of \\space relevant \\space results}\n$$\n\nNote that for *AP@K* we are taking the average *precision@k* score for all values of $k = [1, ..., K]$. Meaning that for AP@5 we calculate *precision@k* where $k=[1, 2, 3, 4, 5]$.\n\nWe have not seen the $rel_k$ parameter before. It is a *relevance* parameter that (for *AP@K*) is equal to *1* when the $k^{th}$ item is relevant or *0* when it is not.\n\n<center>\n<img loading=\"lazy\" src=\"/images/offline-evaluation-8.png\" width=\"80%\" />\n</center>\n<small>We calculate $precision@k$ and $rel_k$ iteratively using $k = [1, ..., K]$.</small>\n\nAs we multiply *precision@k* and $rel_k$, we only need to consider *precision@k* where the $k^{th}$ item is relevant.\n\n<center>\n<img loading=\"lazy\" src=\"/images/offline-evaluation-9.png\" width=\"50%\" />\n</center>\n<small>$Precision@k$ and $rel_k$ values for all relevant items across three queries $q = [1, ..., Q]$.</small>\n\nGiven these values, for each query $q$, we can calculate the $AP@K_q$ score where $K=8$ as:\n\n$$\nAP@8_1 = \\frac{0.5\\*1 + 0.5\\*1 + 0.6\\*1 + 0.57\\*1}{4} = 0.54\n$$\n\n$$\nAP@8_2 = \\frac{1\\*1 + 0.5\\*1 + 0.4\\*1 + 0.43\\*1}{4} = 0.67\n$$\n\n$$\nAP@8_3 = \\frac{0.2\\*1 + 0.25\\*1}{2} = 0.22\n$$\n\nEach of these individual $AP@K_q$ calculations produces a single **A**verage **P**recision@K score for each query $q$. To get the **M**ean **A**verage **P**recision@K (*MAP@K*) score for all queries, we simply divide by the number of queries $Q$:\n\n$$\nMAP@K = \\frac{1}{Q}\\sum_{q=1}^{Q}AP@K_q = \\frac{1}{3}*(0.54+0.67+0.22) = 0.48\n$$\n\nThat leaves us with a final *MAP@K* score of *0.48*. To calculate all of this with Python, we write:\n\n{{< notebook file=\"metrics-map\" height=\"full\" >}}\n\nReturning the same *MAP@K* score of $0.48$.\n\n### Pros and Cons\n\nMAP@K is a simple offline metric that allows us to consider the *order* of returned items. Making this ideal for use cases where we expect to return multiple relevant items.\n\nThe primary disadvantage of MAP@K is the $rel_K$ relevance parameter is binary. We must either view items as *relevant* or *irrelevant*. It does not allow for items to be slightly more/less relevant than others.\n\n## Normalized Discounted Cumulative Gain (NDCG@K)\n\n***N**ormalized **D**iscounted **C**umulative **G**ain **@K*** ($NDCG@K$) is another *order-aware metric* that we can derive from a few simpler metrics. Starting with **C**umulative **G**ain ($CG@K$) calculated like so:\n$$\nCG@K = \\sum_{k=1}^{K}rel_k\n$$\nThe $rel_k$ variable is different this time. It is a range of relevance ranks where *0* is the least relevant, and some higher value is the most relevant. The number of ranks does not matter; in our example, we use a range of $0 \\rightarrow 4$.\n\n<center>\n<img loading=\"lazy\" src=\"/images/offline-evaluation-10.png\" width=\"70%\" />\n</center>\n<small>Using $rel_k$ we rank every item based on its relevance to a particular query $q$.</small>\n\nLet's try applying this to another example. We will use a similar eight-image dataset as before. The circled numbers represent the IR system's *predicted* ranking, and the diamond shapes represent the $rel_k$ *actual ranking*.\n\n![ndcg_relevance](/images/offline-evaluation-11.png)\n<small>A small dataset with predicted ranks (circles) and actual ranks (diamonds).</small>\n\nTo calculate the cumulative gain at position *K* (*CG@K*), we sum the relevance scores up to the *predicted* rank *K*. When $K = 2$:\n\n$$\nCG@2 = \\sum_{k=1}^{2}rel_k = rel_1 + rel_2 = 0 + 4 = 4\n$$\n\nIt's important to note that *CG@K* is *not* order-aware. If we swap images *1* and *2*, we will return the same score when $K \\geq 2$ despite having the more relevant item placed first.\n\n![ndcg_relevance_two](/images/offline-evaluation-12.png)\n<small>Images *1* and *2* have been swapped.</small>\n\n$$\nCG@2 = \\sum_{k=1}^{2}rel_k = rel_1 + rel_2 = 4 + 0 = 4\n$$\n\nTo handle this lack of order awareness, we modify the metric to create *DCG@K*, adding a penalty in the form of $log_{2}(1+k)$ to the formula:\n\n$$\nDCG@2 = \\sum_{k=1}^{K}\\frac{rel_k}{log_2(1+k)}\n$$\n\nNow when we calculate *DCG@2* and swap the position of the first two images, we return different scores:\n\n$$\noriginal: \\space DCG@2 = \\frac{0}{\\log_2(1+1)}+\\frac{4}{\\log_2(1+2)} = 0 + 2.52 = 2.52\n$$\n\n$$\nswapped: \\space DCG@2 = \\frac{4}{\\log_2(1+1)}+\\frac{0}{\\log_2(1+2)} = 4 + 0 = 4\n$$\n\n```python\nfrom math import log2\n\n# initialize variables\nrelevance = [0, 7, 2, 4, 6, 1, 4, 3]\nK = 8\n\ndcg = 0\n# loop through each item and calculate DCG\nfor k in range(1, K+1):\n    rel_k = relevance[k-1]\n    # calculate DCG\n    dcg += rel_k / log2(1 + k)\n```\n\n![DGC@K_graph](/images/offline-evaluation-13.png)\n<small>$DCG@K$ score as $K$ increases using the *query #1* order of results.</small>\n\nUsing the *order-aware* $DCG@K$ metric means the preferred swapped results returns a better score.\n\nUnfortunately, *DCG@K* scores are very hard to interpret as their range depends on the variable $rel_k$ range we chose for our data. We use the **N**ormalized **DCG@K** (*NDCG@K*) metric to fix this.\n\n---\n\n*$NDCG@K$ is a special modification of standard NDCG that cuts off any results whose rank is greater than $K$. This modification is prevalent in use-cases measuring search performance<sup>[5]</sup>.*\n\n---\n\n*NDCG@K* normalizes *DCG@K* using the **I**deal **DCG@K** (*IDCG@K*) rankings. For *IDCG@K*, we assume that the most relevant items are ranked highest and in order of relevance.\n\nCalculating *IDCG@K* takes nothing more than reordering the assigned ranks and performing the same *DCG@K* calculation:\n\n$$\nIDCG@2 = \\frac{4}{\\log_2(1+1)}+\\frac{4}{\\log_2(1+2)} = 4 + 2.52 = 6.52\n$$\n\n```python\n# sort items in 'relevance' from most relevant to less relevant\nideal_relevance = sorted(relevance, reverse=True)\n\nprint(ideal_relevance)\n\nidcg = 0\n# as before, loop through each item and calculate *Ideal* DCG\nfor k in range(1, K+1):\n    rel_k = ideal_relevance[k-1]\n    # calculate DCG\n    idcg += rel_k / log2(1 + k)\n```\n\n![DCG_IGC@K_graph](/images/offline-evaluation-15.png)\n<small>*IDCG@K* score as $K$ increases compared against the *DCG@K* score calculated with using the *query #1* order of results.</small>\n\nNow all we need to calculate *NDCG@K* is to normalize our *DCG@K* score using the *IDCG@K* score:\n\n$$\nNDCG@K = \\frac{DCG@K}{IDCG@K} = \\frac{2.52}{6.52} = 0.39\n$$\n\n```python\ndcg = 0\nidcg = 0\n\nfor k in range(1, K+1):\n    # calculate rel_k values\n    rel_k = relevance[k-1]\n    ideal_rel_k = ideal_relevance[k-1]\n    # calculate dcg and idcg\n    dcg += rel_k / log2(1 + k)\n    idcg += ideal_rel_k / log2(1 + k)\n    # calcualte ndcg\n    ndcg = dcg / idcg\n```\n\n![NDGC@K_graph](/images/offline-evaluation-16.png)\n<small>*NDCG@K* score as $K$ increases calculated by normalizing *DCG@K* using *IDCG@K*.</small>\n\nUsing *NDCG@K*, we get a more interpretable result of *0.41*, where we know that *1.0* is the *best* score we can get with all items ranked perfectly (e.g., the *IDCG@K*).\n\n### Pros and Cons\n\n*NDCG@K* is one of the most popular offline metrics for evaluating IR systems, in particular web search engines. That is because *NDCG@K* optimizes for highly relevant documents, is *order-aware*, and is easily interpretable.\n\nHowever, there is a significant disadvantage to *NDCG@K*. Not only do we need to know which items are relevant for a particular query, but we need to know whether each item is more/less relevant than other items; the data requirements are more complex.\n\n![example of data for other metrics vs for NDCG](/images/offline-evaluation-17.png)\n<small>Example of data for the other metrics (left) and the more complex data required for NDCG@K (right).</small>\n\n---\n\nThese are some of the most popular offline metrics for evaluating information retrieval systems. A single metric can be a good indicator of system performance. For even more confidence in retrieval performance you can use several metrics, just as Spotify did with recall@1, recall@30, and MRR@30.\n\nThese metrics are still best supported with online metrics during A/B testing, which act as *the next step* before deploying your retrieval system to the world. However, these offline metrics are the foundation behind any retrieval project.\n\nWhether you’re prototyping your very first product, or evaluating the latest iteration of Google search, evaluating your retrieval system with these metrics will help you deploy the best retrieval system possible.\n\n{{< newsletter text=\"Subscribe to new content in search and recommendation!\" inputText=\"Email address...\" buttonText=\"Submit\">}}\n\n## References\n[Code Walkthrough](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/algos-and-libraries/offline-evaluation/offline-evaluation.ipynb)\n\n[1] G. Linden et al., [Item-to-Item Collaborative Filtering](https://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf) (2003), IEEE\n\n[2] J. Solsman, [YouTube's AI is the puppet master over most of what you watch](https://www.cnet.com/tech/services-and-software/youtube-ces-2018-neal-mohan/) (2018)\n\n[3] P. Covington et al., [Deep Neural Networks for YouTube Recommendations](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45530.pdf) (2016), RecSys\n\n[4] A. Tamborrino, [Introducing Natural Language Search for Podcast Episodes](https://engineering.atspotify.com/2022/03/introducing-natural-language-search-for-podcast-episodes/) (2022), Engineering at Spotify Blog\n\n[5] Y. Wang et al., [A Theoretical Analysis of NDCG Ranking Measures](http://proceedings.mlr.press/v30/Wang13.pdf) (2013), JMLR\n\n## Nomenclature\n\n$K: number \\space of \\space retrieved \\space results$\n\n$k: position \\space k \\space of \\space retrieved \\space items$\n\n$Q: number \\space of \\space queries$\n\n$q: query$\n\n$rank_q: rank \\space of \\space first \\space relevant \\space result \\space for \\space query \\space q$\n\n$rel_k: relevance \\space of \\space item \\space at \\space position \\space k$\n\n$p: actual \\space relevant \\space result$\n\n$\\hat{p}: predicted \\space relevant \\space result$\n\n$n: actual \\space irrelevant \\space result $\n\n$\\hat{n}: predicted \\space irrelevant \\space result$\n\n<span onMouseOver=\"this.style.backgroundColor='#8CF1FF'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$p\\hat{p}: true \\space positive$</span>\n\n<span onMouseOver=\"this.style.backgroundColor='#FAFF00'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$n\\hat{p}: false \\space positive$</span>\n\n<span onMouseOver=\"this.style.backgroundColor='#738FAB'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$n\\hat{n}: true \\space negative $</span>\n\n<span onMouseOver=\"this.style.backgroundColor='#DFECF9'\" onMouseOut=\"this.style.backgroundColor='transparent'\">$p\\hat{n}: false \\space negative$</span>\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbc4"
  },
  "filename": "train-sentence-transformers-softmax.md",
  "title": "ebook-post",
  "category": "\"Training Sentence Transformers with Softmax Loss\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Training Sentence Transformers with Softmax Loss\"\nheadline: \"Training Sentence Transformers the OG Way (with Softmax Loss)\"\ncategories:\n  - NLP for Semantic Search\ntoc: >-\nweight: 3\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: The original way of training sentence transformers like SBERT for semantic search.\n# Open Graph\nimages: ['/images/train-sentence-transformer-1.jpg']\n---\n\nOur article introducing [sentence embeddings and transformers](/learn/sentence-embeddings/) explained that these models can be used across a range of applications, such as semantic textual similarity (STS), semantic clustering, or information retrieval (IR) using concepts rather than words.\n\nThis article dives deeper into the training process of the first sentence transformer, *sentence-BERT*, or more commonly known as *SBERT*. We will explore the **N**atural **L**anguage **I**nference (NLI) training approach of *softmax loss* to fine-tune models for producing sentence embeddings.\n\nBe aware that softmax loss is no longer the preferred approach to training sentence transformers and has been superseded by other methods such as MSE margin and multiple negatives ranking loss. But we’re covering this training method as an important milestone in the development of ever improving sentence embeddings.\n\nThis article also covers *two approaches* to fine-tuning. The first shows how NLI training with softmax loss works. The second uses the excellent training utilities provided by the `sentence-transformers` library — it’s more abstracted, making building good sentence transformer models *much easier*.\n\n<p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/aSx0jg9ZILo\" title=\"Sentence Embeddings\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>\n\n## NLI Training\n\nThere are several ways of training sentence transformers. One of the most popular (and the approach we will cover) is using Natural Language Inference (NLI) datasets.\n\nNLI focus on identifying sentence pairs that *infer* or *do not infer* one another. We will use two of these datasets; the Stanford Natural Language Inference (SNLI) and Multi-Genre NLI (MNLI) corpora.\n\nMerging these two corpora gives us 943K sentence pairs (550K from SNLI, 393K from MNLI). All pairs include a `premise` and a `hypothesis`, and each pair is assigned a `label`:\n\n* **0** — *entailment*, e.g. the `premise` suggests the `hypothesis`.\n* **1** — *neutral*, the `premise` and `hypothesis` could both be true, but they are not necessarily related.\n* **2** — *contradiction*, the `premise` and `hypothesis` contradict each other.\n\nWhen training the model, we will be feeding sentence A (the `premise`) into BERT, followed by sentence B (the `hypothesis`) on the next step.\n\nFrom there, the models are optimized using *softmax loss* using the `label` field. We will explain this in more depth soon.\n\nFor now, let's download and merge the two datasets. We will use the `datasets` library from Hugging Face, which can be downloaded using `!pip install datasets`. To download and merge, we write:\n\n{{< notebook file=\"prep-snli-mnli\" height=\"full\" >}}\n\nBoth datasets contain `-1` values in the `label` feature where no confident class could be assigned. We remove them using the `filter` method.\n\n{{< notebook file=\"remove-empties\" height=\"full\" >}}\n\nWe must convert our human-readable sentences into transformer-readable tokens, so we go ahead and tokenize our sentences. Both `premise` and `hypothesis` features must be split into their own `input_ids` and `attention_mask` tensors.\n\n{{< notebook file=\"nli-tokenize\" height=\"full\" >}}\n\nNow, all we need to do is prepare the data to be read into the model. To do this, we first convert the `dataset` features into PyTorch tensors and then initialize a data loader which will feed data into our model during training.\n\n```python\n# covert dataset features to PyTorch tensors\ndataset.set_format(type='torch', columns=all_cols)\n\n# initialize the dataloader\nbatch_size = 16\nloader = torch.utils.data.DataLoader(\n    dataset, batch_size=batch_size, shuffle=True\n)\n```\n\nAnd we're done with data preparation. Let's move on to the training approach.\n\n\n\n## Softmax Loss\n\nOptimizing with *softmax loss* was the primary method used by Reimers and Gurevych in the original SBERT paper [1].\n\nAlthough this was used to train the first sentence transformer model, it is no longer the go-to training approach. Instead, [the MNR loss approach](/learn/fine-tune-sentence-transformers-mnr/) is most common today. We will cover this method in another article.\n\nHowever, we hope that explaining softmax loss will help demystify the different approaches applied to training sentence transformers. We included a comparison to MNR loss at the end of the article.\n\n#### Model Preparation\n\nWhen we train an SBERT model, we don't need to start from scratch. We begin with an already pretrained BERT model (and tokenizer).\n\n```python\nfrom transformers import BertModel\n\n# start from a pretrained bert-base-uncased model\nmodel = BertModel.from_pretrained('bert-base-uncased')\n```\n\nWe will be using what is called a *'siamese'*-BERT architecture during training. All this means is that given a sentence pair, we feed *sentence A* into BERT first, then feed *sentence B* once BERT has finished processing the first.\n\nThis has the effect of creating a *siamese*-like network where we can imagine two identical BERTs are being trained in parallel on sentence pairs. In reality, there is just a single model processing two sentences one after the other.\n\n![Start SBERT](/images/train-sentence-transformer-2.jpg)\n<small>Siamese-BERT processing a sentence pair and then pooling the large token embeddings tensor into a single dense vector.</small>\n\nBERT will output 512 768-dimensional embeddings. We will convert these into an *average* embedding using *mean-pooling*. This *pooled output* is our sentence embedding. We will have two per step — one for sentence A that we call `u`, and one for sentence B, called `v`.\n\nTo perform this mean pooling operation, we will define a function called `mean_pool`.\n\n```python\n# define mean pooling function\ndef mean_pool(token_embeds, attention_mask):\n    # reshape attention_mask to cover 768-dimension embeddings\n    in_mask = attention_mask.unsqueeze(-1).expand(\n        token_embeds.size()\n    ).float()\n    # perform mean-pooling but exclude padding tokens (specified by in_mask)\n    pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(\n        in_mask.sum(1), min=1e-9\n    )\n    return pool\n```\n\nHere we take BERT's token embeddings output (we'll see this all in full soon) and the sentence's `attention_mask` tensor. We then resize the `attention_mask` to align to the higher `768`-dimensionality of the token embeddings. \n\nWe apply this resized mask `in_mask` to those token embeddings to exclude padding tokens from the mean pooling operation. Our mean pooling takes the average activation of values across each dimension to produce a single value. This brings our tensor sizes from `(512*768)` to `(1*768)`.\n\nThe next step is to concatenate these embeddings. Several different approaches to this were presented in the paper:\n\n| Concatenation | NLI Performance |\n| -------------------- | --------------- |\n| `(u, v)` | 66.04 |\n| `(|u-v|)` | 69.78 |\n| `(u*v)` | 70.54 |\n| `(|u-v|, u*v)` | 78.37 |\n| `(u, v, u*v)` | 77.44 |\n| **`(u, v, |u-v|)`** | **80.78** |\n| `(u, v, |u-v|, u*v)` | 80.44 |\n\n<small>Concatenation methods for sentence embeddings `u` and `v` and their performance on STS benchmarks.</small>\n\nOf these, the best performing is built by concatenating vectors `u`, `v`, and `|u-v|`. Concatenation of them all produces a vector three times the length of each original vector. We label this concatenated vector `(u, v, |u-v|)`. Where `|u-v|` is the element-wise difference between vectors `u` and `v`.\n\n![UV Vectors](/images/train-sentence-transformer-3.jpg)\n<small>We concatenate `(u, v, |u-v|)` to merge the sentence embeddings from sentence A and B.</small>\n\nWe will perform this concatenation operation using PyTorch. Once we have our mean-pooled sentence vectors `u` and `v` we concatenate with:\n\n```python\nuv_abs = torch.abs(torch.sub(u, v))  # produces |u-v| tensor\n# then we concatenate\nx = torch.cat([u, v, uv_abs], dim=-1)\n```\n\nVector `(u, v, |u-v|)` is fed into a feed-forward neural network (FFNN). The FFNN processes the vector and outputs three activation values. One for each of our `label` classes; *entailment*, *neutral*, and *contradiction*.\n\n```python\n# we would initialize the feed-forward NN first\nffnn = torch.nn.Linear(768*3, 3)\n\t...\n# then later in the code process our concatenated vector with it\nx = ffnn(x)\n```\n\nAs these activations and `label` classes are aligned, we now calculate the softmax loss between them.\n\n![SBERT Training](/images/train-sentence-transformer-4.jpg)\n<small>The final steps of training. The concatenated `(u, v, |u-v|)` vector is fed through a feed-forward NN to produce three output activations. Then we calculate the softmax loss between these predictions and the true labels.</small>\n\nSoftmax loss is calculated by applying a softmax function across the three activation values (or nodes), producing a predicted label. We then use [cross-entropy loss](/learn/cross-entropy-loss/) to calculate the difference between our predicted label and true `label`.\n\n```python\n# as before, we would initialize the loss function first\nloss_func = torch.nn.CrossEntropyLoss()\n\t...\n# then later in the code add them to the process\nx = loss_func(x, label)  # label is our *true* 0, 1, 2 class\n```\n\nThe model is then optimized using this loss. We use an Adam optimizer with a learning rate of `2e-5` and a linear warmup period of *10%* of the total training data for the optimization function. To set that up, we use the standard PyTorch `Adam` optimizer alongside a learning rate scheduler provided by HF transformers:\n\n```python\nfrom transformers.optimization import get_linear_schedule_with_warmup\n\n# we would initialize everything first\noptim = torch.optim.Adam(model.parameters(), lr=2e-5)\n# and setup a warmup for the first ~10% steps\ntotal_steps = int(len(dataset) / batch_size)\nwarmup_steps = int(0.1 * total_steps)\nscheduler = get_linear_schedule_with_warmup(\n\t\toptim, num_warmup_steps=warmup_steps,\n  \tnum_training_steps=total_steps - warmup_steps\n)\n\t...\n# then during the training loop we update the scheduler per step\nscheduler.step()\n```\n\nNow let's put all of that together in a PyTorch training loop.\n\n{{< notebook file=\"softmax-loss-training\" height=\"full\" >}}\n\nWe only train for a single epoch here. Realistically this should be enough (and mirrors what was described in the original SBERT paper). The last thing we need to do is save the model.\n\n{{< notebook file=\"save-softmax-loss\" height=\"full\" >}}\n\nNow let's compare everything we've done so far with `sentence-transformers` training utilities. We will compare this and other sentence transformer models at the end of the article.\n\n## Fine-Tuning With Sentence Transformers\n\nAs we already mentioned, the `sentence-transformers` library has excellent support for those of us just wanting to train a model without worrying about the underlying training mechanisms.\n\nWe don't need to do much beyond a little data preprocessing (but less than what we did above). So let's go ahead and put together the same fine-tuning process, but using `sentence-transformers`.\n\n### Training Data\n\nAgain we're using the same SNLI and MNLI corpora, but this time we will be transforming them into the format required by `sentence-transformers` using their `InputExample` class. Before that, we need to download and merge the two datasets just like before.\n\n{{< notebook file=\"quick-prep-nli\" height=\"full\" >}}\n\nNow we're ready to format our data for `sentence-transformers`. All we do is convert the current `premise`, `hypothesis`, and `label` format into an *almost* matching format with the `InputExample` class.\n\n{{< notebook file=\"quick-data-for-st\" height=\"full\" >}}\n\nWe've also initialized a `DataLoader` just as we did before. From here, we want to begin setting up the model. In `sentence-transformers` we build models using different *modules*.\n\nAll we need is the transformer model module, followed by a mean pooling module. The transformer models are loaded from HF, so we define `bert-base-uncased` as before.\n\n{{< notebook file=\"init-model-st\" height=\"full\" >}}\n\nWe have our data, the model, and now we define how to optimize our model. Softmax loss is *very* easy to initialize.\n\n{{< notebook file=\"softmax-loss\" height=\"full\" >}}\n\nNow we're ready to train the model. We train for a single epoch and warm up for 10% of training as before.\n\n{{< notebook file=\"train-st\" height=\"full\" >}}\n\nWith that, we're done, the new model is saved to `./sbert_test_b`. We can load the model from that location using either the `SentenceTransformer` or HF's `from_pretrained` methods! Let's move on to comparing this to other SBERT models.\n\n## Compare SBERT Models \n\nWe're going to test the models on a set of random sentences. We will build our mean-pooled embeddings for each sentence using *four* models; *softmax-loss* SBERT, *multiple-negatives-ranking-loss* SBERT, the original SBERT `sentence-transformers/bert-base-nli-mean-tokens`, and BERT `bert-base-uncased`.\n\n```python\nsentences = [\n    \"the fifty mannequin heads floating in the pool kind of freaked them out\",\n    \"she swore she just saw her sushi move\",\n    \"he embraced his new life as an eggplant\",\n    \"my dentist tells me that chewing bricks is very bad for your teeth\",\n    \"the dental specialist recommended an immediate stop to flossing with construction materials\",\n    \"i used to practice weaving with spaghetti three hours a day\",\n    \"the white water rafting trip was suddenly halted by the unexpected brick wall\",\n    \"the person would knit using noodles for a few hours daily\",\n    \"it was always dangerous to drive with him since he insisted the safety cones were a slalom course\",\n    \"the woman thinks she saw her raw fish and rice change position\"\n]\n```\n\nAfter producing sentence embeddings, we will calculate the cosine similarity between all possible sentence pairs, producing a simple but insightful semantic textual similarity (STS) test.\n\nWe define two new functions; `sts_process` to build the sentence embeddings and compare them with cosine similarity and `sim_matrix` to construct a similarity matrix from all possible pairs.\n\n```python\nimport numpy as np\n\n# build embeddings and calculate cosine similarity\ndef sts_process(sentence_a, sentence_b, model):\n    vecs = []  # init list of sentence vecs\n    for sentence in [sentence_a, sentence_b]:\n        # build input_ids and attention_mask tensors with tokenizer\n        input_ids = tokenizer(\n            sentence, max_length=512, padding='max_length',\n            truncation=True, return_tensors='pt'\n        )\n        # process tokens through model and extract token embeddings\n        token_embeds = model(**input_ids).last_hidden_state\n        # mean-pool token embeddings to create sentence embeddings\n        sentence_embeds = mean_pool(token_embeds, input_ids['attention_mask'])\n        vecs.append(sentence_embeds)\n    # calculate cosine similarity between pairs and return numpy array\n    return cos_sim(vecs[0], vecs[1]).detach().numpy()\n\n# controller function to build similarity matrix\ndef sim_matrix(model):\n    # initialize empty zeros array to store similarity scores\n    sim = np.zeros((len(sentences), len(sentences)))\n    for i in range(len(sentences)):\n        # add similarity scores to the similarity matrix\n        sim[i:,i] = sts_process(sentences[i], sentences[i:], model)\n    return sim\n```\n\nThen we just run each model through the `sim_matrix` function.\n\n```python\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('./sbert_test_a')\n\nsim = sim_matrix(model)  # build similarity scores matrix\nsns.heatmap(sim, annot=True)  # visualize heatmap\n```\n\nAfter processing all pairs, we visualize the results in heatmap visualizations.\n\n![SBERT Heatmaps](/images/train-sentence-transformer-5.jpg)\n<small>Similarity score heatmaps for four BERT/SBERT models.</small>\n\nIn these heatmaps, we ideally want all dissimilar pairs to have very low scores (near white) and similar pairs to produce distinctly higher scores.\n\nLet's talk through these results. The bottom-left and top-right models produce the correct top three pairs, whereas BERT and *softmax loss* SBERT return 2/3 of the correct pairs.\n\nIf we focus on the standard BERT model, we see minimal variation in square color. This is because almost every pair produces a similarity score of between *0.6* to *0.7*. This lack of variation makes it challenging to distinguish between more-or-less similar pairs. Although this is to be expected as BERT has *not* been fine-tuned for semantic similarity.\n\nOur PyTorch *softmax loss* SBERT (top-left) misses the *9-1* sentence pair. Nonetheless, the pairs it produces are much more distinct from dissimilar pairs than the vanilla BERT model, so it's an improvement. The `sentence-transformers` version is better still and did *not* miss the *9-1* pair.\n\nNext up, we have the SBERT model trained by Reimers and Gurevych in the 2019 paper (bottom-left) [1]. It produces better performance than our SBERT models but still has little variation between similar and dissimilar pairs.\n\nAnd finally, we have an SBERT model trained using *MNR loss*. This model is easily the highest performing. Most dissimilar pairs produce a score *very* close to *zero*. The highest non-pair returns *0.28* — roughly half of the true-pair scores.\n\nFrom these results, the SBERT MNR model seems to be the highest performing. Producing much higher activations (with respect to the average) for true pairs than any other model, making similarity much easier to identify. SBERT with softmax loss is clearly an improvement over BERT, but unlikely to offer any benefit over the SBERT with MNR loss model.\n\n---\n\nThat's it for this article on fine-tuning BERT for building sentence embeddings! We delved into the details of preprocessing SNLI and MNLI datasets for NLI training and how to fine-tune BERT using the *softmax loss* approach.\n\nFinally, we compared this *softmax-loss* SBERT against vanilla BERT, the original SBERT, and [an *MNR loss* SBERT](/learn/fine-tune-sentence-transformers-mnr/) using a simple STS task. We found that although fine-tuning with *softmax loss* does produce valuable sentence embeddings — it still lacks quality compared to more recent training approaches.\n\nWe hope this has been an insightful and exciting exploration of how transformers can be fine-tuned for building sentence embeddings.\n\n## References\n\n[1] N. Reimers, I. Gurevych, [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084) (2019), ACL"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbc5"
  },
  "filename": "nlp-financial-services.md",
  "title": "post",
  "category": "\"Language Embedding Models in Financial Services\"",
  "content": "---\nlayout: post\ntitle: \"Language Embedding Models in Financial Services\"\nheadline: \"How Language Embedding Models Will Change Financial Services\"\ncategories:\n  - Applications of Vector Search\ntoc: >-\nweight: 5\nauthor:\n  name: Diego Lopez Yse\n  position: Data Scientist\n  src: /images/diego-lopez-yse.jpeg\n  href: \"https://lopezyse.medium.com/\"\ndescription: How financial players are using vectorized data to gain insights.\n#Open Graph\nimages: ['/images/nlp-financial-services-8.png']\n---\n\n**How financial players are using vectorized data to gain insights.**\n\nIn finance, big profits are usually reserved for the ones who get the insights first. Each second counts, and high frequency trading algorithms are clear examples of this. But it’s not only a matter of speed: Data aggregation and knowledge discovery have become essential abilities. This is the reason why so many financial players are turning into data science companies.\n\nTraditional financial data sources, like company filings, are widely available and accessible to everyone. Moreover, so many companies mask their accounts that financial statements are becoming less and less valuable to get to the insight first. So what are financial companies doing? Turning to alternative data.\n\nAlternative data refers to non-traditional data that [can provide an indication of future performance](https://www.refinitiv.com/en/financial-data/alternative-data) of a company outside of traditional sources. You can think of [examples](http://forbes.com/sites/forbesinsights/2019/12/12/alternative-data-what-is-it-who-uses-it-and-why-is-it-interesting/) like records from credit card transactions, web-scraped data, geolocation data from cell phones, satellite images and weather forecasts. These sources are less structured and usually less accessible than the traditional ones, making them ideal for uncovering insights.\n\n![Sources of alternative data](/images/nlp-financial-services-1.png)\n<small>Experts divide alternative data roughly into three categories: data generated by individuals, data generated through business processes, and data generated by sensors. Source: [Caserta](https://caserta.com/alternative-data/)</small>\n\nFrom this growing universe of alternative data, there’s one in particular gaining super-fast traction. The accelerated growth of **text data** has been identified as a valuable source of opportunities to gain insights. Think about it: Text data includes text from social media, consumer reviews, news, documents, and even media formats like video and audio files. We’re surrounded by this type of data, and they carry precious amounts of information.\n\nBut how can we decode the information integrated in text data? **Text data is highly unstructured**, which means it doesn’t have a predefined format. Although this has always been a problem for analytical and predictive models, modern AI solutions are capable of processing this type of data effectively. There’s only one caveat. For models to be effective, they have to deal with another important characteristic of text data: its **high dimensionality**.\n\n## The problem of high dimensionality\n\nData dimensionality refers to how many attributes a dataset has, and high dimensional data is characterized by having multiple dimensions. How many? There can be hundreds, thousands, or millions of dimensions in a single dataset. A sample of thirty-word Twitter messages that use only the one thousand most common words in the English language, for example, has roughly [as many dimensions as there are atoms in the universe](https://web.stanford.edu/~gentzkow/research/text-as-data.pdf)!\n\nThe **Curse of Dimensionality** is the name given to the problem of the exponential increase in volume associated with adding extra dimensions to the data space. High dimensional data brings all sorts of challenges, but there’s one in particular that deserves our attention: the problem of data sparsity. Sparsity of data occurs when moving to higher data dimensions, as the volume of the space represented grows so quickly that the data cannot keep up and thus becomes sparse.\n\n![Curse of dimensionality](/images/nlp-financial-services-2.png)\n<br><small>As the data moves from one dimension (at left) to two dimensions (in the middle) and finally to three dimensions (at right), it fills less and less of the data space. In order to maintain an accurate representation of the space, the data for analysis needs to grow exponentially. Source: [Deep AI](https://deepai.org/machine-learning-glossary-and-terms/curse-of-dimensionality)</small>\n\nSo how can we manage this complexity and discover insights in our current massive data volumes? The answer is: using **[vector embeddings](/learn/vector-embeddings/)**.\n\n## What are vector embeddings?\n\nMachine Learning models expect inputs as numbers, not words, images, or videos. In order for these models to extract patterns and make predictions, data has to be transformed into a vector first. **Vector embeddings represent inputs like text as numbers**, so that we can train and deploy our Machine Learning models for tasks like classification or sentiment analysis.\n\nVector embeddings are low-dimensional representations of high-dimensional data. Typically, a standard vector won’t capture all information contained in the original data, but a good vector embedding will capture enough to solve the problem at hand.\n\nWhen we use vector embeddings as inputs, the main benefit is their ability to encode information in a format that our models can process and then output something useful to our end goal. In the case of text data, we can use vectors to represent the features we want our model to learn. This vectorization produces meaningful numerical representations for the machines, enabling them to perform different tasks.\n\nThis way, a text representation shifts from a sequence of words to **points that occupy a vector embedding space**. Points in space can be close together or far apart, tightly clustered or evenly distributed.\n\n<video autoplay loop muted playsinline class=\"responsive\"><source src=\"/images/nlp-financial-services-3.mp4\" type=\"video/mp4\"></video>\n\n<small class=\"video\">Visualization of vectorized words in 3D through the [TensorFlow Embedding Projector](https://projector.tensorflow.org/)</small>\n\nThis vector embedding space is therefore mapped in such a way that words and sentences with similar meanings are closer together, and those that are different are farther apart. By encoding **[similarity](/learn/semantic-search/) as distance**, we can begin to derive the primary components of texts and draw decision boundaries in our semantic space. Once words or sentences get represented as numerical vectors, you can perform mathematical operations with them like addition and subtraction. This is an amazing property of vector embeddings because it means that they carry [important relational information](https://github.com/sid321axn/bank_fin_embedding) that can be used in many different Natural Language Processing (NLP) tasks.\n\n![Addition and subtraction with financial vectors](/images/nlp-financial-services-4.png)\n<small>Since vectors can be added and subtracted, in this example we can perform operations like adding the words “Sales” and “Grow”, which will result in the word “Inflation”. Source: [BankFin Embeddings](https://github.com/sid321axn/bank_fin_embedding)</small>\n\nDifferent models including neural-net language models (NNLM), global vectors for word representation (GloVe), deep contextualized word representations (ELMo), and Word2vec are [designed to learn word embeddings](http://www.scholarpedia.org/article/Neural_net_language_models), which are real-valued feature vectors for each word.\n\nWord embeddings built from a general corpus of sources like Wikipedia or Google News are widely available to use and provide acceptable results to solve general tasks. These are general purpose models, however, which need to be [fine-tuned](/learn/fine-tune-sentence-transformers-mnr/) to the specific vocabulary if you want to increase your model performance. For example, it’s possible to redefine word embeddings by adding text from 10-K filings to improve results on tasks such as document classification, document similarity, sentiment analysis, or readability index.\n\n## Gain insights with vectorization\n\nExtracting information from unstructured data can be of enormous value. Text from news, social media, and company filings and communications is used to predict asset price movements and study the causal impact of new information. Through vector embeddings, you can perform exploratory analysis and visualizations to derive insights and discover patterns.\n\n### News\n\n[More than 2 million articles are published every day on the web.](https://www.semanticscholar.org/paper/Text-Similarity-Measures-in-News-Articles-by-Vector-Singh-Singh/2f3bfd8f11cc55aea33b61e23457572236664df8) That’s more than 1,300 per minute. How can you keep pace with them? One strategy is to automatically extract insights from these massive volumes using Machine Learning. Through vectorization, it’s possible to monitor news media in an automated fashion: from digital newspapers to video and audio channels. By embedding all data into the same vectorized space, we can perform searches and look for similarities between data sources that previously seemed dissimilar. News that contains useful information on companies and markets can be exploited for profits, no matter their original format.\n\n### Consumer sentiment\n\nThrough vectorization, it’s also possible to derive people’s sentiment. The idea behind it is to analyze pieces of content (e.g., documents, videos) to determine the emotional tone they carry, usually classifying them by positive, negative, or neutral. The goal? To understand attitudes towards a topic. This is a highly used resource by companies that want to get insights on their customers, through the analysis of reviews, conversations, and posts on social media. Stock market investors also use customer sentiment to anticipate potential market trends.\n\n![Consumer sentiment over time](/images/nlp-financial-services-5.png)\n<small>Tesla, Inc. share price over time (left axis) and cumulative news sentiment scores (right axis). Source: [Dow Jones](https://visit.dowjones.com/factiva/content/unlocking-hidden-potential/#lp-pom-text-1115)</small>\n\nFinancial data companies like Moody’s have integrated these concepts to develop a [Credit Sentiment Score](https://www.moodysanalytics.com/product-list/credit-sentiment-score) that compiles adverse credit signals from news articles, backed by extensive research and state of the art NLP, text analytics, and Machine Learning techniques. This score helps firms assess credit in the loan origination and portfolio risk monitoring process and track unfavorable media. The higher the score, the stronger the credit distress signal.\n\n![Credit sentiment over time](/images/nlp-financial-services-6.png)\n<small>The figure shows the monthly average credit sentiment score of the companies in the run up to a credit event (blue line). For comparison, we also show the long-term average score for the control group (green dashed line) and three times this average (green dotted line). We see that the average credit sentiment score moves away from the long-term average as it moves toward a credit event. At around nine months before a credit event, the score is already around about three times this monthly average. Source: [Moody’s Analytics](https://www.moodysanalytics.com/-/media/products/moodys-analytics-credit-sentiment-score-product-overview.pdf)</small>\n\nStandard & Poors (S&P) is another company exploiting the benefits of vector embeddings. They launched [Textual Data Analytics (TDA)](https://apnews.com/press-release/pr-prnewswire/62e90ac6f5ea7bb0fa5c501e805de6f4), a sophisticated data offering which applies NLP to generate sentiment scores and behavioural metrics based on company transcripts. This solution allows to incorporate more qualitative measures of company performance into investment strategies by quantifying sentiment and behaviour during company calls.\n\n### Better data interaction\n\nBesides allowing us to create target solutions, vector embeddings also allow you to interact with the data in a much easier way.\n\n*What if you could ask questions to your data like you would ask a human being?*\n\nSolutions like Spawner AI enable you to ask questions about financial instruments and get back [natural language answers](https://towardsdatascience.com/financial-nlp-the-internets-financial-membrane-4da9482781f9). You can ask all sorts of questions about income statements, balance sheets, and cash flows. Instead of using query languages, why not ask “What is the revenue of General Electric”?\n\n<video autoplay loop muted playsinline class=\"responsive\"><source src=\"/images/nlp-financial-services-7.mp4\" type=\"video/mp4\">\n</video>\n\n<small class=\"video\">Answers from the [Spawner API](https://spawner.ai/) using Python library. Source: [Towards Data Science](https://towardsdatascience.com/financial-nlp-the-internets-financial-membrane-4da9482781f9)</small>\n\n### What the future looks like\n\nSectors beyond finance are paying strong attention to the value of alternative data. Decision makers across all industries are demanding more actionable insights before making inferences about what actions to take. To fill this need, lots of companies have been fast enough to gather, clean, analyse, and interpret useful data from non-traditional sources.\n\n![Future of alternative data](/images/nlp-financial-services-8.png)\n<small>Alternative data is quickly becoming the key driver of investment strategies. Source: [Nasdaq](https://data.nasdaq.com/monetize-alternative-data/guide-to-selling-data-to-wall-street.pdf)</small>\n\nToday more than ever, alternative data can give insights that traditional data cannot. Additionally, mobility and smartphones have brought wide possibilities: all  cell phone apps are rich sources of data that can be used while we circulate and interact with the physical world.\n\nBut how can we make sense of all these data sources to get insights? It’s estimated that nearly [90% of today’s data is unstructured](https://venturebeat.com/2021/07/22/why-unstructured-data-is-the-future-of-data-management/), and this massive volume will only keep growing. But since unstructured data can’t be easily processed or stored in traditional databases, we need to think about different strategies.\n\nWhether you’re dealing with numeric data, text, images, sounds or rich media, everything can be vectorized into the same space, and [the semantic similarity of these objects and concepts can be quantified by how close they are to each other as points in this vector space](https://www.pinecone.io/learn/vector-embeddings/). Vectorizing unstructured data can allow us to store that information in a much more efficient way and use it to feed Machine Learning models for clustering, recommendation, or classification tasks. Machines only process numbers, and vectorization is an effective way to translate our highly diverse data sources into a language that machines can understand.\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbc6"
  },
  "filename": "pinecone-gcp-marketplace.md",
  "title": "post",
  "category": "Pinecone is now available on the Google Cloud Marketplace",
  "content": "---\nlayout: post\ntitle: Pinecone is now available on the Google Cloud Marketplace\nheadline: Pinecone is now available on the Google Cloud Marketplace\ncategories:\n  - Company News\ntoc: >-\nauthor:\n  name: Gibbs Cullen\n  position: Senior Product Marketing Manager\n  src: /images/gibbs-cullen.jpg\n  href: https://www.linkedin.com/in/gibbscullen/\ndate: \"2022-12-22\"\n# Open Graph\ndescription: Start building faster with Pinecone through the Google Cloud Marketplace\nimages: [\"/images/pinecone-gcp-marketplace-ogimage.png\"]\nthumbnail: \"/images/pinecone-gcp-thumbnail.png\"\n---\n\nWe are excited to announce that Pinecone is now available on the Google Cloud Platform (GCP) Marketplace (and as the first vector database, no less). With Pinecone, you can build AI-powered search into your applications without needing to manage your own or modify legacy infrastructures. Companies ranging from small startups to enterprises trust Pinecone’s vector database to securely and efficiently manage their data, whether it’s hundreds of thousands or a billion items. And now, Google Cloud users can get started faster through our [Marketplace offering](https://console.cloud.google.com/marketplace/product/pinecone-public/pinecone).\n\nThe GCP Marketplace provides software solutions for users to easily discover, procure, and deploy solutions available on Google Cloud. Joining the GCP Marketplace has many benefits for existing Google Cloud users including:\n\n- Consolidated Google Cloud billing: You can now meet committed spend levels faster by applying Pinecone spend to your account. View your spend across GCP services in a single place.\n- Faster, simpler procurement: Skip the approvals needed to integrate a new solution, and get started in production faster when purchasing Pinecone through the Marketplace.\n\nWant to learn more? Here are some FAQs for existing Pinecone users. Make sure to also check out our [integration guide](https://docs.pinecone.io/docs/setting-up-gcp-marketplace-billing) and visit our [Marketplace listing](https://console.cloud.google.com/marketplace/product/pinecone-public/pinecone).\n\n## FAQs:\n\n**Q**: Which plans and pricing are available through GCP Marketplace?\n\n**A**: We currently support Standard and Enterprise plans, with support for annual pre-commitment contracts coming soon. The Starter (free) plan is not available.\n\n**Q**: Can I change plans while being billed through GCP Marketplace?\n\n**A**: Yes, you can still upgrade and downgrade plans through the billing page in the Pinecone console.\n\n**Q**: Do I need a GCP marketplace account for this?\n\n**A**: Yes, you need a GCP Marketplace account. To purchase Pinecone, you must be logged in to the GCP Marketplace and your project must be enabled for purchase by your billing administrator.\n\n**Q**: Where will I see how much I’m being charged for Pinecone?\n\n**A**: All billing through marketplaces will be charged through your cloud provider. Check the billing console from your cloud provider.\n\n**Q**: If I’m already a Pinecone customer, how do I switch to billing on the GCP marketplace?\n\n**A**: Subscribing to GCP marketplace will automatically create a new organization for you. If you want to use GCP billing for an existing organization or projects, please reach out to support@pinecone.io and we’ll help migrate them for you.\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbc7"
  },
  "filename": "question-answering.md",
  "title": "ebook-post",
  "category": "\"Question Answering\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Question Answering\"\nheadline: \"An Introduction to Open Domain Question-Answering\"\ncategories:\n  - NLP for Semantic Search\ntoc: >-\nweight: 7\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: Illustrated guide to open domain question-answering.\nimages: ['/images/qa-overview-1.jpg']\n---\n\nSearch is a crucial functionality in many applications and companies globally. Whether in manufacturing, finance, healthcare, or *almost* any other industry, organizations have vast internal information and document repositories.\n\nUnfortunately, the scale of many companies’ data means that the organization and accessibility of information can become incredibly inefficient. The problem is exacerbated for language-based information. Language is a tool for people to communicate often abstract ideas and concepts. Naturally, ideas and concepts are harder for a computer to comprehend and store in a meaningful way.\n\n<p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/-td57YvJdHc\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>\n\nMost organizations rely on a cluster of keyword-based search interfaces hosted on various *'internal portals'* to deal with language data. If done well, this can satisfy business requirements for *some* of that data.\n\nIf a person knows what they're looking for and they know the keywords and terminology of the information they need, a keyword-based search is ideal. When the keywords and terminology of the answer are unknown, keyword search is inadequate. People searching for unknown answers in large repositories of documents is a drain on productivity. \n\nHow do we minimize this problem? The answer lies with *semantic search*, specifically with the question-answering (QA) flavor of semantic search.\n\nSemantic search allows us to [search based on concepts and ideas](/learn/sentence-embeddings/) rather than keywords. Given a phrase, a semantic search tool returns the most *semantically similar* phrases from a repository.\n\nQuestion-answering takes this idea further by searching using a natural language question and returning relevant documents and specific answers. QA aims to mimic natural language as much as possible. If we asked a shop assistant *\"where are those tasty, freshly baked things that are not cookies but look like cookies?\"*, we would expect directions that take us to those things. This natural form of conversation is what QA aims to reproduce.\n\nThis article will introduce the different forms of QA, the components of these *'QA stacks'*, and where we might use them.\n\n## Question Answering at a Glance\n\nBefore we dive into the details, let us paint a high-level picture of QA. First, our focus is on *open-domain* QA (ODQA). ODQA systems deal with questions across broad topics and cannot rely on a specific set of rules in your code. The alternative to *open-domain* is *closed-domain*, which focuses on a limited domain/scope and *can* often rely on explicit logic. We will **not** cover *closed-domain* QA.\n\nFor the remainder of the article, I will use **OD**QA and QA interchangeably. ODQA models can be split into a few subcategories.\n\n![qa_types](/images/qa-overview-2.jpg)\n<small>There are a few approaches to question answering (QA).</small>\n\nThe most common form of QA is **open-book extractive QA** (top-left above). Here we combine an information retrieval (IR) step and a reading comprehension (RC) step.\n\nAny *open-book* QA requires an IR step to *retrieve* relevant information from the 'open-book'. Just as with open-book exams where students can refer to their books for information during the exam, the model can refer to an external source of information. That source of information may be internal company documents, Wikipedia, Reddit, or any other information source that *is not* the model itself.\n\nThe IR step retrieves relevant documents and passes them to the RC (reader) step. RC consists of *extracting* a succinct answer from a sentence or paragraph, typically referred to as the *document* or *context*.\n\n![question_context_answer](/images/qa-overview-3.jpg)\n<small>Example of a *question*, relevant *context*, and an *answer*.</small>\n\nThe other two types of QA rely on *generating* answers rather than *extracting* them. OpenAI's GPT models are well-known generative transformer models.\n\nIn *open-book* abstractive QA, the first IR step is the same as extractive QA; relevant contexts are *retrieved* from an external source. These contexts are passed to the text generation model (such as GPT) and used to *generate* (not extract) an answer.\n\nAlternatively, we can use *closed-book* abstractive QA. Here there is only a text generation model and *no* IR step. The generator model will generate an answer based on its own internal learned representation of the world. It *cannot* refer to any external source of information hence the name *closed-book*.\n\nLet's dive into each of these approaches and learn where we might apply each.\n\n### Extractive QA\n\nExtractive QA is arguably the most widely applicable form of question-answering. It allows us to ask a question and then *extract* an answer from a short text. For example, we have the text (or *context*):\n\n```\nSuper Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\n```\n\nTo which we could ask the question, `\"which team represented the AFC at Super Bowl 50?\"`  and we should expect to return `\"Denver Broncos\"`.\n\nThe example where we present a single *context* and extract an answer is reading comprehension (RC). Alone, this is not particularly useful, but we can couple it with an external data source and search through *many contexts*, not just one. We call this 'open-book extractive QA'. More commonly referred to as just *extractive QA*. It is not a single model but actually consists of *three* components:\n\n* Indexed data (document store/vector database)\n* Retriever model\n* Reader model\n\nBefore beginning to ask questions, open-book QA requires indexing data that our retriever model can later access. Typically this will be chunks of sentence-to-paragraph-sized text.\n\n![retriever_reader_stack](/images/qa-overview-4.jpg)\n<small>The open-book extractive QA stack includes the 'open-book' database, a retriever model, and a reader model.</small>\n\nLet’s work through an example. First, we need data. A popular QA dataset is the Stanford Question and Answering Dataset (SQuAD). We can download this dataset using Hugging Face's `datasets` library like so:\n\n{{< notebook file=\"get-squad\" height=\"full\" >}}\n\nHere we have the *context* feature. It is these contexts that should be indexed in our database.\n\nOptions for the type of database vary based on the retriever model. A traditional retriever uses *sparse vector* retrieval with TF-IDF or BM25. These models return *contexts* based on the frequency of matching words between a *context* and the *question*. More word matches equate to higher relevance. Elasticsearch is the most popular database solution for this thanks to their scalable and strong keyword search capabilities.\n\nThe other option is to use *dense vector* retrieval with sentence vectors built by [transformer models](/learn/transformers/) like [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)). Dense vectors have the advantage of enabling search via *semantics*. Searching with the meaning of a question as described in the *'tasty, freshly baked things'* example. For this, a vector database like [Pinecone](/) or a standalone vector index like [Faiss](/learn/faiss/) is needed.\n\nWe will try the *dense vector* approach. First, we encode our *contexts* with a QA model like `multi-qa-MiniLM-L6-cos-v1` from sentence-transformers. We initialize the model with:\n\n{{< notebook file=\"init-qa-model\" height=\"full\" >}}\n\nUsing the model, we encode the contexts inside our dataset object `qa` to create the sentence vector representations to be indexed in our vector database.\n\n{{< notebook file=\"build-encodings\" height=\"full\" >}}\n\nNow we can go ahead and store these inside a vector database. We will use Pinecone in this example (which does require a [free API key](https://app.pinecone.io)). First, we initialize a connection to Pinecone, create a new index, and connect to it.\n\n{{< notebook file=\"init-pinecone-1\" height=\"full\" >}}\n\nFrom there, all we need to do is `upsert` (*up*load and in*sert*) our vectors to the Pinecone index. We do this in batches where each sample is a tuple of `(id, vector)`.\n\n{{< notebook file=\"pinecone-upsert\" height=\"full\" >}}\n\nOnce the contexts have been indexed inside the database, we can move on to the QA process.\n\nGiven a question/query, the *retriever* creates a sparse/dense vector representation called a *query vector*. This query vector is compared against all of the already indexed *context vectors* in the database. The *n* most similar are returned.\n\n{{< notebook file=\"query\" height=\"full\" >}}\n\nThese most similar contexts are passed (one at a time) to the *reader* model alongside the original question. Given a question and context, the reader predicts the start and end positions of an answer.\n\n![start_end_reader](/images/qa-overview-5.jpg)\n<small>The reader model predicts the start and end positions of an answer given a question and a context containing the answer.</small>\n\nWe will use the `deepest/electra-base-squad2` model from HuggingFace’s `transformers` as our reader model. All we do is set up a `'question-answering'` pipeline and pass our *query* and *contexts* to it one by one.\n\n{{< notebook file=\"reader\" height=\"full\" >}}\n\nThe reader prediction is repeated for each context. From here — if preferred — we can order the 'answers' using the scores output by the retriever and/or reader models.\n\nAs we can see, the model returns the correct answer of `'Denver Broncos'` with a score of 0.99. Most other answers return only minuscule scores, showing that our reader model easily distinguishes between good and bad answers.\n\n### Abstractive QA\n\nAs we saw before, abstractive QA can be split into two types: open-book and *closed*-book. We will start with *open-book* as the natural continuation of the previous extractive QA pipeline.\n\n#### Open Book\n\nBeing **open-book** *abstractive* QA, we can use the same database and retriever components used for *extractive QA*. These components work in the same way and deliver a set of *contexts* to our *generator* model, which replaces the *reader* from extractive QA.\n\n![retriever_reader_stack](/images/qa-overview-6.jpg)\n<small>Open-book abstractive QA pipeline, note that the *reader* model has been replaced with a *generator* model (highlighted) when compared to the extractive QA stack.</small>\n\nRather than *extracting* answers, contexts are used as input (alongside the question) to a generative sequence-to-sequence (seq2seq) model. The model uses the question and context to *generate* an answer.\n\nLarge transformer models store 'representations' of knowledge in their parameters. By passing relevant contexts and questions into the model, we *hope* that the model will use the context alongside its 'stored knowledge' to answer more *abstract* questions.\n\nThe seq2seq model used is commonly BART or T5-based. We will go ahead and initialize a seq2seq pipeline using a BART model fine-tuned for abstractive QA — `yjernite/bart_eli5`.\n\n{{< notebook file=\"bart-eli5-init\" height=\"full\" >}}\n\nThe question we asked before is specific. We're looking for a short and concise answer of `Denver Broncos`. Abstractive QA is not ideal for these types of questions:\n\n{{< notebook file=\"bert-eli5-out-0\" height=\"full\" >}}\n\nInstead, the benefit of abstractive QA comes with more 'abstract' questions like `\"Do NFL teams only care about playing at the Super Bowl?\"` Here, we're almost asking for an opinion. There is unlikely to be an *exact* answer. Let's see what the abstractive QA method thinks about this.\n\n{{< notebook file=\"abstract-question\" height=\"full\" >}}\n\nThese answers look *much* better than our 'specific' question. The returned contexts don't include direct information about whether the teams care about being in the Super Bowl. Instead, they contain snippets of concrete NFL/Super Bowl details.\n\nThe seq2seq model combines those details and its own internal 'knowledge' to produce some insightful thoughts on the question:\n\n* *\"No, because it is the pinnacle of professional football\"* — points out that teams in the Super Bowl (whether they win or not) already know they're at the top; in a way, they've *'already won'*.\n* *\"They don't care if they lose, they just care if they get a nice, big crowd to cheer\"* — players are happy that they get to entertain their fans; that is, the Super Bowl is less important.\n* *\"They are paid a lot of money to be in the Superbowl\"* — points out the more obvious 'who *wouldn't* want bucket loads of money?'.\n\nThere is plenty of contradiction and opinion, but that is often the case with more abstract questioning, particularly with the question we asked.\n\nAlthough these results are interesting, they're not perfect. We can tweak parameters such as `temperature` to increase/decrease randomness in the answers, but abstractive QA can be limited in its coherence.\n\n#### Closed Book\n\nThe final architecture we will look at is *closed-book* abstractive QA. In reality, this is nothing more than a generative model that takes a question and relies on *nothing more* than its own internal knowledge. There is **no** retrieval step.\n\n![retriever_reader_stack](/images/qa-overview-7.jpg)\n<small>The closed-book architecture is much simpler, there is nothing more than a *generator* model.</small>\n\nAlthough we're dropping the retriever model, that doesn't mean we stick with the same reader model. As we saw before, the `yjernite/bart_eli5` model requires input like:\n\n```\nquestion: <our question> context: <a (hopefully) relevant context>\n```\n\nWithout the context input, the previous model does not perform as well. This is to be expected. The seq2seq model is optimized to produce coherent answers when given both question *and* context. If our input is in a new, unexpected format, performance suffers:\n\n{{< notebook file=\"bart-eli5-no-context\" height=\"full\" >}}\n\nThe model doesn't know the answer and flips the direction of questioning. Unfortunately, this isn't really what we want. However, there are many alternative models we can try. The GPT models from OpenAI are well-known examples of generative transformers and can produce good results.\n\nGPT-3, the most recent GPT from OpenAI, is locked behind an API, but there are open-source alternatives like GPT-Neo from Eleuther AI. Let's try one of the smaller GPT-Neo models.\n\n{{< notebook file=\"gpt-neo-0\" height=\"full\" >}}\n\nHere we're using the `'text-generation'` pipeline. All we do here is generate text following a question. We do get an interesting answer which is true but doesn't necessarily answer the question. We can try a few more questions.\n\n{{< notebook file=\"gpt-neo-answers\" height=\"full\" >}}\n\nWe can tweak parameters to reduce the likelihood of repetition.\n\n{{< notebook file=\"with-do-sample\" height=\"full\" >}}\n\nWe do get some interesting results, although it is clear that *closed-book* abstractive QA is a challenging task. Larger models store more internal knowledge; thus, closed-book performance is very much tied to model size. With bigger models, we can get better results, but for consistent answers, the open-book alternatives tend to outperform the closed-book approach.\n\n\n\nThat's it for our article on open-domain question answering (ODQA). We've worked through the idea behind semantic similarity and how it is applied to QA models.  We explored the various components that produce these 'QA stacks', like [vector databases](/learn/vector-database/), retrievers, readers, and generators. Alongside that, we've learned how to implement these different stacks using different tools and models. All of this should provide a strong foundation for exploring the world and opportunities of ODQA further.\n\n\n\n## Further Reading\n\n* L. Weng, [How to Build an Open-Domain Question Answering System?](https://lilianweng.github.io/lil-log/2020/10/29/open-domain-question-answering.html), GitHub Blog\n\n* D. Khashabi, et al., [UnifiedQA: Crossing Format Boundaries with a Single QA System](https://arxiv.org/pdf/2005.00700.pdf) (2020), EMNLP\n\n* [Extractive Question Answering](https://huggingface.co/transformers/usage.html#extractive-question-answering), Hugging Face Docs\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbc8"
  },
  "filename": "classifier-train-vector-search.md",
  "title": "post",
  "category": "\"Optimize Classifier Training with Vector Search\"",
  "content": "---\nlayout: post\ntitle: \"Optimize Classifier Training with Vector Search\"\nheadline: \"Optimize Classifier Training with Vector Search\"\ncategories:\n  - Applications of Vector Search\ntoc: >-\nweight: 1\nauthors:\n  - name: James Briggs\n    position: Developer Advocate\n    src: /images/james-briggs.jpeg\n    href: \"https://www.youtube.com/c/jamesbriggs\"\n  - name: Edo Liberty\n    position: Founder and CEO\n    src: /images/company-edo.png\n    href: https://edoliberty.github.io/\n  \ndescription: How to supercharge fine-tuning for classification with vector search\n# Open graph\nimages: ['https://www.pinecone.io/images/classifier-train-vector-search-0.png']\n---\n\nPretrained models dominate the world of machine learning. Very few ML projects begin by training a new model from scratch. Instead, people often start by taking an off-the-shelf model like Resnet or BERT and fine-tuning it for another domain, or using an existing in-house model for the same purpose.\n\nThe ecosystem of pretrained models, both external and in-house, has allowed us to push the limits of what is possible. This doesn't mean, however, that there are no challenges.\n\nFortunately, we can tackle some of these problems across many different pretrained models, because they often share similar points of failure. One of those is the excessive compute and data needed to fine-tune a pretrained model for classification.\n\nA common scenario will have some model containing a linear layer that outputs a classification. Preceding this linear layer, we can have anything from a small neural network to a billion-parameter language model. In either case, it’s the classification layer producing the final prediction.\n\nThat means  we can almost ignore the preceeding model layers, and focus on the classification layer alone. This classification layer can become a single point of failure (or success) for accurate predictions.\n\nThe classification layer alone can be fine-tuned, and it often is. A common approach for fine-tuning this layer may look like this:\n\n1. Collect a dataset that focuses on enabling the model to adapt to a new domain or handle data drift,\n2. Slog through this dataset, labeling records as per their classification, and\n3. Once the records have all been labeled, fine-tune the classifier.\n\nThis approach works, but it isn't efficient. There is a better way...\n\nWe need to focus fine-tuning efforts on *essential samples* that would have the greatest impact on the performance of the classifier. Otherwise, we waste time and compute by annotating and fine-tuning on samples that make little-to-no difference to model performance.\n\nThe question becomes: How do you determine which samples are essential? That’s where vector search comes in. You can use vector search to identify and focus on the essential records that *really* make a difference in model performance. This will save valuable time and compute by skipping all non-essential records when fine-tuning the model.\n\n---\n\n*All code covering the content of this article can be [found here](https://github.com/pinecone-io/examples/tree/master/analytics-and-ml/model-training/classifier-train-vector-search).*\n\n---\n\n## Training with Vector Search\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/pfwBut7E60Q\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\nVector search will play a key role in optimizing our training steps. First, let's understand where vector search fits into all of this.\n\nMany state-of-the-art (SOTA) models are available for use as *pretrained models*. That includes models like Google's BERT and T5, and OpenAI's CLIP. These models use millions, even billions, of parameters and perform many complex operations. Yet, when applied to classification, these models rely on simple linear or feedforward network layers to make the final prediction.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n <source src=\"./images/classifier-train-vector-search-1.mp4\" type=\"video/mp4\">\n</video>\n\n\nThe reason for this is that these models are *not* trained to make class predictions; they're trained to make [vector embeddings](https://www.pinecone.io/learn/vector-embeddings/).\n\nVectors created by these models are full of helpful information that belong to a learned structure in a high-dimensional vector space. That helpful information is abstracted beyond human comprehension, but the effect is that similar items are located close to one another in vector space, whereas dissimilar items are not.\n\nThe result is that each of these models creates a \"map\" of information. Using this map, they can consume data, like images and text, and output a meaningful vector *representation* of said data.\n\n![vector-maps](./images/classifier-train-vector-search-2.png)\n\nIn these maps, we will find that sentences, images, or whatever form of data you're working with belongs to a specific region based on the data's characteristics.\n\nPretrained models are very good at producing accurate maps of information. Because of that, all we need to translate these into accurate class predictions is a simple layer that learns to *identify the different regions* in this map.\n\n### Linear Classifiers\n\nA typical architecture for classification consists of a pretrained model followed by a linear layer. A binary linear classifier (that predicts one of two labels) works by taking the dot product between an input vector $X$ and its own internal weights $W$. Based on a threshold, the output of this operation will be categorized as one of two classes.\n\n![w-dot-x](./images/classifier-train-vector-search-8.png)\n\n\nThe dot product of two vectors returns a *positive* score if they share a similar direction, $0$ if they are orthogonal, and a *negative* score if they have opposite directions.\n\nThere is one key problem with dot product similarity, it considers *both* direction and *magnitude*. Magnitude is troublesome because vectors with greater magnitudes often overpower more similar, lower-magnitude vectors. To avoid this, we normalize the vectors being output by our pretrained models.\n\nThe result is that a linear classifier must learn to align its internal weights $W$ with the vectors $X$ labeled as $+1$ and push its internal weights away from vectors labeled as $-1$.\n\n![vector-maps-learn-binary](./images/classifier-train-vector-search-9.png)\n\nFine-tuning the classifier like this works, but there are some unecessary limitations. First, imagine we return *only* irrelevant samples for a training batch. They will all be marked as $-1$. The classifier knows to move away from these values but it cannot know which direction to move towards. In high-dimensional spaces, this is problematic and will cause the classifier to move at random.\n\nSecond, many training samples may be more or less relevant. \"A dog\" is more relevant than \"a truck\" to the query \"dogs in the snow\", yet, \"a dog in the snow\" is *not* equally relevant as \"a dog\".\n\n![vector-maps-learn-contrastive](./images/classifier-train-vector-search-10.png)\n\nWhat we need is a *gradient* of relevance, a continuous range from -1 to +1. The first problem is solved as the *range* of scores gives the classifier information on the best direction of movement. And the second problem is solved as we can now be more precise with our relevance scores.\n\nAll of this allows a linear classifier to learn where to place itself within the vector space produced by the model layers preceding it.\n\nThat describes the fine-tuning process, but we cannot do this across our entire dataset. It would take too much time annotating everything. To do this efficiently, we must capitalize on the idea of identifying relevant vs. irrelevant vectors within proximity of the model’s learned weights.\n\n![with-without-efficient-samples](./images/classifier-train-vector-search-11.png)\n\nBy identifying the vectors with the highest proximity to the classifier's learned boundaries, we are able to skip irrelevant samples that make little-to-no impact on the classifier performance. Instead, we hone-in on the critical area of vectors near the target vector space.\n\n### Training Efficiently with Vector Search\n\nDuring training, we need to feed vectors generated by the preceding layers into our linear classifier. Those vectors also need to be labeled. But, if our classifier is already tuned to understand the vector space generated by the previous layers, most training data is unlikely to be helpful.\n\nWe need to focus our fine-tuning efforts on records that are similar enough to our target class to confuse our model. For an already trained classifier, these are the false positives and false negatives predicted by the classifier.\n\nHowever, we don’t usually have a list of false positives and false negatives. But we do know that the solvable errors will be present near the classifiers decision boundary; the line that separates the positive predictions from negative predicitons.\n\nDue to the proximity of these samples, it is harder for the classifier to find the exact boundary that best identifies true positives vs. true negatives.\n\nVector search allows us to retrieve the high proximity samples most similar to the model weights $W$. We can then label the returned samples and use them for training our model. The model optimizes its internal weights; we extract them again, search, and repeat.\n\n![training-process](./images/classifier-train-vector-search-12.png)\n\nWe focus annotation and training on essential samples by retrieving the most similar vectors. Doing this avoids wasting time and compute on samples that make little to no difference to our model performance.\n\n---\n\n## Putting it All Together\n\nNow let's combine all this to fine-tune a linear classifier with vector search.\n\nThere are two parts to our training process:\n\n1. **Indexing** our data: Here we must embed everything as vectors using the \"preceding\" model layers (BERT, ResNet, CLIP, etc.).\n2. **Fine-tuning** the classifier: We will query using model weights $W$, return the most similar (or high scoring) records, annotate, and fine-tune the model.\n\n*If you already have an indexed dataset, you can skip ahead to the **Fine-tuning** section. If not, we'll work through the indexing steps next.*\n\n### Indexing\n\nGiven a dataset of images (or other formats), we first need to process everything through the preceding model layers to generate a list of vectors to be indexed. These vectors will later be used as the training data for the model.\n\n---\n\n*The terms vectors, embeddings, and vector embeddings will be used interchangeably. When specifying embeddings produced by a specific medium (such as images or text), we will refer to them as \"image embeddings\" or \"text embeddings\".*\n\n---\n\nFor our example, we will use a model capable of comparing *both* text and images called CLIP. OpenAI's CLIP has been trained to match similar natural language prompts to images. It does this by encoding pairs as closely as possible in a vector space. \n\n#### Initialization of Dataset and CLIP\n\nWe need an image dataset and CLIP (swap these for your dataset and model where relevant). We will use the `frgfm/imagenette` dataset found on Hugging Face datasets.\n\n{{< notebook file=\"query-train-dataset\" height=\"full\" >}}\n\nIn the *\"image\"* feature of the dataset, we have ~9.4K images of various sizes stored as PIL objects. Inside a Jupyter notebook, we can view them like so:\n\n{{< notebook file=\"query-train-show-image\" height=\"full\" >}}\n\nWe embed these images using CLIP, which we initialize through the HuggingFace *Transformers* library.\n\n```python\n# !pip install transformers torch\nfrom transformers import CLIPProcessor, CLIPModel\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel_id = \"openai/clip-vit-base-patch32\"\nmodel = CLIPModel.from_pretrained(model_id).to(device)\nprocessor = CLIPProcessor.from_pretrained(model_id)\n```\n\nWe can embed an image and transform it into a *flat* Python list (ready for indexing) like so:\n\n{{< notebook file=\"query-train-embed-image\" height=\"full\" >}}\n\n#### Normalization is Important\n\nThe later linear classifier uses dot product to calculate predictions. That means we must also use dot product to measure the similarity between image embeddings during the vector search. Given two similar images of dogs and an image of a radio, we would expect the two dog images to return a higher score.\n\n![no-norm-image-embeds](./images/classifier-train-vector-search-3.png)\n\n<small>We would expect two nearby embeddings like **a** and **b** to return a higher similarity score than with **c**. Yet, when we calculate the dot product between these embeddings, the magnitude of **c** outputs a higher output.</small>\n\n{{< notebook file=\"query-train-dot-product\" height=\"full\" >}}\n\nDot product is heavily influenced by vector magnitude. This means two very similar vectors with low magnitude can score lower than if they were compared to a dissimilar vector with greater magnitude.\n\nWe solve this problem by normalizing all of our vectors beforehand. By doing this, we \"flatten\" the magnitude across vectors, leaving just the angular difference between them.\n\n![with-norm-image-embeds](./images/classifier-train-vector-search-4.png)\n\n<small>Normalization \"flattens\" the magnitude of our vectors.</small>\n\n{{< notebook file=\"query-train-dot-product-norm\" height=\"full\" >}}\n\nAfter normalization of our embedding with `emb = emb / np.linalg.norm(emb)`, we can move on to indexing it in our vector database.\n\n#### Vector Database and Indexing\n\nHere we will use the [Pinecone vector database](https://www.pinecone.io/). All we need is a *free* API key and `environment` variable that can be [found here](https://app.pinecone.io/). To install the Pinecone Python client, we use `pip install pinecone-client`. Finally, we import and initialize the connection.\n\n```python\nimport pinecone\n\npinecone.init(api_key=\"YOUR_API_KEY\", environment=\"YOUR_ENV\")\n# (default env is 'us-east1-gcp')\n```\n\nAfter connecting to Pinecone, we create a new index where we will store our vectors.\n\n```python\nindex_name = \"imagenet-query-trainer-clip\"\n\npinecone.create_index(\n    index_name,\n    dimension=emb.shape[0],\n    metric=\"dotproduct\",\n    metadata_config={\"indexed\": [\"seen\"]}\n)\n# connect to the index\nindex = pinecone.Index(index_name)\n```\n\nWe specify four parameters for our index:\n\n* `index_name`: The name of our vector index, it can be anything.\n* `dimensions`: The dimensionality of our vector embeddings. This must match the vector dimensionality output by CLIP. All future vectors must have the same dimensionality. Our vectors have `768` dimensions.\n* `metric`: This is the similarity metric we will use. Pinecone accepts `\"euclidean\"`, `\"cosine\"`, and `\"dotproduct\"`. As discussed, we will be using `\"dotproduct\"`.\n* `metadata_config`: Pinecone has both *indexed* and non-indexed metadata. Indexed metadata can be used in [metadata filtering](https://www.pinecone.io/learn/vector-search-filtering/), and we need this for *\" exploring \"* the image dataset. So, we index a single field called `\"seen\"`.\n\nWith this, we have indexed a single vector (`emb`) in our Pinecone index. We can check this by running `index.describe_index_stats()` which will return:\n\n```json\n{'dimension': 512,\n'index_fullness': 0.0,\n'namespaces': {'': {'vector_count': 1}},\n'totalVectorCount': 1.0}\n```\n\nThose are all the steps we need to embed and index an image. Let's apply these steps to the remainder of the dataset.\n\n### Index Everything\n\nThere's little we can do with a single vector, so we will repeat the previous steps on the rest of our dataset. We place the previous logic into a loop, iterate once over the dataset, and we're done.\n\n```python\nfrom tqdm.auto import tqdm\nbatch_size = 64\n\nfor i in tqdm(range(0, len(imagenet), batch_size)):\n    # select the batch start and end\n    i_end = min(i + batch_size, len(imagenet))\n    # some images are grayscale (mode=='L') we only keep 'RGB' images\n    images = [img for img in imagenet[i:i_end]['image'] if img.mode == 'RGB']\n    # process images and extract pytorch tensor pixel values\n    image = processor(\n        text=None,\n        images=images,\n        return_tensors='pt',\n        padding=True\n    )['pixel_values'].to(device)\n    # feed tensors to model and extract image features\n    out = model.get_image_features(pixel_values=image)\n    out = out.squeeze(0)\n    # take the mean across each dimension to create a single vector embedding\n    embeds = out.cpu().detach().numpy()\n    # normalize and convert to list\n    embeds = embeds / np.linalg.norm(embeds, axis=0)\n    embeds = embeds.tolist()\n    # create ID values\n    ids = [str(i) for i in range(i, i_end)]\n    # prep metadata\n    meta = [{'seen': 0} for image in images]\n    # zip all data together and upsert\n    to_upsert = zip(ids, embeds, meta)\n    index.upsert(to_upsert)\n```\n\nThere's a lot of code here, but it's nothing more than a compact version of the previous steps. We can check the number of records added using the `describe_index_stats` method.\n\n```json\n{'dimension': 512,\n'index_fullness': 0.0,\n'namespaces': {'': {'vector_count': 9296}},\n'totalVectorCount': 9296.0}\n```\n\nWe have slightly fewer records here because we drop grayscale images in the upsert loop (line 8).\n\n## Fine-Tuning\n\nWith everything indexed, we're ready to take our classifier model and optimize it on the most relevant samples in our dataset. You can follow along live using [this Colab notebook](https://colab.research.google.com/github/pinecone-io/examples/blob/master/analytics-and-ml/model-training/classifier-train-vector-search/01-fine-tune-vector-search-contrastive.ipynb).\n\nYou may or may not have a classifier already trained. If you **do** have a classifier, you can skip ahead a few paragraphs to the **Classifier** section.\n\nIf you **do not** have a classifier, we can begin by setting the model weights $W$ equal to the vector produced by a relevant query. This is where the *text-to-image* capabilities of CLIP come into use. Given a natural language prompt like *\"dogs in the snow\"*, we can use CLIP to embed this into the same vector space as our image embeddings.\n\n{{< notebook file=\"query-train-text-prompt\" height=\"full\" >}}\n\nWe will set our initial model weights equal to `xq`, but first, let's retrieve the first batch of training samples.\n\nAs with the image embeddings, we need to transform the CLIP output into a flat list for querying with Pinecone and retrieving the image `idx` and vector `values`:\n\n```python\nxc = index.query(xq, top_k=10, include_values=True)\n\n# get the index values\nidx = [int(match['id']) for match in xc['matches']]\n# get the vectors\nvalues = [match['values'] for match in xc['matches']]\n```\n\n![dogs-in-snow-results](./images/classifier-train-vector-search-5.png)\n\n<small>The \"dogs in the snow\" query is mostly accurate, with the two exceptions showing dogs on non-snow yet white backgrounds.</small>\n\nThese images and their embeddings act as the training data for our classifier. The embeddings themselves will become the inputs `X`. We allow the user to create the labels ` y` by entering a score from `-1` to `+1`. All of this will be performed by a function called `score_images`, the code for this can be [found here](https://colab.research.google.com/github/pinecone-io/examples/blob/master/analytics-and-ml/model-training/classifier-train-vector-search/01-fine-tune-vector-search-contrastive.ipynb).\n\n{{< notebook file=\"query-train-score\" height=\"full\" >}}\n\nAbove we can see the images followed by a printout of their ID values and the scores assigned to them, all of these pairs are stored in `scores` as a dictionary. These scores are our training data; all that is left is to train our model with it. So, we initialize the classifier.\n\n### Classifier\n\nHere, we will use a simple linear binary classifier in PyTorch. The model weights will act as our future query vectors. As the model learns to distinguish between relevant and irrelevant vectors, it will optimize its internal weights to produce a vector more like the vectors we marked with the label `1` (relevant).\n\n```python\nimport torch\n\n# initialize the model with 512 input size (equal to vector size) and one output\nmodel = torch.nn.Linear(512, 1)\n\n# convert initial query `xq` to tensor paramater for initial model weights\ninit_weight = torch.Tensor(xq).reshape(1, -1)\nmodel.weight = torch.nn.Parameter(init_weight)\n\n# init loss and optimizer\nloss = torch.nn.BCEWithLogitsLoss()\n# we set the lr high for these examples, in real-world use case this\n# may need to be lower for more stable fine-tuning\noptimizer = torch.optim.SGD(model.parameters(), lr=0.2)\n```\n\nOn lines 7-8, we set the model weights to the initial query we made. If you **already have a classifier**, this part is not necessary. By initializing the model weights like this, we start in a more relevant vector space, from which we can begin fine-tuning the model and optimizing our query.\n\nWe will write a small PyTorch training loop and place it in a function called `fit`. The number of iterations `iters` can be set to move slower/faster through the vector space for each training batch.\n\n```python\nmodel.train()  # switch model to training mode\n\ndef fit(X: list, y: list, iters: int = 5):\n    for _ in range(iters):\n        # get predictions\n        out = model(torch.Tensor(X))\n        # calculate loss\n        loss_value = loss(out, torch.Tensor(y).reshape(-1, 1))\n        # reset gradients\n        optimizer.zero_grad()\n        # backpropagate\n        loss_value.backward()\n        # update weights\n        optimizer.step()\n        \n# train\nfit(X, y)\n```\n\nAfter we've run the training loop, we can extract the new model weights to use as our next query vector, `xq`.\n\n```python\nxq = model.weight.detach().numpy()[0].tolist()\n```\n\nWe can return many of the same items during training if querying with this slightly fine-tuned `xq` vector. Increasing `lr` and `iters` to shift the fine-tuned `xq` values more quickly might avoid this, but it will struggle to converge on an optimal query. On the other hand, decreasing `lr` and `iters` will mean we keep seeing the same set of images and will overfit them.\n\nWe want the classifier to see a broader range of both positive and negative images without needing excessive values for `lr` and `iters`. Instead, we keep these two parameters low and filter out all previously seen images.\n\n---\n\n*These examples use excessively high `lr` and `iters` parameters to demonstrate the movement across vector space. We recommended using lower values to provide a more stable training process.*\n\n---\n\nFiltering is done via Pinecone's metadata filtering. Earlier we initialized the index with `metadata_config={\"indexed\": [\"seen\"]}` and added `{\"seen\": 0}` to the metadata of every record. All of that was for this next step. We set their metadata for the previous 10 retrieved records to `{\"seen\": 1}`.\n\n```python\n# we must update one record at a time\nfor i in idx:\n    index.update(str(i), set_metadata={\"seen\": 1})\n```\n\nWhen we query again, we can add a filter condition `filter={\"seen\": 0}` to return only *unseen* records.\n\n```python\n# retrieve most similar records\nxc = index.query(\n    xq,\n    top_k=10,\n    include_values=True,\n    filter={\"seen\": 0}\n)\n# extract index and vector values\nidx = [int(match['id']) for match in xc['matches']]\nvalues = [match['values'] for match in xc['matches']]\n```\n\nStarting with dogs in the snow, let's imagine we'd like to adjust our already well-trained \"dogs in snow\" classifier to become a \"dogs at dog shows\" classifier. How do we influence the model to retrieve images from this new domain?\n\n<video autoplay loop muted playsinline class=\"responsive\">\n <source src=\"./images/classifier-train-vector-search-6.mp4\" type=\"video/mp4\">\n</video>\n<small>Traversing across clusters of similar images in the [semantic query trainer app](https://huggingface.co/spaces/pinecone/semantic-query-trainer), this example uses irrelevant/relevant labels. The app has since been updated to use contrastive sliders to score images.</small>\n\nThe example above starts with our slightly fine-tuned \"dogs in the snow\" embedding in both windows. We then change what is marked as relevant. The left window shows us traversing from dogs in the snow to garbage trucks and back to dogs. In the right window, we traverse to dogs in fields and finally to dog shows.\n\nWe can replicate this process by repeating the logic we have already worked through. You can find an example that wraps this code into a few [training/retrieval functions here](https://colab.research.google.com/github/pinecone-io/examples/blob/master/analytics-and-ml/model-training/classifier-train-vector-search/01-fine-tune-vector-search-contrastive.ipynb).\n\nAs we keep doing this, the number of retrieved dog images will quickly increase until they dominate the returned images, or we simply exhaust all relevant images. At this point, we can stop and test our newly trained query vector on the unfiltered dataset. We can do this in one of two ways:\n\n1. We drop the `filter` argument in `query`. This is ideal if performing a quick test but will not work if planning to perform a second loop through the dataset or train another query.\n2. We reset the filter values, switching all records with `{\"seen\": 1}` back to `{\"seen\": 0}`.\n\nTo apply method *2*, we iteratively query the index with a filter of `{\"seen\": 1}`, resetting the metadata, and stop *only* when we return no more records.\n\n```python\nwhile True:\n    xc = index.query(xq, top_k=100, filter={\"seen\": 1})\n    idx = [match['id'] for match in xc['matches']]\n    if len(idx) == 0: break\n    for i in idx:\n        index.update(str(i), set_metadata={\"seen\": 0})\n```\n\nWhen we search again, we will return a completely unfiltered view of the search results.\n\n```python\nxc = index.query(\n    xq,\n    top_k=10\n)\n# extract index and vector values\nidx = [int(match['id']) for match in xc['matches']]\n\n# show the results\nfor i in idx:\n    print(i)\n    plt.imshow(imagenet[i]['image'])\n    plt.show()\n```\n\n![dog-show-results](./images/classifier-train-vector-search-7.png)\n\n<small>Images retrieved after fine-tuning the query for retrieving dog shows.</small>\n\nOur query has clearly been optimized for finding images of dog shows. We can go ahead and save our classifier model.\n\n```python\nwith open(\"classifier.pth\", \"wb\") as f:\n    torch.save(model, f)\n```\n\nIn the next section, we'll look at classifying images using our model fine-tuned with vector search.\n\n## Classifier Predictions\n\nWe know how to optimize our query and hone in on specific concepts and clusters of images. With this, our classifier has hopefully become great at identifying images of dog shows. Its internal weights $W$ should have aligned to the vectors $X$ that best represent the concept of \"dog shows\".\n\nThere is just one more step. How do we make and interpret predictions with our new classifier? We start by loading the classifier from file (you can skip the save/load if preferred and use the same instance).\n\n```python\nwith open(\"classifier.pth\", \"rb\") as f:\n    clf = torch.load(f)\n```\n\nWe will test the predictions on the *validation* split of the imagenette dataset. To download this, we run the same `load_dataset` function as before but change the `split` parameter to `validation`.\n\n```python\nimagenet = load_dataset(\n    'frgfm/imagenette',\n    'full_size',\n    split='validation',  # here we switch to validation set\n    ignore_verifications=False  # set to True if seeing splits Error\n)\n```\n\nLet's start with a dog show image and see what model outputs. As before, we will process and create the image embedding using CLIP.\n\n{{< notebook file=\"query-train-make-pred-1\" height=\"full\" >}}\n\nThe prediction is positive, meaning the model predicts an image of a dog show! Let's try another.\n\n{{< notebook file=\"query-train-make-pred-2\" height=\"full\" >}}\n\nA negative value means the model predicts this is *not* a dog show. We can use this same logic to make predictions for the complete validation set and look at what the model predicts as dog shows.\n\n```python\nfrom tqdm.auto import tqdm\n\nbatch_size = 64\n\npreds = []\n\nfor i in tqdm(range(0, len(imagenet), batch_size)):\n    i_end = min(i+batch_size, len(imagenet))\n    image = processor(\n        text=None,\n        images=imagenet[i:i_end]['image'],\n        return_tensors='pt',\n        padding=True\n    )['pixel_values'].to(device)\n    out = clip.get_image_features(pixel_values=image)\n    logits = clf(out)\n    preds.extend(logits.detach().cpu().numpy().reshape(1, -1)[0].tolist())\n```\n\nWe add these predictions to our dataset, filter out any results where the prediction is negative, and then sort the results.\n\n{{< notebook file=\"query-train-get-preds\" height=\"full\" >}}\n\nThese look like great results. There are *23* results in total, and all but two of them are images of dog shows (find the [complete set of results here](https://colab.research.google.com/github/pinecone-io/examples/blob/master/analytics-and-ml/model-training/classifier-train-vector-search/02-classifier-test.ipynb).\n\n---\n\nThat is how we can optimize fine-tuning for linear classification layers with vector search. With this, we can hone in on what is important for our classifier and focus on these critical samples rather than slogging through the entire dataset and fine-tuning the model at random.\n\nDoing this for an image classifier is just one example. We can apply this to various use cases, from anomaly detection to recommendation engines. The pool of use cases involving vector search is growing daily.\n\n---\n\n## Resources\n\n[Semantic query trainer demo](https://huggingface.co/spaces/pinecone/semantic-query-trainer)\n\n[GitHub Notebooks](https://github.com/pinecone-io/examples/tree/master/analytics-and-ml/model-training/classifier-train-vector-search)\n\n[Colab 00: CLIP Indexing](https://colab.research.google.com/github/pinecone-io/examples/blob/master/analytics-and-ml/model-training/classifier-train-vector-search/00-indexer-clip.ipynb)\n\n[Colab 01: Fine-tuning](https://colab.research.google.com/github/pinecone-io/examples/blob/master/analytics-and-ml/model-training/classifier-train-vector-search/01-fine-tune-vector-search-contrastive.ipynb)\n\n[Colab 02: Classifier Testing](https://colab.research.google.com/github/pinecone-io/examples/blob/master/analytics-and-ml/model-training/classifier-train-vector-search/02-classifier-test.ipynb)"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbc9"
  },
  "filename": "bag-of-visual-words.md",
  "title": "ebook-post",
  "category": "\"Bag of Visual Words\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Bag of Visual Words\"\nheadline: \"Bag of Visual Words\"\ncategories:\n  - Embedding Methods for Image Search\ntoc: >-\nweight: 2\nauthors:\n  - name: Laura Carnevali\n    position: Developer\n    src: /images/laura-carnevali.jpeg\n    href: \"https://www.linkedin.com/in/laura-carnevali-071a01b7/\"\n  - name: James Briggs\n    position: Developer Advocate\n    src: /images/james-briggs.jpeg\n    href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: A look at one of the best pre-DL embedding methods for classification and retrieval\n# Open graph\nimages: ['https://www.pinecone.io/images/bag-of-visual-words-0.png']\n---\n\nIn computer vision, bag of visual words (BoVW) is one of the pre-deep learning methods used for building image embeddings. We can use BoVW for content-based image retrieval, object detection, and image classification.\n\nAt a high level, comparing images with the bag of visual words approach consists of five steps:\n\n1. Extract visual features,\n2. Create *visual words*,\n3. Build sparse frequency vectors with these visual words,\n4. Adjust frequency vectors for relevant with tf-idf,\n5. Compare vectors with similarity or distance metrics.\n\nWe will start by walking through the theory of how all of this works. In the second half of this article we will look at how to implement all of this in Python.\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/jjQetJtQDS4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n## How Bag of Visual Words Works\n\n### Visual Features\n\nThe model derives from **bag of words** in natural language processing (NLP), where a chunk of text is split into words or sub-words and those components are collated into an unordered list, the so-called \"bag of words\" (BoW).\n\n<center><div> <img src=\"/images/bag-of-visual-words-1.png\" alt=\"Drawing\" style=\"width:70%;\"/></div> </center>\n\nSimilarly, in bag of *visual* words the images are represented by patches, and their unique patterns (or *visual features*) are extracted from the image.\n\n<center><div> <img src=\"/images/bag-of-visual-words-2.png\" alt=\"Drawing\" style=\"width:80%;\"/></div> </center>\n\nHowever, despite the similarity, these visual features are *not* visual words just yet; we must perform a few more steps. For now, let's focus on understanding what these visual features are.\n\nVisual features consist of two items:\n\n* **Keypoints** are points in an image, which do **not** change if the image is rotated, expanded, or scaled and\n\n* **Descriptors** are vector representations of an image patch found at a given keypoint.\n\nThese visual features can be detected and extracted using a feature detector, such as **SIFT (Scale Invariant Feature Transform)**, **ORB (Oriented FAST and Rotated BRIEF)**, or **SURF (Speeded Up Robust Features)**.\n\nThe most common is **SIFT** as it is invariant to scale, rotation, translation, illumination, and blur. SIFT converts each image patch into a $128$-dimensional vector (i.e., the **descriptor** of the visual feature).\n\nA single image will be represented by many SIFT vectors. The order of these vectors is not important, only their presence within an image.\n\n### Codebooks and Visual Words\n\nAfter extracting visual features we build a **codebook**, also called a dictionary or vocabulary. This codebook acts as a repository of all existing **visual words** (similar to an **actual** dictionary, like the Oxford English Dictionary).\n\nWe use this codebook as a way to translate a potentially infinite variation of visual features into a predefined set of visual words.\n\nHow? The idea is to group similar visual features into clusters. Each cluster is assigned a central point which represents the visual word translation (or mapping) for that group of visual features. The standard approach for grouping visual features into visual words is [**k-means clustering**](https://www.pinecone.io/learn/k-means-clustering/).\n\nK-means divides the data into $k$ clusters, where $k$ is chosen by us. Once the data is grouped, k-means calculates the mean for each cluster, i.e., a central point between all of the vectors in a group. That central point is a **centroid** (i.e., a **visual word**).\n\n<center>\n <div>\n   <img src=\"/images/bag-of-visual-words-3.png\" alt=\"Drawing\" style=\"width:90%;\"/>\n </div>\n</center>\n\nAfter finding the centroids, k-means iterates through each data point (visual feature) and checks which centroid (visual word) is nearest. If the nearest centroid has changed, the data point switches grouping, being assigned to the new nearest centroid.\n\nThis process is repeated over a given number of iterations or until the centroid positions have stabilized.\n\nWith that in mind, how do we choose the number of centroids, $k$?\n\nIt is more of an art than a science, but there are a few things to consider. Primarily, how many visual words can cover the various **relevant** visual features in the dataset.\n\nThat's not an easy thing to figure out, and it's always going to require some guesswork. However, we can think of it using the language equivalent, **bag of words**.\n\nIf our language dataset covered several documents about a specific topic in a single language, we would find fewer unique words than if we had thousands of documents, spanning several languages about a range of topics.\n\nThe same is true for images; dogs and/or animals could be a topic, and buildings could be another topic. As for the equivalent of different languages, this is not a perfect metaphor but we could think of different photography styles, drawings, or cartoons. All of these added layers of complexity increase the number of visual words needed to accurately represent the dataset.\n\nHere, we could start with choosing a smaller $k$ value (e.g., $100$ or $150$) and re-run the code multiple times changing $k$ until convergence and/or our model seems to be identifying images well.\n\nIf we choose $k=150$, k-means will generate $150$ centroids and, therefore, $150$ visual words.\n\nWhen we perform the mapping from new visual feature vectors to the nearest centroid (i.e., visual word), we categorize visual features into a more limited set of visual words. This process of reducing the number of possible unique vectors is called **vector quantization**.\n\n<center><div> <img src=\"/images/bag-of-visual-words-4.png\" alt=\"Drawing\" style=\"width:70%;\"/></div> </center>\n\nUsing a limited set of visual words allows us to compress our image descriptions into a set of visual word IDs. And, more importantly, it helps us represent similar features across images using a shared set of visual words.\n\nThat means that the **visual words** shared by two images of churches may be quite large, meaning they're similar. However, an image of a church and an image of a dog will share far fewer visual words, meaning they're dissimilar.\n\nAfter those steps, our images will be represented by a varying number of visual words. From here we move on to the next step of transforming these visual words into image-level frequency vectors.\n\n### Frequency Vectors\n\nWe can count the frequency of these visual words and visualize them with histograms.\n\nThe **x-axis** of the histogram is the codebook, while the **y-axis** is the frequency of each visual word (in the codebook) for that image.\n\nIf we consider $2$ images, we can represent the image histograms as follows:\n\n<center><div> <img src=\"/images/bag-of-visual-words-5.png\" alt=\"Drawing\" style=\"width:80%;\"/></div> </center>\n\nTo create these representations, we have converted each image into a sparse vector where each value in the vector represents an item in the codebook (i.e., the x-axis in the histograms). Most of the values in each vector will be **zero** because most images will only contain a small percentage of total number of visual words, which is why we refer to them as **sparse** vectors.\n\nAs for the non-zero values in our sparse vector, they are calculated in the same way that we calculated our histogram bar heights. They are equal to the frequency of a particular visual word in an image.\n\nThis works, but it's a crude way to create these sparse vector representations. Because many visual words are actually not that important, we add one more step.\n\n### Tf-idf\n\nIn language there are some words that are more important than others in that they give us more information. If we used the sentence \"the history of Rome\" to search through a set of articles, the words \"the\" and \"of\" should not be given the same importance as \"history\" or \"Rome\".\n\nThese less important words are often very common. If we only consider the frequency of words shared with our \"the history of Rome\" query, the article with the most \"the\"s could be scored highest.\n\nThis problem is also found in images. A visual word extracted from a patch of sky in an image is unlikely to tell us whether this image is of a church or a dog. Some visual words are more relevant than others.\n\n<center>\n <div>\n   <img src=\"/images/bag-of-visual-words-9.png\" alt=\"Drawing\" style=\"width:70%;\"/>\n </div>\n</center>\n\nIn the example above, we would expect a visual word representing the sky *1* to be less relevant than a visual word representing the cross on top of the bell tower *2*.\n\nThat is why it is important to adjust the values of our sparse vector to give more weight to more relevant visual words and less weight to less relevant visual words.\n\nTo do that, we can use the *tf-idf* (*term-frequency inverse document frequency*) formula, which is calculated as follows:\n\n$$\ntf\\textrm{--}idf_{t,d} = tf_{t,d} * idf_t = tf_{t,d} * log\\frac{N}{df_t}\n$$\n\nWhere:\n\n* $tf_{t,d}$ is the term frequency of the visual word $t$ in the image $d$ (the number of times $t$ occurs in $d$),\n* $N$ is the total number of images,\n* $df_t$ number of images containing visual word $t$,\n* $log\\frac{N}{df_t}$ measures how common the visual word $t$ is across all images in the database. This is low if the visual word $d$ occurs many times in the image, high otherwise.\n\nAfter **tf-idf**, we can visualize the vectors via our histogram again, which will better reflect the image's features.\n\n<center>\n <div>\n   <img src=\"/images/bag-of-visual-words-6.png\" alt=\"Drawing\" style=\"width:80%;\"/>\n </div>\n</center>\n\nBefore we were giving the same importance to image's patches in an image; now they're adjusted based on relevance and then normalized.\n\nWe've now trained our codebook and learned how to process each vector for better relevance and normalization. When wanting to embed new images with this pipeline, we repeat the process but avoid retraining the codebook. Meaning we:\n\n1. Extract the visual features,\n2. Transform them into visual words using the existing codebook,\n3. Use these visual words to create a sparse frequency vector,\n4. Adjust the frequency vector based on relevance with tf-idf, giving us our final sparse vector representations.\n\nAfter that, we're ready to compare these sparse vectors to find similar or dissimilar images.\n\n### Measuring Similarity\n\nThere are several metrics we can use to calculate similarity or distance between two vectors. The most common are:\n\n1. **Cosine similarity**,\n\n2. **Euclidean distance**, and\n\n3. **Dot product similarity**.\n\nWe will use **cosine similarity** which measures the angle between vectors. Vectors pointing in a similar direction have a lower angular separation and therefore *higher* cosine similarity.\n\nCosine similarity is calculated as:\n\n$$\ncossim(A,B)= cos(\\theta)=\\frac{A \\cdot B}{||A|| \\space ||B||}\n$$\n\nCosine similarity generally gives a value ranging $[-1,1]$. However, if we think about the **frequency** of visual words, we cannot consider them as negative. Therefore, the angle between two term frequency vectors cannot be greater than $90°$, and cosine similarity ranges between $[0,1]$.\n\nIt equals $1$ if the vectors are pointing in the same direction (the angle equals $0$) and $0$ if vectors are perpendicular.\n\n<center><div> <img src=\"/images/bag-of-visual-words-7.png\" alt=\"Drawing\" style=\"width:100%;\"/></div> </center>\n\nIf we consider three different images and we build a matrix based on cosine similarity:\n\n<center><div> <img src=\"/images/bag-of-visual-words-8.png\" alt=\"Drawing\" style=\"width:80%;\"/></div> </center>\n\nWe can see that cosine similarity is $1$ when the image is exactly the same (i.e., in the main diagonal). The cosine similarity approaches $0$ as the images have less in common.\n\nLet's now move on to implementing bag of visual words with Python.\n\n## Implementing Bag of Visual Words\n\nThe next section will work through the implementation of everything we've just learned in Python. If you'd like to follow along, use [this Colab notebook](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/image-retrieval/bag-of-visual-words/bag-of-visual-words.ipynb).\n\n### Imagenette Dataset Preprocessing\n\nFirst, we want to import a dataset of images to train the model.\n\nFeel free to use any images you like. However, if you’d like to follow along with the same dataset, we will use the `frgfm/imagenette` dataset from HuggingFace Datasets.\n\n{{< notebook file=\"bovw-dataset\" height=\"full\" >}}\n\nThe dataset contains 9469 images, covering a range of images with dogs, radios, fishing, cities, etc. The `image` feature contains the images themselves stored as PIL object, meaning we can view them in a notebook like so:\n\n{{< notebook file=\"bovw-show-image\" height=\"full\" >}}\n\nTo process these images we need to transform them from PIL objects to numpy arrays.\n\n```python\nimport numpy as np\n\n# initialize list\nimages_training = []\n\nfor n in range(0,len(imagenet)):\n    # generate np arrays from the dataset images\n    images_training.append(np.array(imagenet[n]['image']))\n```\n\nThe dataset mostly consists of color images containing three color channels (red, green, and blue), but some are also grayscale containing just a single channel (brightness). To optimize processing time and keep everything as simple as possible, we will transform color images to grayscale.\n\n```python\nimport cv2  # pip install opencv-contrib-python opencv-python\n\n# convert images to grayscale\nbw_images = []\nfor img in images_training:\n    # if RGB, transform into grayscale\n    if len(img.shape) == 3:\n        bw_images.append(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY))\n    else:\n        # if grayscale, do not transform\n        bw_images.append(img)\n```\n\nThe arrays in `bw_images` are what we will be using to create our visual features, visual words, frequency vectors, and tf-idf vectors.\n\n### Visual Features\n\nWith our dataset prepared we're ready to move on to extracting visual features (both keypoints and descriptors). As mentioned earlier, we will use the SIFT feature detection algorithm.\n\n```python\n# defining feature extractor that we want to use (SIFT)\nextractor = cv2.xfeatures2d.SIFT_create()\n\n# initialize lists where we will store *all* keypoints and descriptors\nkeypoints = []\ndescriptors = []\n\nfor img in bw_images:\n    # extract keypoints and descriptors for each image\n    img_keypoints, img_descriptors = extractor.detectAndCompute(img, None)\n    keypoints.append(img_keypoints)\n    descriptors.append(img_descriptors)\n```\n\nIt's worth noting that if an image doesn't have any noticeable features (e.g., it is a flat image without any edges, gradients, etc.), extraction with SIFT can return `None`. We don't have that problem with this dataset, but it's something to watch out for with others.\n\nNow that we have extracted the visual features, we can visualize them with `matplotlib`.\n\n{{< notebook file=\"bovw-show-visual-features\" height=\"full\" >}}\n\nThe centre of each circle is the keypoint location, and the lines from the centre of each circle represent keypoint orientation. The size of each circle is the scale at which the features were detected.\n\nWith our visual features ready, we can move onto the next step of creating visual words.\n\n### Visual Words and the Codebook\n\nEarlier we described the \"codebook\". The codebook acts as a vocabulary where we store *all* of our visual words. To create the codebook we use k-means clustering to quantize our visual features into a smaller set of visual words.\n\nOur full set of visual features is big, and training k-means with the full set will take some time. So, to avoid that and also emulate a real-world scenario where we are unlikely to train on all images that we'll ever process, we will use a smaller sample of 1000 images.\n\n```python\n# set numpy seed for reproducability\nnp.random.seed(0)\n# select 1000 random image index values\nsample_idx = np.random.randint(0, len(imagenet)+1, 1000).tolist()\n\n# extract the sample from descriptors\n# (we don't need keypoints)\ndescriptors_sample = []\n\nfor n in sample_idx:\n    descriptors_sample.append(np.array(descriptors[n]))\n```\n\nOur `descriptors_sample` contains a single array for each image, and each array can contain a varying number of SIFT feature vectors. When training k-means, we only care about the feature vectors, we don't care about which image they're coming from. So, we need to flatten `descriptors_sample` into a single array containing *all* descriptors.\n\n{{< notebook file=\"bovw-prep-sample\" height=\"full\" >}}\n\nFrom this, we get `all_descriptors`, a single array containing all feature vectors from our sample. There are ~1.3M of these.\n\nWe now want to group similar visual features (descriptors) using **k-means**. After a few tests, we chose $k=200$ for our model.\n\nAfter k-means, all images will have been reduced to **visual words**, and the full set of these visual words become our codebook.\n\n```python\n# perform k-means clustering to build the codebook\nfrom scipy.cluster.vq import kmeans\n\nk = 200\niters = 1\ncodebook, variance = kmeans(all_descriptors, k, iters)\n```\n\nOnce built, the codebook does not change. No matter how many more visual features we process, no more visual words are added as we will use it solely as a mapping between new visual features and the existing visual words.\n\n---\n\n*It can be difficult to find the optimal size of our codebook - if too small, visual words could be unrepresentative of all image regions, and if too large, there could be too many visual words with little to no of them being shared between images (making comparisons very hard or impossible).*\n\n---\n\nWith our codebook complete, we can use it to transform the full dataset of visual features into visual words.\n\n{{< notebook file=\"bovw-map-to-visual-words\" height=\"full\" >}}\n\nWe can see here that image `0` contains `397` visual words; the first five of those are represented by `[84, 22, 45, 172, 172]`, which are the index values of the visual word vector found in the codebook. This visual word vector shares the same dimensionality as our SIFT feature vectors because it represents a cluster centroid from those feature vectors.\n\n### Sparse Frequency Vectors\n\nAfter building our codebook and creating our image representations with visual words, we can move on to building sparse vector representations from these visual words.\n\nWe do this to compress the *many* visual word vectors representing our images into a single vector of set dimensionality. By doing this, we are able to directly compare our image representations using metrics like cosine similarity and Euclidean distance.\n\nTo create these frequency vectors, we look at how many times each visual word is found in an image. There are only 200 unique visual words (the length of our codebook), so each of these frequency vectors will have dimensionality 200, where each value becomes a count for a specific visual word.\n\n{{< notebook file=\"bovw-create-freq-vec\" height=\"full\" >}}\n\nAfter creating the frequency vectors, we're left with a single vector representation for each image. We can see an example of the frequency vector for image *0* below:\n\n{{< notebook file=\"bovw-show-freq-vec\" height=\"full\" >}}\n\n### Tf-idf\n\nOur frequency vector can already be used for comparing images using our similarity and distance metrics. However, it is not ideal as it does not consider the different levels of relevance of each visual word. So, we must use tf-idf to adjust the frequency vector to consider relevance.\n$$\ntf\\textrm{-}idf_{t,d} = tf_{t,d} * idf_t = tf_{t,d} * log\\frac{N}{df_t}\n$$\nWe first calculate $N$ and $df_t$, both of which are shared across the entire dataset as the image $d$ is not considered by either parameter. Naturally, $idf_t$ also produces a single vector shared by the full dataset.\n\n{{< notebook file=\"bovw-idf\" height=\"full\" >}}\n\nWith $idf_t$ calculated, we just need to multiply it by each $tf_{t,d}$ vector to get our $tf \\textrm{-} idf_{t,d}$ vectors. Fortunately, we already have the $tf_{t,d}$ vectors, as they are our frequency vectors.\n\n{{< notebook file=\"bovw-tfidf\" height=\"full\" >}}\n\nWe now have *9469* 200-dimensional sparse vector representations of our images.\n\n### Search\n\nThese sparse vectors have been built in such a way that images that share many similar visual features should share similar sparse vectors. We can use cosine similarity to compare these images and identify similar images.\n\nWe will start by searching with image `1200`:\n\n![image-20220801174017452](./images/bag-of-visual-words-12.png)\n\n{{< notebook file=\"bovw-search\" height=\"full\" >}}\n\nThe top image is of course the same image; as they are exact matches we can see the expected cosine similarity score of `1.0`. Following this, we have two highly similar  results. Interestingly, the fourth image seems to have been pulled through due to similarity in background foliage.\n\nHere are a few more sets of results, showing the varied performance of the approach with different items.\n\n![retrieval-1](/images/bag-of-visual-words-10.png)\n\nHere we get a good first result, followed by irrelevant images and a final good result in fifth position, a 50% success rate.\n\n![retrieval-2](/images/bag-of-visual-words-11.png)\n\nBag of visual words seems to work well with golf balls, identifying 3/4 of relevant images.\n\nIf you're interested in seeing more results, check out the [Colab notebook here](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/image-retrieval/bag-of-visual-words/bag-of-visual-words.ipynb).\n\n---\n\nThat's it for this article on bag of visual words, one of the most successful methods for image classification and retrieval *without* the use of neural networks or other deep learning methods.\n\nOne great aspect of this approach is that it is fairly reliable and interpretable. There is no black box of AI here, so when applied to a lot of data we will rarely get too many surprising results.\n\nWe know that images with similar edges, textures, and colors are likely to be identified as similar; the features being identified are set by the SIFT (or other) algorithms.\n\nAll of this makes bag of visual words a good option for image retrieval or classification, where we need to focus on the features that we know algorithms like SIFT can deal with, i.e. we're focused on finding similar object edges (that are resistant to scale, noise, and illumination changes). Finally, these well-defined algorithms give us a huge advantage when interpretability is important.\n\n\n## Resources\n\n[Code Notebook](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/image-retrieval/bag-of-visual-words/bag-of-visual-words.ipynb), GitHub [Examples Repo](https://github.com/pinecone-io/examples/tree/master/learn/image-retrieval/bag-of-visual-words)"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbca"
  },
  "filename": "fine-tune-sentence-transformers-mnr.md",
  "title": "ebook-post",
  "category": "\"Training Sentence Transformers with MNR Loss\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Training Sentence Transformers with MNR Loss\"\nheadline: \"Next-Gen Sentence Embeddings with Multiple Negatives Ranking Loss\"\ncategories:\n  - NLP for Semantic Search\ntoc: >-\nweight: 4\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: How to create sentence transformers by fine-tuning with MNR loss.\n#Open Graph\nimages: ['/images/fine-tuning-sentence-transformers-mnr-loss-1.jpg']\n---\n\nTransformer-produced sentence embeddings have come a long way in a very short time. Starting with the slow but accurate similarity prediction of BERT cross-encoders, the world of [sentence embeddings](/learn/sentence-embeddings/) was ignited with the introduction of SBERT in 2019 [1]. Since then, many more sentence transformers have been introduced. These models quickly made the original SBERT obsolete.\n\nHow did these newer sentence transformers manage to outperform SBERT so quickly? The answer is *multiple negatives ranking (MNR) loss*.\n\nThis article will cover what MNR loss is, the data it requires, and how to implement it to fine-tune our own high-quality sentence transformers.\n\nImplementation will cover two training approaches. The first is more involved, and outlines the exact steps to fine-tune the model. The second approach makes use of the `sentence-transformers` library's excellent utilities for fine-tuning.\n\n<p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/or5ew7dqA-c\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>\n\n## NLI Training\n\nAs explained in our article on [softmax loss](/learn/train-sentence-transformers-softmax/), we can fine-tune sentence transformers using **N**atural **L**anguage **I**nference (NLI) datasets.\n\nThese datasets contain many sentence pairs, some that *imply* each other, and others that *do not imply* each other. As with the softmax loss article, we will use two of these datasets: the Stanford Natural Language Inference (SNLI) and Multi-Genre NLI (MNLI) corpora.\n\nThese two corpora total to 943K sentence pairs. Each pair consists of a `premise` and `hypothesis` sentence, which are assigned a `label`: \n\n* **0** — *entailment*, e.g. the `premise` suggests the `hypothesis`.\n\n* **1** — *neutral*, the `premise` and `hypothesis` could both be true, but they are not necessarily related.\n* **2** — *contradiction*, the `premise` and `hypothesis` contradict each other.\n\nWhen fine-tuning with MNR loss, we will be dropping all rows with *neutral* or *contradiction* labels — keeping only the positive *entailment* pairs.\n\nWe will be feeding sentence A (the `premise`, known as the *anchor*) followed by sentence B (the `hypothesis`, when the label is **0**, this is called the *positive*) into BERT on each step. Unlike softmax loss, we do not use the `label` feature.\n\nThese training steps are performed in batches. Meaning several anchor-positive pairs are processed at once.\n\nThe model is then optimized to produce similar embeddings between pairs while maintaining different embeddings for non-pairs. We will explain this in more depth soon.\n\n### Data Preparation\n\nLet's look at the data preparation process. We first need to download and merge the two NLI datasets. We will use the `datasets` library from Hugging Face.\n\n{{< notebook file=\"prep-nli\" height=\"full\" >}}\n\nBecause we are using MNR loss, we only want anchor-positive pairs. We can apply a filter to remove all other pairs (including erroneous `-1` labels).\n\n{{< notebook file=\"filter-mnr\" height=\"full\" >}}\n\nThe dataset is now prepared differently depending on the training method we are using. We will continue preparation for the more involved PyTorch approach. If you'd rather just train a model and care less about the steps involved, feel free to skip ahead to the next section.\n\nFor the PyTorch approach, we must tokenize our own data. To do that, we will be using a `BertTokenizer` from the `transformers` library and applying the `map` method on our `dataset`.\n\n{{< notebook file=\"tokenizer-mnr\" height=\"full\" >}}\n\nAfter that, we're ready to initialize our `DataLoader`, which will be used for loading batches of data into our model during training.\n\n{{< notebook file=\"dataloader-mnr\" height=\"full\" >}}\n\nAnd with that, our data is ready. Let's move on to training.\n\n\n\n### PyTorch Fine-Tuning\n\nWhen training SBERT models, we don't start from scratch. Instead, we begin with an already pretrained BERT — all we need to do is *fine-tune* it for building sentence embeddings.\n\n```python\nfrom transformers import BertModel\n\n# start from a pretrained bert-base-uncased model\nmodel = BertModel.from_pretrained('bert-base-uncased')\n```\n\nMNR and softmax loss training approaches use a * 'siamese'*-BERT architecture during fine-tuning. Meaning that during each step, we process a *sentence A* (our *anchor*) into BERT, followed by *sentence B* (our *positive*).\n\n![Start SBERT](/images/fine-tuning-sentence-transformers-mnr-loss-2.jpg)\n<small>Siamese-BERT network, the *anchor* and *positive* sentence pairs are processed separately. A mean pooling layer converts token embeddings into sentence embeddings.sentence A is our *anchor* and sentence B the *positive*.</small>\n\nBecause these two sentences are processed *separately*, it creates a *siamese*-like network with two identical BERTs trained in parallel. In reality, there is only a single BERT being used twice in each step.\n\nWe can extend this further with *triplet*-networks. In the case of triplet networks for MNR, we would pass three sentences, an *anchor*, it's *positive*, and it's *negative*. However, we are *not* using triplet-networks, so we have removed the negative rows from our dataset (rows where `label` is `2`).\n\n![Triplet networks](/images/fine-tuning-sentence-transformers-mnr-loss-3.jpg)\n<small>Triplet networks use the same logic but with an added sentence. For MNR loss this other sentence is the *negative* pair of the *anchor*.</small>\n\nBERT outputs 512 768-dimensional embeddings. We convert these into *averaged* sentence embeddings using *mean-pooling*. Using the siamese approach, we produce two of these per step — one for the *anchor* that we will call `a`, and another for the *positive* called `p`.\n\n```python\n# define mean pooling function\ndef mean_pool(token_embeds, attention_mask):\n    # reshape attention_mask to cover 768-dimension embeddings\n    in_mask = attention_mask.unsqueeze(-1).expand(\n        token_embeds.size()\n    ).float()\n    # perform mean-pooling but exclude padding tokens (specified by in_mask)\n    pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(\n        in_mask.sum(1), min=1e-9\n    )\n    return pool\n```\n\nIn the `mean_pool` function, we're taking these token-level embeddings (the 512) and the sentence `attention_mask` tensor. We resize the `attention_mask` to match the higher `768`-dimensionality of the token embeddings.\n\nThe resized mask `in_mask` is applied to the token embeddings to exclude padding tokens from the mean pooling operation. Mean-pooling takes the average activation of values across each dimension but *excluding* those padding values, which would reduce the average activation. This operation transformers our token-level embeddings (shape `512*768`) to sentence-level embeddings (shape `1*768`).\n\nThese steps are performed in *batches*, meaning we do this for many *(anchor, positive)* pairs in parallel. That is important in our next few steps.\n\n{{< notebook file=\"a-p-shapes\" height=\"full\" >}}\n\nFirst, we calculate the cosine similarity between each anchor embedding (`a`) and *all* of the positive embeddings in the same batch (`p`).\n\n{{< notebook file=\"cos-sim-mnr\" height=\"full\" >}}\n\nFrom here, we produce a vector of cosine similarity scores (of size `batch_size`) for each anchor embedding `a_i` *(or size `2 * batch_size` for triplets)*. Each anchor should share the highest score with its positive pair, `p_i`.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/fine-tuning-sentence-transformers-mnr-loss-4.mp4\" type=\"video/mp4\">\n</video>\n\n<small class=\"video\">Cosine similarity scores using five pairs/triples in a triplet network (with `(a, p, n)`). A siamese network is the same but excluding the dark blue `n` blocks (`n`).</small>\n\nTo optimize for this, we use a set of increasing label values to mark where the highest score should be for each `a_i`, and categorical [cross-entropy loss](/learn/cross-entropy-loss/).\n\n{{< notebook file=\"mnr-loss-func\" height=\"full\" >}}\n\nAnd that's every component we need for fine-tuning with MNR loss. Let's put that all together and set up a training loop. First, we move our model and layers to a CUDA-enabled GPU *if available*.\n\n{{< notebook file=\"mnr-layer-prep\" height=\"full\" >}}\n\nThen we set up the optimizer and schedule for training. We use an Adam optimizer with a linear warmup for 10% of the total number of steps.\n\n{{< notebook file=\"mnr-warmup-setup\" height=\"full\" >}}\n\nAnd now we define the training loop, using the same training process that we worked through before.\n\n{{< notebook file=\"mnr-training-loop\" height=\"full\" >}}\n\nWith that, we've fine-tuned our BERT model using MNR loss. Now we save it to file.\n\n{{< notebook file=\"mnr-save\" height=\"full\" >}}\n\nAnd this can now be loaded using either the `SentenceTransformer` or HF `from_pretrained` methods. Before we move on to testing the model performance, let's look at how we can replicate that fine-tuning logic using the *much simpler* `sentence-transformers` library.\n\n\n\n## Fast Fine-Tuning\n\nAs we already mentioned, there is an easier way to fine-tune models using MNR loss. The `sentence-transformers` library allows us to use pretrained sentence transformers and comes with some handy training utilities.\n\nWe will start by preprocessing our data. This is the same as we did before for the first few steps.\n\n{{< notebook file=\"mnr-preprocess\" height=\"full\" >}}\n\nBefore, we tokenized our data and then loaded it into a PyTorch `DataLoader`. This time we follow a *slightly different format*. We * don't* tokenize; we reformat into a list of `sentence-transformers` `InputExample` objects and use a slightly different `DataLoader`.\n\n{{< notebook file=\"mnr-fast-data-prep\" height=\"full\" >}}\n\nOur `InputExample` contains just our `a` and `p` sentence pairs, which we then feed into the `NoDuplicatesDataLoader` object. This data loader ensures that each batch is duplicate-free — a helpful feature when ranking pair similarity across *randomly* sampled pairs with MNR loss.\n\nNow we define the model. The `sentence-transformers` library allows us to build models using *modules*. We need just a transformer model (we will use `bert-base-uncased` again) and a mean pooling module.\n\n{{< notebook file=\"mnr-fast-model\" height=\"full\" >}}\n\nWe now have an initialized model. Before training, all that's left is the loss function — MNR loss.\n\n{{< notebook file=\"mnr-fast-loss\" height=\"full\" >}}\n\nAnd with that, we have our data loader, model, and loss function ready. All that's left is to fine-tune the model! As before, we will train for a single epoch and warmup for the first 10% of our training steps.\n\n{{< notebook file=\"mnr-fast-train\" height=\"full\" >}}\n\nAnd a couple of hours later, we have a new sentence transformer model trained using MNR loss. It goes without saying that using the `sentence-transformers` training utilities makes life *much easier*. To finish off the article, let's look at the performance of our MNR loss SBERT next to other sentence transformers.\n\n\n\n## Compare Sentence Transformers\n\nWe're going to use a semantic textual similarity (STS) dataset to test the performance of *four models*; our *MNR loss* SBERT (using PyTorch and `sentence-transformers`), the *original* SBERT, and an MPNet model trained with MNR loss on a [1B+ sample dataset](https://huggingface.co/spaces/flax-sentence-embeddings/sentence-embeddings).\n\nThe first thing we need to do is download the STS dataset. Again we will use `datasets` from Hugging Face.\n\n{{< notebook file=\"sts-load\" height=\"full\" >}}\n\nSTSb (or STS benchmark) contains sentence pairs in features `sentence1` and `sentence2` assigned a similiarity score from *0 -> 5*.\n\n| sentence1 | sentence2 | label | idx |\n| ------------------------------------ | ------------------------------------ | ----- | ---- |\n| A man with a hard hat is dancing. | A man wearing a hard hat is dancing. | 5.0 | 0 |\n| A man is riding a bike. | A woman is riding a horse. | 1.4 | 149 |\n| A man is buttering a piece of bread. | A slow loris hanging on a cord. | 0.0 | 127 |\n\n<small>Three samples from the validation set of STSb.</small>\n\nBecause the similarity scores range from 0 -> 5, we need to normalize them to a range of 0 -> 1. We use `map` to do this.\n\n{{< notebook file=\"sts-norm-labels\" height=\"full\" >}}\n\nWe're going to be using `sentence-transformers` evaluation utilities. We first need to reformat the STSb data using the `InputExample` class — passing the sentence features as `texts` and similarity scores to the `label` argument.\n\n{{< notebook file=\"sts-input-example\" height=\"full\" >}}\n\nTo evaluate the models, we need to initialize the appropriate evaluator object. As we are evaluating continuous similarity scores, we use the `EmbeddingSimilarityEvaluator`.\n\n{{< notebook file=\"sts-evaluator\" height=\"full\" >}}\n\nAnd with that, we're ready to begin evaluation. We load our model as a `SentenceTransformer` object and pass the model to our `evaluator`.\n\nThe evaluator outputs the * Spearman's rank correlation* between the cosine similarity scores calculated from the model's output embeddings and the similarity scores provided in STSb. A high correlation between the two values outputs a value close to *+1*, and no correlation would output *0*.\n\n{{< notebook file=\"sts-eval\" height=\"full\" >}}\n\nFor the model fine-tuned with `sentence-transformers`, we output a correlation of *0.84*, meaning our model outputs good similarity scores according to the scores assigned to STSb. Let's compare that with other models.\n\n| Model | Score |\n| --------------------------------------------------- | ----- |\n| `all_datasets_v3_mpnet-base` | 0.89 |\n| Custom SBERT with MNR (`sentence-transformers`) | 0.84 |\n| Original SBERT `bert-base-nli-mean-tokens` | 0.81 |\n| Custom SBERT with softmax (`sentence-transformers`) | 0.80 |\n| Custom SBERT with MNR (PyTorch) | 0.79 |\n| Custom SBERT with softmax (PyTorch) | 0.67 |\n| `bert-base-uncased` | 0.61 |\n\nThe top two models are trained using MNR loss, followed by the original SBERT.\n\nThese results support the advice given by the authors of `sentence-transformers`, that models trained with MNR loss outperform those trained with softmax loss in building high-performing sentence embeddings [2].\n\nAnother key takeaway here is that despite our best efforts and the complexity of building these models with PyTorch, *every* model trained using the easy-to-use `sentence-transformers` utilities far outperformed them.\n\nIn short; fine-tune your models with MNR loss, and do it with the `sentence-transformers` library.\n\n---\n\nThat's it for this walkthrough and guide to fine-tuning sentence transformer models with multiple negatives ranking loss — the current best approach for building high-performance models.\n\nWe covered preprocessing the two most popular NLI datasets — the Stanford NLI and multi-genre NLI corpora — for fine-tuning with MNR loss. Then we delved into the details of this fine-tuning approach using PyTorch before taking advantage of the excellent training utilities provided by the `sentence-transformers` library.\n\nFinally, we learned how to evaluate our sentence transformer models with the semantic textual similarity benchmark (STSb). Identifying the highest performing models.\n\n{{< newsletter text=\"Subscribe for more NLP and semantic search tutorials!\" inputText=\"Email address...\" buttonText=\"Submit\">}}\n\n## References\n\n[1] N. Reimers, I. Gurevych, [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084) (2019), ACL\n\n[2] N. Reimers, [Sentence Transformers NLI Training Readme](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/nli), GitHub\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbcb"
  },
  "filename": "rise-vector-data.md",
  "title": "post",
  "category": "\"The Rise of Vector Data\"",
  "content": "---\nlayout: post\ntitle: \"The Rise of Vector Data\"\nheadline: \"The Rise of <span>Vector Data</span>\"\ncategories:\n  - Company News\ntoc: >-\nauthor:\n  name: Edo Liberty\n  position: Founder and CEO\n  src: /images/company-edo.png\n  href: https://edoliberty.github.io/\ndate: \"2021-05-21\"\n# Date: May 21, 2021\n#Open Graph\ndescription: What happens in your brain when you see someone you recognize? First, the rods and cones in your eyes record the light intensity. Those signals then travel..\nimages: ['/images/rise-of-vector-data-7.png']\nthumbnail: \"/images/rise-vector-data-thumbnail.jpg\"\n---\n\nWhat happens in your brain when you see someone you recognize?\n\nFirst, the rods and cones in your eyes record the light intensity. Those signals then travel to the visual cortex in the back of your head, where they activate neural cells through several layers in your visual cortex. In the end, you have millions of neurons activated in varying intensities. Those activations are transmitted to your temporal lobe, where your brain interprets as: “I see Julie.”\n\n![](/images/rise-of-vector-data-6.png)\n\nThe higher functions related to vision happen on information that hardly resembles the initial intensity of the light that hit your eye. Instead, they deal with the much richer representations output by your visual cortex. When you interpret what you see or read, your brain operates on those neural representations and not the original image.\n\nDeep learning applications process the world in a similar way. Information is converted into [vector embeddings](/learn/vector-embeddings/) — or simply “vectors” — which are then used for predictions, interpretation, comparison, and other cognitive functions.\n\n\n![](/images/rise-of-vector-data-5.png)\n\nIn Machine Learning, transformer models — or more generally “embedding models” — serve the role of converting raw data into vector embeddings. They generate vector data.\n\nThere are embedding models for all kinds of data: audio, images, text, logs, video, structured, unstructured, and so on. By converting raw data into vectors, they enable functions such as image search, audio search, deduplication, semantic search, object and facial recognition, question-answering, and more.\n\nEmbedding models are [growing in numbers](https://www.blog.google/technology/ai/lamda), capability, and adoption. They’re also getting easier to access and use. Deep-learning frameworks such as MXNet, TensorFlow, PyTorch, and Caffe have pre-trained models included and accessible with as few as two lines of code.\n\n![](/images/rise-of-vector-data-4.png)\n\n```python\nimport torchvision.models as models\nmodel = models.squeezenet1_0(pretrained=True)\n```\n\nThe more models are used, the more vector data gets generated. Often, vectors get immediately discarded after they are generated. But what if you save the vector data you generate? That, it turns out, can be quite valuable. So valuable that Google, Microsoft, Amazon, Facebook, Netflix, Spotify, and other AI trailblazers have already put it at the core of their applications.\n\n## Making Something of Vector Data\n\nWhat higher cognitive functions could we unlock by aggregating millions or billions of semantically rich vectors?\n\n![](/images/rise-of-vector-data-7.png)\n\nOne of the most helpful and fundamental things unlocked by storing vectors is simple: **search**.\n\nGiven some new vector, find other known vectors that are similar. Since this [similarity search](/learn/what-is-similarity-search/) (or “vector search”) acts on rich vector representations, it performs a lot more like our brains do when we look for similar objects: we use pattern recognition, semantic meaning, relevant context, memory, association, and even intuition.\n\nThis fundamentally new method of information retrieval can make many things better: search engines, recommendation systems, chatbots, security systems, analysis tools, and any other application involving user-facing or internal search functions.\n\nAnd not just *a little* better. If you’ve recently marveled at the personalized product recommendations from Amazon, the sublime music recommendations from Spotify, the mystifyingly relevant search results from Google/Bing, or the can’t-look-away activity feeds from Facebook/LinkedIn/Twitter, then you’ve experienced the power of similarity search.\n\nSome of those companies have written about their use of vector embeddings for search. [Google](http://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html), [Spotify](https://github.com/spotify/annoy), and [Facebook](/learn/faiss/) have even open-sourced the core components of their similarity search technology.\n\n![](/images/rise-of-vector-data-3.png)\n\nVector data is growing, and there’s a clear benefit to using it for search. However, there’s a reason why only a few companies with some of the largest and most sophisticated engineering teams are doing similarity searches at scale.\n\n## The Tangle of Vector Search Algorithms\n\nVectors have a unique format that requires novel indexing and search algorithms.\n\nThere are well-established tools for searching through relational databases, key-value stores, text documents, and even graphs. Vector data requires an entirely new index and search methods involving the geometric relationships — proximity and angles — between items represented as vector embeddings.\n\n![](/images/rise-of-vector-data-1.png)\n\nVectors don’t contain discrete attributes or terms you could just filter through. Instead, each vector embedding is an array of hundreds or thousands of numbers. Treating those numbers as coordinates lets you treat vectors as points in a multi-dimensional  [Euclidean space](https://en.wikipedia.org/wiki/Euclidean_space). Then, searching for similar items is equivalent to finding the neighboring points in that space.\n\n![](/images/rise-of-vector-data-8.png)\n\nIt’s relatively easy to do this with two-dimensional vectors: Dissect the space in a way that you can say, apriori, the red circle only intersects the gray rectangles, then focus your search for nearest neighbors there. That describes the well-known k-d tree algorithm. It works well in low dimensions but fails in higher dimensions. In higher dimensions (three-dimensional in the figure above for illustration), there is no simple way to dissect the space into “rectangles” to accelerate the search procedure.\n\nWe need a much more complex search algorithm for high-dimension spaces. Fortunately, there are [over a dozen open-source libraries](http://ann-benchmarks.com/) dedicated to solving this problem efficiently. Less fortunately, each of those contains multiple algorithms to choose from, each with varying trade-offs between speed and accuracy, each with different parameters to tune.\n\n![](/images/rise-of-vector-data-2.png)\n*Source: ann-benchmarks.com*\n\nAs a practical matter, choosing a library, algorithm, and parameters for your data is the first hurdle. There is no “right” answer. Each algorithm comes with a complex set of trade-offs, limitations, and behaviors that may not be obvious. For example, the fastest algorithm might be wildly inaccurate; a performant index could be immutable or very slow to update; memory consumption can grow super linearly; and more surprises like that.\n\n## The Tall Barrier to Scalable Vector Search\n\nStoring and searching through vector data at scale looks a lot like running a database in production, and building the infrastructure takes just as much work.\n\nDepending on the size of your vector data and your throughput, latency, accuracy, and availability requirements, you may need to build a system with sharding, replication, live index updates, namespacing, filtering, persistence, and consistency. Then you need monitoring, alerting, auto-recovery, auto-scaling, etc, to ensure high availability and operational health.\n\nThis work becomes a significant undertaking that companies like Google, Microsoft, and Amazon can afford in terms of time and resources. Most other companies can’t, so they can’t use vector search.\n\nOr can they?\n\n## The Rise of Vector Tooling\n\nIn recent years, [the rise of ML models](https://www.wsj.com/articles/models-will-run-the-world-1534716720) spurred an ecosystem of tools that made it easier to develop and deploy models. As we witness the rise of vector data, we need new tools for working with that data.\n\nWe hope to lead the way with our [managed vector search solution](/). We specifically designed it for use in production with just a few lines of code without the user needing to worry about algorithm tuning or distributed infrastructure.\n\nThe rise of vector data will have limited impact until more companies have the tools to use it and make their products better. Search is the first and fundamental step in this process, so that’s where we begin.\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbcc"
  },
  "filename": "domain-transfer.md",
  "title": "ebook-post",
  "category": "\"Domain Transfer with BERT\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Domain Transfer with BERT\"\nheadline: \"Making the Most of Data: Domain Transfer with BERT\"\ncategories:\n  - NLP for Semantic Search\ntoc: >-\nweight: 11\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: How to apply AugSBERT in domain transfer.\n# Open graph\nimages: ['/images/augsbert-domain-transfer-0.png']\n---\n\nWhen building language models, we can spend months optimizing training and model parameters, but it’s useless if we don't have the correct data.\n\nThe success of our language models relies first and foremost on data. We covered a part way solution to this problem by applying the [Augmented SBERT training strategy to in-domain problems](/learn/data-augmentation/). That is, given a small dataset, we can artificially enlarge it to enhance our training data and improve model performance.\n\nIn-domain assumes that our target use case aligns to that small initial dataset. But what if the only data we have *does not* align? Maybe we have Quora question duplicate pairs, but we want to identify similar questions on StackOverflow.\n\nGiven this scenario, we must transfer information from the out-of-domain (or *source*) dataset to our target domain. We will learn how to do this here. First, we will learn to assess which source datasets align best with our target domain quickly. Then we will explain and work through the AugSBERT domain-transfer training strategy [2].\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/a8jyue22SJM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n## Will it Work?\n\nBefore we even begin training our models, we can get a good approximation of whether the method will work with some simple *n-gram* matching statistics [1].\n\nWe count how many n-grams two different domains share. If our *source* domain shares minimal similarity to a *target* domain, as measured by *n-gram* matches, it is less likely to output good results.\n\nThis behavior is reasonably straightforward to understand; given our two *source*-*target* domains, overlapping n-grams indicate the linguistic and semantic gap (or *overlap*) between the two domains.\n\n![linguistic_gap](/images/augsbert-domain-transfer-1.jpg)\n<small>Small n-gram overlap indicates a more significant gap between domains. More significant gaps require larger bridges (better models). The closer the two domains, the easier it is to bridge the gap.</small>\n\nThe greater the gap, the more difficult it is to bridge it using our training strategy. Although models are becoming better at generalization, they’re [still *brittle* when compared to our human-level ability](https://www.nature.com/articles/d41586-019-03013-5) to adapt knowledge across domains. \n\nThe *brittleness* of language models means a small change can hamper performance. The more significant that change, the less likely our model will successfully translate its existing knowledge to the new domain.\n\nWe are similar. Although people are much more flexible and can apply pre-existing knowledge across domains incredibly well, we’re not perfect.\n\nGiven a book, we can tilt the pages at a slight five-degree angle, and most people will hardly notice the difference and continue reading. Turn the book upside-down, and many people will be unable to read. Others will begin to read slower. Our performance degrades with this small change.\n\nIf we are then given the same book in another language, most of us will have difficulty comprehending the book. It is still the same book, presented differently.\n\nThe knowledge transfer of models across different domains works in the same way: Greater change results in lower performance.\n\n### Calculating Domain Correlation\n\nWe will measure the *n-gram overlap* between **five** domains, primarily from *[Hugging Face Datasets](https://huggingface.co/datasets)*.\n\n| Dataset                                     | Download Script                                              |\n| ------------------------------------------- | ------------------------------------------------------------ |\n| STSb                                        | `load_dataset('glue', 'stsb')`                               |\n| Quora Question Pairs (QQP)                  | `load_dataset('glue', 'qqp')`                                |\n| Microsoft Research Paraphrase Corpus (MRPC) | `load_dataset('glue', 'mrpc')`                               |\n| Recognizing Textual Entailment (RTE)        | `load_dataset('glue', 'rte')`                                |\n| Medical Question Pairs (Med-QP)             | [see here](https://gist.github.com/jamescalam/2dbc9874b599dde95d8ddcdd018dfcf6) |\n\nTo calculate the similarity, we perform three operations:\n\n1. Tokenize datasets\n\n{{< notebook file=\"tokenize-1\" height=\"full\" >}}\n\n2. Merge tokens into bi-grams (two-token pairs)\n\n{{< notebook file=\"ngrams\" height=\"full\" >}}\n\n3. Calculate the Jaccard similarity between different n-grams.\n\n{{< notebook file=\"jaccard\" height=\"full\" >}}\n\n*([Full script here](https://gist.github.com/jamescalam/15b48b1d9689e70ab9073e374ba3dc4a))*\n\nAfter performing each of these steps and calculating the [Jaccard similarity](/learn/semantic-search/) between each dataset, we should get a *rough indication* of how transferable models trained in one domain could be to another.\n\n![ngram_similarity](/images/augsbert-domain-transfer-2.jpg)\n<small>Jaccard similarity scores between each of the five datasets.</small>\n\nWe can see that the *MedQP* dataset has the lowest similarity to other datasets. The remainder are all reasonably similar.\n\nOther factors contribute to how well we can expect domain transfer to perform, such as the size of the source dataset and subsequent performance of the source cross encoder model within its own domain. We'll take a look at these statistics soon.\n\n\n\n## Implementing Domain Transfer\n\nThe AugSBERT training strategy for domain transfer follows a similar pattern to that explained in our [in-domain AugSBERT article](/learn/data-augmentation/). With the one exception that we train our cross-encoder in one domain and the bi-encoder (sentence transformer) in another.\n\nAt a high-level it looks like this:\n\n![domain_transfer_steps](/images/augsbert-domain-transfer-3.jpg)\n<small>AugSBERT training strategy for cross-domain use.</small>\n\nWe start with a labeled dataset from our *source domain* and an unlabeled dataset in our *target domain*. The source domain should be as similar as possible to our target domain.\n\nThe next step is to train the source domain cross-encoder. For this, we want to maximize cross encoder performance, as the bi-encoder will essentially learn to replicate the cross-encoder. Better cross-encoder performance translates to better bi-encoder performance.\n\nIf the target dataset is very small (1-3K pairs), we may need to augment the dataset. We do this because bi-encoder models require more data to be trained to the same level as a cross-encoder model. A good target dataset should contain 10K or more pairs, although this can vary by use case.\n\nWe label the previously *unlabeled* (and possibly *augmented*) target domain dataset with the trained cross-encoder.\n\nThe final step is to take the now labeled target domain data and use it to train the bi-encoder model.\n\nThat is all there is to it. We will add additional evaluation steps to confirm that the models are performing as expected, but otherwise, we'll stick with the described process.\n\nWe already have our five datasets, and we will use each as both source and target data to see the difference in performance between domains.\n\nWhen using a dataset for the *target domain*, we emulate a real-world use case (where we have no target data labels) by *not* including existing labels and instead relying solely on the cross-encoder-generated labels.\n\n### Cross Encoder Training\n\nAfter downloading our labeled source data, we train the cross encoder. To do this, we need to format the source data into `InputExample` objects, then load them into a PyTorch `DataLoader`.\n\n```python\nfrom sentence_transformers import InputExample\nfrom torch.utils.data import DataLoader\n\ndata = []\n# iterate through each row in dataset\nfor row in ds:\n    # append InputExample object to the list\n    data.append(InputExample(\n        texts=[row['sentence1'], row['sentence2']],\n        label=float(row['label'])\n    ))\n\n# initialize PyTorch DataLoader using data\nsource = DataLoader(\n    data, shuffle=True, batch_size=16\n)\n```\n\nIt can be a good idea to take validation samples for either the source or target domains and create an `evaluator` that can be passed to the cross encoder training function. With this, the script will output Pearson and Spearman correlation scores that we can use to assess model performance.\n\n```python\nfrom sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator\n\ndev_data = []\n# iterate through each row again (this time the validation split)\nfor row in dev:\n    # build up using InputExample objects\n    dev_data.append(InputExample(\n        texts=[row['sentence1'], row['sentence2']],\n        label=float(row['label'])\n    ))\n\n# the dev data goes into an evaluator\nevaluator = CECorrelationEvaluator.from_input_examples(\n    dev_data\n)\n```\n\nTo train the cross encoder model, we initialize a `CrossEncoder` and use the `fit` method. `fit` takes the source data dataloader, evaluator (optional), where we would like to save the trained model `output_path`, and a few training parameters.\n\n```python\n# initialize the cross encoder\ncross_encoder = CrossEncoder('bert-base-uncased', num_labels=1)\n\n# setup the number of warmup steps, 0.2 == 20% warmup\nnum_epochs = 1\nwarmup = int(len(source) * num_epochs * 0.2)\n\ncross_encoder.fit(\n    train_dataloader=source,\n    evaluator=evaluator,\n    epochs=num_epochs,\n    warmup_steps=warmup,\n    optimizer_params={'lr': 5e-5},  # default 2e-5\n    output_path=f'bert-{SOURCE}-cross-encoder'\n)\n```\n\nFor the training parameters, it is a good idea to test various learning rates and warm-up steps. A single epoch is usually enough to train the cross-encoder, and anything beyond this is likely to cause overfitting. Overfitting is bad when the target data is *in-domain*, and, when it is out-of-domain, it's *even* worse.\n\nFor the five models being trained (plus one more trained on a restricted Quora-QP dataset containing 10K rather than 400K training pairs), the following learning rate and percentage of warm-up steps were used.\n\n| Model                       | Learning Rate | Warmup | Evaluation (Spearman, Pearson) |\n| --------------------------- | ------------- | ------ | ------------------------------ |\n| `bert-mrpc-cross-encoder`   | 5e-5          | 35%    | (0.704, 0.661)                 |\n| `bert-stsb-cross-encoder`   | 2e-5          | 30%    | (0.889, 0.887)                 |\n| `bert-rte-cross-encoder`    | 5e-5          | 30%    | (0.383, 0.387)                 |\n| `bert-qqp10k-cross-encoder` | 5e-5          | 20%    | (0.688, 0.676)                 |\n| `bert-qqp-cross-encoder`    | 5e-5          | 20%    | (0.823, 0.772)                 |\n| `bert-medqp-cross-encoder`  | 5e-5          | 40%    | (0.737, 0.714)                 |\n\nThe Spearman and Pearson correlation values measure the correlation between the predicted and true labels for sentence pairs in the validation set. A value of 0.0 signifies no correlation, 0.5 is a moderate correlation, and 0.8+ represents strong correlation.\n\nThese results are fairly good, in particular the `bert-stsb-cross-encoder` and *full* `bert-qqp-cross-encoder` models return great performance. However, the RTE model `bert-rte-cross-encoder` performance is far from good.\n\nThe poor RTE performance is in part likely due to the small dataset size. However, as it is not significantly smaller than other datasets (Med-QP and MRPC in particular), we can assume the dataset is (1) not as clean or (2) that RTE is a more complex task.\n\n| Dataset  | Size    |\n| -------- | ------- |\n| MRPC     | 3,668   |\n| STSb     | 5,749   |\n| **RTE**  | 2,490   |\n| Quora-QP | 363,846 |\n| Med-QP   | 2,753   |\n\nWe will find that this poor RTE performance doesn't necessarily translate to poor performance in other domains. Indeed, *very good* performance in the source domain can actually hinder performance in the target domain because the model must be able to *generalize* well and not *specialize* too much in a particular domain.\n\nWe will later be taking a pretrained BERT model, which already has a certain degree of performance in the target domains. Overtraining in the source domain can pull the pretrained model alignment away from the target domain, hindering performance.\n\nA better measure of potential performance is to evaluate against a small (or big if possible) validation set in the target domain.\n\n![source_ce_to_target_dev](/images/augsbert-domain-transfer-4.jpg)\n<small>Correlation scores between source cross-encoder models (x-axis) and target domain dev sets (x-axis). The bottom row indicates baseline performance using a pretrained `Bert-base-uncased` model *without fine-tuning*. Lower scores are marked with red, above with cyan, and *roughly equal* with grey.</small>\n\nThese correlation values are a good indication of the performance we can expect from our bi-encoder model. Immediately it is clear that the MedQP domain is not easily bridged as expected from the earlier n-gram analysis.\n\nAt this point, we can consider dropping the low performing source domains. Although we will keep them to see how these low cross-encoder scores translate to bi-encoder performance.\n\n### Labeling the Target Data\n\nThe next step is to create our labeled target dataset. We use the cross-encoder trained in the source domain to label the *unlabeled* target data.\n\nThis is relatively straightforward. We take the unlabeled sentence pairs, transform them into a *list* of sentence pairs, and feed them into the `cross_encoder.predict` method.\n\n```python\n# target data is from the training sets from prev snippets\n# (but we ignore the label feature, otherwise there is nothing to predict)\npairs = list(zip(target['sentence1'], target['sentence2']))\n\nscores = cross_encoder.predict(pairs)\n```\n\nWe return a set of similarity scores, which we can append to the target data and use it to train our bi-encoder.\n\n```python\nimport pandas as pd\n\n# store everything in a pandas DataFrame\ntarget = pd.DataFrame({\n    'sentence1': target['sentence1'],\n    'sentence2': target['sentence2'],\n    'label': scores.tolist()  # cross encoder predicted labels\n})\n# and save to file\ntarget.to_csv('target_data.tsv', sep='\\t', index=False)\n```\n\n### Training the Bi-Encoder\n\nThe final step in the training process is training the bi-encoder/sentence transformer itself. Everything we've done so far has been to label the target dataset.\n\nNow that we have the dataset, we first need to reformat it using `InputExample` objects and a `DataLoader` as before.\n\n```python\nfrom torch.utils.data import DataLoader\nfrom sentence_transformers import InputExample\n\n# create list of InputExamples\ntrain = []\nfor i, row in target.iterrows():\n  \ttrain.append(InputExample(\n      \ttexts=[row['sentence1'], row['sentence2']],\n      \tlabel=float(row['label'])\n    ))\n# and place in PyTorch DataLoader\nloader = DataLoader(\n  \ttrain, shuffle=True, batch_size=BATCH_SIZE\n)\n```\n\nThen we initialize the bi-encoder. We will be using a pretrained `bert-base-uncased` model from *[Hugging Face Transformers](https://huggingface.co/docs/transformers/index)* followed by a mean pooling layer to transform word-level embeddings to sentence embeddings.\n\n{{< notebook file=\"init-sentence-transformer\" height=\"full\" >}}\n\nThe labels output by our cross encoder are continuous values in the range 0.0 -> 1.0, which means we can use a loss function like `CosineSimilarityLoss`. Then we're ready to train our model as we have done before.\n\n```python\n# setup loss function\nloss = losses.CosineSimilarityLoss(model=model)\n\n# and training\nepochs = 1\n# warmup for first 30% of training steps (test diff values here)\nwarmup_steps = int(len(loader) * epochs * 0.3)\n\nmodel.fit(\n  train_objectives=[(loader, loss)],\n  epochs=epochs,\n  warmup_steps=warmup_steps,\n  output_path=f'bert-target'\n)\n```\n\n### Evaluation and Augmentation\n\nAt this point, we can evaluate the bi-encoder model performance on a validation set of the target data. We use the `EmbeddingSimilarityEvaluator` to measure how closely the predicted, and true labels correlate ([script here](https://gist.github.com/jamescalam/64e38a2a8e84db61e5739f9fe41c12f2)).\n\n![target_biencoder_dev](/images/augsbert-domain-transfer-5.jpg)\n<small>Bi-encoder correlation scores. Many reach very close to the equivalent cross-encoder performance (and some even exceed it). Yellow highlights indicate an improved performance after augmentation via random sampling.</small>\n\nThe first bi-encoder results are reasonable, with most scoring higher than the Bert benchmark. Highlighted results indicate the original score (in the center) followed by scores *after* augmenting target datasets with random sampling. Where data augmentation showed little-to-no improvement, scores were excluded.\n\nOne reason we might see improvement is quite simple. Bi-encoders require relatively large training sets. Our datasets are all tiny, except for QQP (which does produce a 72% correlation score in `bert-Smedqp-Tqqp`). Augmented datasets help us satisfy the data-hungry nature of bi-encoder training.\n\nFortunately, we already set up most of what we needed to *augment* our target datasets. We have the cross-encoders for labeling, and all that is left is to generate new pairs.\n\nAs covered in our [in-domain AugSBERT article](https://www.pinecone.io/learn/data-augmentation/), we can generate new pairs with *random sampling*. All this means is that we create new sentence pairs by mixing-and-matching sentences from features A and B.\n\nAfter generating these new pairs, we score them using the relevant cross-encoder. And [like magic](https://gist.github.com/jamescalam/062673282c2a8da13e8084bb7a5bbb35), we have thousands of new samples to train our bi-encoders with.\n\nWith or without random sampling, we can see results that align with the performance of our cross-encoder models, which is precisely what we would expect. This similarity in results means that the knowledge from our cross-encoders is being distilled successfully into our *faster* bi-encoder models.\n\n\n\nThat is it for the Augmented SBERT training strategy and its application to domain transfer. Effective domain transfer allows us to broaden the horizon of sentence transformer use across many more domains.\n\nThe most common blocker for new language tools that rely on BERT or other transformer models is a lack of data. We do not eliminate the problem entirely using this technique, but we can reduce it.\n\nGiven a new domain that is not *too far* from the domain of existing datasets, we can now build better-performing sentence transformers. Sometimes in the range of just a few percentage point improvements, and at other times, we see much more significant gains.\n\nThanks to AugSBERT, we can now tackle a few of those previously inaccessible domains.\n\n\n\n## References\n\n[1] D. Shah, et al., [Adversarial Domain Adaption for Duplicate Question Detection](https://aclanthology.org/D18-1131/) (2018), EMNLP Proc.\n\n[2] N. Thakur, et al., [Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks](https://arxiv.org/abs/2010.08240) (2021), NAACL\n\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbcd"
  },
  "filename": "semantic-search.md",
  "title": "post",
  "category": "\"Semantic Search",
  "content": "---\nlayout: post\ntitle: \"Semantic Search: Measuring Meaning From Jaccard to Bert\"\nheadline: \"Semantic Search: <span>Measuring Meaning From Jaccard to Bert</span>\"\ncategories:\n  - Algorithms & Libraries\ntoc: >-\nweight: 1\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: Supercharge search with these stellar technologies.\n# Open Graph\nimages: ['/images/semantic-search-1.png']\n---\n\nSimilarity search is one of the fastest-growing domains in AI and machine learning. At its core, it is the process of matching relevant pieces of information together.\n\nThere's a strong chance that you found this article through a search engine — most likely Google. Maybe you searched something like \"what is semantic similarity search?\" or \"traditional vs vector similarity search\".\n\nGoogle processed your query and used many of the same similarity search essentials that we will learn about in this article, to bring you to — this article.\n\n---\n\n**Note: Want to replace your keyword search with semantic search powered by NLP? [Pinecone](/) makes it easy, scalable, and free — [start now](https://app.pinecone.io/).**\n\n---\n\nIf similarity search is at the heart of the success of a $1.65T company — the world's fifth most valuable company in the world<sup>[1]</sup>, there's *a good chance* it's worth learning more about.\n\n[Similarity search](/learn/what-is-similarity-search/) is a complex topic and there are countless techniques for building effective search engines.\n\nIn this article, we'll cover a few of the most interesting — and powerful — of these techniques — focusing specifically on semantic search. We'll learn how they work, what they're good at, and how we can implement them ourselves.\n\nWatch the videos or continue reading:\n\n<p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/AY62z7HrghY\" title=\"Traditional methods for semantic similarity search\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>\n<p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/ziiF1eFM3_4\" title=\"Vector-based methods for semantic similarity search\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>\n\n## Traditional Search\n\nWe start our journey down the road of search in the traditional camp, here we find a few key players like:\n\n- **Jaccard Similarity**\n- **w-shingling**\n- Pearson Similarity\n- **Levenshtein distance**\n- Normalized Google Distance\n\nAll are great metrics to use with similarity search — of which we'll cover three of the most popular, Jaccard similarity, w-shingling, and Levenshtein distance.\n\n### Jaccard Similarity\n\nJaccard similarity is a simple, but sometimes powerful similarity metric. Given two sequences, **A** and **B** — we find the number of shared elements between both and divide this by the total number of elements from both sequences.\n\n![Jaccard similarity measures the intersection between two sequences over the union between the two sequences.](/images/semantic-search-1.png)\n\n<small>Jaccard similarity measures the intersection between two sequences over the union between the two sequences.</small>\n\nGiven two sequences of integers, we would write:\n\n{{< notebook file=\"jac-sim\" height=\"full\" >}}\n\nHere we identified **two** shared *unique* integers, `3` and `4` — between two sequences with a total of ten integers in both, of which **eight** are unique values — `2/8` gives us our Jaccard similarity score of `0.25`.\n\nWe could perform the same operation for text data too, all we do is replace *integers* with *tokens*.\n\n![Jaccard similarity calculated between two sentences a and b.](/images/semantic-search-2.png)\n\n<small>Jaccard similarity calculated between two sentences **a** and **b**.</small>\n\n{{< notebook file=\"jaccard-1\" height=\"full\" >}}\n\nWe find that sentences `b` and `c` score much better, as we would expect. Now, it isn't perfect — two sentences that share nothing but words like *'the'*, *'a'*, *'is'*, etc — could return high Jaccard scores despite being semantically dissimilar.\n\nThese shortcomings can be solved partially using preprocessing techniques like stopword removal, stemming/lemmatization, and so on. However, as we'll see soon — some methods avoid these problems altogether.\n\n### w-Shingling\n\nAnother similar technique is **w-shingling**. w-shingling uses the exact same logic of *intersection / union* — but with 'shingles'. A 2-shingle of sentence **a** would look something like:\n\n```python\na = {'his thought', 'thought process', 'process is', ...}\n```\n\nWe would then use the same calculation of `intersection / union` between our *shingled* sentences like so:\n\n{{< notebook file=\"w-shingling\" height=\"full\" >}}\n\nUsing a 2-shingle, we find three matching shingles between sentences **b** and **c**, resulting in a similarity of **0.125**.\n\n### Levenshtein Distance\n\nAnother popular metric for comparing two strings is the Levenshtein distance. It is calculated as the number of operations required to change one string into another — and it's calculated with:\n\n![Levenshtein distance formula.](/images/semantic-search-3.png)\n\n<small>Levenshtein distance formula.</small>\n\nNow, this is a pretty complicated-looking formula — if you understand it, great! If not, don't worry — we'll break it down.\n\nThe variables `a` and `b` represent our two strings, `i` and `j` represent the character position in `a` and `b` respectively. So given the strings:\n\n![Levenshtein' and a mispelling 'Livinshten](/images/semantic-search-30.png)\n\n<small>'Levenshtein' and a mispelling 'Livinshten'.</small>\n\nWe would find:\n\n![We index the word itself from 1 to the length of the word, the zeroth index does exist as a none character (more on that next)](/images/semantic-search-4.png)\n\n<small>We index the word itself from 1 to the length of the word, the zeroth index does exist as a none character (more on that next).</small>\n\nEasy! Now, a great way to grasp the logic behind this formula is through visualizing the Wagner-Fischer algorithm — which uses a simple matrix to calculate our Levenshtein distance.\n\nWe take our two words `a` and `b` and place them on either axis of our matrix — we include our *none* character as an empty space.\n\n![Our empty Wagner-Fischer matrix — we'll be using this to calculate the Levenshtein distance between 'Levenshtein' and 'Livinshten'.](/images/semantic-search-5.png)\n\n<small>Our empty Wagner-Fischer matrix — we'll be using this to calculate the Levenshtein distance between **'Levenshtein'** and **'Livinshten'**.</small>\n\n{{< notebook file=\"levenshtein-init\" height=\"full\" >}}\n\n<small>Initializing our empty Wagner-Fischer matrix in code.</small>\n\nThen we iterate through every position in our matrix and apply that complicated formula we saw before.\n\nThe first step in our formulae is `if min(i, j) = 0` — all we're saying here is, out of our two positions `i` and `j`, are either `0`? If so, we move across to `max(i, j)` which tells us to assign the current position in our matrix the higher of the two positions `i` and `j`:\n\n![We start on the right, along the edges where i and/or j is 0, the matrix position will be populated with max(i, j).](/images/semantic-search-6.png)\n\n<small>We start on the right, along the edges where i and/or j is 0, the matrix position will be populated with **max(i, j)**.</small>\n\n{{< notebook file=\"levenshtein-max-ij\" height=\"full\" >}}\n\n<small>The min(i,j) == 0 followed by the max(i,j) operation visualized above — translated into code.</small>\n\nNow, we've dealt with the outer edges of our matrix — but we still need to calculate the inner values — which is where our optimal path will be found.\n\nBack to `if min(i, j) = 0` — what if neither are `0`? Then we move onto that complex part of the equation inside the `min {` section. We need to calculate a value for each row, then we take the **min**imum value.\n\nNow, we already know these values — they're in our matrix:\n\n![For each new position in our matrix, we take the minimum value from the three neighboring positions (circled — top-left).](/images/semantic-search-7.png)\n\n<small>For each new position in our matrix, we take the minimum value from the three neighboring positions (circled — top-left).</small>\n\n`lev(i-1, j)` and the other operations are all indexing operations — where we extract the value in that position. We then take the minimum value of the three.\n\nThere is just one remaining operation. The `+1` on the left should only be applied if `a[i] != b[i]` — this is the penalty for mismatched characters.\n\n![If a[i] != b[j] we add 1 to our minimum value — this is the penalty for mismatched characters.](/images/semantic-search-8.png)\n\n<small>If a[i] != b[j] we add 1 to our minimum value — this is the penalty for mismatched characters.</small>\n\nPlacing all of this together into an iterative loop through the full matrix looks like this:\n\n{{< notebook file=\"levenshtein-full\" height=\"full\" >}}\n\n<small>The full Levenshtein distance calculation using a Wagner-Fischer matrix.</small>\n\nWe've now calculated each value in the matrix — these represent the number of operations required to convert from string `a` up to position `i` to string `b` up to position `j`.\n\nWe're looking for the number of operations to convert `a` to `b` — so we take the bottom-right value of our array at `lev[-1, -1]`.\n\n![The optimal path through our matrix — in position [-1, -1] at the bottom-right we have the Levenshtein distance between our two strings.](/images/semantic-search-9.png)\n\n<small>The optimal path through our matrix — in position [-1, -1] at the bottom-right we have the Levenshtein distance between our two strings.</small>\n\n{{< notebook file=\"levenshtein-get-val\" height=\"full\" >}}\n\n---\n\n## Vector Similarity Search\n\nFor vector-based search, we typically find one of several vector building methods:\n\n- **TF-IDF**\n- **BM25**\n- word2vec/doc2vec\n- **BERT**\n- USE\n\nIn tandem with some implementation of *approximate* nearest neighbors (ANN), these vector-based methods are the MVPs in the world of similarity search.\n\nWe'll cover TF-IDF, BM25, and BERT-based approaches — as these are easily the most common and cover both sparse and dense [vector representations](/learn/vector-embeddings/).\n\n### 1. TF-IDF\n\nThe respected grandfather of vector similarity search, born back in the 1970s. It consists of two parts, **T**erm **F**requency (TF) and **I**nverse **D**ocument **F**requency (IDF).\n\nThe TF component counts the number of times a term appears within a document and divides this by the total number of terms in that same document.\n\n![](/images/semantic-search-10.png)\n\n<small>The term frequency (TF) component of TF-IDF counts the frequency of our query ('bananas') and divides by the frequency of all tokens.</small>\n\nThat is the first half of our calculation, we have the frequency of our **q**uery within the current **D**ocument `f(q,D)` — over the frequency of all **t**erms within the current **D**ocument `f(t,D)`.\n\nThe **T**erm **F**requency is a good measure, but doesn't allow us to differentiate between common and uncommon words. If we were to search for the word 'the' — using TF alone we'd assign this sentence the same relevance as had we searched 'bananas'.\n\nThat's fine until we begin comparing documents, or searching with longer queries. We don't want words like *'the'*,* 'is'*, or *'it'* to be ranked as highly as *'bananas'* or *'street'*.\n\nIdeally, we want matches between rarer words to score higher. To do this, we can multiply TF by the second term — IDF. The **I**nverse **D**ocument **F**requency measures how common a word is across *all* of our documents.\n\n![](/images/semantic-search-11.png)\n\n<small>The inverse document frequency (IDF) component of TF-IDF counts the number of documents that contain our query.</small>\n\nIn this example, we have three sentences. When we calculate the IDF for our common word *'is'*, we return a much lower number than that for the rarer word *'forest'*.\n\nIf we were to then search for both words *'is'* and *'forest'* we would merge TF and IDF like so:\n\n![We calculate the TF('is', D) and TF('forest', D) scores for docs a, b, and c.](/images/semantic-search-12.png)\n\n<small>We calculate the **TF('is', D)** and **TF('forest', D)** scores for docs **a**, **b**, and **c**. The IDF value is across all docs — so we calculate just **IDF('is')** and **IDF('forest')** once. Then, we get TF-IDF values for both words in each doc by **multiplying** the **TF** and **IDF** components. Sentence **a** scores highest for **'forest'**, and **'is'** always scores **0** as the **IDF('is')** score is **0**.</small>\n\n{{< notebook file=\"tf-idf-calculation\" height=\"full\" >}}\n\nThat's great, but where does *vector* similarity search come into this? Well, we take our vocabulary (a big list of all words in our dataset) — and calculate the TF-IDF for each and every word.\n\n![We calculate the TF-IDF value for every word in our vocabulary to create a TF-IDF vector. This process is repeated for each document.](/images/semantic-search-13.png)\n\n<small>We calculate the TF-IDF value for every word in our vocabulary to create a TF-IDF vector. This process is repeated for each document.</small>\n\nWe can put all of this together to create our TF-IDF vectors like so:\n\n{{< notebook file=\"tf-idf-vector\" height=\"full\" >}}\n\nFrom there we have our TF-IDF vector. It's worth noting that vocab sizes can easily be in the 20K+ range, so the vectors produced using this method are incredibly sparse — which means we cannot encode any semantic meaning.\n\n<a name=\"bm25\"> </a>\n### 2. BM25\n\nThe successor to TF-IDF, Okapi BM25 is the result of optimizing TF-IDF primarily to normalize results based on document length.\n\nTF-IDF is great but can return questionable results when we begin comparing several mentions\n\nIf we took two 500 word articles and found that article A mentions 'Churchill' six times, and article B mentions 'Churchill' twelve times — should we view article A as half as relevant? Likely not.\n\nBM25 solves this by modifying TF-IDF:\n\n![The BM25 formula.](/images/semantic-search-14.png)\n\n<small>The BM25 formula.</small>\n\nThat's a pretty nasty-looking equation — but it's nothing more than our TF-IDF formula with a few new parameters! Let's compare the two TF components:\n\n![The TF part of BM25 (left) compared to the TF of TF-IDF (right).](/images/semantic-search-15.png)\n\n<small>The TF part of BM25 (left) compared to the TF of TF-IDF (right).</small>\n\nAnd then we have the IDF part, which doesn't even introduce any new parameters — it just rearranges our old IDF from TF-IDF.\n\n![The IDF part of BM25 (left) compared to the IDF of TF-IDF (right).](/images/semantic-search-16.png)\n<small>The IDF part of BM25 (left) compared to the IDF of TF-IDF (right).</small>\n\nNow, what is the result of this modification? If we take a sequence containing 12 tokens, and gradually feed it more and more 'matching' tokens — we produce the following scores:\n\n![TF-IDF](/images/semantic-search-17.png)\n\n![BM25](/images/semantic-search-18.png)\n\n<small>Comparison of TF-IDF (top) and BM25 (bottom) algorithms using a sentence of 12 tokens, and an incremental number of relevant tokens (x-axis).</small>\n\nThe TF-IDF score increases linearly with the number of relevant tokens. So, if the frequency doubles — so does the TF-IDF score.\n\nSounds cool! But how do we implement it in Python? Again, we'll keep it nice and simple like the TF-IDF implementation.\n\n{{< notebook file=\"bm25\" height=\"full\" >}}\n\nWe've used the default parameters for `k` and `b` — and our outputs look promising. The query `'purple'` only matches sentence `a`, and `'bananas'` scores reasonable for both `b` and `c` — but slightly higher in `c` thanks to the smaller word count.\n\nTo build vectors from this, we do the exact same thing we did for TF-IDF.\n\n{{< notebook file=\"bm25-vecs\" height=\"full\" >}}\n\nAgain, just as with our TF-IDF vectors, these are *sparse* vectors. We will not be able to encode semantic meaning — but focus on syntax instead. Let's take a look at how we can begin considering semantics.\n\n### 3. BERT\n\nBERT — or Bidirectional Encoder Representations from Transformers — is a hugely popular transformer model used for *almost* everything in NLP.\n\nThrough 12 (or so) encoder layers, BERT encodes a huge amount of information into a set of *dense* vectors. Each dense vector typically contains 768 values — and we usually have 512 of these vectors for each sentence encoded by BERT.\n\nThese vectors contain what we can view as numerical representations of language. We can also extract those vectors — from different layers if wanted — but typically from the final layer.\n\nNow, with two correctly encoded [dense vectors](/learn/dense-vector-embeddings-nlp/), we can use a similarity metric like Cosine similarity to calculate their semantic similarity. Vectors that are more aligned are more semantically alike, and vise-versa.\n\n![A smaller angle between vectors (calculated with cosine similarity) means they are more aligned. For dense vectors, this correlates to greater semantic similarity.](/images/semantic-search-19.png)\n\n<small>A smaller angle between vectors (calculated with cosine similarity) means they are more aligned. For dense vectors, this correlates to greater semantic similarity.</small>\n\nBut there's one problem, each sequence is represented by 512 vectors — not one vector.\n\nSo, this is where another — brilliant — adaption of BERT comes into play. Sentence-BERT allows us to create a single vector that represents our full sequence, otherwise known as a *sentence vector* <sup>[2]</sup>.\n\nWe have two ways of implementing SBERT — the easy way using the `sentence-tranformers` library, or the slightly less easy way using `transformers` *and* PyTorch.\n\nWe'll cover both, starting with the `transformers` with PyTorch approach so that we can get an intuition for how these vectors are built.\n\nIf you've used the HF transformers library, the first few steps will look very familiar. We initialize our SBERT model and tokenizer, tokenize our text, and process our tokens through the model.\n\n{{< notebook file=\"sbert-process\" height=\"full\" >}}\n\nWe've added a new sentence here, sentence **g** carries the same *semantic* meaning as **b** — without the same keywords. Due to the lack of shared words, all of our previous methods would struggle to find similarity between these two sequences — remember this for later.\n\n{{< notebook file=\"sbert-last-hidden-state\" height=\"full\" >}}\n\nWe have our vectors of length 768 — but these are **not** *sentence vectors* as we have a vector representation for each token in our sequence (128 here as we are using SBERT — for BERT-base this is 512). We need to perform a **mean pooling** operation to create the sentence vector.\n\nThe first thing we do is multiply each value in our `embeddings` tensor by its respective `attention_mask` value. The `attention_mask` contains **ones** where we have 'real tokens' (eg not padding tokens), and **zeros** elsewhere — this operation allows us to ignore non-real tokens.\n\n{{< notebook file=\"sbert-mean-pooling\" height=\"full\" >}}\n\nAnd those are our sentence vectors, using those we can measure similarity by calculating the cosine similarity between each.\n\n{{< notebook file=\"sbert-cos-sim\" height=\"full\" >}}\n\nIf we visualize our array, we can easily identify higher similarity sentences:\n\n![Heatmap showing cosine similarity between our SBERT sentence vectors — the score between sentences b and g is circled.](/images/semantic-search-20.png)\n\n<small>Heatmap showing cosine similarity between our SBERT sentence vectors — the score between sentences b and g is circled.</small>\n\nNow, think back to the earlier note about sentences b and g having essentially identical meaning whilst not sharing *any* of the same keywords.\n\nWe'd hope SBERT and its superior semantic representations of language to identify these two sentences as similar — and lo-and-behold the similarity between both is our second-highest score at 0.66 (circled above).\n\nNow, **the alternative (easy) approach is to use sentence-transformers**. To get the exact same output as we produced above we write:\n\n{{< notebook file=\"sentence-transformers\" height=\"full\" >}}\n\nWhich, of course, is much easier.\n\n---\n\nThat's all for this walk through history with Jaccard, Levenshtein, and Bert!\n\nWe covered a total of **six** different techniques, starting with the straight-forward Jaccard similarity, w-shingling, and Levenshtein distance. Before moving onto search with sparse vectors — TF-IDF and BM25, and finishing up with state-of-the-art dense vector representations with SBERT.\n\n## References\n\n[1] [Market Capitalization of Alphabet (GOOG)](https://companiesmarketcap.com/alphabet-google/marketcap/), Companies Market Cap\n\n[2] N. Reimers, I. Gurevych, [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084) (2019), Proceedings of the 2019 Conference on Empirical Methods in 2019\n\n[Notebooks Repo](https://github.com/pinecone-io/examples/tree/master/semantic_search_intro)\n\nRunnable Colab notebooks: [Jaccard](https://colab.research.google.com/github/pinecone-io/examples/blob/master/semantic_search_intro/jaccard.ipynb), [Levenshtein](https://colab.research.google.com/github/pinecone-io/examples/blob/master/semantic_search_intro/levenshtein.ipynb), [TF-IDF](https://colab.research.google.com/github/pinecone-io/examples/blob/master/semantic_search_intro/tfidf.ipynb), [BM25](https://colab.research.google.com/github/pinecone-io/examples/blob/master/semantic_search_intro/bm25.ipynb), [SBERT](https://colab.research.google.com/github/pinecone-io/examples/blob/master/semantic_search_intro/sbert.ipynb)\n\n**All images are by the author except where stated otherwise*\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbce"
  },
  "filename": "clip.md",
  "title": "ebook-post",
  "category": "\"Multi-modal ML with OpenAI's CLIP\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Multi-modal ML with OpenAI's CLIP\"\nheadline: \"Multi-modal ML with OpenAI's CLIP\"\ncategories:\n  - Embedding Methods for Image Search\ntoc: >-\nweight: 9\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: An introduction to OpenAI's CLIP and multi-modal ML\n# Open graph\nimages: ['https://www.pinecone.io/images/clip-0.png']\n---\n\nLanguage models (LMs) can not rely on language alone. That is the idea behind the \"Experience Grounds Language\" paper, that proposes a framework to measure LMs' current and future progress. A key idea is that, beyond a certain threshold LMs need other forms of data, such as visual input [1] [2].\n\n![world-scopes](./images/clip-1.png)<small>**W**orld **S**copes (WS), as datasets become larger in scope and span multiple modalities, the capabilities of models trained with them increase.</small>\n\nThe next step beyond well-known language models; BERT, GPT-3, and T5 is *”World Scope 3”*. In World Scope 3, we move from large text-only datasets to large multi-modal datasets. That is, datasets containing information from multiple forms of media, like *both* images and text.\n\nThe world, both digital and real, is multi-modal. We perceive the world as an orchestra of language, imagery, video, smell, touch, and more. This chaotic ensemble produces an inner state, our \"model\" of the outside world.\n\nAI must move in the same direction. Even specialist models that focus on language or vision must, at some point, have input from the other modalities. How can a model fully understand the concept of the word \"person\" without *seeing* a person?\n\nOpenAI **C**ontrastive **L**earning **I**n **P**retraining (CLIP) is a world scope three model. It can comprehend concepts in both text and image and even connect concepts between the two modalities. In this chapter we will learn about multi-modality, how CLIP works, and how to use CLIP for different use cases like encoding, classification, and object detection.\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/fGwH2YoQkDM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n---\n\n## Multi-modality\n\nThe multi-modal nature of CLIP is powered by two encoder models trained to \"speak the same language\". Text inputs are passed to a text encoder, and image inputs to an image encoder [3]. These models then create a *vector representation* of the respective input.\n\nBoth models \"speak the same language\" by encoding similar concepts in text and images into similar vectors. That means that the text \"two dogs running across a frosty field\" would output a vector similar to an *image* of two dogs running across a frosty field.\n\n![multi-modal-similarity](./images/clip-2.png)\n\n<small>Similar text and images will be encoded into a similar vector space. Dissimilar text and images do not share a similar vector space.</small>\n\nWe can think of the language these models speak as the vector space in which they encode vectors. These two models can express nuanced information about text and images through this vector space. However, this \"vector language\" is far too abstract for us to directly understand.\n\nRather than directly reading this \"language\", we can train other simple neural networks to understand it and make predictions that we can understand. Or we use vector search to identify similar concepts and patterns across text and image domains.\n\nLet's take a look at an example of CLIP in action.\n\n### Text-to-Image Search\n\n<iframe src=\"https://hf.space/streamlit/pinecone/semantic-query-trainer/+\" data-src=\"https://hf.space/streamlit/pinecone/semantic-query-trainer/+\" data-sdk=\"streamlit\" title=\"Streamlit app\" style=\"width:100%;height:1000px;overflow: hidden;\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\" scrolling=\"yes\"></iframe>\n\nEntering a prompt in the search bar above allows us to search through images based on their *content* rather than any attached textual metadata. We call this **C**ontent **B**ased **I**mage **R**etrieval (CBIR).\n\nWith CBIR, we can search for specific phrases such as \"two dogs running across a frosty field\". We can even drop the word \"dogs\" and replace it with everyday slang for dogs like \"good boy\" or \"mans best friend\", and we return the same images showing dogs running across fields.\n\nCLIP can accurately understand language. It understands that *in the context* of running across a field, we are likely referring to dogs and do not literally mean good children or someone's \"human\" best friend.\n\nAmusingly, the dataset contains no images of the food hot dogs (other than one). So, suppose we search for \"hot dogs\". In that case, we first get an image containing a hot dog (and a dog), a dog looking toasty in a warm room, another dog looking warm with wooly clothing, and another dog posing for the camera. All of these portray a hot dog in one sense or another.\n\n---\n\n\n*After being processed by CLIP's text or image encoder, we are left with vectors. That means we can search across **any** modality with **any** modality; we can search in either direction. We can also stick to a single modality, like text-to-text or image-to-image.*\n\n---\n\nNow that we've seen what CLIP can do, let's take a look at *how* it can do this.\n\n## CLIP\n\nCLIP actually consists of two models trained in parallel. A 12-layer text transformer for building text embeddings and a ResNet or vision transformer (ViT) for building image embeddings [3].\n\n![clip-architecture](./images/clip-3.png)\n<small>Architecture diagram of CLIP with the text encoder and ViT *or* ResNet as the image encoder.</small>\n\nThe text encoder and image encoder (ResNet *or* ViT) output single vector embeddings for each text/image record fed into the encoders. All vectors are 512 dimensional and can be represented in the same vector space, meaning similar images and text produce vectors that appear near each other.\n\n### Contrastive Pretraining\n\nAcross both [**N**atural **L**anguage **P**rocessing (NLP)](https://www.pinecone.io/learn/nlp/) and computer vision (CV), large pretrained models dominate the SotA. The idea is that by giving a big model a lot of data, they can learn general patterns from the dataset.\n\nFor language models, that may be the general rules and patterns in the English language. For vision models, that may be the characteristics of different scenes or objects.\n\nThe problem with multi-modality is that these models are trained separately and, by default, have no understanding of one another. CLIP solves this thanks to image-text *contrastive pretraining*. With CLIP, text and image encoders are trained while considering the other modality and context. Meaning that the text and image encoders share an \"indirect understanding\" of patterns in both modalities; language and vision.\n\nContrastive pretraining works by taking a *(text, image)* pair – where the text describes the image – and learning to encode the pairs as closely as possible in vector space.\n\nFor this to work well, we also need negative pairs to provide a contrastive comparison. We need positive pairs that should output similar vectors and negative pairs that should output dissimilar vectors.\n\nThis is the general idea behind contrastive learning, which can be found in the training functions of many models, particularly those that produce embedding vectors.\n\nThe negative pairs can be extracted directly from positive pairs. If we have positive pairs $(T_1, I_1)$ and $(T_2, I_2)$, we simply swap the components, giving us the negative pairs $(T_1, I_2)$ and $(T_2, I_1)$.\n\nWith this, we can apply a loss function that maximizes the similarity between $(T_1, I_1)$ and $(T_2, I_2)$, and minimizes the similarity between $(T_1, I_2)$ and $(T_2, I_1)$. Altogether, this looks like this:\n\n![pretraining](./images/clip-4.png)\n<small>Contrastive pretraining with CLIP.</small>\n\nIn this image, we can see a single pretraining step on a single batch. The loss function assumes pairs in the diagonal should have a maximized dot product score, and all other pairs should have a minimized dot product score. Both text and image encoder models are optimized for this.\n\nA fundamental assumption is that there are no other positive pairs within a single batch. For example, we assume that \"two dogs running across a frosty field\" is only relevant to the image it is paired with. We assume there are no other texts or images with similar meanings.\n\nThis assumption is possible because the datasets used for pretraining are diverse and large enough that the likelihood of two similar pairs appearing in a single batch is negligible. Therefore, rare enough to have a little-to-no negative impact on pretraining performance.\n\n## Using CLIP\n\nWe have a good idea of what CLIP can be used for and how it is trained. With that, how can we get started with it?\n\nOpenAI released a few implementations of CLIP via the Hugging Face library; this is the fastest way to get started. First, we need to install the necessary libraries.\n\n```\npip install transformers torch datasets\n```\n\nBefore we can do anything with CLIP, we need some text and images. The `jamescalam/image-text-demo` dataset contains a small number of image-text pairs we can use in our examples.\n\n```python\nfrom datasets import load_dataset\n\ndata = load_dataset(\n    \"jamescalam/image-text-demo\",\n    split=\"train\"\n)\n```\n\n![text-image](./images/clip-5.png)\n<small>Example of text-image pair found in the dataset. Text is stored in the `\"text\"` feature and images in the `\"image\"` feature.</small>\n\nWith these sample records ready, we can move on to initializing CLIP and an image/text preprocessor like so:\n\n```python\nfrom transformers import CLIPProcessor, CLIPModel\nimport torch\n\nmodel_id = \"openai/clip-vit-base-patch32\"\n\nprocessor = CLIPProcessor.from_pretrained(model_id)\nmodel = CLIPModel.from_pretrained(model_id)\n\n# move model to device if possible\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmodel.to(device)\n```\n\nThe `model` is CLIP itself. Note that we use the ViT image encoder (the model is `clip-vit`). Text and image data cannot be fed directly into CLIP. The text must be preprocessed to create \"tokens IDs\", and images must be resized and normalized. The `processor` handles both of these functions.\n\n### Encoding Text\n\nWe will start with encoding text using the CLIP text transformer. Before feeding text into CLIP, it must be preprocessed and converted into token IDs. Let's take a batch of sentences from the `unsplash` data and encode them.\n\n{{< notebook file=\"clip-tokenize\" height=\"full\" >}}\n\nThis returns the typical text transformer inputs of `input_ids` and `attention_mask`.\n\nThe `input_ids` are token ID values where each token ID is an integer value ID that maps to a specific word or sub-word. For example the phrase *\"multi-modality\"* may be split into tokens *\\[\"multi\", \"-\", \"modal\", \"ity\"\\]*, which are then mapped to IDs *\\[1021, 110, 2427, 425\\]*.\n\nA text transformer maps these token IDs to semantic vector embeddings that the model learned during pretraining.\n\nThe `attention_mask` is a tensor of 1s and 0s used by the model's internal mechanisms to \"pay attention\" to real token IDs and ignore padding tokens.\n\n---\n\n*Padding tokens are a special type of token used by text transformers to create input sequences of a fixed length from sentences of varying length. They are appended to the end of shorter sentences, so \"hello world\" may become \"hello world \\[PAD\\] \\[PAD\\] \\[PAD\\]\".*\n\n---\n\nWe then use CLIP to encode all of these text descriptions with `get_text_features` like so:\n\n```python\ntext_emb = model.get_text_features(\n    **tokens\n)\n```\n\nOne important thing to note here is that these embeddings are *not* normalized. If we plan on using a similarity metric like the dot product, we must normalize the embeddings:\n\n{{< notebook file=\"clip-get-text-features\" height=\"full\" >}}\n\nAlternatively, we can use cosine similarity as our metric as this only considers angular similarity and not vector magnitude (like dot product). For our examples, we will normalize and use dot product similarity.\n\nWe now have our text embeddings; let's see how to do the same for images.\n\n### Encoding Images\n\nImages will be encoded using the ViT portion of CLIP. Similar to text encoding, we need to preprocess these images using the `preprocessor` like so:\n\n{{< notebook file=\"clip-image-preprocess\" height=\"full\" >}}\n\nPreprocessing images does *not* produce token IDs like those we saw from preprocessing our text. Instead, preprocessing images consists of resizing the image to a 244x244 array with three color channels (red, green, and blue) and normalizing pixel values into a $[0, 1]$ range.\n\nAfter preprocessing our images, we get the image features with `get_image_features` and normalize them as before:\n\n{{< notebook file=\"clip-get-image-features\" height=\"full\" >}}\n\nWith this, we have created CLIP embeddings for both text and images. We can move on to comparing items across the two modalities.\n\n### Calculating Similarity\n\nCLIP embedding similarities are represented by their angular similarity. Meaning we can identify similar pairs using cosine similarity:\n\n$$\ncossim(A, B) = \\frac{A \\cdot B}{||A|| * ||B||} = \\frac{\\sum_i^nA_iB_i}{\\sqrt{\\sum_i^nA_i^2} \\sqrt{\\sum_i^nB_i^2}}\n$$\n\nOr, if we have normalized the embeddings, we can use dot product similarity:\n\n$$\ndotproduct(A, B) = A \\cdot B = \\sum_{i=0}^{n-1}A_iB_i\n$$\nLet's try both. First, for cosine similarity, we do:\n\n{{< notebook file=\"clip-cosine-sim\" height=\"full\" >}}\n\nAnd if we perform the same operation for dot product similarity, we should return the same results:\n\n{{< notebook file=\"clip-dot-prod-sim\" height=\"full\" >}}\n\nBoth of these similarity score arrays look the same, and if we check for the difference between the two arrays, we will see that the scores are the same. We see some slight differences due to floating point errors.\n\n{{< notebook file=\"clip-sim-metrics\" height=\"full\" >}}\n\nUsing the embedding functions of CLIP in this way, we can perform a semantic search across the modalities of text and image in any direction. We can search for images with text, text with images, text with text, and images with images.\n\nThese use cases are great, but we can make slight modifications to this for many other tasks.\n\n### Classification\n\nOne of the most impressive demonstrations of CLIP is its unparalleled zero-shot performance on various tasks. For example, given the `fragment/imagenette` dataset from Hugging Face *Datasets*, we can write a list of brief sentences that align with the ten class labels.\n\n![label-to-sentence](./images/clip-6.png)<small>We take the original *imagenette* labels and preappend `\"a photo of a ...\"` to each to create a set of CLIP-friendly sentence representations.</small>\n\nFrom this, we can calculate the cosine similarity between the text embeddings of these ten labels against an image we'd like to classify. The text that returns the highest similarity is our predicted class.\n\n### Object Detection\n\nAnother compelling use case of zero-shot CLIP is object detection. We can do this by splitting our images into smaller patches and running each patch through the image encoder of CLIP. We then compare these patch embeddings to a text encoding describing what we are looking for. After calculating the similarity scores for all patches, we can collate them into a map of relevance.\n\nFor example, given an image of a butterfly and a cat, we could break it into many small patches. Given the prompt `\"a fluffy cat\"`, we will return an outline of the cat, whereas the prompt `\"a butterfly\"` will produce an outline of the butterfly.\n\n![patches](./images/clip-7.png)\n<small>Zero-shot object detection with CLIP allows us to find specific objects with natural language prompts.</small>\n\nThese are only a few of the use cases of CLIP and only scratch the surface of what is possible with this model and others in the scope of multi-modal ML.\n\n---\n\nThat's it for this introduction to multi-modal ML with OpenAI's CLIP. The past years since the CLIP release have seen ever more fascinating applications of the model.\n\nDALL-E 2 is a well-known example of CLIP. The incredible images generated by DALL-E 2 start by embedding the user’s text prompt with CLIP [4]. That text embedding is then passed to the diffusion model, which generates some mind-blowing images.\n\nThe fields of NLP and CV have mainly progressed independently of each other for the past decade. However, with the introduction of world scope three models, they're becoming more entwined into a majestic multi-modal field of Machine Learning.\n\n## Resources\n\n[1] Y. Bisk et al., [Experience Grounds Language](https://arxiv.org/abs/2004.10151) (2020), EMNLP\n\n[2] J. Alammar, [Experience Grounds Language: Improving language models beyond the world of text](https://www.youtube.com/watch?v=WQm7-X4gts4) (2022), YouTube\n\n[3] A. Radford et al., [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) (2021), arXiv\n\n[4] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, M. Chen, [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/abs/2204.06125) (2022), arXiv\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbcf"
  },
  "filename": "pods-for-performance.md",
  "title": "post",
  "category": "High-throughput vector indexes now generally available and free",
  "content": "---\nlayout: post\ntitle: High-throughput vector indexes now generally available and free\nheadline: High-throughput vector indexes now generally available and free\ncategories:\n  - Company News\ntoc: >-\nauthor:\n  name: Jeff Zhu\n  position: Staff Product Manager\n  src: /images/jeff-zhu.jpg\n  href: https://www.linkedin.com/in/jeffrey-s-zhu/\ndate: \"2022-12-08\"\n# Open Graph\ndescription: P2 pods are now generally available to all users, dot product compatible, and 50% lower in cost.\nimages: [\"/images/pods-for-performance-banner.png\"]\nthumbnail: \"/images/pods-for-performance-thumbnail-2.png\"\n---\n\nBack in August, we [announced](/learn/faster-easier-scalable/) a new performance pod type - p2 - in public preview. Different from the existing p1 pods, p2 pods use a new graph-based index to provide blazing fast search speeds (up to 10x faster than p1) and higher throughput (up to 200 QPS) per replica.\n\nToday, we are excited to announce that p2 pods are now generally available for all users. Since the preview announcement, we have made the pods even more efficient and accessible for high-throughput use cases. Along with being generally available, p2 pods are now:\n\n- **Available in the Starter (free) plan**: Users can now easily experiment with high-throughput use cases with a single p2 pod. Try a p2 pod today without any commitment!\n- **50% lower in cost**: We’ve continued to invest in optimizing p2 memory utilization throughout public preview. These optimizations allow us to reduce the p2 price by 50%, making p2 a more cost-competitive option than ever before.\n- **Dot product compatible**: Along with support of cosine and euclidean distance metrics, p2 pods now support dot product. Choose the metric that works best for you.\n\nUpdated pricing for p2 pods has been in effect since December 1, 2022, starting at $0.144/hour and up depending on plan, pod size, and cloud environment. See the [pricing page](/pricing/) for more details.\n\n## Deciding between pod types? When to consider p2\n\nIn general, p2 pods are designed for applications that require minimal latency (<10ms) and/or high throughput (>100 QPS). Examples of performance-focused use cases are movie recommendations on video streaming applications or personalization in social media feeds.\n\nHere are some sample query latencies using a single p2.x1 pod:\n\n<div class=\"responsive-table centered-table\">\n\n| Vector Dimension | # of vectors | TopK | P50 Query Latency | P95 Query Latency |\n| ---------------- | ------------ | ---- | ----------------- | ----------------- |\n| 128              | 1M           | 10   | 7.7 ms            | 9.6 ms            |\n| 768              | 1M           | 10   | 9.2 ms            | 11.1 ms           |\n| 2048             | 100k         | 100  | 8.3 ms            | 10.3 ms           |\n\n</div>\n\nVertically scaling p2 pods also improves each pod’s throughput. For example, with a single p2.x8 pod, you can support over 1000 QPS searching across 10 million 256 dimension vectors.\n\nAs always, your performance and accuracy may vary and we encourage you to test with your own data and follow our [tips for performance tuning](https://docs.pinecone.io/docs/performance-tuning). Performance is dependent on vector dimensionality, topK, filter conditions, cloud provider, and other factors.  \n\nIf you have high storage and low QPS requirements, consider using our [s1 pod type](https://docs.pinecone.io/docs/indexes#s1-pods).    \n\n## Get started\n\nCheck out the [documentation](https://docs.pinecone.io/docs/indexes#p2-pods) to learn more and how to start using p2 pods. We will share benchmarks against p1 in the near future, so stay tuned!\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbd0"
  },
  "filename": "haystack-lfqa.md",
  "title": "post",
  "category": "\"Long Form Question Answering in Haystack\"",
  "content": "---\nlayout: post\ntitle: \"Long Form Question Answering in Haystack\"\nheadline: \"Long Form Question Answering in Haystack\"\ncategories:\n  - Question Answering\ntoc: >-\nweight: 1\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: Learn how to build generative question-answering pipelines with Haystack.\n# Open graph\nimages: [\"/images/haystack-lfqa-0.png\"]\nthumbnail: \"https://www.pinecone.io/images/haystack-lfqa-0.png\"\n---\n\n**Q**uestion-**A**nswering (QA) has exploded as a subdomain of **N**atural **L**anguage **P**rocessing (NLP) in the last few years. QA is a widely applicable use case in NLP yet was out of reach until the introduction of [transformer models](/learn/transformers/) in 2017.\n\nWithout transformer models, the level of language comprehension required to make something as complex as QA work simply was not possible.\n\nAlthough QA is a complex topic, it comes from a simple idea. The automatic retrieval of information via a more human-like interaction. The task of information retrieval (IR) is performed by almost every organization in the world. Without other options, organizations rely on person-to-person IR and rigid keyword search tools. This haphazard approach to IR generates a lot of friction, particularly for larger organizations.\n\nConsider that many large organizations contain thousands of employees, each producing pages upon pages of unstructured text data. That data quickly gets lost in the void of unused directories and email archives.\n\nQA offers a solution to this problem. Rather than these documents being lost in an abyss, they can be stored within a space where an intelligent QA agent can access them. Unlike humans, our QA agent can scan millions of documents in seconds and return answers from these documents almost instantly.\n\nTo interact with a QA agent, we don't need to know any fancy search logic or code. Instead, we just ask a question as we would ask another human being. Suppose we want to understand why process X exists. In that case, we can ask, \"why do we follow process X?\" and the relevant information will be returned within milliseconds.\n\nQA capability is not a \"nice to have\". It is a key that can unlock ~90% of your organization's data. Without it, unstructured data is lost almost as soon as it is made, akin to searching in the dark.\n\nWith QA tools, employees can stop wasting time searching for snippets of information and focus on their *real*, value-adding tasks.\n\nA small investment in QA is, for most organizations, a no-brainer.\n\n---\n\n## Long-Form Question-Answering\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/O9lrWt15wH8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\nThere are [two overarching approaches to QA](/learn/question-answering/), *abstractive* (/*generative*) and *extractive*. The difference between the two lies in how the answer is constructed.\n\nFor *abstractive* QA, an answer is **generated** by an NLP *generator* model, usually based on external documents. Extractive QA differs from this approach. Rather than *generating* answers, it uses a *reader* model to **extract** them directly from external documents, similar to cutting out snippets from a newspaper.\n\nWe are using external documents to inform our generator/reader models in both cases. We call this *open-book* QA as it emulates open-book exams where students (the models) can refer to a book (external documents) during the exam. In our case, the void of unstructured data becomes our *open-book*.\n\nAn open-book abstractive QA pipeline looks like this:\n\n![lfqa-pipeline](/images/haystack-lfqa-1.png)\n\n<small>Open-book abstractive QA pipeline. We start by indexing documents into a document store, and then perform searches with queries encoded by a retriever model. The retrieved contexts and the initial query are passed to a generator to produce an answer.</small>\n\nOne form of open-book abstractive QA is **L**ong-**F**orm **Q**uestion-**A**nswering (LFQA). LFQA focuses on the generation of multi-sentence answers to open-ended questions.\n\nLet's work through an implementation of LFQA using the [Haystack library](https://github.com/deepset-ai/haystack).\n\n## LFQA in Haystack\n\nHaystack is a popular Python library for building QA pipelines, allowing us to create an LFQA pipeline with just a few lines of code. You can find the [full LFQA code here](https://github.com/pinecone-io/examples/tree/master/integrations/haystack).\n\nWe start by installing the necessary libraries.\n\n```bash\n!pip install -U 'farm-haystack[pinecone]'>=1.3.0 pinecone-client datasets\n```\n\n### Data Preparation\n\nOur first task is to find a dataset to emulate our *\"void\"* of unstructured data. For that, we will use the Wikipedia Snippets dataset. The full dataset contains over 17M passages from Wikipedia, but for this demo, we will restrict the dataset to ~50K passages. Feel free to use the whole dataset, but it will take some time to process.\n\n```python\nfrom datasets import load_dataset\n\nwiki_data = load_dataset(\n    'vblagoje/wikipedia_snippets_streamed',\n    split='train',\n    streaming=True\n)\n```\n\nWe can find the dataset in Hugging Face's *Datasets* library. The `streaming=True` parameter allows us to *stream* the dataset rather than download it. The full dataset is over 9GB, and we don't need it all; `streaming` allows us to iteratively download records one at a time.\n\nThe dataset contains *eight* features, of which we are most interested in the `passage_text` and `section_title`.\n\n{{< notebook file=\"lfqa-show-data\" height=\"full\" >}}\n\nAs we are limiting our dataset to ~50K passages, We will tighten the scope of topics and only extract records where the `section_title` feature is `History`.\n\n```python\n# filter only documents with History as section_title\nhistory = wiki_data.filter(\n    lambda d: d['section_title'].startswith('History')\n)\n```\n\nOur dataset is now prepared. We can move on to initializing the various components in our LFQA pipeline.\n\n### Document Store\n\nThe document store is (not surprisingly) where we *store* our *documents*. Haystack allows us to use various document stores, each with its pros and cons. A key consideration is whether we want to support a sparse keyword search or enable a full [*semantic search*](/learn/sentence-embeddings/). Naturally, a human-like QA system requires full semantic search capability.\n\nWith that in mind, we must use a document store that supports [dense vectors](/learn/dense-vector-embeddings-nlp/). If we'd like to scale to larger datasets, we must also use a document store that supports [**A**pproximate **N**earest **N**eighbors (ANN) search](/learn/vector-indexes/).\n\nTo satisfy these requirements, we use the `PineconeDocumentStore`, which also supports:\n\n* Single-stage metadata filtering, if using different `section_title` documents, we could use metadata filtering to tighten the search scope.\n* Instant index updates, meaning we can add millions of new documents and immediately see these new documents reflected in new queries.\n* Scalability to billions of documents.\n* Free hosting for up to 1M documents.\n\nWe first need to sign up for a [free API key](https://app.pinecone.io/). After signing up, API keys can be found by clicking on a project and navigating to *API Keys*. Next, we initialize the document store using:\n\n{{< notebook file=\"lfqa-doc-store-init\" height=\"full\" >}}\n\nHere we specify the name of the `index` where we will store our documents, the `similarity` metric, and the embedding dimension `embedding_dim`. The similarity metric and embedding dimension can change depending on the *retriever* model used. However, most retrievers use `\"cosine\"` and `768`.\n\nWe can check the current document and embedding count of our document store like so:\n\n{{< notebook file=\"lfqa-get-counts1\" height=\"full\" >}}\n\nIf there is an existing index called `haystack-lfqa`, the above will connect to the existing index rather than initialize a new one. Existing indexes can be found and managed by visiting the active project in the [Pinecone dashboard](https://app.pinecone.io/).\n\nWe can start adding documents to our document store. To do this, we first create Haystack `Document` objects, where we will store the text *content* alongside some *metadata* for each document. The indexing process will be done in batches of `10_000`.\n\n```python\nfrom haystack import Document\nfrom tqdm.auto import tqdm  # progress bar\n\ntotal_doc_count = 50000\nbatch_size = 10000\n\ncounter = 0\ndocs = []\nfor d in tqdm(history, total=total_doc_count):\n    # create haystack document object with text content and doc metadata\n    doc = Document(\n        content=d[\"passage_text\"],\n        meta={\n            \"article_title\": d[\"article_title\"],\n            'section_title': d['section_title']\n        }\n    )\n    docs.append(doc)\n    counter += 1\n    if counter % batch_size == 0:\n        # writing docs everytime 10k docs are reached\n        document_store.write_documents(docs)\n        docs.clear()\n    if counter == total_doc_count:\n        break\n```\n\nNow, if we check the *document* count, we will see that ~50K documents have been added:\n\n{{< notebook file=\"lfqa-get-counts2\" height=\"full\" >}}\n\nWhen looking at the *embedding* count, we still see *zero*; this reflects  the embedding count in the Pinecone dashboard. This is because we have not created any *embeddings* of our documents. That is another step that requires the *retriever* component.\n\n### Retriever\n\nThe QA pipeline relies on retrieving relevant information from our document store. In reality, this document store is what is called a *vector database*. Vector databases store *vectors* (surprise!), and each vector represents a single document's text content (the *context*).\n\n![lfqa-semantic-similarity](/images/haystack-lfqa-2.png)\n\n<small>Queries and contexts with similar meaning are embedding into a similar vector space.</small>\n\nUsing the retriever model, we can take text content and encode it into a vector embedding that numerically represents the text's original *\"human\"* meaning.\n\nThe vector database is where we store these vectors. If we introduce a new *query* vector to an already populated vector database, we could use similarity metrics to measure its proximity to existing vectors. From there, we return the top *k* most similar vectors (e.g., the most *semantically* similar contexts).\n\n![lfqa-sim-search](/images/haystack-lfqa-3.png)\n\n<small>We return the top *k* (in this case, `k=5`) most similar context vectors to our query vector. This visual demonstrates the top k vectors using Euclidean (or L2) distance, other common metrics include cosine similarity and dot product.</small>\n\nWe will use Haystack's `EmbeddingRetriever` component, which allows us to use any retriever model from the *Sentence Transformers* library hosted via the [Hugging Face Model Hub](https://huggingface.co/models?sort=downloads&search=sentence-transformers).\n\n![lfqa-flax-model](/images/haystack-lfqa-4.png)\n\n<small>The model we use on [Hugging Face Model Hub](https://huggingface.co/flax-sentence-embeddings/all_datasets_v3_mpnet-base).</small>\n\nFirst, we check that we are using the GPU, as this will make the retriever embedding process *much faster*.\n\n```python\nimport torch\n# confirm GPU is available, outputs True if so\ntorch.cuda.is_available()\n```\n\nThe embedding step will still run if you do not have access to a CUDA-enabled GPU, but it may be slow. We initialize the `EmbeddingRetriever` component with the `all_datasets_v3_mpnet-base` model shown above.\n\n```python\nfrom haystack.retriever.dense import EmbeddingRetriever\n\nretriever = EmbeddingRetriever(\n   document_store=document_store,\n   embedding_model=\"flax-sentence-embeddings/all_datasets_v3_mpnet-base\",\n   model_format=\"sentence_transformers\"\n)\n```\n\nWe call the `document_store.update_embeddings` method and pass in our new `retriever` to begin the embedding process.\n\n```python\ndocument_store.update_embeddings(\n   retriever,\n   batch_size=128\n)\n```\n\nThe `batch_size` parameter can be increased to reduce the embedding time. However, it is limited by GPU/CPU hardware and cannot be increased beyond those limits.\n\nWe can confirm that our document store now contains the embedded documents by calling `document_store.get_embedding_count()` or checking the embedding count in the Pinecone dashboard.\n\n![lfqa-pinecone-vecs](/images/haystack-lfqa-5.png)\n\n<small>Screenshot from the Pinecone dashboard showing *49,995* vectors have been stored. Calling `document_store.get_embedding_count()` will return the same number.</small>\n\nBefore moving on to the next step, we can test our document retrieval:\n\n{{< notebook file=\"lfqa-test-retrieval\" height=\"full\" >}}\n\nIt looks like we're returning good results. We now have the first two components of our LFQA pipeline: the document store and retriever. Let's move on to the final component.\n\n### Generator\n\nThe generator is the component that builds our answer. Generators are sequence-to-sequence (*Seq2Seq*) models that take the query and retrieved contexts as input and use them to generate an output, the answer.\n\n![lfqa-query-contexts](/images/haystack-lfqa-6.png)\n\n<small>The input to our generator model is a concatenation of the question and any retrieved contexts. In this example each context is preceded by *\"<P>\"* to mark that the following is a new chunk of information for the generator to consider.</small>\n\nWe initialize the generator using Haystack's `Seq2SeqGenerator` with a model trained specifically for LFQA, for example, `vblagoje/bart_lfqa` or `yjernite/bart_eli5` [1].\n\n```python\nfrom haystack.generator.transformers import Seq2SeqGenerator\n\ngenerator = Seq2SeqGenerator(model_name_or_path=\"vblagoje/bart_lfqa\")\n```\n\nNow we can initialize the entire abstractive QA pipeline using Haystack's `GenerativeQAPipeline` object. This pipeline combines all three components, with the document store included as part of the `retriever`.\n\n```python\nfrom haystack.pipelines import GenerativeQAPipeline\n\npipe = GenerativeQAPipeline(generator, retriever)\n```\n\nWith that, our abstractive QA pipeline is ready, and we can move on to making some queries.\n\n## Querying\n\nWhen querying, we can specify the number of contexts for our retriever to return and the number of answers for our generator to generate using the `top_k` parameters.\n\n{{< notebook file=\"lfqa-ask\" height=\"full\" >}}\n\nThere's a lot here, but we can see that we have `1` final answer, followed by the `3` retrieved contexts. It's hard to understand what is happening here, so we can use Haystack's `print_answer` util to clean up the output.\n\n{{< notebook file=\"lfqa-ask2\" height=\"full\" >}}\n\nOur output is now much more readable. The answer looks good, but there is not much detail. When we find an answer is either not good or lacks detail, there can be two reasons for this:\n\n* The generator model has not been trained on data that includes information about the *\"war on currents\"*, so it has not *memorized* this information within its model weights.\n\n* We have not returned any contexts that contain the answer, so the generator has no reliable external sources of information.\n\nIf neither condition is satisfied, the generator cannot produce a factually correct answer. However, in our case, we are returning some good external context. We can try and return more detail by increasing the number of contexts retrieved.\n\n{{< notebook file=\"lfqa-ask3\" height=\"full\" >}}\n\nNow we're seeing much more information. The latter half does descend into nonscensical gibberish, most likely because the higher `top_k` value retrieved several irrelevant contexts. However, given a larger dataset we could likely avoid this. We can also compare these results to an answer generated *without* any context by querying the generator directly.\n\n{{< notebook file=\"lfqa-ask4\" height=\"full\" >}}\n\nClearly, the retrieved contexts are important. Although this isn't always the case, for example, if we ask about a more well-known fact:\n\n{{< notebook file=\"lfqa-ask5\" height=\"full\" >}}\n\nFor general knowledge queries, the generator model can often pull the answer directly from its own *\"memory\"* (the model weights optimized during training), where it may have seen training data containing the answer. Larger models have a larger memory and, in turn, are better at direct answer generation.\n\nWhen we ask more specific questions, like our question about the war on currents, both smaller and larger generators rarely return good answers without an external data source.\n\nWe can ask a few more questions:\n\n{{< notebook file=\"lfqa-ask6\" height=\"full\" >}}\n\nTo confirm that this answer is correct, we can check the contexts used to generate the answer.\n\n{{< notebook file=\"lfqa-ask6-contexts\" height=\"full\" >}}\n\nIn this case, the answer looks correct. If we ask a question and no relevant contexts are retrieved, the generator will typically return nonsensical or false answers, like with this question about COVID-19:\n\n{{< notebook file=\"lfqa-ask7\" height=\"full\" >}}\n\nThe issue with nonsensical or false answers is one drawback of the LFQA approach. However, it can be mitigated somewhat by implementing thresholds to filter our low confidence answers and referring to the sources behind generated answers.\n\nLet's finish with a final few questions.\n\n{{< notebook file=\"lfqa-ask8\" height=\"full\" >}}\n\n\n\nThat's it for this walkthrough of Long-Form Question-Answering with Haystack. As mentioned, there are many approaches to building a pipeline like this. Many different retriever and generator models can be tested by simply switching the model names for other retriever/generator models.\n\nWith the wide variety of off-the-shelf models, many use cases can be built with little more than what we have worked through here. All that is left is to find potential use cases and try implementing LFQA (or [other QA pipelines](/learn/question-answering/)) and reap the benefits of enhanced data visibility and workplace efficiency that come with it.\n\n## Resources\n\n[1] A. Fan, et al., [ELI5: Long Form Question Answering](https://arxiv.org/abs/1907.09190) (2019)\n\n[Haystack Example Notebooks](https://github.com/pinecone-io/examples/tree/master/integrations/haystack)\n\n[PineconeDocumentStore Integration Docs](/docs/integrations/haystack/)\n\n[Haystack Github Repo](https://github.com/deepset-ai/haystack)"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbd1"
  },
  "filename": "softmax-activation.md",
  "title": "post",
  "category": "\"Softmax Activation Function",
  "content": "---\nlayout: post\ntitle: \"Softmax Activation Function: Everything You Need to Know\"\nheadline: \"Softmax Activation Function: Everything You Need to Know\"\ncategories:\n  - Algorithms & Libraries\ntoc: >-\nweight: 1\nauthor:\n  name: Bala Priya C\n  position: Technical Writer\n  src: /images/bala-priya.jpg\n  href: https://www.linkedin.com/in/bala-priya/\ndescription: \"Softmax Activation Function: Everything You Need to Know\"\n# Open graph\nimages: ['/images/softmax-activation.png']\n---\n\n![Softmax Activation](/images/softmax-activation.png)\n\nHave you ever trained a neural network to solve the problem of multiclass classification? If yes, you know that the raw outputs of the neural network are often very difficult to interpret. The **softmax activation function** simplifies this for you by making the neural network’s outputs easier to interpret!\n\nThe softmax activation function transforms the raw outputs of the neural network into a vector of *probabilities*, essentially a probability distribution over the input classes. Consider a multiclass classification problem with `N` classes. The softmax activation returns an output vector that is `N` entries long, with the entry at index `i` corresponding to the probability of a particular input belonging to the class `i`.\n\nIn this tutorial, you’ll learn all about the softmax activation function. You’ll start by reviewing the basics of multiclass classification, then proceed to understand why you cannot use the sigmoid or argmax activations in the output layer for multiclass classification problems.\n\nFinally, you’ll learn the mathematical formulation of the softmax function and implement it in Python.\n\nLet’s get started.\n\n## Multiclass Classification Revisited\n\nRecall that in *binary* classification, there are *only two* possible classes. For example, a ConvNet trained to classify whether or not a given image is a panda is a binary classifier, whereas, in *multiclass* classification, there are *more than* two possible classes. \n\nLet’s consider the following example: You’re given a dataset containing images of pandas, seals, and ducks. You’d like to train a neural network to predict whether a previously unseen image is that of a seal, a panda, or a duck. \n\nNotice how the input class labels below are one-hot encoded, and the classes are *mutually exclusive*. In this context, mutual exclusivity means that a given image can only be *one* of {seal, panda, duck} at a time.\n\n![Multiclass Classification](/images/multiclass-classification.png)\n<small>Multiclass Classification Example (Image by the author)</small>\n\n## Can You Use Sigmoid or Argmax Activations Instead?\n\nIn this section, you’ll learn why the sigmoid and argmax functions are not the optimal choices for the output layer in a multiclass classification problem.\n\n### Limitations of the Sigmoid Function \n\nMathematically, the sigmoid activation function is given by the following equation, and it squishes all inputs onto the range [0, 1].\n\n![Sigmoid Function Equation](/images/sigmoid-func-formula.png)\n<small>Sigmoid Function Equation (Image by the author)</small>\n\nThe sigmoid function takes in any *real* number as the input and maps it to a number between 0 and 1. This is exactly why it’s well-suited for binary classification. \n\n▶️ You may run the following code cell to plot the values of the sigmoid function over a range of numbers.\n\n```python\nimport numpy as np\nimport seaborn as sns\n\ndef sigmoid(x):\n  exp_x = np.exp(x)\n  return np.divide(exp_x,(1 + exp_x))\n  \nx = np.linspace(-10,10,num=200)\nexp_x = np.exp(x)\nsigmoid_arr = sigmoid(x)\n\nsns.set_theme()\nsns.lineplot(x = x,y = sigmoid_arr).set(title='Sigmoid Function')\n```\n\n![Sigmoid Function Plot](/images/sigmoid-plot.png)\n<small>Plot of the Sigmoid Function</small>\n\nLet’s go back to our example of classifying whether an input image is that of a panda or not. In this case, let z be the raw output of the neural network. If σ(z) is the probability that the given image belongs to class 1 (is a panda), then  1 - σ(z) is the probability that the given image does not belong to class 1 and is not a panda. You can think of σ(z) as a *probability score*.\n\nYou can now fix a threshold, say T, and predict that class whose probability score is *greater* than the chosen threshold.\n\nHowever, this won't quite work when you have more than two classes. Softmax to the rescue!  \n\nIn fact, you can think of the softmax function as a **vector generalization** of the sigmoid activation. We’ll revisit this later to confirm that for *binary* classification—when N = 2—the softmax and sigmoid activations are *equivalent*.\n\n### Limitations of the Argmax Function \n\nThe argmax function returns the **index** of the maximum value in the input array. \n\nLet's suppose the neural network’s raw output vector is given by **z** = [0.25, 1.23, -0.8]. In this case, the maximum value is 1.23 and it occurs at index 1. In our image classification example, index 1 corresponds to the second class—and the image is predicted to be that of a panda.\n\nIn vector notation, you’ll have 1 at the index where the maximum occurs (at index 1 for the vector *z*). And you’ll have 0 at all other indices.\n\n![Argmax Output](/images/argmax-output.png)\n<small>Argmax Output (Image by the author)</small>\n\nOne limitation with using the argmax function is that its *gradients* with respect to the raw outputs of the neural networks are always *zero*. As you know, it’s the [backpropagation of gradients](https://cs231n.github.io/optimization-2/) that facilitates the learning process in neural networks.\n\nAs you’ll have to plug in the value 0 for all gradients of the argmax output during backpropagation, you cannot use the argmax function in training. Unless there's backpropagation of gradients, the parameters of the neural network cannot be adjusted, and there's effectively no learning!\n\nFrom a probabilistic viewpoint, notice how the argmax function puts all the mass on index 1: the predicted class and 0 elsewhere. So it's straightforward to infer the predicted class label from the argmax output. However, we would like to know how likely the image is to be that of a panda, a seal, or a duck, and the softmax scores help us with just that!\n\n## The Softmax Activation Function, Explained\n\nIt's finally time to learn about softmax activation. The softmax activation function takes in a vector of **raw outputs** of the neural network and returns a vector of **probability scores**.\n\nThe equation of the softmax function is given as follows:\n\n![Softmax Function Equation](/images/softmax-formula.png)\n<small>Softmax Function Equation (Image by the author)</small>\n\nHere,\n- **z** is the vector of raw outputs from the neural network \n- The value of e ≈ 2.718\n- The i-th entry in the softmax output vector softmax(**z**) can be thought of as the predicted probability of the test input belonging to class i.\n\nFrom the plot of e^x, you can see that, regardless of whether the input x is positive, negative, or zero, e^x is always a positive number.\n\n![Plot of expx](/images/plot-of-expx.png)\n<small>Plot of exp(x)</small>\n\nRecall that in our example, N = 3 as we have 3 classes: {seal, panda, duck}, and the valid indices are 0, 1, and 2. Suppose you’re given the vector **z** = [0.25, 1.23, -0.8] of raw outputs from the neural network.\n\nLet's apply the softmax formula on the vector **z**, using the steps below: \n1. Calculate the exponent of each entry.\n2. Divide the result of step 1 by the sum of the exponents of all entries.\n\n![Computing softmax scores](/images/computing-softmax-scores.png)\n<small>Computing softmax scores for the 3 classes (Image by the author)</small>\n\n▶️ Now that we’ve computed the softmax scores, let’s collect them into a vector for succinct representation, as shown below:\n\n![Softmax Output](/images/softmax-output.png)\n<small>Softmax Output (Image by the author)</small>\n\nFrom the softmax output above, we can make the following observations:\n\n- In the vector **z** of raw outputs, the maximum value is 1.23, which on applying softmax activation maps to 0.664: the largest entry in the softmax output vector. Likewise, 0.25 and -0.8 map to 0.249 and 0.087: the second and the third largest entries in the softmax output respectively. Thus, applying softmax preserves the *relative ordering* of scores.\n- All entries in the softmax output vector are between 0 and 1.\n- In a multiclass classification problem, where the classes are mutually exclusive, notice how the entries of the softmax output sum up to **1**: 0.664 + 0.249 + 0.087 = 1.\n\nThis is exactly why you can think of softmax output as a probability distribution over the input classes, that makes it *readily interpretable*.\n\nAs a next step, let's examine the softmax output for our example. \n\nIn the vector softmax(**z**) = [0.664, 0.294, 0.087],  0.664 at index 1 is the largest value. This means there’s a 66.4% chance that the given image belongs to class 1, which from our one-hot encoding is a class *panda*.\n\nAnd the input image has a 29.4% chance of being a seal and around 8.7% chance of being a duck. \n\nTherefore, applying softmax gives *instant* interpretability, as you know how *likely* the test image is to belong to each of the 3 classes. In this particular example, it’s *highly likely* to be a panda and *least likely* to be a duck.\n\nIt now makes sense to call the argmax function on the softmax output to get the predicted class label. As the predicted class label is the one with the highest probability score, you can use `argmax(softmax(z))` to obtain the predicted class label. In our example, the highest probability score of 0.664 occurs at index 1, corresponding to class 1 (panda).\n\n### How to Implement the Softmax Activation in Python\n\nIn the previous section, we did some simple math to compute the softmax scores for the output vector **z**. \n\nNow let's translate the math operations into equivalent operations on NumPy arrays. You may use the following code snippet to get the softmax activation for any vector **z**.\n\n```python\nimport numpy as np\n\ndef softmax(z):\n  '''Return the softmax output of a vector.'''\n  exp_z = np.exp(z)\n  sum = exp_z.sum()\n  softmax_z = np.round(exp_z/sum,3)\n  return softmax_z\n```\n\nWe can parse the definition of the softmax function:\n- The function takes in one required parameter **z**, a vector, and returns the softmax output vector `softmax_z`.\n- We use `np.exp(z)` to compute `exp(z)` for each `z` in **z**; call the resultant array `exp_z`.\n- Next, we call sum on the array exp_z to compute the sum of exponents.\n- We then divide each entry in exp_z by the sum and round off the result to 3 decimal places, storing the result in a variable, say, `softmax_z`.\n- Finally, the function returns the array `softmax_z`.\n\nYou may now call the function with the output array z as the argument and verify that the scores are identical to what we had computed manually.\n\n```python\nz = [0.25, 1.23, -0.8]\nsoftmax(z)\n\n# Output\narray([ 0.249, 0.664, 0.087])\n```\n\nAre you wondering if normalizing each value by the sum of entries will suffice, to get relative scores? Let's see why it’s not an efficient solution.\n\n### Why Won't Normalization by the Sum Suffice\n\nWhy use something math-heavy as the softmax activation? Can we not just divide each of the output values by the sum of all outputs?\n\nWell, let's try to answer this by taking a few examples.\n\nUse the following function to return the array normalized by the sum.\n\n```python\ndef div_by_sum(z):\n  sum_z = np.sum(z)\n  out_z = np.round(z/sum_z,3)\n  return out_z\n```\n1️⃣ Consider **z1** = [0.25, 1.23, -0.8], and call the function `div_by_sum`. In this case, though the entries in the returned array sum up to 1, it has both positive and negative values. We still aren’t able to interpret the entries as probability scores.\n\n```python\nz1 = [0.25,1.23,-0.8]\ndiv_by_sum(z1)\n\n# Output\narray([ 0.368,  1.809, -1.176])\n```\n\n2️⃣ Let **z2** = [-0.25, 1, -0.75]. In this case, all elements in the vector sum up to zero, so the denominator will always be 0. When you divide by the sum to normalize, you’ll face runtime warnings, as division by zero is not defined.\n\n```python\nz2 = [-0.25,1,-0.75]\ndiv_by_sum(z2)\n\n# Output\nRuntimeWarning: divide by zero encountered in true_divide\narray([-inf,  inf, -inf])\n```\n\n3️⃣ In this example, **z3** = [0.1, 0.9, 0.2]. Let’s check both the softmax and normalized scores.\n\n```python\nz3 = [0.1,0.9,0.2] # ratio: 1:9:2\nprint(div_by_sum(z3))\nprint(softmax(z3))\n\n# Output\n[0.083 0.75  0.167] # ratio: 1:9:2\n[0.231 0.514 0.255]\n```\nAs shown in the code cell above, when all the inputs are positive, you may interpret the normalized scores as probability scores, but the scores are in the same ratio as in the array **z3**. In this example, the predicted class is still that of a panda.\n\nHowever, you can’t guarantee that the neural network’s raw output won’t sum up to 0 or have negative entries.\n\n4️⃣ In this example,  **z4** = [0, 0.9, 0.1]. Let’s check both the softmax and normalized scores.\n\n```python\nz4 = [0,0.9,0.1]\nprint(div_by_sum(z4))\nprint(softmax(z4))\n\n# Output\n[0.  0.9 0.1]\n[0.219 0.539 0.242]\n```\nAs you can see, when one of the entries is 0, upon calling the `div_by_sum` function, the entry is still 0 in the normalized array. However, in the softmax output, you can see that 0 has been mapped to a score of 0.219.\n\nIn some sense you can think of the softmax activation function as a softer version of the argmax function: It *maximizes* the probability score corresponding to the predicted output label. At the same time, it's *soft* because it does assign some probability mass to the less likely classes as well, unlike the argmax function that puts the entire probability mass of 1 on the maximum, and 0 everywhere else. \n\nIn essence, the softmax activation can be perceived as a smooth approximation to the argmax function.\n\n## Equivalence of the Sigmoid, Softmax Activations for N = 2\n\nNow let's revisit our earlier claim that the sigmoid and softmax activations are equivalent for binary classification when N = 2.\n\nRecall that in binary classification, you apply the sigmoid function to the neural network’s output to get a value in the range [0, 1].\n\nWhen you’re using the softmax function for multiclass classification, **the number of nodes in the output layer = the number of classes N**.\n\nYou can think of binary classification as a special case of multiclass classification. Assume that the output layer has two nodes: one outputting the score z and the other 0.\n\nEffectively, there’s *only one* node as the other is not given any weight at all. The raw output vector now becomes **z** =  [z, 0]. Next, we may go ahead and apply softmax activation on this vector **z** and check how it’s equivalent to the sigmoid function we looked at earlier.\n\n![Equivalence of Sigmoid & Softmax Activations](/images/sigmoid-softmax-equiv.png)\n<small>Equivalence of Sigmoid & Softmax Activations (Image by the author)</small>\n\nObserve how the softmax activation scores in this case are the same as the sigmoid activation scores: σ(z) and 1 - σ(z).\n\nAnd with this, we wrap up our discussion on the softmax activation function. Let’s quickly summarize all that we’ve learned.\n\n## Summing Up\n\nIn this tutorial, you’ve learned the following:\n- How to use the softmax function as output layer activation in a multiclass classification problem.\n- The working of the softmax function—how it transforms a vector of raw outputs into a vector of probabilities. And how you can interpret each entry in the softmax output as the probability of the corresponding class.\n- How to interpret the softmax activation as an extension of the sigmoid function to multiclass classification, and their equivalence for binary classification where the number of classes N = 2. \n\nIn the next tutorial, we’ll delve deep into [cross-entropy loss](/learn/cross-entropy-loss/)—a widely-used metric to assess how well your multiclass classification model performs.\n\nUntil then, check out other interesting [NLP tutorials on vector search, algorithms, and more](/learn/). Happy learning!\n\n{{< newsletter text=\"Subscribe for more deep learning tutorials!\" inputText=\"Email address...\" buttonText=\"Submit\">}}\n\n## Further Reading\n\n[1] [Lesson on Backpropagation, Chain Rule, and Vectorization from CS231n](https://cs231n.github.io/optimization-2/)\n\n[2] [Layer Activation Functions: Keras API Reference](https://keras.io/api/layers/activations/)\n\n[3] [Implementation of Softmax Regression from Scratch](https://d2l.ai/chapter_linear-networks/softmax-regression-scratch.html)"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbd2"
  },
  "filename": "color-histograms.md",
  "title": "ebook-post",
  "category": "\"Color Histograms in Image Retrieval\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Color Histograms in Image Retrieval\"\nheadline: \"Color Histograms in Image Retrieval\"\ncategories:\n  - Embedding Methods for Image Search\ntoc: >-\nweight: 1\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: An introduction to color histograms, one of the earliest image retrieval techniques\n# Open graph\nimages: ['https://www.pinecone.io/images/color-histograms-0.jpg']\n---\n\nBrowsing, searching, and retrieving images has never been easy. Traditionally, many technologies relied on manually appending metadata to images and searching via this metadata. This approach works for datasets with high-quality annotation, but most datasets are too large for manual annotation.\n\nThat means any large image dataset must rely on **C**ontent-**B**ased **I**mage **R**etrieval (CBIR). Search with CBIR focuses on comparing the *content* of an image rather than its metadata. Content can be color, shapes, textures – or with some of the latest advances in ML — the *\"semantic meaning\"* behind an image.\n\nColor histograms represent one of the first CBIR techniques, allowing us to search through images based on their color profiles rather than metadata.\n\n![color-histograms-1](/images/color-histograms-1.png)\n\n<small>Using the top query image we return the top five most similar images (including the same image) based on their color profiles.</small>\n\nThese examples demonstrate the core idea of color histograms. That is, we take an image, translate it into color-based histograms, and use these histograms to retrieve images with similar color profiles.\n\nThere are many pros and cons to this technique, as we will outline later. For now, be aware that this is a one of the earliest methods for CBIR, and many newer methods may be more useful (particularly for more advanced use-cases). Let's begin by focusing on understanding color histograms and how we can implement them in Python.\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/I3na13AESjw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n---\n\n*The original code notebooks covering the content of this article can be [found here](https://github.com/pinecone-io/examples/tree/master/learn/image-retrieval/color-histograms).*\n\n---\n\n## Color Histograms\n\nTo create our histograms we first need images. Feel free to use any images you like, but, if you'd like to follow along with the same images, you can download them using HuggingFace *Datasets*.\n\n```python\nfrom datasets import load_dataset  # !pip install datasets\n\ndata = load_dataset('pinecone/image-set', split='train', revision='e7d39fc')\n```\n\nInside the *image_bytes* feature of this dataset we have base64 encoded representations of 21 images. We decode them into OpenCV compatible Numpy arrays like so:\n\n```python\nfrom base64 import b64decode\nimport cv2\nimport numpy as np\n\ndef process_fn(sample):\n    image_bytes = b64decode(sample['image_bytes'])\n    image = cv2.imdecode(np.frombuffer(image_bytes, np.uint8), cv2.IMREAD_COLOR)\n    return image\n\nimages = [process_fn(sample) for sample in data]\n```\n\nThis code leaves us with the images in the list `images`. We can display them with *matplotlib*.\n\n{{< notebook file=\"color-histograms-show-bgr\" height=\"full\" >}}\n\nThe three dogs look strangely blue; that's not intentional. OpenCV loads images in a **B**lue **G**reen **R**ed (BGR) format. Matplotlib expected RGB, so we must flip the *color channels* of the array to get the true color image.\n\n![color-histograms-5](/images/color-histograms-5.png)\n\n<small>OpenCV reads images using BGR format, we flip the arrays to RGB so that we can view the true color image in matplotlib.</small>\n\n{{< notebook file=\"color-histograms-invert-color\" height=\"full\" >}}\n\nNote that while the `shape` of the array has remained the same, the three values have reversed order. Those three values are the BGR-to-RGB channel values for a single pixel in the image. As shown, after flipping the order of these channels, we can display the true color image.\n\n---\n\n*Just want to create some color histogram embeddings? Skip ahead to the [**OpenCV Color Histograms**](https://www.pinecone.io/learn/color-histograms/#:~:text=a%20better%20way.-,OpenCV%20Histograms,-Building%20histograms%20can) section!*\n\n---\n\n### Step-by-Step with Numpy\n\nTo help us understand how an image is transformed into a color histogram we will work through a step-by-step example using Numpy. We already have our Numpy arrays. For the first image we can see the three BGR color values at pixel zero with:\n\n<script src=\"https://gist.github.com/jamescalam/08140ff66b6004ef9df9e4cb790c44b4.js\"></script>\n\nEvery pixel in each image has three BGR color values like this that range on a scale of `0` (no color) to `255` (max color). Using this, we can manually create *RGB* arrays to display colors with Matplotlib like so:\n\n{{< notebook file=\"color-histogram-color-example\" height=\"full\" >}}\n\nFrom the first pixel of our image with the three dogs, we have the BGR values:\n\n| Blue | Green | Red  |\n| ---- | ----- | ---- |\n| 165  | 174   | 134  |\n\nWe can estimate that this pixel will be a relatively neutral green-blue color, as both of these colors *slightly* overpower red. We will see this color by visualizing that pixel with matplotlib:\n\n{{< notebook file=\"color-histogram-top-left-color\" height=\"full\" >}}\n\nThe color channel values for all pixels in the image are presently stored in an array of equal dimensions to the original image. When comparing image embeddings the most efficient techniques rely on comparing *vectors* not arrays. To handle this, we first reshape the rows and columns of the image array into a single row.\n\n<script src=\"https://gist.github.com/jamescalam/172491f0da52ebb47d1cf5c282e69e9d.js\"></script>\n\nWe can see that the top left three pixels are still the same:\n\n{{< notebook file=\"color-histogram-top-left-color-vec\" height=\"full\" >}}\n\nEven now, we still don't have a \"vector\" because there are three color channels. We must extract those into their own vectors (and later during comparison we will concatenate them to form a single vector).\n\n<script src=\"https://gist.github.com/jamescalam/04e3102b4fe9ead468d555e3573faf86.js\"></script>\n\nNow we visualize each with a histogram.\n\n{{< notebook file=\"color-histogram-channel-histograms\" height=\"full\" >}}\n\nHere we can see the three color channels RGB. On the x-axis we have the pixel color value from *0* to *255* and, on the y-axis, is a count of the number of pixels with that color value.\n\nTypically, we would discretize the histograms into a smaller number of bins. We will add this to a function called `build_histogram` that will take our image array `image` and a number of `bins` and build a histogram for us.\n\n```python\ndef build_histogram(image, bins=256):\n    # convert from BGR to RGB\n    rgb_image = np.flip(image, 2)\n    # show the image\n    plt.imshow(rgb_image)\n    # convert to a vector\n    image_vector = rgb_image.reshape(1, -1, 3)\n    # break into given number of bins\n    div = 256 / bins\n    bins_vector = (image_vector / div).astype(int)\n    # get the red, green, and blue channels\n    red = bins_vector[0, :, 0]\n    green = bins_vector[0, :, 1]\n    blue = bins_vector[0, :, 2]\n    # build the histograms and display\n    fig, axs = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\n    axs[0].hist(red, bins=bins, color='r')\n    axs[1].hist(green, bins=bins, color='g')\n    axs[2].hist(blue, bins=bins, color='b')\n    plt.show()\n```\n\nWe can apply this to a few images to get an idea of how the color profile of an image can change the histograms.\n\n{{< notebook file=\"color-histogram-visuals\" height=\"full\" >}}\n\nThat demonstrates color histograms and how we build them. However, there is a better way.\n\n## OpenCV Histograms\n\nBuilding histograms can be abstracted to be done more easily using the OpenCV library. OpenCV has a function called `calcHist` specifically for building histograms. We apply it like so:\n\n<script src=\"https://gist.github.com/jamescalam/2aa67abb1b94194d6cd32cbe2da6b011.js\"></script>\n\nThe values used here are:\n\n```python\ncv2.calcHist(\n [images], [channels], [mask], [bins], [hist_range]\n)\n```\n\nWhere:\n\n* `images` is our *cv2* loaded image with a BGR color channel. This argument expects a list of images which is why we have placed a single image inside square brackets `[]`.\n* `channels` is the color channel (BGR) that we'd like to create a histogram for; we do this for a single channel at a time.\n* `mask` is another image array consisting of `0` and `1` values that allow us to mask (e.g. hide) part of `images` if wanted. We will not use this so we set it to `None`.\n\n![color-histograms-4](/images/color-histograms-4.png)\n<small>Example of how the masking layer works to \"hide\" part of an image.</small>\n\n* `bins` is the number of buckets/histogram bars we place our values in. We can set this to 256 if we'd like to keep all of the original values.\n* `hist_range` is the range of color values we expect. As we're using RGB/BGR, we expect a min value of *0* and max value of *255*, so we write `[0, 256]` (the upper limit is exclusive).\n\nAfter calculating these histogram values we can visualize them again using `plot`.\n\n{{< notebook file=\"color-histograms-calchist-plot\" height=\"full\" >}}\n\nThe `calcHist` function has effectively performed the same operation but with much less code. We now have our histograms; however, we're not done yet.\n\n## Vectors and Similarity\n\nWe have a function for transforming our images into three vectors representing the three color channels. Before comparing our images we must concatenate these three vectors into a single vector. We will pack all of this into `get_vector`:\n\n{{< notebook file=\"color-histograms-get-vec\" height=\"full\" >}}\n\nUsing the default `bins=32` this function will return a vector with *96* dimensions, where values *[0, ... 32]* are red, *[32, ... 64]* are green, and *[64, ... 96]* are blue.\n\nOnce we have these vectors we can compare them using typical similarity/distance metrics such as Euclidean distance and cosine similarity. To calculate the cosine similarity we use the formula:\n\n\n![color-histograms-6 cosine similarity](/images/color-histograms-6.png)\n\n\nWhich we write in Python with just:\n\n```python\ndef cosine(a, b):\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n```\n\nUsing our `cosine` function we can calculate the similarity which varies from *0* (highly dissimilar) to *1* (identical). We can apply this alongside everything else we have done so far to create another `search` function that will return the `top_k` most similar images to a particular query image specified by its index `idx` in `images`.\n\n{{< notebook file=\"color-histogram-search-func\" height=\"full\" >}}\n\nWe can add a few more components to this search function to output the images themselves rather than the image index positions; the code for this can [be found here](https://github.com/pinecone-io/examples/blob/master/learn/image-retrieval/color-histograms/01-search-histogram.ipynb). Here are some of the results:\n\n![color-histograms-1](/images/color-histograms-1.png)\n<small>Color histogram retrieval with query image (top) and retrieved similar images (bottom).</small>\n\nHere, it is clear that the teal-orange color profile of the first query is definitely shared by the returned results. If we were looking for images with a similar aesthetic, I'd view this as a good result.\n\n![color-histograms-2](/images/color-histograms-2.png)\n<small>Color histogram retrieval with query image (top) and retrieved similar images (bottom).</small>\n\nThe dog with orange background query returns a cat with orange background as the most similar result (excluding the same image). Again, in terms of color profiles and image aesthetics, the top result is good, and the histogram is also clearly similar.\n\n---\n\nThese are some great results, and you can test the color histogram retrieval using the [notebook here](https://github.com/pinecone-io/examples/blob/master/learn/image-retrieval/color-histograms/01-search-histogram.ipynb). However, this isn't perfect, and these results can highlight some of the drawbacks of using color histograms.\n\nTheir key limitation is that they rely solely on image color profiles. That means the textures, edges, or actual meaning behind the content of images is *not* considered.\n\nFurther work on color histograms helped improve performance in some of these areas, such as comparing textures and edges, but these were still limited. Other deep learning methods greatly enhanced the performance of retrieving images based on semantic meaning, which is the focus of most modern technologies.\n\nDespite these drawbacks, for a simple content-based image retrieval system, this approach provides several benefits:\n\n* It is incredibly **easy to implement**; all we need to do is extract pixel color values and transform these into a vector to be compared with a simple metric like Euclidean distance or cosine similarity.\n* The results are highly relevant for color-focused retrieval. If the meaningful content of an image is not important, this can be useful.\n* Results are highly interpretable. There is no black box operation happening here; we know that every result is returned because it has a similar color profile.\n\nWith this in mind, embedding images using color histograms can produce great results for *simple* and interpretable image retrieval systems where aesthetics or color-profiles are important.\n\n{{< newsletter text=\"Subscribe for more computer vision and retrieval content!\" inputText=\"Email address...\" buttonText=\"Submit\">}}\n\n## Resources\n\n[Color Histogram Notebooks](https://github.com/pinecone-io/examples/tree/master/learn/image-retrieval/color-histograms)\n\nJ. Smith, [Integrated Spatial and Feature Image Systems: Retrieval, Analysis and Compression](https://www.ee.columbia.edu/ln/dvmm/publications/PhD_theses/jrsmith-thesis.pdf) (2007)\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbd3"
  },
  "filename": "predict-perform-control.md",
  "title": "post",
  "category": "\"February Release",
  "content": "---\nlayout: post\ntitle: \"February Release: Performance, Predictability, and Control\"\nheadline: \"February Release: Performance at Scale, Predictability, and Control\"\ncategories:\n  - Company News\ntoc: >-\nauthor:\n  name: Greg Kogan\n  position: VP Marketing\n  src: /images/company-greg.png\n  href: https://www.linkedin.com/in/gkogan/\ndate: \"2022-02-16\"\n# Date: February 16, 2022\n# Open Graph\ndescription: \"Latest version of the Pinecone gives you greater performance, predictability, and control of your vector search applications.\"\n# No image in article, default will be used\nthumbnail: \"/images/pinecone-logo-thumbnail.jpg\"\n---\n\n**The latest version of Pinecone gives you greater performance, predictability, and control of your vector search applications.**\n\nLow-latency vector search at scale is one of the biggest reasons engineering teams choose Pinecone. This update significantly lowers search latency for large indexes even further. For example, an index with 100M vectors is now 3.4x faster than before.\n\nEngineers also choose Pinecone because they can start and scale a vector search service during their lunch break, without any infrastructure or algorithm hassles. This release provides more predictability and control while minimizing overhead, with a redesigned user console and additional deployment options across GCP and AWS.\n\nThis update is effective on all new indexes starting today. Indexes created before today will be automatically updated one month from now, on March 15. If you’d like your existing indexes updated sooner, we can perform a zero-downtime update for you by request.\n\nContinue reading to learn more, then [try it](https://app.pinecone.io) and [join us for a live demo and Q&A](https://pinecone-io.zoom.us/webinar/register/WN_kd6WSbQ5TSONOSqMF0vRdg) on Tuesday, February 22nd.\n\n## Performance at scale\n\nYou’ve always had fast [vector search](/learn/vector-search-basics/) with Pinecone, and now it *stays* remarkably fast even as you scale. Previously, as you added pods to accommodate a growing index, you experienced increasing search latencies. This release flattens the impact of scaling, so the search latency stays low even with *hundreds of millions* of vectors.\n\nIn our benchmark tests, indexes running on our performance-optimized pods (p1) maintain search speeds well below 120ms (p95) as they scale from zero to tens of millions of vectors. At 10M 768-dimensional vectors, Pinecone is now 1.6x faster than before, and at 20M vectors it is a whopping 2.5x faster than before.\n\n*Note: These tests used the minimum number of pods required. This is best case in terms of cost (fewer pods) and the \"worst case\" in terms of performance (each pod is at full capacity). Users can reduce latencies by adding more pods, and/or applying filters to queries. In practice, many customers see sub-100ms latencies from Pinecone. Since Pinecone is a cloud service, these latencies include network overhead.*\n\n![Query latencies at scale on p1 pods](/images/pinecone-query-latencies-on-p1-pods.svg)\n\nThe difference is even starker for indexes running on our storage-optimized pods (s1). These pods were designed as a cost-efficient option for teams with larger catalogs and a tolerance for higher latency. However, their progressively slower search speeds at larger index sizes made them impractical for real-time applications... Until today.\n\nWith this release, indexes running on s1 pods maintain search latencies under 500ms (p95) even as you scale to 100M+ vectors. **At 50M vectors, Pinecone is 2x faster than before, and at 100M vectors (20 pods) it's an incredible 3.4x faster than before.**\n\n![Query latencies at scale on s1 pods](/images/pinecone-query-latencies-on-s1-pods.svg)\n\nIt doesn’t stop there. If you need to index billions of vectors while keeping sub-second latencies — like some of our customers — [contact us](/contact/) for help in setting up your index.\n\nAs always, your performance may vary and we encourage you to test with your own data. Latencies are dependent on vector dimensionality, metadata size, network connection, cloud provider (more on this below), and other factors.\n\nThis improvement came from months of engineering efforts to build the most performant, scalable, and reliable vector database. It included rewriting core parts of the Pinecone engine in Rust, optimizing I/O operations, implementing dynamic caching, re-configuring storage formats, and more. This effort is never-ending, so expect even more performance improvements very soon.\n\n## Predict performance and usage\n\nYou need to know what to expect from your search applications. How fast will it be? How consistent is that speed? How much hardware do I need? What will it cost? How long will it take you to upload your data? This release helps you answer all of these questions and puts your mind at ease.\n\nThe first thing you want to predict is how many pods you'll need, what they'll cost, and what's the expected latency. We’ve made this planning easier with our [new usage estimator tool](/pricing/#estimate).\n\nNext, you need to know that search speed will be consistent for your users without erratic spikes from one query to the next. This update drastically lowers the variance between p50 and p95 search latencies: It is now within 20% for p1 pods, and just 10% for s1 pods.\n\n![Query latency variance on s1 pods](/images/pinecone-query-latency-variance-on-s1-pods.svg)\n\nAnd finally, when you start loading data into Pinecone you want to know it'll be indexed quickly and completely. We've made [data ingestion](https://www.pinecone.io/docs/insert-data/) faster and more reliable. Before, upserts slowed down as the index approached capacity, and if you exceeded capacity then the index would fail. Now, upserts stay fast all the way, and trying to upload beyond capacity will result in a gentle error message — the index will remain up.\n\n## Control projects and environments\n\nWhether it’s to minimize latencies or to comply with data regulations, many Pinecone users asked for the ability to choose between cloud providers and regions. Now they have it.\n\n**Users on the [Standard plan](/pricing/) can now choose from GCP US-West, GCP EU-West (new), and AWS US-East (new)**. Even more regions are coming soon.\n\nAs before, users on the Dedicated plan get a single-tenant environment on GCP or AWS in any region of their choice.\n\nGCP US-West remains the default environment for new projects, and the only one available for users on the Free plan. The environment is set when creating a project, and different projects can use different environments. \n\nAnd now, creating and managing projects is even easier with the completely redesigned management console. It includes a new page for managing projects, along with a more powerful page for managing indexes and data.\n\n![Screenshot of the redesigned console](/images/pinecone-february-console-indexes.png)\n\nAnd, let’s be honest, it’s also easier on the eyes. [See it!](https://app.pinecone.io/)\n\n## Get Started\n\nFor existing users:\n\n* All *new* indexes starting from today come with this update.\n* If you use the Python client, install the latest version with `pip install pinecone-client` (see [installation docs](https://www.pinecone.io/docs/quickstart/)).\n* This is a non-breaking change. The updated API is backward compatible.\n* Existing indexes will be rolled over by March 15th, with zero downtime.\n* To update existing indexes before March 15, users on the Free plan should re-create the index, and users on the Standard or Dedicated plans should contact us with a preferred time when you want us to update your indexes.\n\nFor new users:\n\n* [Create a free account](https://app.pinecone.io) and start building vector-search applications.\n* [Register for the live office hour](https://pinecone-io.zoom.us/webinar/register/WN_kd6WSbQ5TSONOSqMF0vRdg) on Tuesday, Feb 22, to learn more, see a demo, and get your questions answered!\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbd4"
  },
  "filename": "vector-database.md",
  "title": "post",
  "category": "\"What is a Vector Database?\"",
  "content": "---\nlayout: post\ntitle: \"What is a Vector Database?\"\nheadline: \"What is a <span>Vector Database</span>?\"\ndescription: The nature of vector embeddings require new methods of storage and retrieval. We need a new kind of database.\ncategories:\n  - Vector Search 101\ntoc: >-\nweight: 3\nauthor:\n  name: Bryan Turriff\n  position: Director of Product Marketing\n  src: /images/company-bryan.jpeg\n  href: https://www.linkedin.com/in/bryanturriff/\n# Open Graph\nimages: ['/images/vector-database-1.png']\n---\n\n![Traditional vs Vector Data](/images/vector-database-1.jpg)\n\n**Pinecone is a vector database that makes it easy for developers to add vector-search features to their applications, using just an API. [Try it now](https://app.pinecone.io) to see for yourself, or continue reading to learn about vector databases.**\n\n## Introduction\n\nComplex data is growing at break-neck speed. These are unstructured forms of data that include documents, images, videos, and plain text on the web. Many organizations would benefit from storing and analyzing complex data, but complex data can be difficult for traditional databases built with structured data in mind. Classifying complex data with keywords and metadata alone may be insufficient to fully represent all of its various characteristics.\n\nFortunately, Machine Learning (ML) techniques can offer a far more helpful representation of complex data by transforming it into vector embeddings. Vector embeddings describe complex data objects as numeric values in hundreds or thousands of different dimensions. \n\nMany technologies exist for building vectors, ranging from vector representations of words or sentences, to cross-media text, images, audio, and video. There are [several existing public models](https://www.sbert.net/) that are high-performance and easy to use as-is. These models can be [fine-tuned for specific applications](https://www.pinecone.io/learn/fine-tune-sentence-transformers-mnr/) and you can also train a new model from scratch, although that is less common.\n\nVector databases are purpose-built to handle the unique structure of vector embeddings. They index vectors for easy search and retrieval by comparing values and finding those that are most similar to one another. They are, however, difficult to implement.\n\nUntil now, vector databases have been reserved for only a handful of tech giants that have the resources to develop and manage them. Unless properly calibrated, they may not provide the performance users require without costing a fortune.\n\nUsing a well-constructed vector database gives your applications superior search capability while also meeting performance and cost goals. There are several solutions available to make it easier to implement. These solutions range from plugins and open-source projects to fully-managed services that handle security, availability, and performance. This document will describe common uses of vector databases, core components, and how to get started.\n\n## What is a Vector Database?\n\n**A vector database indexes and stores vector embeddings for fast retrieval and similarity search, with capabilities like CRUD operations, metadata filtering, and horizontal scaling.**\n\n<div class=\"post-group\">\n\n**vector** noun\n\nˈvek-tər\n\nin machine learning, an array of numerical measurements that describe and represent the various characteristics of an object\n</div>\n\n<div class=\"post-group\">\n\n**database** noun\n\nˈdā-tə-ˌbās\n\na large collection of data organized especially for rapid search and retrieval (as by a computer)\n</div>\n\nWhen we say that vector databases index [vector embeddings](https://www.pinecone.io/learn/vector-embeddings/), we mean that they organize them in a way that we can compare any vector to one another or to the vector of a search query. We will cover algorithms used to index vectors further down. Vector databases are also responsible for executing CRUD operations (create, read, update, and delete) and [metadata filtering](https://www.pinecone.io/learn/vector-search-filtering/). The combination of traditional database functionality with the ability to search and compare vectors in an index makes vector databases the powerful tools that they are.\n\nVector databases excel at [similarity search](https://www.pinecone.io/learn/what-is-similarity-search/), or “vector search.” Vector search enables users to describe what they want to find without having to know which keywords or metadata classifications are ascribed to the stored objects. Vector search can also return results that are similar or near-neighbor matches, providing a more comprehensive list of results that otherwise may have remained hidden. \n\n## Why Use a Vector Database?\n\nVector search in production is the most common reason to use a vector database. Vector search compares the similarity of multiple objects to a search query or subject item. In order to find similar matches, you convert the subject item or query into a vector using the same ML embedding model used to create your vector embeddings. The vector database compares the similarity of these objects to find the closest matches, providing accurate results while eliminating irrelevant results that traditional search technology might have returned.\n\nLet’s look at some common use cases for vector search:\n\n### 1. [Semantic search](/semantic-search)\n\nSearching text and documents can generally be done in two ways. Lexical search looks for patterns and exact word or string matches, while semantic search uses the meaning of your search query or question and puts it into context. Vector databases store and index vector embeddings from Natural Language Processing models to understand the meaning and context of strings of text, sentences, and whole documents for more accurate and relevant search results.\n\nUsing natural language queries to find relevant results is a better experience and allows users to find what they need more quickly without having to know specifics about how the data is classified.\n\nSee example code: [Semantic Search](https://www.pinecone.io/docs/examples/semantic-text-search/), [Hybrid Search with Filtering](https://www.pinecone.io/docs/examples/basic-hybrid-search/)\n\n### 2. Similarity search for images, audio, video, JSON, and other forms of unstructured data\n\nImages, audio, video, and other unstructured datasets can be very challenging to classify and store in a traditional database. This often requires keywords, descriptions, and metadata to be manually applied to each object. The way one individual classifies one of the complex data objects may not be obvious to another. As a result, searching for complex data can be very hit and miss. This approach requires the searcher to understand something about how the data is structured and construct queries that match the original data model.\n\nSee example code: [Image Similarity Search](https://www.pinecone.io/docs/examples/image-similarity-search/)\n\n### 3. Ranking and recommendation engines\n\nVector databases are a great solution for powering ranking and recommendation engines. For online retailers, they can be used to suggest items similar to past purchases or a current item the customer is researching. Streaming media services can apply a user’s song ratings to create perfectly matched recommendations tailored to the individual rather than relying on collaborative filtering or popularity lists.\n\nThe ability to find similar items based on nearest matches makes vector databases ideal for offering relevant suggestions, and can easily rank items based on similarity scores.\n\nSee example code: [Movie Recommender](https://www.pinecone.io/docs/examples/movie-recommender/)\n\n### 4. Deduplication and record matching\n\nAnother use case for vector similarity search is record matching and deduplication. Using the similarity service to find near-duplicate records can be used in a wide range of applications.  Consider an application that removes duplicate items from a catalog to make it far more usable and relevant.\n\nSee example code: [Document Deduplication](https://www.pinecone.io/docs/examples/document-deduplication/)\n\n### 5. Anomaly detection\n\nAs good as vector databases are in finding similar objects, they can also find objects that are distant or dissimilar from an expected result. These anomalies are valuable in applications used for threat assessment, fraud detection, and IT Operations. It’s possible to identify the most relevant anomalies for further analysis without overwhelming resources with a high rate of false alarms. \n\nSee example code: [IT Threat Detection](https://www.pinecone.io/docs/examples/it-threat-detection/)\n\n## Required Capabilities of a Vector Database\n\n### 1. Vector Indexes for Search and Retrieval\n\nVector databases use [algorithms specifically designed to index and retrieve vectors efficiently](https://www.pinecone.io/learn/vector-indexes/). Different use cases require the prioritization of accuracy, latency, or memory usage which can be fine-tuned using different algorithms. Choosing and optimizing these algorithms is a science in itself, and finding the optimum algorithm for different datasets that satisfies use-case requirements can be challenging.\n\n![Vector Indexes](/images/vector-database-2.jpg)\n\nAlongside indexes, there are also similarity and distance metrics. These metrics are what measure the relevance/similarity between vector embeddings. Some metrics have better recall and precision performance than others. Common metrics in vector indexes include Euclidean distance, cosine similarity, and dot products. \n\nVector databases use “nearest neighbor” indexes to assess how closely similar objects are to one another or to a search query. Traditional nearest neighbor search is problematic for large indexes as they require a comparison between the search query and every indexed vector. Comparing every vector takes time.\n\nApproximate Nearest Neighbor (ANN) search circumvents this problem by approximating and retrieving a best guess of most similar vectors. While ANN does not guarantee to return the exact closest match, it balances very good precision with very fast performance.\n\nTechniques such as [HNSW](https://www.pinecone.io/learn/hnsw/), IVF, or [PQ](https://www.pinecone.io/learn/product-quantization/) are some of the most popular components used in building effective ANN indexes. Where each technique focuses on improving a particular performance property, such as memory reduction with PQ or fast but accurate search times with HNSW and IVF. It is common practice to mix several components to produce a ‘composite’ index to achieve optimal performance for a given use case.\n\nWithout a vector database, designing and building an effective index is not easy. If using a stand-alone framework such as [Faiss](/learn/faiss-tutorial/), the design and deployment of an index requires a team of experienced engineers with a good grasp of indexing and retrieval algorithms. At a minimum, these vectors must be mapped back to the original data using another storage and retrieval pipeline (as stand-alone indexes do not support this). Indexes require periodic retraining and mechanisms for tracking deleted, replaced, or new data. A team must account for these added requirements and any ongoing operations.\n\n### 2. Single-Stage Filtering\n\nFiltering allows you to limit search results based on vector metadata. This can improve the relevance of search results by returning a subset of available matches based on limiting criteria.  \n\nPost-filtering applies approximate nearest neighbor search first and then restricts the results to metadata filter restrictions. ANN typically returns a requested set of nearest matches but does not know how many (if any) of them will match the metadata criteria. This is usually fast but may return too few vectors that match the filter if any at all.\n\n![Filtering](/images/vector-database-3.png)\n\nPre-filtering vectors with metadata shrinks the dataset and may return highly relevant results. However, because pre-filtering applies the matching criteria on each vector in the index first, it can also severely slow the performance of vector databases.\n\nSingle-stage filtering is a must for effective vector databases. It combines the accuracy and relevance of pre-filtering with speeds that are as fast or faster than post-filtering. By merging vector and metadata indexes into a single index, single-stage filtering offers the best of both approaches.\n\n### 3. Data Sharding\n\nWhat is a vector database without scaling? ANN algorithms search vectors with remarkable efficiency. But whatever their efficiency, hardware limits what’s possible on a single machine. You can scale vertically — increase the capacity of a single machine and parallelize aspects of the ANN routine. But you’ll hit a limit to how far you can take this, be it cost or availability of behemoth machines. Enter horizontal scaling. We can divide the vectors into shards and replicas to scale across many commodity-level machines to achieve scalable and cost-effective performance.\n\nImagine a friend filled a bucket with 100 little slips of paper. And suppose on each slip of paper she wrote someone’s name along with their birthday, month and day, and the actual time of birth. Then she requests: “find the person whose birth date and time is closest to yours”. So you sift through the bucket to find the closest match. In this way, the slips of paper are like vectors, you are like a CPU, and the bucket is like RAM.\n\nNow suppose your friend gave you a bucket with 1000 names and birthdays — you’re going to be searching for a while! Instead, you split the 1000 names into 10 buckets and invite 10 friends to help. Each of you searches only 100 names for the best match in the bucket and then compares the results each of you found to find the very best match. As a result, you find the best match among 1000 names in almost the same amount of time it took you to find the best match among 100 names. You’ve horizontally scaled yourself! \n\nA vector database divides the vectors equally into shards, searches each shard, and combines the results from all the shards at the end to determine the best match. Often, it will use Kubernetes and grant each shard its own Kubernetes pod with at least one CPU and some RAM. The pods work in parallel to search the vectors.\n\nAs a result, you get the answer in just a little over the time it takes one pod to search one shard. Have 20M vectors? Use 20 pods and get results in the time it takes one pod to search 1M vectors or use 40 pods (500K vectors per shard) to get results even faster. There is more to it, but put simply, fewer vectors per pod lower query latency and allow you to search as many as billions of vectors in a reasonable amount of time.\n\n![Shard Router](/images/vector-database-4.jpg)\n\n### 4. Replication\n\nVector databases need to handle many requests gracefully. Shards allow it to employ many pods in parallel to perform a vector search faster. But what if you need to perform many different vector searches at the same time or in rapid succession? Even speedy vector searches will get backed up if new requests are coming in fast enough. Enter replicas.\n\nAs their name implies, replicas replicate the whole set of pods to handle more requests in parallel. If we think back to our names-in-buckets analogy, this is like creating a copy of the ten buckets and asking another ten friends to handle any new matching request. Suppose ten pods can search 10M vectors in 100 ms. If you issue one request a second, you’re good. If you issue 20 different requests every second, you need backup. Add a replica (ten more pods in this case) to keep up with the demand.\n\nReplicas also improve availability. Machines fail — it’s a fact of life. A vector database needs to bring pods back up as quickly as possible after a failure. But “as quickly as possible” isn’t always quick enough. Ideally, it needs to handle failures immediately without missing a beat. Cloud providers offer so-called availability zones that are highly unlikely to fail simultaneously. \n\nThe vector database can spread replicas to different availability zones to ensure high availability. But you, the user, have a part to play here, too — you need to have multiple replicas and replica capacity, such that fewer replicas can handle the query load with acceptable latency in the case of a failure. \n\n### 5. Hybrid Storage\n\nVector searches typically run completely in-memory (RAM). For companies with over a billion items in their catalog, the memory costs alone could make vector search too expensive to consider. Some vector search libraries have the option to store everything on disk, but this could come at the expense of search latencies becoming unacceptably high.\n\nIn a hybrid storage configuration, a compressed vector index is stored in memory, and the original, full-resolution vector index is stored on disk. The in-memory index is for locating a small set of candidates to search within the complete index on disk. This method provides fast and accurate search results yet cuts infrastructure costs by up to 10x.\n\nHybrid storage allows you to store more vectors across the same data footprint, lowering the cost of operating your vector database by improving overall storage capacity without negatively impacting database performance.\n\n### 6. API\n\nVector databases should take the burden of building and maintaining vector search capability away from developers so they can focus on making their applications the best they can be. An API makes it easy for developers to use or manage the vector database from any other application.\n\nThe application makes API calls to the vector database to perform an action such as upserting vectors into the database, retrieving query results, or deleting vectors.   \n\nREST APIs add flexibility by initiating the functionality of the vector database from any environment that can make HTTPS calls. Developers may also access it directly through clients using languages like Python, Java, and Go.\n\n## Getting Started with Vector Databases\n\nCombined with machine learning transformer models, vector databases offer a more intuitive way to find similar objects, answer complex questions, and understand the hidden context of complex data.\n\nSo how should you get started?\n\n### Learn More about Vector Databases\n\nVisit the [Pinecone learning center](/learn/) and read more about key concepts, including vector embeddings, vector indexes, and NLP for semantic search. Here are some of the most popular topics:\n\n- [Sentence Transformers: Meanings in Disguise](/learn/sentence-embeddings/) - This guide discusses core techniques for converting text and documents into vector embeddings and details some of the most popular NLP embedding models.\n\n- [The Missing WHERE Clause in Vector Search](/learn/vector-search-filtering/) - This article explains two common methods for adding metadata filters to vector search, and explores their limitations. Then, we cover how Single-Stage Filtering bridges some of these gaps.\n\n- [Nearest Neighbor Indexes for Similarity Search](/learn/vector-indexes/) - This article explores the pros and cons of some of the most important indexes including Flat, LSH, HNSW, and IVF. It also gives tips for deciding which to use and the impact of parameters in each index.\n\n### Launch Your First Vector Database\n\nOnce you have your vector embeddings, you’ll need a vector database to index, store, and retrieve them.\n\n[Create an account](https://app.pinecone.io) and launch your first vector database.\n\nWith Pinecone, you can do this in just a few minutes. Pinecone is a fully managed vector database that makes it easy to add vector search to production applications. It combines vector search libraries, capabilities such as filtering, and distributed infrastructure to provide high performance and reliability at any scale.\n\n{{< newsletter text=\"Subscribe for more on vector databases!\" inputText=\"Email address...\" buttonText=\"Submit\">}}\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbd5"
  },
  "filename": "v2-pricing.md",
  "title": "post",
  "category": "\"Pinecone 2.0 is Available and Free\"",
  "content": "---\nlayout: post\ntitle: \"Pinecone 2.0 is Available and Free\"\nheadline: \"Pinecone 2.0 is Available and Free\"\ncategories:\n  - Company News\ntoc: >-\nauthor:\n  name: Edo Liberty\n  position: Founder and CEO\n  src: /images/company-edo.png\n  href: https://edoliberty.github.io/\ndate: \"2021-10-04\"\n# Date: October 4, 2021\n# Open Graph\ndescription: Every use case comes with its own requirements for performance, number of vectors, and throughput. Our new pricing gives you the choice to optimize Pinecone to meet your needs and pay (less) as you go.\n# No image in article, default will be used\nthumbnail: \"/images/pinecone-logo-thumbnail.jpg\"\n---\n\n**Pinecone 2.0 is generally available as of today, with many [new features](/learn/pinecone-v2/) and [new pricing](/pricing/) which is up to 10x cheaper for most customers and, for some, completely free!**\n\n\nOn September 19, 2021, we announced Pinecone 2.0, which introduced many new features that get vector similarity search applications to production faster. Today, we are pleased to announce Pinecone 2.0 is generally available. The all-new pricing lets you choose the plan that works best for you.\n\nEvery use case comes with its own requirements for performance, number of vectors, and throughput. Our new pricing gives you the choice to optimize Pinecone to meet your needs and pay (less) as you go.\n\nThe new pricing is based on hardware usage (number and types of pods) and better aligns with your actual consumption patterns. We believe that pricing by hardware is simpler, more transparent, and gives you the flexibility to optimize cost and performance for your dataset and use case.\n\nWe are introducing three new pricing plans: Free, Standard, and Dedicated.\n\n## Free\n\nWe are committed to helping companies create better products using vector search. We want to remove the obstacles to getting these projects into production. That includes the obstacle — for some — of cost.\n\nOur new Free plan lets you use the fully managed Pinecone service for small workloads without any cost. It comes with a single P1 pod, which is enough to search through roughly a million vectors in around 100ms, or through 100K vectors in around 20ms. For many users this is all they’ll need.\n\nWith the Free plan you get the same production-ready, secure, and fully managed vector database as the other plans.\n\n## Standard\n\nWhen you are ready to scale up, the new Standard plan lets you choose the type of pod, the number of pods, and the number of replicas to optimize your Pinecone service.\n\nIn addition to the P1 pods, you have the option of S1 pods. The S1 pods are storage-optimized and suitable for a large number of vectors when you are more tolerant of slightly higher latency. S1 pods are compatible with Pinecone’s hybrid index. Each S1 pod includes 1 vCPU and 20GB SSD storage. \n\nWith the Standard plan, you can achieve low latencies across any number of vectors. Pricing in this plan is based on the number of pods used per hour, with P1 pods priced at $0.070/hour and S1 pods at $0.075/hour. Connect with us for help with choosing the best configuration to meet your requirements.\n\nThe Standard plan offers priority support within a 24-hour timeframe and increased reliability with multiple availability zones.\n\n## Dedicated\n\nIf your workload is very large or if you have unique compliance or deployment requirements, our Dedicated plan is for you. Our team will work with you to configure a custom deployment that meets your requirements and is optimized for your workload, in a dedicated GCP or AWS environment in any region.\n\nAs with our Standard plan, our Dedicated plan ensures high availability with multi-AZ, with the added peace of mind of a fully dedicated cluster and high-priority support within a 4-hour time window.\n\n## Get Started\n\nPinecone 2.0 brings vector similarity search from R&D labs to production applications, for companies of all sizes. Our new hardware-based pricing plans provide the flexibility to get started quickly and scale effortlessly.\n\nWhat will you build with the Pinecone vector database? Get started now:\n\n* [Visit our pricing page](/pricing/) for more detailed information\n* [Sign up](https://app.pinecone.io/) and start using Pinecone for free\n* [Contact us](/contact/) to discuss your project\n* [Learn more](/learn/pinecone-v2/) about Pinecone 2.0"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbd6"
  },
  "filename": "class-activation-maps.md",
  "title": "post",
  "category": "\"How to Explain ConvNet Predictions Using Class Activation Maps\"",
  "content": "---\nlayout: post\ntitle: \"How to Explain ConvNet Predictions Using Class Activation Maps\"\nheadline: \"How to Explain ConvNet Predictions Using Class Activation Maps\"\ncategories:\n  - Algorithms & Libraries\ntoc: >-\nweight: 1\nauthor:\n  name: Bala Priya C\n  position: Technical Writer\n  src: /images/bala-priya.jpg\n  href: https://www.linkedin.com/in/bala-priya/\ndescription: \"Learn how class activation maps (CAMs) can be used to explain a convolutional neural net (ConvNet), and how to generate them in PyTorch.\"\n# Open graph\nimages: [\"/images/class-activation-maps.png\"]\n---\n\n![Class activation maps](/images/class-activation-maps.png)\n\nHave you ever used deep learning to solve computer vision tasks? If so, you probably trained a convolutional neural network (ConvNet or CNN) for tasks such as image classification and visual question answering. \n\nIn practice, ConvNets are often viewed as black boxes that take in a dataset and give a task-specific output: predictions in image classification, captions in image captioning, and more. For example, in image classification, you’ll optimize the model for prediction accuracy.\n\n**But how do you know *which* parts of the image the network was looking at when it made a prediction? And how do you go from black box to interpretable models?**\n\nAdding a layer of explainability to ConvNets can be helpful in applications such as medical imaging for disease prognosis. For example, consider a classification model trained on medical images, namely, brain scans and X-rays, to predict the presence or absence of a medical condition. Ensuring that the model is using the relevant parts of the images for its predictions makes it more trustworthy than a black box model with a high prediction accuracy.\n\n**Class activation maps** can help explain the predictions of a ConvNet. Class activation maps, commonly called CAMs, are **class-discriminative** saliency maps. While saliency maps give information on the most important parts of an image for a particular class, class-discriminative saliency maps help distinguish between classes. \n\nIn this tutorial, you’ll learn how class activation maps (CAM) and their generalizations, Grad-CAM and Grad-CAM++, can be used to explain a ConvNet. You’ll then learn how to generate class activation maps in PyTorch. \n\nLet's begin!\n\n## Class Activation Maps Explained\n\nIn general, a ConvNet consists of a series of convolutional layers, each consisting of a set of filters, followed by fully connected layers. \n\nActivation maps indicate the salient regions of an image for a particular prediction. Class activation map (CAM) uses a global average pooling (GAP) layer after the last convolutional layer. Let’s understand how this works.\n\n![CAM intro](/images/CAM-intro.png)\n<small>GAP Layer After the Last CONV Layer (Image by the author)</small>\n\nIf there are `n` filters in the last convolutional layer, then there are `n` feature maps. The activation map for a particular output class is the *weighted combination* of all the `n` feature maps. \n\n*So how do we learn these weights?*\n\n**Step 1**: Apply global average pooling to each of the feature maps. \n\nThe average value of all pixels in a feature map is its global average. Here’s an example of how global average pooling works. The qualifier global means that the average is computed over *all pixel locations* in the feature map.\n\n![How GAP works](/images/how-GAP-works.png)\n<small>How GAP Works - An Example (Image by the author)</small>\n\nAfter computing the global average for each of the feature maps, we’ll have `n` scalars, $k_1, k_2, …, k_n$. Let’s call them GAP outputs.\n\n![Feature maps to GAP](/images/feature-maps-to-GAP.png)\n<small>From Feature Maps to Scalars through GAP (Image by the author)</small>\n\nStep 2: The next step is to learn a linear model from these GAP outputs onto the class labels. For each of the `N` output classes, we should learn a model with weights $w_1, w_2,...,w_n$.  Therefore, we’ll have to learn `N` linear models in all.\n\n![Linear Models from GAP output](/images/GAP-to-output-classes.png)\n<small>Linear Models from GAP Output onto the Class Labels (Image by the author)</small>\n\n**Step 3**: Once we’ve obtained the `n` weights for each of the `N` classes, we can weight the feature maps to generate the class activation maps. Therefore, *different* weighted combinations of the *same* set of feature maps give the class activation maps for the different classes. \n\n![Feature maps](/images/feature-maps-weighted.png)\n<small>Class Activation Maps as Weighted Combinations of Feature Maps (Image by the author)</small>\n\nMathematically, the class score for an output class `c` in the CAM model is given by:\n\n\\begin{align}\ny^c = \\sum_{k} {w_{k}}^c \\frac{1}{Z}\\sum_{i}\\sum_{j} {A_{ij}}^k \\text{ }\\text{ }(1)\\\\\\\\\nA_{ij}^k: \\text{ }pixel\\text{ } at\\text{ } location\\text{ } (i,j)\\text{ } in\\text{ } the\\text{ } k-th\\text{ } feature\\text{ } map\\\\\\\\\nZ: total\\text{ }number\\text{ }of\\text{ }pixels\\text{ }in\\text{ }the\\text{ }feature\\text{ }map\\\\\\\\\n{w_k}^c: weight\\text{ }of\\text{ }the\\text{ }k-th\\text{ }feature\\text{ }map\\text{ }for\\text{ }class \\text{ }c\n\\end{align}\n\n### Advantages of CAM \n\nEven though we need to train `N` linear models to learn the weights, CAM does not require a backward pass through the network again. A backward pass through the layers of the network is more expensive than learning a linear mapping.\n\nCAM uses the inherent localization capability of the convolutional layers, so the activation maps can be generated [without any positional supervision on the location of the target](https://arxiv.org/abs/1412.6856) in the image.\n\n### Limitations of CAM\n\nUsing class activation maps involves the overhead of learning `N` linear models to learn the weights $w_1, w_2,..., w_n$ for each of the `N` classes. Training a ConvNet is a computationally intensive task in itself. This overhead can be a limiting factor when both `n`, the number of filters in the last convolutional layer, and `N`, the number of output classes, are especially large. \n\nThe introduction of the global average pooling (GAP) layer after the last convolutional layer imposes a *restriction* on the ConvNet architecture. Though CAM is helpful in explaining the predictions in an image classification task, it cannot be used for computer vision tasks such as visual question answering (VQA). As explained, the GAP layer outputs are scalars that are global averages of the preceding convolutional layer’s feature maps. There is no known performance degradation for image classification. However, this requirement for the GAP layer after the convolutional layers may be too restrictive for tasks like VQA.\n\n## How Gradient-Weighted Class Activation Maps Work \n\nAs mentioned, the key limitation of CAM is the overhead of learning the weights for linear mapping. Gradient-weighted class activation map (Grad-CAM) is a generalization to CAM that overcomes this limitation.\n\nLet’s start by making a simple substitution in the equation for output class score $y^c$ in CAM.\n\n\\begin{align}\nLet\\text{ }F^k = \\frac{1}{Z}\\sum_{i}\\sum_{j} {A_{ij}}^k \\\\\\\\\nSubstituting\\text{ }F^k\\text{ }in\\text{ }eqn(1), y^c = \\sum_{k} {w_{k}}^cF^k\\\\\\\\\n\\end{align}\n\nNext, let’s compute the derivative of the output class score with respect to the pixels $A_{i,j}$ in the feature map.\n\n\\begin{align}\n\\frac{\\partial{y^c}}{\\partial{F^k}} = {w_{k}}^c\\\\text{ }(2)\\\\\\\\\n\\frac{\\partial{y^c}}{\\partial{F^k}} = \\frac{\\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}}}{\\frac{\\partial{F^k}}{\\partial{{A_{ij}}^k}}}\\\\\\\\\n\\frac{\\partial{F^k}}{\\partial{{A_{ij}}^k}} = \\frac{1}{Z}\\\\\\\\\n\\frac{\\partial{y^c}}{\\partial{F^k}} = \\frac{\\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}}}{\\frac{1}{Z}}\\\\\\\\\n\\frac{\\partial{y^c}}{\\partial{F^k}} = \\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}}.{Z}\\\\text{ }(3)\\\\\\\\\nFrom \\text{ }(2) \\text{ }and\\text{ } (3),\\text{ } we \\text{ }have,\\\\\\\\\n\\frac{\\partial{y^c}}{\\partial{F^k}} = \\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}}.{Z} = {w_{k}}^c\n\\end{align}\n\nSumming the above quantities over all the pixels in the feature map, we have the following:\n\n\\begin{align}\n\\sum_{i}\\sum_{j}{w_{k}}^c = \\sum_{i}\\sum_{j}\\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}}.{Z}\\\\\\\\\n{Z}.{w_{k}}^c = {Z}.\\sum_{i}\\sum_{j}\\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}}\\\\\\\\\n{w_{k}}^c = \\sum_{i}\\sum_{j}\\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}}-->gradients!\n\\end{align}\n\nAs seen in the above equation, the weights $w_k$ evaluate to the **gradient** of the output score with respect to the kth feature map. This means there’s no need to retrain `N` linear models to learn the weights!\n\nWe’ve summed over all pixel locations (i,j). Adding the normalization factor 1/Z  back in, we get:\n\n\\begin{align}\n{w_{k}}^c = \\frac{1}{Z}\\sum_{i}\\sum_{j}\\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}}\n\\end{align}\n\nIn essence, Grad-CAM uses the global average of the gradients flowing into the feature maps of the last convolutional layer.\n\n![How Grad-CAM Works](/images/gradcam-working.png)\n<small>How Grad-CAM Works  (Image by the author)</small>\n\nTo retain only the positive correlations in the final activation map, we apply the ReLU function on the weighted combination of feature maps. \n\n\\begin{align}\n{L^c_{Grad-CAM}} = ReLU\\left(\\sum_{k}{w_k}^cA^k\\right)\n\\end{align}\n\n- - - \n*ReLU function: f(x) = ReLU(x) = x if x >= 0 and 0 otherwise. The ReLU function filters all the negative inputs and passes the positive inputs as they are.*\n- - - \n\n### Grad-CAM: Counterfactual Explanations\n\nGiven that the gradients of the output with respect to the feature maps identify salient patches in the image, what do negative gradients signify?\n\n\\begin{align}\n{w_{k}}^c = \\frac{1}{Z}\\sum_{i}\\sum_{j}-\\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}}\n\\end{align}\n\nUsing negative gradients in the weights will give those patches in the image that *adversarially affect* a particular prediction. For example, in an image containing a cat and a dog, if the target class is cat, then the pixel patch corresponding to the dog class affects prediction.\n\n![Grad-CAM Counterfactual Explanations](/images/gradcam-counterfactual.png)\n<small>Grad-CAM Counterfactual Explanations  (Image Source: [arxiv](https://arxiv.org/pdf/1610.02391.pdf))</small>\n\nTherefore, by identifying and removing these patches from the images, we can suppress the adversarial effect on prediction. As a result, the confidence of prediction increases.\n\n### Guided Grad-CAM: Grad-CAM + Guided Backprop\n\nEven though Grad-CAM provides activation maps with good target localization, it fails to capture certain minute details. [Pixel-space gradient visualization](https://arxiv.org/abs/1412.6806) techniques, which were used in earlier approaches to explainability, can provide more granular information on which pixels have the most influence. \n\nTo obtain a detailed activation map, especially to understand misclassifications among similar classes, we can use guided backpropagation in conjunction with Grad-CAM. This approach is called **guided Grad-CAM**.\n\n- - - -\n*The concept of **guided backpropagation** was introduced in [2]. Given a feedforward neural network, the influence of an input x_j on a hidden layer unit h_i is given by the **gradient** of h_i with respect to x_j. This gradient can be interpreted as follows:*\n\n- *a zero-valued gradient indicates no influence,*\n- *a positive gradient indicates a significant positive influence, and*\n- *a negative gradient indicates negative influence.*\n\n*So to understand the fine-grained details, we only backpropagate along the path with positive gradients. Since this approach uses information from higher layers during the backprop, it’s called guided backpropagation.*\n- - - - \n\n### Advantages of Grad-CAM\n\n- Given that we have the gradients of the output score with respect to the feature maps, Grad-CAM uses these gradients as the weights of the feature maps. This eliminates the need to retrain `N` models to explain the ConvNet’s prediction. \n- As we have the gradients of the task-specific output with respect to the feature maps, Grad-CAM can be used for all computer vision tasks such as visual question answering and image captioning.\n\n### Limitations of Grad-CAM\nWhen there are multiple occurrences of the target class within a single image, the spatial footprint of each of the occurrences is substantially lower. Grad-CAM fails to provide convincing explanations under such *“low spatial footprint”* conditions. \n\n## Understanding Grad-CAM++\n\nGrad-CAM++ provides better localization when the targets have a low spatial footprint in the images.  \n\nLet's start by reviewing the equation for the Grad-CAM weights.\n\n\\begin{align}\n{w_{k}}^c = \\frac{1}{Z}\\sum_{i}\\sum_{j}\\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}}\n\\end{align}\n\nFrom the above equation, we see that Grad-CAM scales all pixel gradients $\\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}}$ by the same factor 1/Z. This means that each pixel gradient has the same significance in generating the final activation map. However, in images where the target has a low spatial footprint, the pixel gradients that actually help the prediction should have greater significance.\n\nTo achieve this, Grad-CAM++ proposes the following:\n\n- The pixel gradients that are important for a particular class should be scaled by a larger factor, and\n- The pixel gradients that do not contribute to a particular class prediction should be scaled by a smaller factor.\n\nMathematically, this can be expressed as:\n\n\\begin{align}\n{w_{k}}^c = \\sum_{i}\\sum_{j}\\alpha_{ij}^{kc}ReLU\\left(\\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}}\\right)\n\\end{align}\n\nLet’s parse what $\\alpha_{ij}^{kc}$ means. \n\n- $\\alpha^{kc}$ denotes the values of α for the k-th feature map corresponding to the output class c.\n- $\\alpha_{ij}^{kc}$ is the value of α at pixel location (i,j) for the k-th feature map corresponding to the output class c.\n\nApplying the ReLU function on the gradients ensures that only the gradients that have a positive contribution to the class prediction are retained.\n\n[Working out the math](https://arxiv.org/abs/1710.11063) like we did for Grad-CAM, the values of $\\alpha_{ij}$ can be given by the following closed-form expression:\n\n\\begin{align}\n\\alpha_{ij}^{kc} = \\frac{\\frac{\\partial^2{y^c}}{(\\partial{A_{ij}}^k)^2}}{2.\\frac{\\partial^2{y^c}}{(\\partial{A_{ij}}^k)^2} + \\sum_a\\sum_b{A_{ab}}^k \\frac{\\partial^3{y^c}}{(\\partial{A_{ij}}^k)^3}}\\\\\\\\\n\\end{align}\n\nUnlike Grad-CAM weights that use first-order gradients, Grad-CAM++ weights use higher order gradients (second and third-order gradients).\n\nThe output activation map is given by:\n\n\\begin{align}\n{L^c_{Grad-CAM++}} = ReLU\\left(\\sum_{k}{w_k}^cA^k\\right)\\\\\\\\\nwhere, \\text{ }{w_{k}}^c = \\sum_{i}\\sum_{j}\\alpha_{ij}^{kc}ReLU\\left(\\frac{\\partial{y^c}}{\\partial{{A_{ij}}^k}}\\right)\n\\end{align}\n\nNow that you’ve learned how class activation maps and the variants, Grad-CAM and Grad-CAM++, work, let's proceed to generate class activation maps for images.\n\n## How to Generate Class Activation Maps in PyTorch \n\nThe [PyTorch Library for CAM Methods](https://jacobgil.github.io/pytorch-gradcam-book/) by  [Jacob Gildenblat](https://github.com/jacobgil) and contributors on GitHub has ready-to-use PyTorch implementations of Grad-CAM, Grad-CAM++, EigenCAM, and much more. This library `grad-cam` is available as a PyPI package that you can install using `pip`.\n\n```bash\npip install grad-cam\n```\n\n📥 [Download the Colab notebook and follow along](https://github.com/balapriyac/CAM-Tutorial).\n\nYou can customize [this generic CAM example](https://github.com/jacobgil/pytorch-grad-cam#using-from-code-as-a-library) depending on the computer vision task to which you'd like to add explainability. Let’s start by importing the necessary modules.\n\n```python\nfrom torchvision import models\nimport numpy as np\nimport cv2\nimport PIL\n```\n\nNext, we import the necessary classes from the grad_cam library. \n\n```python\nfrom pytorch_grad_cam import GradCAM,GradCAMPlusPlus\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom pytorch_grad_cam.utils.image import show_cam_on_image,preprocess_image\n```\n\nIn this example, we’ll use the [pre-trained ResNet50](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html) model from the [PyTorch Torchvision library](https://pytorch.org/vision/stable/index.html) that contains datasets and pre-trained models. We then define the target class, the layer after which we’d like to generate the activation map. In this example, we’ve used the following [ImageNet classes](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a): `Goldfish`, `Siberian Husky`, and `Mushroom`.\n\n```python\n# use the pretrained ResNet50 model\nmodel = models.resnet50(pretrained=True)\nmodel.eval()\n\n# fix target class label (of the Imagenet class of interest!)\n# 1: goldfish, 250: Siberian Husky, 947: mushroom\n\ntargets = [ClassifierOutputTarget(<target-class-number>)] \n\n# fix the target layer (after which we'd like to generate the CAM)\ntarget_layers = [model.layer4]\n```\n\nWe can instantiate the model, preprocess the image, generate and display the class activation map.\n\n```python\n# instantiate the model\ncam = GradCAM(model=model, target_layers=target_layers) # use GradCamPlusPlus class\n\n# Preprocess input image, get the input image tensor\nimg = np.array(PIL.Image.open('<image-file-path>'))\nimg = cv2.resize(img, (300,300))\nimg = np.float32(img) / 255\ninput_tensor = preprocess_image(img)\n\n# generate CAM\ngrayscale_cams = cam(input_tensor=input_tensor, targets=targets)\ncam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n\ncam = np.uint8(255*grayscale_cams[0, :])\ncam = cv2.merge([cam, cam, cam])\n\n# display the original image & the associated CAM\nimages = np.hstack((np.uint8(255*img), cam_image))\nPIL.Image.fromarray(images)\n```\n\nWe can interpret the class activation map as a heatmap in which the regions in red are the most salient for a particular prediction, and the regions in blue are the least salient.\n\n![Goldfish Grad-CAM](/images/goldfish-gradcam.jpg)\n<small>Activation Map for Class Goldfish (ImageNet Class #1)</small>\n\n![Husky Grad-CAM](/images/husky-gradcam.jpg)\n<small>Activation Map for Class Siberian Husky (ImageNet Class #250)</small>\n\nSo far, the targets were present only once in the entire image. Now, consider the following image with many small mushrooms, each having a very small spatial footprint.\n\n![Mushroom spacial footprint](/images/multiple--instances-mushroom.jpg)\n\nIn this case, the activation map generated using GradCAM++ better identifies all instances of mushroom than the one from GradCAM.\n\n![Mushroom Grad-CAM](/images/mushrooms-gradcam.png)\n<small>Grad-CAM Output for Multiple Occurrences of Class Mushroom (ImageNet Class #947)</small>\n\n![Mushroom Grad-CAM++](/images/mushrooms-gradcam++.png)\n<small>Grad-CAM++ Output for Multiple Occurrences of Class Mushroom (ImageNet Class #947)</small>\n\nAs a next step, you can try generating activation maps for any class or other vision task of your choice. \n\n## Summing Up\n\nI hope you enjoyed this tutorial on explaining ConvNets with activation maps. Here’s a summary of what you’ve learned.\n\n- Class activation map (CAM) uses the notion of global average pooling (GAP) and learns weights from the output of the GAP layer onto the output classes. The class activation map of any target class is a weighted combination of feature maps.\n- Grad-CAM uses the gradients available in the network and does not require learning additional models to explain the ConvNet’s predictions. The gradients of the output with respect to the feature maps from the last convolutional layer are used as the weights.\n- Grad-CAM++ provides better performance under *low spatial footprint*. Instead of scaling all pixels in a feature map by a constant factor, Grad-CAM++ uses *larger* scaling factors for pixel locations that are *salient* for a particular class. These scaling factors are obtained from higher-order gradients in the ConvNet.\n\nIf you’d like to delve deeper, consider checking out the resources below. Happy learning!\n\n## References\n\n[1] Bolei Zhou et al., [Learning Deep Features for Discriminative Localization](https://arxiv.org/abs/1512.04150), 2015\n\n[2] Springenberg and Dosovitskiy et al., [Striving for Simplicity: The All Convolutional Net](https://arxiv.org/abs/1412.6806), ICLR 2015\n\n[3] R Selvaraju et al., [Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localizations](https://arxiv.org/abs/1610.02391), ICCV 2017\n\n[4] A Chattopadhyay et al., [Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks](https://arxiv.org/abs/1710.11063), WACV 2018\n\n[5] B Zhou et al., [Object Detectors Emerge in Deep Scene CNNs](https://arxiv.org/abs/1412.6856), ICLR 2015\n\n[6] [Jacob Gildenblat](https://github.com/jacobgil) and contributors, [PyTorch Library for CAM Methods](https://jacobgil.github.io/pytorch-gradcam-book/), GitHub, 2021\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbd7"
  },
  "filename": "mem-semantic-search.md",
  "title": "ebook-post",
  "category": "\"Building the Self-Organizing Workspace at Mem\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Building the Self-Organizing Workspace at Mem\"\nheadline: \"Building the Self-Organizing Workspace at Mem\"\ncategories:\n  - Vector Search in the Wild\ntoc: >-\nweight: 2\nauthor:\n  name: Isabella Fulford\n  position: Software Engineer, Mem Labs\n  src: /images/isabella-fulford-2.png\n  href: \"https://www.linkedin.com/in/isabella-fulford/\"\ndescription: How Mem is harnessing large language models and vector search to unlock the collective intelligence of humanity.\n# Open graph\nimages: [\"/images/building-mem-x.png\"]\nthumbnail: \"/images/building-mem-x.png\"\n---\n\n![Building Mem X](/images/building-mem-x.png)\n\n*Written by [Isabella Fulford](https://www.linkedin.com/in/isabella-fulford/) for [the Mem.ai blog](https://get.mem.ai/blog/building-mem-x). Reposted with permission.*\n\nOver the course of our lives, we spend a vast amount of time creating and capturing information. Yet we lack the ability to usefully draw from this well of knowledge, as it often becomes lost in folders or information silos.\n\nAt [Mem](https://mem.ai), we are building a world in which every person has access to the information they need when they need it. We leverage AI technology to create a self-organizing workspace that automatically organizes all of the information in your work life and proactively surfaces relevant knowledge.\n\nOur long-term mission is to unlock the collective intelligence of humanity. To realize our vision for the future, we are harnessing a technological inflection point: the quality of publicly available foundation models.\n\nRecent breakthroughs in large language models (LLMs) like GPT-3 have drastically changed the field of Natural Language Processing (NLP). Unlike previous generations of NLP that required the construction of separate models for each specific language task, these LLMs are not specialized for any particular task. With a small amount of fine-tuning, we have been able to optimize these pre-trained LLMs for our own use cases.\n\nMost recently, creators of LLMs have also started to open up their black boxes, releasing access to internal layers from within those models. [OpenAI’s embeddings models](https://beta.openai.com/docs/guides/embeddings/what-are-embeddings), for example, allow users to access the embedding layers that encode a text’s meaning, giving more insight into the fundamental building blocks of their NLP AI than the direct GPT-3 output alone can provide. [Embeddings](/learn/vector-embeddings/) are high-dimensional vectors that encode different features of text documents, including meaning, structure, content, theme and topic. Texts with similar meanings will have similar vector representations, and by comparing embeddings of different pieces of text, we can measure the similarity between them. Embeddings make natural language tasks such as [semantic search](/learn/nlp/) and clustering of similar documents easy to perform.\n\nThe ability to carry out these similarity calculations at query time is critical when building products that rely on embeddings. [Pinecone](/) is a leader in the vector search space, and their vector database allows users to store embeddings and quickly query for similar embeddings based on various similarity measures and filters.\n\nWe leverage both OpenAI embeddings models and Pinecone [vector search](/learn/vector-search-basics/) as fundamental pillars of Mem X. These technologies power features such as *similar mems* and *smart results*, among others. *Similar mems* surfaces documents that are semantically similar to the document a user is viewing, allowing users to discover knowledge from across their team, re-discover knowledge they forgot they had, and make new connections between pieces of information they might not have otherwise seen. *Smart results* allows users to ask Mem questions as though it were a person – e.g., “How many people did we add to the Mem X waitlist in March?”.  With *smart results*, Mem understands the semantic meaning of a user's search query and then finds the most relevant results.\n\nOpenAI offers different embeddings models specialized for different functionalities. We use the text similarity and text search models. The similarity embeddings are good at capturing semantic similarity between multiple pieces of text, and the text search embeddings are trained to measure whether long documents are relevant to a short search query.\n\n![Mem X overview](/images/mem-x-overview.png)\n\nWe transform each document into a format that can be embedded, and use OpenAI's embeddings API to create two embeddings for the document, one with a similarity model and the other with a search model. The embeddings are stored in a Pinecone index, along with metadata about the document. We leverage Pinecone's namespaces to create divisions between vectors that are produced by different models. As a user edits a document, we continuously re-compute the embeddings for this document and upsert the new embeddings to the Pinecone index to ensure that our embeddings are always up to date.\n\n![Smart results Pinecone index](/images/mem-x-pinecone-index.png)\n\nIn the case of *smart results*, when a user makes a search, we parse and transform the search query before creating an embedding with one of OpenAI's search query models, and then query the Pinecone index to find the most similar search documents (i.e. documents with the highest cosine similarity score). Pinecone's metadata filtering functionality allows us to query for only those embeddings that represent documents to which the currently signed-in user has access. We then reconcile the search results returned from Pinecone with our non-semantic search service to improve keyword results, and display the documents corresponding to these embeddings.\n\n![Similar mems Pinecone index](/images/mem-x-similar-mems.png)\n\nIn the *similar mems* feature, when a user views a document, we fetch the embedding for the document from the Pinecone index, then query the index for the most similar embeddings according to metadata filters. We re-rank and re-weight these similar embeddings based on our own clustering and length normalization algorithms, and surface the documents that the embeddings most closely correspond to.\n\nOver time, we will be able to automatically organize **all** of the information that exists within an organization, from employee information to customer data, internal documents, research, emails, Slack messages, and more.\n\n\n**Learn more about Mem:**\n\n- [Mem X Demo Video](https://www.loom.com/share/55d176c8bd70400dac7b4a017dc907d8)\n- [Introducing Mem X and Teams](https://get.mem.ai/blog/introducing-mem-x-and-teams)\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbd8"
  },
  "filename": "locality-sensitive-hashing.md",
  "title": "ebook-post",
  "category": "\"Locality Sensitive Hashing (LSH)",
  "content": "---\nlayout: ebook-post\ntitle: \"Locality Sensitive Hashing (LSH): The Illustrated Guide\"\nheadline: \"Locality Sensitive Hashing (LSH): The Illustrated Guide\"\ncategories:\n  - \"Faiss: The Missing Manual\"\ntoc: >-\nweight: 3\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: The magic, theory, and practice of Locality Sensitive Hashing.\n#Open Graph\nimages: ['/images/locality-sensitive-hashing-1.jpeg']\n---\n\n**Locality sensitive hashing (LSH)** is a widely popular technique used in *approximate* nearest neighbor (ANN) search. The solution to efficient similarity search is a profitable one — it is at the core of several billion (and even trillion) dollar companies.\n\nBig names like Google, Netflix, Amazon, Spotify, Uber, and countless more rely on similarity search for many of their core functions.\n\nAmazon uses similarity search to compare customers, finding new product recommendations based on the purchasing history of their highest-similarity customers.\n\nEvery time you use Google, you perform a similarity search between your query/search term — and Google’s indexed internet.\n\nIf Spotify manages to recommend good music, it’s because their similarity search algorithms are successfully matching you to other customers with a similarly good (or not so good) taste in music.\n\nLSH is one of the original techniques for producing high quality search, while maintaining lightning fast search speeds. In this article we will work through the theory behind the algorithm, alongside an easy-to-understand implementation in Python!\n\nYou can find a video walkthrough of this article here:\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/e_SBq3s20M8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n## Search Complexity\n\nImagine a dataset containing millions or even *billions* of samples — how can we efficiently compare all of those samples?\n\nEven on the best hardware, comparing all pairs is out of the question. This produces an at best complexity of O(n²). Even if comparing a single query against the billions of samples, we still return an at best complexity of O(n).\n\nWe also need to consider the complexity behind a single similarity calculation — every sample is stored as a vector, often very highly-dimensional vectors — this increases our complexity even further.\n\nHow can we avoid this? Is it even possible to perform a search with sub-linear complexity? *Yes, it is!*\n\nThe solution is *approximate* search. Rather than comparing every vector (*exhaustive* search) — we can approximate and limit our search scope to only the most relevant vectors.\n\nLSH is one algorithm that provides us with those sub-linear search times. In this article, we will introduce LSH and work through the logic behind the magic.\n\n---\n\n## Locality Sensitive Hashing\n\nWhen we consider the complexity of finding similar pairs of [vectors](/learn/dense-vector-embeddings-nlp), we find that  the number of calculations required to compare everything is unmanageably enormous even with reasonably small datasets.\n\nLet’s consider a vector index. If we were to introduce just one new vector and attempt to find the closest match — we must compare that vector to every other vector in our database. This gives us a *linear time complexity* — which cannot scale to fast search in larger datasets.\n\nThe problem is even worse if we wanted to compare all of those vectors against each other — the optimal approach sorting method to achieve this is at best *log-linear time complexity*.\n\nSo we need a way to reduce the number of comparisons. Ideally, we want only to compare vectors that we believe to be potential matches — or *candidate pairs*.\n\nLocality sensitive hashing (LSH) allows us to do this.\n\nLSH consists of a variety of different methods. In this article, we’ll be covering the traditional approach — which consists of multiple steps — shingling, MinHashing, and the final banded LSH function.\n\nAt its core, the final LSH function allows us to segment and hash the same sample several times. And when we find that a pair of vectors has been hashed to the same value *at least once* , we tag them as *candidate pairs* — that is, *potential* matches.\n\nIt is a very similar process to that used in Python dictionaries. We have a key-value pair which we feed into the dictionary. The key is processed through the dictionary hash function and mapped to a specific bucket. We then connect the respective value to this bucket.\n\n![A typical hash function aims to place different values (no matter how similar) into separate buckets.](/images/locality-sensitive-hashing-2.jpeg)<small>A typical hash function aims to place different values (no matter how similar) into separate buckets.</small>\n\nHowever, there is a key difference between this type of hash function and that used in LSH. With dictionaries, our goal is to minimize the chances of multiple key-values being mapped to the same bucket — we *minimize collisions*.\n\nLSH is almost the opposite. In LSH, we want to *maximize collisions* — although ideally only for *similar* inputs.\n\n![An LSH function aims to place similar values into the same buckets.](/images/locality-sensitive-hashing-3.jpeg)<small>An LSH function aims to place similar values into the same buckets.</small>\n\nThere is no *single* approach to hashing in LSH. Indeed, they all share the same *‘bucket similar samples through a hash function’* logic , but they can vary a lot beyond this.\n\nThe method we have briefly described and will be covering throughout the remainder of this article could be described as the *traditional* approach, using *shingling*, *MinHashing*, and *banding*.\n\nThere are several other techniques, such as [Random Projection](/learn/locality-sensitive-hashing-random-projection/) which we cover in another article.\n\n---\n\n## Shingling, MinHashing, and LSH\n\nThe LSH approach we’re exploring consists of a three-step process. First, we convert text to sparse vectors using *k-shingling (and one-hot encoding)*, then use *minhashing* to create ‘signatures’ — which are passed onto our LSH process to weed out *candidate pairs*.\n\n![A high-level view of the LSH process we will be working through in this article.](/images/locality-sensitive-hashing-4.jpeg)<small>A high-level view of the LSH process we will be working through in this article.</small>\n\nWe will discuss some of the other LSH methods in future articles. But for now, let’s work through the traditional process in more depth.\n\n### k-Shingling\n\nk-Shingling, or simply shingling — is the process of converting a string of text into a set of ‘shingles’. The process is similar to moving a window of length k down our string of text and taking a picture at each step. We collate all of those pictures to create our *set* of shingles.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/locality-sensitive-hashing-5.mp4\" type=\"video/mp4\">\n</video>\n\n<small class=\"video\">k-Shingling consists of moving through a string and adding **k** characters at a time to a ‘shingle set’.</small>\n\nShingling also removes duplicate items (hence the word ‘set’). We can create a simple k-shingling function in Python like so:\n\n{{< notebook file=\"shingles\" height=\"full\" >}}\n\nAnd with this, we have our shingles. Next, we create our sparse vectors. To do this, we first need to union all of our sets to create one big set containing *all* of the shingles across all of our sets — we call this the vocabulary (or vocab).\n\n![All of our shingled sets are merged to create our **vocab**.](/images/locality-sensitive-hashing-6.jpeg)*All of our shingled sets are merged to create our **vocab**.*\n\nWe use this vocab to create our sparse vector representations of each set. All we do is create an empty vector full of zeros and the same length as our vocab — then, we look at which shingles appear in our set.\n\n![To create our one-hot encoding our single shingle set is matched up to our **vocab** which indicates where in our zero vector we should place **ones** (we use a shingle-to-index dictionary in our code).](/images/locality-sensitive-hashing-7.jpeg)*To create our one-hot encoding our single shingle set is matched up to our **vocab** which indicates where in our zero vector we should place **ones** (we use a shingle-to-index dictionary in our code).*\n\nFor every shingle that appears, we identify the position of that shingle in our vocab and set the respective position in our new zero-vector to 1. Some of you may recognize this as *one-hot encoding*. \n\n### Minhashing\n\nMinhashing is the next step in our process, allowing us to convert our sparse vectors into dense vectors. Now, as a pre-warning — this part of the process can seem confusing initially — but it’s very simple once you get it.\n\nWe have our sparse vectors, and what we do is randomly generate one minhash function for every position in our signature (e.g., the dense vector).\n\nSo, if we wanted to create a dense vector/signature of 20 numbers — we would use 20 minhash functions.\n\nNow, those MinHash functions are simply a randomized order of numbers — and we count from *1* to the final number (which is len(vocab)). Because the order of these numbers has been randomized, we may find that number *1 *is in position *57* (for example) of our randomized MinHash function.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/locality-sensitive-hashing-8.mp4\" type=\"video/mp4\">\n</video>\n\n<small class=\"video\">Our signature values are created by first taking a randomly permuted count vector (from **1** to **len(vocab)+1**) and finds the minimum number that aligns with a **1** in our sparse vector.</small>\n\nAbove, we’re using a smaller vocab containing *six* values so we can easily visualize the process.\n\nWe look at our sparse vector and say, “did this shingle at vocab[1] exist in our set?”. If it did — the sparse vector value will be 1 — in this case, it did *not* exist (hence the 0 value). So, we move to number *2*, identify its position (0) and ask the same question. This time, the answer is *yes,* and so our minhash output is **2**.\n\nThat’s how we produce one value in our minhash signature. But we need to produce 20 (or more) of these values. So, we assign a different minhash function to each signature position — and repeat the process.\n\n![Here we using four minhash functions/vectors to create a four-digit signature vector. If you count (from **one**) in each minhash function, and identify the first value that aligns with a **one** in the sparse vector — you will get **2412**.](/images/locality-sensitive-hashing-9.jpeg)*Here we using four minhash functions/vectors to create a four-digit signature vector. If you count (from **one**) in each minhash function, and identify the first value that aligns with a **one** in the sparse vector — you will get **2412**.*\n\nAt the end of this, we produce our minhash signature — or dense vector.\n\nLet’s go ahead and write that in code. We have three steps:\n\n**1.** Generate a randomized MinHash vector.**\n\n{{< notebook file=\"hash-examples\" height=\"full\" >}}\n\n**2.** Loop through this randomized MinHash vector (starting at 1), and match the index of each value to the equivalent values in the sparse vector a_1hot. If we find a 1 — that index is our signature value.\n\n{{< notebook file=\"create-signature-val\" height=\"full\" >}}\n\n**3.** Build a signature from multiple iterations of **1** and **2** (we’ll formalize the code from above into a few easier to use functions):\n\n{{< notebook file=\"build-full-signatures\" height=\"full\" >}}\n\nAnd that is MinHashing — it’s really nothing more complex than that. We’ve taken a sparse vector and compressed it into a more densely packed, 20-number signature.\n\n### Information Transfer from Sparse to Signature\n\nIs the information truly maintained between our much larger sparse vector and much smaller dense vector? It’s not easy for us to visually identify a pattern in these new dense vectors — but we can calculate the similarity between vectors.\n\nIf the information truly has been retained during our downsizing — surely the similarity between vectors will be similar too?\n\nWell, we can test that. We use Jaccard similarity to calculate the similarity between our sentences in *shingle* format — then repeat for the same vectors in signature format:\n\n{{< notebook file=\"check-jaccard-a\" height=\"full\" >}}\n\nWe see pretty close similarity scores for both — so it seems that the information is retained. Let’s try again for b and c:\n\n{{< notebook file=\"check-jaccard-bc\" height=\"full\" >}}\n\nHere we find much higher similarity, as we would expect — it looks like the similarity information is maintained between our sparse vectors and signatures! So, we’re now fully prepared to move onto the LSH process.\n\n---\n\n## Band and Hash\n\nThe final step in identifying similar sentences is the LSH function itself.\n\nWe will be taking the banding approach to LSH — which we could describe as the traditional method. It will be taking our signatures, hashing segments of each signature, and looking for hash collisions — as we described earlier in the article.\n\n![A high-level view of the signature-building process. We take our text, build a shingle set, one-hot encode it using our **vocab**, and process it through our minhashing process.](/images/locality-sensitive-hashing-10.jpeg)<small>A high-level view of the signature-building process. We take our text, build a shingle set, one-hot encode it using our **vocab**, and process it through our minhashing process.</small>\n\nThrough this method, we produce these vectors of equal length that contain positive integer values in the range of 1 → len(vocab) — these are the signatures that we typically input into *this* LSH algorithm.\n\nNow, if we were to hash each of these vectors as a whole, we may struggle to build a hashing function that accurately identifies similarity between them — we don’t require that the full vector is equal, only that parts of it are similar.\n\nIn most cases, even though parts of two vectors may match perfectly — if the remainder of the vectors are not equal, the function will likely hash them into *separate* buckets.\n\nWe don’t want this. We want signatures that share even some similarity to be hashed into the same bucket , thus being identified as candidate pairs.\n\n### How it Works\n\nThe banding method solves this problem by splitting our vectors into sub-parts called *bands* b. Then, rather than processing the full vector through our hash function, we pass each band of our vector through a hash function.\n\nImagine we split a 100-dimensionality vector into 20 bands. That gives us 20 opportunities to identify matching sub-vectors between our vectors.\n\n![We split our signature into **b** sub-vectors, each is processed through a hash function (we can use a single hash function, or **b** hash functions) and mapped to a hash bucket.](/images/locality-sensitive-hashing-11.jpeg)<small>We split our signature into **b** sub-vectors, each is processed through a hash function (we can use a single hash function, or **b** hash functions) and mapped to a hash bucket.</small>\n\nWe can now add a more flexible condition — given a collision between any two sub-vectors, we consider the respective full vectors as candidate pairs.\n\n![We split the signatures into subvectors. Each equivalent subvector across all signatures must be processed through the same hash function. However, it is not necessary to use different hash functions for each subvector (we can use just one hash function for them all).](/images/locality-sensitive-hashing-12.jpeg)<small>We split the signatures into subvectors. Each equivalent subvector across all signatures must be processed through the same hash function. However, it is not necessary to use different hash functions for each subvector (we can use just one hash function for them all).</small>\n\nNow, only part of the two vectors must match for us to consider them. But of course, this also increases the number of false positives (samples that we mark as candidate matches where they are not similar). However, we do try to minimize these as far as possible.\n\nWe can implement a simple version of this. First, we start by splitting our signature vectors **a**, **b**, and **c**:\n\n{{< notebook file=\"split-vectors\" height=\"full\" >}}\n\nThen we loop through the lists to identify any matches between sub-vectors. If we find *any* matches — we take those vectors as candidate pairs.\n\n{{< notebook file=\"candidates\" height=\"full\" >}}\n\nWe find that our two more similar sentences, **b**, and **c **— are identified as candidate pairs. The less similar of the trio, **a** — is not identified as a candidate. This is a good result, but if we want to really test LSH, we will need to work with more data.\n\n---\n\n## Testing LSH\n\nWhat we have built thus far is a very inefficient implementation — if you want to implement LSH, this is certainly not the way to do it. Rather, use a library built for similarity search — like [Faiss](https://www.pinecone.io/learn/faiss/), or a managed solution like Pinecone.\n\nBut working through the code like this should — if nothing else — make it clear how LSH works. However, we will now be replicating this for much more data, so we will rewrite what we have so far using Numpy.\n\nThe code will function in the same way — and you can find each of the functions (alongside explanations) in [this notebook](https://github.com/pinecone-io/examples/blob/master/locality_sensitive_hashing_traditional/testing_lsh.ipynb).\n\n### Getting Data\n\nFirst, we need to get data. There is a great repository [here](https://github.com/brmson/dataset-sts) that contains several datasets built for similarity search testing. We will be extracting a set of sentences from [here](https://github.com/brmson/dataset-sts/blob/master/data/sts/sick2014/SICK_train.txt).\n\n{{< notebook file=\"download\" height=\"full\" >}}\n\n### Shingles\n\nOnce we have our data, we can create our one-hot encodings — this time stored as a NumPy array ([find full code and functions here](https://github.com/pinecone-io/examples/blob/master/locality_sensitive_hashing_traditional/testing_lsh.ipynb)).\n\n{{< notebook file=\"shingle\" height=\"full\" >}}\n\nNow we have our one-hot encodings. The shingles_1hot array contains *4500 *sparse vectors, where each vector is of length *36466*.\n\n### MinHashing\n\nAs before, we will compress our sparse vectors into dense vector *‘signatures’* with minhashing. Again, we will be using our NumPy implementation, which you can find the full code here.\n\n{{< notebook file=\"minhash\" height=\"full\" >}}\n\nWe’ve compressed our sparse vectors from a length of *36466* to signatures of length *100*. A big difference, but as we demonstrated earlier, this compression technique retains similarity information very well.\n\n### LSH\n\nFinally, onto the LSH portion. We will use a Python dictionary here to hash and store our candidate pairs — again. [The full code is here.](https://github.com/pinecone-io/examples/blob/master/locality_sensitive_hashing_traditional/testing_lsh.ipynb)\n\n{{< notebook file=\"lsh\" height=\"full\" >}}\n\nIt’s important to note that our lsh.buckets variable actually contains a separate dictionary for each band — we do *not* mix buckets between different bands.\n\nWe see in our buckets the vector IDs (row numbers) , so all we need to do to extract our candidate pairs is loop through all buckets and extract pairs.\n\n{{< notebook file=\"check-candidates\" height=\"full\" >}}\n\nAfter identifying our candidate pairs, we would restrict our similarity calculations to those pairs only — we will find that some will be within our similarity threshold, and others will not.\n\nThe objective here is to restrict our scope and reduce search complexity while still maintaining high accuracy in identifying pairs.\n\nWe can visualize our performance here by measuring the candidate pair classification (1 or 0) against actual cosine (or Jaccard) [similarity](/learn/semantic-search/).\n\n![Chart showing the distribution of candidate-pairs (1s) and non-candidates (0s) against the cosine similarity of pair signatures.](/images/locality-sensitive-hashing-13.jpeg)<small>Chart showing the distribution of candidate-pairs (1s) and non-candidates (0s) against the cosine similarity of pair signatures.</small>\n\nNow, this may seem like a strange way to visualize our performance — and you are correct, it is — but we do have a reason.\n\n### Optimizing the Bands\n\nIt is possible to optimize our band value b to shift the similarity threshold of our LSH function. The similarity threshold is the point at which we would like our LSH function to switch from a non-candidate to a candidate pair.\n\nWe formalize this probability-similarity relationship as so:\n\n![Probability (P) of a pair being identified as candidate pairs given a similarity score (s), number of bands (b), and number of rows in each band (r).](/images/locality-sensitive-hashing-14.png)*Probability (P) of a pair being identified as candidate pairs given a similarity score (s), number of bands (b), and number of rows in each band (r).*\n\nNow, if we were to visualize this probability-similarity relationship for our current b and r values we should notice a pattern:\n\n![Candidate classification (left y-axis) and calculated probability **P** (right y-axis) against similarity (calculated or normalized cosine similarity). This shows that our calculated probability **P **and similarity **s** values indicate the general distribution of candidate/non-candidate pairs. The **b** and **r** values are **20** and **5** respectively.](/images/locality-sensitive-hashing-15.jpeg)<small>Candidate classification (left y-axis) and calculated probability **P** (right y-axis) against similarity (calculated or normalized cosine similarity). This shows that our calculated probability **P **and similarity **s** values indicate the general distribution of candidate/non-candidate pairs. The **b** and **r** values are **20** and **5** respectively.</small>\n\nAlthough the alignment isn’t perfect, we can see a correlation between the theoretical, calculated probability — and the genuine candidate pair results. Now, we can push the probability of returning candidate-pairs at different similarity scores left or right by modifying b:\n\n![Calculated probability **P** against similarity **s** for different **b** values. Note that **r** will be **len(signature) / b** (in this case **len(signature) == 100**).](/images/locality-sensitive-hashing-16.jpeg)<small>Calculated probability **P** against similarity **s** for different **b** values. Note that **r** will be **len(signature) / b** (in this case **len(signature) == 100**).</small>\n\nThese are our calculated probability values. If we decided that our previous results where b == 20 required too high a similarity to count pairs as candidate pairs — we would attempt to shift the similarity threshold to the left.\n\nLooking at this graph, a b value of 25 looks like it could shift our genuine results just enough. So, let’s visualize our results when using b == 25:\n\n![Results for real and simulated results when **b == 25** are displayed with blue and magenta. Our previous LSH results (teal) are displayed for comparison. Note that this has created more candidate pairs.](/images/locality-sensitive-hashing-17.jpeg)<small>Results for real and simulated results when **b == 25** are displayed with blue and magenta. Our previous LSH results (teal) are displayed for comparison. Note that this has created more candidate pairs.</small>\n\nBecause we are now returning more candidate pairs, this will naturally result in more false positives — where we return ‘candidate pair’ for dissimilar vectors. This is an unavoidable consequence of modifying b, which we can visualize as:\n\n![Increasing **b** (shifting left) increases  FPs while decreasing FNs.](/images/locality-sensitive-hashing-18.png)<small>Increasing **b** (shifting left) increases  FPs while decreasing FNs.</small>\n\nGreat! We’ve built our LSH process from scratch — and even managed to tune our similarity threshold.\n\nThat’s everything for this article on the principles of LSH. Not only have we covered LSH, but also shingling and the MinHash function!\n\nIn practice, we would most likely want to implement LSH using libraries built specifically for similarity search. We will be covering LSH — specifically the random projection method — in much more detail, alongside its implementation in Faiss.\n\nHowever, if you’d prefer a quick rundown of some of the key indexes (and their implementations) in similarity search, we cover them all in our [overview of vector indexes](https://www.pinecone.io/learn/vector-indexes/).\n\n{{< newsletter text=\"Subscribe for the latest in similarity search!\" inputText=\"Email address...\" buttonText=\"Submit\">}}\n\n---\n\n## Further Resources\n\n* [Jupyter Notebooks](https://github.com/pinecone-io/examples/tree/master/locality_sensitive_hashing_traditional)\n* J. Ullman et al., [Mining of Massive Datasets](http://mmds.org/#ver30)\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbd9"
  },
  "filename": "transformers.md",
  "title": "post",
  "category": "\"Transformers Are All You Need\"",
  "content": "---\nlayout: post\ntitle: \"Transformers Are All You Need\"\nheadline: \"Transformers Are All You Need\"\ncategories:\n  - Algorithms & Libraries\ntoc: >-\nweight: 2\nauthor:\n  name: Diego Lopez Yse\n  position: Data Scientist\n  src: /images/diego-lopez-yse.jpeg\n  href: \"https://lopezyse.medium.com/\"\ndescription: A quick tour through the most popular Neural Net architecture.\nimages: ['/images/transformers-1.png']\n---\n\n**A quick tour through the most popular Neural Net architecture**\n\nHave you ever thought about what happens when you read a book? Unless you have a unique ability, you don’t process and memorize every single word and special character, right? What happens is that we represent events and characters to build our understanding of the story. We do this with a selective memorization that allows us to keep the most relevant pieces of information without needing to accumulate each minor detail.\n\nThis is exactly what [Hochreiter and Schmidhuber](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf) were looking for when they created the **Long Short Term Memory** (LSTM) model in 1997. LSTMs are types of [Recurrent Neural Networks](https://www.ibm.com/cloud/learn/recurrent-neural-networks) that emulate a selective memory approach, allowing them to store relevant information about the past in order to optimize a task. This impressive architecture ruled the sequential data landscape for over two decades and drove huge progress towards the way we understand different disciplines today. Unlike human memory, however, LSTMs can struggle when dealing with long-range contexts, as is the case with human language.\n\nThis limitation was particularly evident in language research that tried to move from keyword-based to more linguistic approaches in tasks like [information searching](https://www.searchenginewatch.com/2016/04/12/everything-you-need-to-know-about-natural-language-search/), [document classification](https://towardsdatascience.com/going-beyond-keywords-with-nlp-42395f7e7c67), or [question-answering](https://wlv.openrepository.com/handle/2436/254613). The nuances of human language were just too many for the Natural Language Processing discipline to keep up.\n\nNatural Language Processing (NLP) is a field of Artificial Intelligence that gives the machines the ability to read, understand, and derive meaning from human languages. It represents the automatic handling of natural human language like speech or text, and, because, as humans, we excel at understanding our language, we tend to underestimate how hard it is for machines to do it.\n\nDespite this,things have significantly changed in past years, and, although far from new, NLP is living a new age thanks to the invention of **Transformers**.\n\nTransformers represent new architectures of Artificial Neural Networks (ANN) that generalize to many NLP tasks with incredible results.\n\nTransformers have improved the performance of language models in a substantial way, significantly extending the model’s contextual processing. Interested in seeing how they perform? You can test Transformers [here](https://transformer.huggingface.co/) and [here](https://app.inferkit.com/demo).\n\n## The road to Transformers\n\nBefore getting to Transformers, let’s start by understanding what an Artificial Neural Network (ANN) is.\n\n**ANNs are computing systems composed of neurons, where each neuron individually performs only a simple computation.**\n\nThe power of an ANN comes from the complexity of the connections these neurons can form. ANNs accept input variables as information, weigh variables as knowledge, and output a prediction. Every ANN works this way.\n\nThe concept is far from new, since the first ANN (the Perceptron) was created in 1958. At the beginning, ANNs were built and used to solve basic tasks, but they rapidly evolved, becoming complex mechanisms able to solve challenges in areas like Computer Vision and Natural Language Processing.\n\n![The Perceptron](/images/transformers-1.png)\n\n<small>The Perceptron is the oldest Artificial Neural Network, created by Frank Rosenblatt in 1958. It has a single neuron and is the simplest form of a neural network. Source: [IBM](https://www.ibm.com/cloud/learn/neural-networks)</small>\n\n![Deep Neural Network](/images/transformers-2.png)\n\n<small>Artificial Neural Networks are composed of node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network. Source: [IBM](https://www.ibm.com/cloud/learn/neural-networks)</small>\n\nANNs architectures were expanded and improved in order to fit the complexity of the data we started gathering; Convolutional Neural Networks (CNNs) were designed to process spatial data like images, while Recurrent Neural Networks (RNNs) and Long Short Term Memories (LSTMs) were built to process sequential data like text.\n\nBut it wasn’t until just recently that something changed the way we conceived most of our challenges. [Introduced in 2017](https://arxiv.org/abs/1706.03762), Transformers rapidly showed effective results at modelling data with long-range dependencies. Originally thought to solve NLP tasks, the application of Transformers has expanded, reaching incredible accomplishments in many disciplines.\n\n**Healthcare**\n\nMany of the world’s greatest challenges, such as developing treatments for diseases or finding enzymes that break down industrial waste, are fundamentally tied to proteins and the role they play. A protein’s shape is closely linked to its function, and the [ability to predict this structure](https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology) unlocks a greater understanding of what it does and how it works. In a major scientific advance, DeepMind’s [AlphaFold](https://deepmind.com/research/case-studies/alphafold) system has been recognised as a solution to this grand challenge.\n\n<video autoplay loop muted playsinline class=\"responsive\"><source src=\"/images/transformers-1.mp4\" type=\"video/mp4\"></video>\n\n<small class=\"video\">Two examples of protein targets in the free modelling category. AlphaFold predicts highly accurate structures measured against experimental results. Source: [DeepMind](https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology)</small>\n\n**Self-driving cars**\n\nTesla’s strategy is built around its Artificial Neural Networks. Unlike many self-driving car companies, Tesla does not use lidar, a more expensive sensor that can see the world in 3D. It relies instead on interpreting scenes by using [neural network algorithms to parse input from its cameras and radar](https://www.wired.com/story/why-tesla-designing-chips-train-self-driving-tech/).\n\n<video autoplay loop muted playsinline class=\"responsive\"><source src=\"/images/transformers-2.mp4\" type=\"video/mp4\"></video>\n\n<small class=\"video\">Tesla’s neural networks can consistently detect objects in various visibility conditions. Source: [VentureBeat](https://venturebeat.com/2021/07/03/tesla-ai-chief-explains-why-self-driving-cars-dont-need-lidar/)</small>\n\n**Art generation**\n\nDALL·E parses text prompts and then responds not with words, but in pictures. It has been specifically trained to generate images from text descriptions, using a dataset of text-image pairs. The amazing thing is that DALL·E can do more than just paint a pretty picture from a caption: It can also, in a sense, [answer questions visually](https://thenextweb.com/news/heres-how-openais-magical-dall-e-generates-images-from-text-syndication). DALL·E is often able to solve matrices that involve continuing simple patterns or basic geometric reasoning.\n\n![DALL-E Output Example](/images/transformers-3.png)\n\n<small>Example of an output from DALL·E when prompted to generate an “armchair in the shape of an avocado”. Source: [OpenAI](https://openai.com/blog/dall-e/)</small>\n\nThese examples are impressive, and, although they seem unrelated, they have one common factor: They all use Transformers architectures. \n\nNow let’s see how Transformers work.\n\n## The anatomy of Transformers\n\nIt’s usually helpful to visualize things when trying to understand them. Let’s see what Transformers look like:\n\n![Transformer Architecture](/images/transformers-4.png)\n\n<small>The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. Source: [HarvardNLP](http://nlp.seas.harvard.edu/2018/04/03/attention.html)</small>\n\nToo much information, right? Let’s start with the basics. In very simple terms, a Transformer’s architecture consists of **encoder and decoder** components. The encoder receives an input (e.g. a sentence to be translated), processes it into a hidden representation, and passes it to the decoder, which returns an output (e.g. the translated sentence).\n\n![Encoding & Decoding Components](/images/transformers-5.png)\n\n<small>An encoding component, a decoding component, and connections between them. Source: [Jay Alammar](https://jalammar.github.io/)</small>\n\nGoing back to the model, we can find the encoder and decoder components as follows:\n\n\n![Encoder & Decoder Blocks](/images/transformers-6.png)\n\n<small>The encoder and decoder blocks are actually multiple identical encoders and decoders stacked on top of each other. Both the encoder stack and the decoder stack have the same number of units. Source: [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/)</small>\n\nTake a look at the image above. The encoder block has one layer of a Multi-Head Attention followed by another layer of Feed Forward Neural Network. The decoder, on the other hand, has an extra Masked Multi-Head Attention. What are those “attention” layers about?\n\nBefore Transformers, ANN architectures, like RNNs, had severe memory problems. In the case of RNNs, there’s a limited scope they can remember about long-range dependencies (the words they saw a long time ago that are somehow related to the next word). That is, [RNNs put too much emphasis on words being close to one another](https://wiki.pathmind.com/attention-mechanism-memory-network) and too much emphasis on upstream context over downstream context. Reading one word at a time, RNNs need to perform multiple steps to make decisions that depend on words far away from each other, which is incredibly slow.\n\nSelf-attention fixes this problem.\n\nUsing **self-attention** mechanisms, Transformers can capture the context of a word from distant parts of a sentence, both before and after the appearance of that word, in order to encode valuable information. Sentences are processed as a whole, rather than word by word. This way, Transformer models avoid suffering from long dependency issues and forgetting past information.\n\n![Encoder & Decoder Blocks](/images/transformers-7.png)\n\n<small>What does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm. As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word. Source: [Jay Alammar](https://jalammar.github.io/)</small>\n\n[Self-attention is computed not once but multiple times in the Transformer’s architecture, in parallel and independently](https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/) (aptly referred to as **Multi-head Attention**).\n\nAnd what about performance? The sequential nature of [RNNs makes it more difficult to fully take advantage of fast modern computing devices](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) such as TPUs and GPUs, which excel at parallel and non-sequential processing. Since the Transformer architecture lends itself to **parallelization**, we can really boost the speed with which these models can be trained.\n\n<video autoplay loop muted playsinline class=\"responsive\"><source src=\"/images/transformers-3.mp4\" type=\"video/mp4\"></video>\n\n<small class=\"video\">The Transformer starts by generating initial representations, or embeddings, for each word. These are represented by the unfilled circles. Then, using self-attention, it aggregates information from all of the other words, generating a new representation per word informed by the entire context, represented by the filled balls. This step is then repeated multiple times in parallel for all words, successively generating new representations. Source: [Google](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)</small>\n\nTransformers' successful results led to their escalation into massive models trained with absurd amounts of data, capable of performing the most diverse tasks.\n\n## Big kids on the block\n\nWhat do you get when you mix Transformers with huge volumes of data? Huge advances happened in the past years after training Transformers with massive data volumes.\n\n**BERT**\n\nBERT, or **B**idirectional **E**ncoder **R**epresentations from **T**ransformers, is one giant model designed by Google. While being conceptually simple, BERT obtains [new state-of-the-art results on eleven NLP tasks](https://www.topbots.com/leading-nlp-language-models-2020/), including question answering, named entity recognition and other tasks related to general language understanding.\n\nTrained on 2.5 billion words, its design allows the model to consider the context from both the left and the right sides of each word. For example, it can understand the semantic meanings of bank in the [following sentences](https://www.ibm.com/blogs/watson/2020/12/how-bert-and-gpt-models-change-the-game-for-nlp/): “Raise your oars when you get to the river bank” and “The bank is sending a new debit card.” To understand this, it uses left-to-right river and right-to-left debit card clues.\n\n**GPT**\n\nDeveloped by [OpenAI](https://openai.com/blog/gpt-3-apps/), **G**enerative **P**re-trained **T**ransformer (GPT) models require a small amount of input text to generate large volumes of relevant and sophisticated outputs. Unlike BERT, GPT models are unidirectional, and their main advantage is the magnitude of data they were pretrained on: GPT-3, the third-generation GPT model, [was trained on 175 billion parameters](https://openai.com/blog/gpt-3-apps/), about 10 times the size of previous models. This gigantic pretrained model provides users with the ability to fine-tune NLP tasks with very little data to accomplish novel tasks, like creating articles, poetry, stories, news reports and dialogue.\n\n![GPT Model Dataset](/images/transformers-8.png)\n\n<small>In general, the more parameters a model has, the more data is required to train the model. As per the creators, the OpenAI GPT-3 model has been trained on about 45 TB text data from multiple sources which include Wikipedia and books. Source: [Springboard Blog](https://www.springboard.com/blog/ai-machine-learning/machine-learning-gpt-3-open-ai/)</small>\n\n## MEGATRON-TURING\n\nYou thought GPT-3 was big? A couple of months ago, Microsoft and Nvidia released the Megatron-Turing Natural Language Generation model (MT-NLG), which is more than triple the size of GPT-3 at 530 billion parameters.\n\nAs you can imagine, getting to 530 billion parameters required quite a lot of input data and just as much computing power. The algorithm was [trained using an Nvidia supercomputer](https://singularityhub-com.cdn.ampproject.org/c/s/singularityhub.com/2021/10/13/microsofts-massive-new-language-ai-is-triple-the-size-of-openais-gpt-3/amp/) made up of 4,480 GPUs and an [estimated cost](https://www.nextplatform.com/2021/02/11/the-billion-dollar-ai-problem-that-just-keeps-scaling/) of over $85 million.\n\nThis massive model is skilled at tasks like completion prediction, reading comprehension, common-sense reasoning, natural language inferences, and word sense disambiguation.\n\n![NLP Models Size Trend](/images/transformers-9.jpg)\n\n<small>Trend of sizes of state-of-the-art NLP models over time. Source: [Microsoft](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/)</small>\n\n## What’s next?\n\nSince Google developed Transformers, most contributions in NLP have been more related to implementation volumes rather than to architectural improvements. And there’s a reason for this: Transformers just work.\n\nTransformers are a fascinating architecture to represent a wide variety of tasks, surprisingly versatile and robust enough to ingest incredible amounts of data. These architectures are so adaptable that we’re witnessing an explosion beyond NLP. Just this year, [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929) emerged as an alternative to Convolutional Neural Networks (CNNs), which are currently state-of-the-art models in computer vision. ViT models are [outperforming CNNs](https://viso.ai/deep-learning/vision-transformer-vit/) in terms of computational efficiency and accuracy, achieving highly competitive performance in tasks like image classification, object detection, and semantic image segmentation.\n\nWe’re probably only in the middle of the Transformers era, as models keep getting bigger and applied to different disciplines. It’s hard to say for how long, but given this pace, Transformers will keep on building great advancements in Machine Learning for years to come."
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbda"
  },
  "filename": "advisors.md",
  "title": "post",
  "category": "\"Former Snowflake CEO Bob Muglia and Couchbase CEO Bob Wiederhold Bet on Pinecone\"",
  "content": "---\nlayout: post\ntitle: \"Former Snowflake CEO Bob Muglia and Couchbase CEO Bob Wiederhold Bet on Pinecone\"\nheadline: \"Former Snowflake CEO Bob Muglia and Couchbase CEO Bob Wiederhold Bet on Pinecone\"\ncategories:\n  - Company News\ntoc: >-\nauthor:\n  name: Edo Liberty\n  position: Founder and CEO\n  src: /images/company-edo.png\n  href: https://edoliberty.github.io/\ndate: \"2021-12-20\"\n# Date: December 20, 2021\n# Open Graph\ndescription: We are ushering in a new era of search.\n# No image in article, default will be used\nthumbnail: \"/images/company-bob.jpeg\"\n---\n\nI’m excited to announce that two former CEOs are joining Pinecone as investors and advisors. Bob Muglia, former CEO of Snowflake, and Bob Wiederhold, former CEO of Couchbase, are working closely with me to help Pinecone become the leader in search infrastructure.\n\nComplex data, including unstructured text documents, call and video transcripts, customer histories, images, and audio, is growing. Companies need to search through this data for their search, personalization, and security applications. Yet they struggle to do so because they are stuck with tooling and infrastructure that was designed for more simple, structured text data. \n\nOur customers and companies like them are increasingly using ML models to transform complex data into vector embeddings. Vector embeddings are representations of complex data that enable searching through that data by similarity and semantic meaning. Existing databases can’t support vector search at scale. While a handful of hyperscalers have already implemented this technology, most companies don't have the time or resources to build and maintain the necessary infrastructure. Pinecone has built a vector database that makes it easy for everyone to leverage this state-of-the-art method of searching through complex data.\n\n## Bob Muglia\n\nAs the former CEO of data warehousing giant, Snowflake, Bob Muglia led the company to five years of unprecedented growth and raised hundreds of millions in funding. Prior to Snowflake, he served as one of four Division Presidents at Microsoft, managing more than 20% of the company's revenue including server products and cloud services.\n\nFrom Bob Muglia:\n\n\"Complex data such as images and video are a potential goldmine of business insight. People can intuitively understand the contents of an image but until the advent of advanced analytics and machine learning, these contents have been opaque to business systems. That has now changed but this requires a new kind of infrastructure that intersects databases and AI. Pinecone is an innovator and early mover in this space. I believe what they are building is cutting edge today but will become a well-understood standard in this decade.\"\n\n## Bob Wiederhold\n\nBob Wiederhold has more than 25 years of high tech experience, having previously served as CEO of Couchbase, a leader in NoSQL databases, and then its Executive Chairman. From 2002 to 2009, Bob was Chairman and CEO of Transitive, the worldwide leader in cross-platform virtualization with over 20M users, where during his tenure the company was acquired by IBM. Until 2001, Bob served as CEO of Tality Corporation, the worldwide leader in electronic design services whose revenues grew to over $200M.\n\nFrom Bob Wiederhold:\n\n\"Hyperscalers such as Google, Microsoft, Facebook, and Amazon invested years in incorporating vector databases and vector search into their applications. They are already seeing improved business outcomes. The majority of the tech world has either not realized it yet or is struggling to catch up. I'm excited to work with Pinecone on bringing this technology to companies who may lack the time, capital, or bandwidth to build such infrastructures in house.\"\n\n## Join Us at Pinecone\n\nBob Wiederhold and Bob Muglia are among the best people in the world to help us scale Pinecone. Their experiences in building great companies map almost exactly to the journey ahead of us. They have already made a huge impact on Pinecone and will undoubtedly continue to do so. \n\nIf you want to help us grow Pinecone into the next great innovator of database technology, check out [our open roles](/careers/). We are hiring in several areas including Engineering, Customer Success, Sales, and Product Management. Experience firsthand why the best minds in tech think Pinecone will be the next big thing in data. Come be a part of it.\n\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbdb"
  },
  "filename": "faiss-tutorial.md",
  "title": "ebook-post",
  "category": "\"Introduction to Facebook AI Similarity Search (Faiss)\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Introduction to Facebook AI Similarity Search (Faiss)\"\nheadline: \"Introduction to Facebook AI Similarity Search (Faiss)\"\ncategories:\n  - \"Faiss: The Missing Manual\"\ntoc: >-\nweight: 1\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: Learn how Facebook AI Similarity Search changes — search.\n# Open Graph\nimages: ['/images/faiss1.png']\n---\n\n<!-- ![Getting Started With FAISS](/images/faiss1.png) -->\n\nFacebook AI Similarity Search (Faiss) is one of the most popular implementations of efficient similarity search, but what is it — and how can we use it?\n\nWhat is it that makes [Faiss](https://github.com/facebookresearch/faiss) special? How do we make the best use of this incredible tool?\n\n---\n\n**Note: [Pinecone](/) lets you implement vector search into your applications with just a few API calls, without knowing anything about Faiss. However, you like seeing how things work, so enjoy the guide!**\n\n---\n\nFortunately, it’s a brilliantly simple process to get started with. And in this article, we’ll explore some of the options FAISS provides, how they work, and — most importantly — how Faiss can make our search faster.\n\nCheck out the video walkthrough here:\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/sKyvsdEv6rk\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n## What is Faiss?\n\nBefore we get started with any code, many of you will be asking — what is Faiss?\n\nFaiss is a library — developed by Facebook AI — that enables efficient similarity search.\n\nSo, given a set of [vectors](/learn/vector-embeddings/), we can index them using Faiss — then using another vector (the query vector), we search for the most similar vectors within the index.\n\nNow, Faiss not only allows us to build an index and search — but it also speeds up search times to ludicrous performance levels — something we will explore throughout this article.\n\n## Building Some Vectors\n\nThe first thing we need is data, we’ll be concatenating several datasets from this semantic test similarity hub repo. We will download each dataset, and extract the relevant text columns into a single list.\n\n{{< notebook file=\"get-sentence-data\" height=\"full\" >}}\n\nNext, we remove any duplicates, leaving us with 14.5K unique sentences. Finally, we build our dense vector representations of each sentence using the [sentence-BERT](/learn/semantic-search/) library.\n\n{{< notebook file=\"create-embeddings\" height=\"full\" >}}\n\nNow, building these sentence embeddings can take some time — so feel free to download them directly from here (you can use [this script](https://github.com/jamescalam/data/blob/main/sentence_embeddings_15K/download.py) to load them into Python).\n\n## Plain and Simple\n\nWe’ll start simple. First, we need to set up Faiss. Now, if you’re on Linux — you’re in luck — Faiss comes with built-in GPU optimization for any CUDA-enabled Linux machine.\n\nMacOS or Windows? Well, we’re less lucky.\n\n_(Don’t worry, it’s still ludicrously fast)_\n\nSo, CUDA-enabled Linux users, type `conda install -c pytorch faiss-gpu`. Everyone else, `conda install -c pytorch faiss-cpu`. If you don’t want to use `conda` there are alternative installation instructions [here](https://github.com/facebookresearch/faiss/blob/master/INSTALL.md).\n\nOnce we have Faiss installed we can open Python and build our first, plain and simple index with `IndexFlatL2`.\n\n## IndexFlatL2\n\n`IndexFlatL2` measures the L2 (or Euclidean) distance between _all_ given points between our query vector, and the vectors loaded into the index. It’s simple, _very_ accurate, but not too fast.\n\n![L2 distance calculation between a query vector xq and our indexed vectors (shown as y)](/images/faiss2.png)\n\n<small>L2 distance calculation between a query vector <b>xq</b> and our indexed vectors (shown as <b>y</b>)</small>\n\nIn Python, we would initialize our `IndexFlatL2` index with our vector dimensionality (`768` — the output size of our sentence embeddings) like so:\n\n{{< notebook file=\"IndexFlatL2-init\" height=\"full\" >}}\n\nOften, we’ll be using indexes that require us to train them before loading in our data. We can check whether an index needs to be trained using the `is_trained` method. `IndexFlatL2` is not an index that requires training, so we should return `False`.\n\nOnce ready, we load our embeddings and query like so:\n\n{{< notebook file=\"IndexFlatL2-add\" height=\"full\" >}}\n\nWhich returns the top `k` vectors closest to our query vector `xq` as `7460`, `10940`, `3781`, and `5747`. Clearly, these are all great matches — all including either people running with a football or in the _context_ of a football match.\n\nNow, if we’d rather extract the numerical vectors from Faiss, we can do that too.\n\n{{< notebook file=\"reconstruct\" height=\"full\" >}}\n\n### Speed\n\nUsing the `IndexFlatL2` index alone is computationally expensive, it doesn’t scale well.\n\nWhen using this index, we are performing an _exhaustive_ search — meaning we compare our query vector `xq` to every other vector in our index, in our case that is 14.5K L2-distance calculations for every search.\n\nImagine the speed of our search for datasets containing 1M, 1B, or even more vectors — and when we include several query vectors?\n\n![Milliseconds taken to return a result (y-axis) / number of vectors in the index (x-axis) — relying solely on IndexFlatL2 quickly becomes slow](/images/faiss3.png)\n\n<small>Milliseconds taken to return a result (y-axis) / number of vectors in the index (x-axis) — relying solely on IndexFlatL2 quickly becomes slow</small>\n\nOur index quickly becomes too slow to be useful, so we need to do something different.\n\n## Partitioning The Index\n\nFaiss allows us to add multiple steps that can optimize our search using many different methods. A popular approach is to partition the index into Voronoi cells.\n\n![We can imagine our vectors as each being contained within a Voronoi cell — when we introduce a new query vector, we first measure its distance between centroids, then restrict our search scope to that centroid’s cell.](/images/faiss4.png)\n\n<small>We can imagine our vectors as each being contained within a Voronoi cell — when we introduce a new query vector, we first measure its distance between centroids, then restrict our search scope to that centroid’s cell.</small>\n\nUsing this method, we would take a query vector `xq`, identify the cell it belongs to, and then use our `IndexFlatL2` (or another metric) to search between the query vector and all other vectors belonging to _that specific_ cell.\n\nSo, we are reducing the scope of our search, producing an _approximate_ answer, rather than exact (as produced through exhaustive search).\n\nTo implement this, we first initialize our index using `IndexFlatL2` — but this time, we are using the L2 index as a quantizer step — which we feed into the partitioning `IndexIVFFlat` index.\n\n{{< notebook file=\"IndexIVFFlat-init\" height=\"full\" >}}\n\nHere we’ve added a new parameter `nlist`. We use `nlist` to specify how many partitions (Voronoi cells) we’d like our index to have.\n\nNow, when we built the previous `IndexFlatL2`-only index, we didn’t need to train the index as no grouping/transformations were required to build the index. Because we added clustering with `IndexIVFFlat`, this is no longer the case.\n\nSo, what we do now is train our index on our data — which we must do _before_ adding any data to the index.\n\n{{< notebook file=\"IndexIVFFlat-train\" height=\"full\" >}}\n\nNow that our index is trained, we add our data just as we did before.\n\nLet’s search again using the same indexed sentence embeddings and the same query vector `xq`.\n\n{{< notebook file=\"IndexIVFFlat-search\" height=\"full\" >}}\n\nThe search time has clearly decreased, in this case, we don’t find any difference between results returned by our exhaustive search, and this approximate search. But, often this can be the case.\n\nIf approximate search with `IndexIVFFlat` returns suboptimal results, we can improve accuracy by increasing the search scope. We do this by increasing the `nprobe` attribute value — which defines how many nearby cells to search.\n\n![Searching the single closest cell when nprobe == 1 (left), and searching the eight closest cells when nprobe == 8 (right)](/images/faiss5.png)\n\n<small>Searching the single closest cell when <b>nprobe == 1</b> (left), and searching the eight closest cells when <b>nprobe == 8</b> (right)</small>\n\nWe can implement this change easily.\n\n{{< notebook file=\"IndexIVFFlat-nprobe\" height=\"full\" >}}\n\nNow, because we’re searching a larger scope by increasing the `nprobe` value, we will see the search speed increase too.\n\n![Query time / number of vectors for the IVFFlat index with different nprobe values — 1, 5, 10, and 20](/images/faiss6.png)\n\n<small>Query time / number of vectors for the IVFFlat index with different <b>nprobe</b> values — 1, 5, 10, and 20</small>\n\nAlthough, even with the larger `nprobe` value we still see much faster responses than we returned with our `IndexFlatL2`-only index.\n\n### Vector Reconstruction\n\nIf we go ahead and attempt to use `index.reconstruct(<vector_idx>)` again, we will return a `RuntimeError` as there is no direct mapping between the original vectors and their index position, due to the addition of the IVF step.\n\nSo, if we’d like to reconstruct the vectors, we must first create these direct mappings using `index.make_direct_map()`.\n\n{{< notebook file=\"make-direct-map\" height=\"full\" >}}\n\nAnd from there we are able to reconstruct our vectors just as we did before.\n\n## Quantization\n\nWe have one more key optimization to cover. All of our indexes so far have stored our vectors as full (eg `Flat`) vectors. Now, in very large datasets this can quickly become a problem.\n\nFortunately, Faiss comes with the ability to compress our vectors using _Product Quantization (PQ)_.\n\nBut, what is PQ? Well, we can view it as an additional approximation step with a similar outcome to our use of **IVF**. Where IVF allowed us to approximate by _reducing the scope_ of our search, PQ approximates the _distance/similarity calculation_ instead.\n\nPQ achieves this approximated similarity operation by compressing the vectors themselves, which consists of three steps.\n\n![Three steps of product quantization](/images/faiss7.png)\n\n<small>Three steps of product quantization</small>\n\n1. We split the original vector into several subvectors.\n2. For each set of subvectors, we perform a clustering operation — creating multiple centroids for each sub-vector set.\n3. In our vector of sub-vectors, we replace each sub-vector with the ID of it’s nearest set-specific centroid.\n\nTo implement all of this, we use the IndexIVF**PQ** index — we’ll also need to `train` the index before adding our embeddings.\n\n{{< notebook file=\"IndexIVFPQ-init\" height=\"full\" >}}\n\nAnd now we’re ready to begin searching using our new index.\n\n{{< notebook file=\"IndexIVFPQ-search\" height=\"full\" >}}\n\n### Speed or Accuracy?\n\nThrough adding PQ we’ve reduced our IVF search time from ~7.5ms to ~5ms, a small difference on a dataset of this size — but when scaled up this becomes significant quickly.\n\nHowever, we should also take note of the slightly different results being returned. Beforehand, with our exhaustive L2 search, we were returning `7460`, `10940`, `3781`, and `5747`. Now, we see a slightly different order of results — and two different IDs, `5013` and `5370`.\n\nBoth of our speed optimization operations, **IVF** and **PQ**, come at the cost of accuracy. Now, if we print out these results we will still find that each item is relevant:\n\n{{< notebook file=\"IndexIVFPQ-results\" height=\"full\" >}}\n\nSo, although we might not get the _perfect_ result, we still get close — and thanks to the approximations, we get a much faster response.\n\n![Query time / number of vectors for our three indexes](/images/faiss8.png)\n\n<small>Query time / number of vectors for our three indexes</small>\n\nAnd, as shown in the graph above, the difference in query times become increasingly relevant as our index size increases.\n\nThat’s it for this article! We’ve covered the essentials to getting started with building high-performance indexes for search in Faiss.\n\nClearly, a lot can be done using `IndexFlatL2`, `IndexIVFFlat`, and `IndexIVFPQ` — and each has many parameters that can be fine-tuned to our specific accuracy/speed requirements. And as shown, we can produce some truly impressive results, at lightning-fast speeds very easily thanks to Faiss.\n\n---\n\n**Want to run Faiss in production? [Pinecone](/) provides vector similarity search that's production-ready, scalable, and fully managed.**"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbdc"
  },
  "filename": "metarank.md",
  "title": "post",
  "category": "Hybrid Search and Learning-to-Rank with Metarank",
  "content": "---\nlayout: post\ntitle: Hybrid Search and Learning-to-Rank with Metarank\nheadline: Hybrid Search and Learning-to-Rank with Metarank\ncategories:\n  - Algorithms & Libraries\ntoc: >-\nauthor:\n  name: Vsevolod Goloviznin\n  position: Co-founder & CEO at Metarank\n  src: /images/vsevolod-goloviznin.jpeg\n  href: \"https://www.linkedin.com/in/vgoloviznin/\"\n# Open Graph\ndescription: How to do hybrid search, recommendation, and learn-to-rank with Elasticsearch, Pinecone, and Metarank.\nimages: [\"/images/metarank-1.jpg\"]\n---\n\nAs more advanced Large Language Models (LLMs) are released, the dream of an accurate semantic search comes closer to reality. But a classical term search is still hard to beat, even with the largest LLMs. So what if you don't need to choose between two approaches and combine them within a single hybrid multi-retriever system?\n\nIn this article, we're going to discuss a case when Elasticsearch, Opensearch, Solr, and [Pinecone](/) are used together to get the best from both words, with the final combined ranking produced with Metarank.\n\n## A Multiple Retrievers Problem\n\nA typical scenario for the multi-retriever setup is to have a traditional Lucene-based term search and a vector search engine installed side-by-side for the same collection of documents and query both  in parallel. A classical term search will return BM25-scored documents, and a vector search engine will try to produce a more semantically correct set of results. \n\n![Multi-retriever setup](/images/metarank-1.jpg)\n\nHowever, when you have multiple sources of documents to merge into a single search page, it may not be obvious how to combine them:\n\n- Different retrievers use non-comparable score types, like BM25 and cosine similarity.\n- Documents may come from single or multiple sources at the same time. There should be a way to deal with duplicates in the final ranking.\n\nIn a beautiful world where BM25 and cosine similarity have the same scale and statistical distribution, a final ranking can be done by just adding both scores together.\n\n### BM25 Score Distribution\n\nLucene-based classical search engines like Elastic, SOLR, and OpenSearch use BM25 as a scoring function to rank multiple found documents for a single search result listing. The BM25 formula depends on the lengths of query, document, and term frequencies, and its values are usually distributed within the low double-digits range.\n\nA practical example of BM25 values distribution is the [MSRD dataset](https://github.com/metarank/msrd): 28320 unique movie search queries over a database of 10k TMDB movie descriptions. In the diagram below, you can see how BM25 scores are distributed across different positions within search results:\n\n![BM25 values distribution, MSRD dataset](/images/metarank-2.png)\n\nFrom another perspective, BM25 scores of items in the first position have a non-even distribution, as you can see from the graph below:\n\n![BM25 values distribution on position=0](/images/metarank-3.png)\n\nSo in a real-world situation, the BM25 scores you may get from an Elasticsearch are unbound and non-evenly distributed, which is perfectly fine until you try to merge it with another non-even distribution.\n\n### Cosine Similarity Value Distribution\n\nDoes text embedding similarity from a vector search also have a non-even distribution? Let's take the same MSRD dataset of queries and movies, and push it through an **all-MiniLM-L6-v2** LLM with [SBERT](https://www.sbert.net/) to compute text embeddings. These embeddings can be later used inside a vector search engine like Pinecone. Still, to simplify things, we're only going to perform an in-memory k-NN search between queries and movie documents and compute the same cosine similarity distribution for top-N documents.\n\n![Cosine distance distribution per position, MSRD dataset](/images/metarank-4.png)\n\nAs you can see from the graphs above, the cosine similarity distribution between query-item embeddings is also not that even and differs quite a lot from the BM25 one:\n\n- Thick-tailed,\n- Has finite upper and lower bounds,\n- Much smaller peak on position-0.\n\nSo, what should we do if there’s no simple way to combine multiple rankings in a statistically correct way?\n\n## Learning-to-Rank for Multiple Retrievers\n\nFrom another perspective, BM25 scores and cosine similarity can be treated as input ranking features for a ranking-focused ML model like [LambdaMART](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf) and implicit customer feedback as labels. \n\nIf a visitor clicks on a document, it’s a positive signal; if ignored, it is negative. So we can ask this ML model to figure out the best way to merge both document sets into a single ranking to maximize a standard search relevance metric like NDCG.\n\n![BM25 score / cosine similarity](/images/metarank-5.jpg)\n\nBut how does the underlying ML model handle holes in the data when a document comes only from a single source and, for example, has only a BM25 score but not the cosine similarity? There are multiple approaches to solving this problem:\n\n- Replace all null values with an average or median value across the dataset/ranking.\n- Use a ranking algorithm that can handle missing values natively, like decision trees - which is a core building block of the [LambdaMART](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf) Learn-to-Rank algorithm.\n\nAll major LambdaMART algorithms implementations like [XGBoost](https://xgboost.readthedocs.io/en/stable/faq.html#how-to-deal-with-missing-values), [LightGBM](https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html#missing-value-handle), and [CatBoost](https://catboost.ai/en/docs/concepts/faq#preprocessing-of-missing-values) all handle missing values in a very similar fashion. A null is a separate, distinct feature value, which can also be part of a split while building yet another decision tree. In other words, each feature split has a default go-to route for a case when the value is missing.\n\nThis approach results in much more stable results for cases when the feature value is sparse, so the majority of its values are missing.\n\n### Metarank as a Secondary Re-ranker\n\nImplementing an in-house solution for a LambdaMART-style multi-retriever problem will require you to build a couple of data pipelines:\n\n- Collecting visitor feedback to log all clicks happening in search results.\n- Query logging: we should also collect both search queries and all the documents from all retrievers.\n- Feature logging: we should persist BM25 and cosine similarity scores for all query-document combinations to use it later during ML model training.\n\nIt takes time to build such a system in-house, and such an adventure is not always possible without a team with proper expertise in ranking optimization. But instead of doing everything from scratch, we suggest using existing open-source tools as it may save us quite some time in the whole journey.\n\n[Metarank](https://metarank.ai) is an open-source project focused on solving common ranking problems. It is flexible enough to be used as a secondary re-ranker for the multi-retriever use case we’re discussing.\n\n![Re-ranking](/images/metarank-6.jpg)\n\n\nA recent release of Pinecone also supports [traditional term search natively](/learn/hybrid-search-intro/), so in some cases you don’t need to configure two separate search engines and just use two different retrieval methods from only a single Pinecone instance.\n\nA typical Metarank integration flow consists of three main processes:\n\n- Metarank receives feedback events with visitor behavior, like clicks and search impressions. Historical feedback events are used for ML model training and real-time events for online model inference and re-ranking.\n- A backend application receives a search request from a visitor and forwards it to Elasticsearch and Pinecone. Both search result sets are concatenated together without specific ordering and then sent to Metarank for a final ranking.\n- Metarank runs the LambdaMART model over the candidate document set and produces the final ranking, which is later presented to the visitor.\n\n![Metarank integration flow](/images/metarank-7.jpg)\n\n### Historical Click-through Events\n\nMetarank has a serious limitation (which is not a limitation per se, but more like a functional requirement) that you need to have a history of behavioral data of your visitors to start working on ranking optimization. But like with a chicken-and-egg problem, you need to implement some basic initial unified ranking over multiple retrievers to collect visitor clicks and impressions.\n\nA common approach to this task is **interleaving**: both result sets are sorted by relevance score and then merged into a single ranking one by one, like a zip line.\n\n![Interleaving](/images/metarank-8.jpg)\n\nThe interleaving process is a great baseline approach: simple enough to implement and hard enough to beat. It can also be extended to support multiple sampling probabilities so that a document may be taken from one of the result sets more frequently than from another.\n\nWith a baseline merging strategy, we can start collecting visitor click-through data. Metarank expects to receive three types of events:\n\n1. Metadata events about users and items. As we’re not using any user/item-specific information in the ranking process, we can skip emitting these.\n2. Ranking events: what was presented to a visitor. \n3. Interaction events: how the visitor reacted to the ranking.\n\nAn example ranking event is a JSON object with query information and per-document scores taken from primary search requests. All per-item fields are optional, so in a case when a document was retrieved only by Pinecone and has no BM25 score, skip the field:\n\n![Ranking event](/images/metarank-9.png)\n\nWhen a visitor makes a click on a document after seeing the ranking, we have to emit an “interaction” event. It describes what was clicked and within which context:\n\n![Interaction event](/images/metarank-10.png)\n\nMetarank can pull historical ranking and interaction events from multiple sources (S3 files, Apache Kafka, Apache Pulsar, AWS Kinesis, and GCP Pubsub are supported), so you’re free to choose the best method to collect and store them. There is also an option to use [Snowplow Analytics telemetry collector](https://docs.metarank.ai/reference/integrations-overview/snowplow) instead of setting up your analytical collection pipeline.\n\n### Re-ranking Setup\n\nWith click-through event collection in place, we can create a Metarank configuration. The config file consists of four main parts: state, click-through, feature, and model sub-sections.\n\n**State configuration**. The place where all ranking state about users and items should be stored. Ironically, in our case, all the ranking features are present directly in the re-ranking request. There is no user/item state apart from the ML model itself.\n\nIn this guide, we’re going to use an in-memory store, but it’s only useful for demonstrational and development purposes. In a production environment, you should prefer Redis.\n\n**Click-through store configuration**. Metarank persists a complete click-chain when a visitor clicks on a document. It includes a list of items presented to the visitor, performed interactions, and ranking features used to build the original ranking event. Such events are quite large, and instead of saving all of them in Redis, there are other more cost-effective alternatives.\n\nThe click-through store section is optional and by default, uses the main state store to persist click-throughs, so for the sake of simplicity, we’ll omit this.\n\n**Feature extractor configuration**. How to map semi-structured incoming metadata/ranking/interaction events to the actual ML ranking features.\n\nIn our use case, we have only two ranking features, using BM25 and cosine scores supplied by the backend app directly in the ranking request:\n\n![Ranking features](/images/metarank-11.png)\n\n**Model configuration**. ML model options and used features. Metarank can serve multiple models at once, but in this guide we’ll define only a single “xgboost” model:\n\n![xgboost model](/images/metarank-12.png)\n\n### Import and Model Training\n\nWith a configuration file in place, we can finally invoke Metarank to process the historical data and build the ML ranking model. Metarank can be run as a JVM application with the **java -jar** command line or as a docker container. We will use the latter in this guide, as it’s more common (but a bit more complicated due to a volume setup).\n\nMetarank has a set of separate sub-commands for data import, model training, and serving (todo: links), but there is a shortcut to run all of them sequentially together, the **standalone mode**. It’s also not very well suited for production but is extremely useful in simple demos and prototype projects.\n\n`docker run -i -t -p 8080:8080 -v <YOUR DIR WITH CONFIG>:/opt/metarank metarank/metarank:latest standalone --config /opt/metarank/config.yml --data /opt/metarank/events.jsonl.gz`\n\nMetarank in the standalone mode will import the historical data, train the ranking ML model and switch to the inference mode to serve real-time re-ranking requests.\n\n### Sending Requests\n\nWhen the visitor-facing backend app receives a search request, the request needs to be passed through to the underlying retrievers. So, for example, we can translate a “terminator” query to the following Elasticsearch request:\n\n![Elasticseach request](/images/metarank-13.png)\n\nThe request must first be converted into a vector representation for a semantic search using your favorite text embedding method. For example, the term “terminator” embedding for a MiniLM model looks like this 384-dimensional vector:\n\n![Vector representation](/images/metarank-14.png)\n\nWith this vector, we can then query a Pinecone vector search engine for semantically close documents:\n\n![Pinecone vector search query](/images/metarank-15.png)\n\nElasticsearch and Pinecone search responses share some of the found documents. We concatenate these responses and send them to Metarank for a final ranking:\n\n![Ranking event](/images/metarank-9.png)\n\nAnd finally Metarank will respond with the final ranking:\n\n![Final ranking](/images/metarank-16.png)\n\nBig success!\n\n## Next steps\n\nIn this article, we attempted to highlight a typical re-ranking setup with Metarank, adapted for a hybrid search with multiple diverse retrievers. \n\nBut in the multi-retriever example here, we only used two specific ranking features: BM25 and cosine similarity scores supplied by search engines. Metarank can also do much more, as it can also compute complex behavioral features like:\n\n- Rates: CTR, conversion rate.\n- User: User-Agent, GeoIP and Referer\n- Profile: interaction history\n- Item: price and category.\n\nThese ranking features can improve the relevance of the final ranking but require a bit more complicated infrastructure setup: the re-ranking process may become stateful (e.g., the next response depends on the previous one).\n\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbdd"
  },
  "filename": "testing-p2-collections-scaling.md",
  "title": "post",
  "category": "\"Testing p2 Pods, Vertical Scaling, and Collections\"",
  "content": "---\nlayout: post\ntitle: \"Testing p2 Pods, Vertical Scaling, and Collections\"\nheadline: \"Testing p2 Pods, Vertical Scaling, and Collections\"\ncategories:\n  - Algorithms & Libraries\ntoc: >-\nweight: 10\nauthors:\n  - name: James Briggs\n    position: Developer Advocate\n    src: /images/james-briggs.jpeg\n    href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: A look at the newest features coming to Pinecone\n# Open graph\nimages: ['https://www.pinecone.io/images/10x-faster-launch-thumb.png']\n---\n\nThe world of machine learning is powered by vectors. Not just any vectors, but *dense* vectors capable of representing human meaning in numeric form. These meaningful vectors are quickly replacing more traditional forms of data as the digital world becomes more ML-powered and more people-centric.\n\nWe expect intuitive natural language search, intelligent recommendations, and much more by *default*. To achieve this, we need dense vectors, but we also need a database for these vectors.\n\nThat is where vector databases like Pinecone come in. The vector database enables scalable, super fast, and accurate retrieval of dense vectors. Already with Pinecone, we see customers searching through billions of vectors and returning results with sub-second latency.\n\nToday, vector search just became up to 10x faster, easier to set up, and vertically scalable. In this article, we will show you how you can get started with the latest features in Pinecone, covering:\n\n* Vertical scaling for p1 and s1 pods.\n\n* Phase one of collections, enabling static snapshots of indexes.\\*\n\n* The latest graph-based p2 pod with  up to 10x faster query times.\n\nAlthough we won't talk about it here, there are also three more upgrades to note:\n\n* p1 and s1 pods now have ~50% lower latency and ~50% more throughput per replica.\n\n* s1 pods are now available on the free Standard plan, meaning you get 5x greater capacity.\n\n* Updated [pricing](https://www.pinecone.io/pricing/) as of September 1st for new customers.\n\nWithout any further ado, let's explore the latest features.\n\n*\\*Future updates to collections will allow import/export between S3 and GCS blob storage, write streaming, and bulk data upload directly to collections.*\n\n---\n\n## Vertical Scaling on p1 and s1\n\n[Pods](https://www.pinecone.io/docs/manage-indexes/#pods-and-pod-types) are the hardware components that all of our vectors are stored in. Naturally, they have limits. A p1 pod is expected to hold ~1M 768-dimensional vectors.\n\n---\n\n*The free Standard tier comes with access to one p1 pod, and as of today, your free capacity can now be increased 5x using the newly included s1 pod.*\n\n---\n\nIn the past, we had to know ahead of time how many pods we needed for our vector database. Unfortunately, this isn't always realistic. Our data needs can change over time, and we often find ourselves outgrowing our initial pod confines or overprovisioning and wasting resources.\n\nAs a result we would need  to create a new index from scratch, which isn't fun - especially when you have millions of vectors.\n\nFortunately, we now have *vertical scaling*. Every index using p1 or s1 pods can be scaled in multiples of two up to *eight-times* their original size with *zero downtime*. Let's see how.\n\n{{< notebook file=\"aug-22-init\" height=\"full\" >}}\n\nStarting with a very full index (see `'index_fullness'`), we need to increase the index size to add more vectors and maintain reasonable latency. We use the new `pinecone.index_config` method to do this.\n\n{{< notebook file=\"aug-22-vertical-scale\" height=\"full\" >}}\n\nBy default when creating an index with `pod_type` as `p1` or `s1`, we are actually creating a `p1.x1` or `s1.x1` pod. From either of those, we can scale up to *eight times*. In this case, we scaled by `x2`, doubling our capacity.\n\n## Collections\n\nAnother major feature of this release is *collections*. In the past, after we created an index, we could only reuse those vectors by keeping a local copy or iteratively retrieving them all. Neither option is ideal. Collections are the solution to this. These are essentially static indexes that we can think of as the *\"source of truth\"* for our vector data.\n\nWe can create a collection using an existing index, like the `oscar-minilm` index we just scaled.\n\n{{< notebook file=\"aug-22-collections\" height=\"full\" >}}\n\n\n\nThe syntax for creating and describing collections mirrors that of the same operations for indexes. We create a new collection with `pinecone.create_collection(\"collection_name\", \"index_name\")`. To view collection information we describe it with `pinecone.describe_collection(\"collection_name\")`.\n\nWe will be able to see the existence of the collection immediately. However, the collection will take some time to fully initialize and be ready for use elsewhere. We can see the collection status after describing it via the `status` value.\n\nOnce the collection status switches to `\"Ready\"` we can use it to create new indexes. All we need is:\n\n{{< notebook file=\"aug-22-index-from-collection\" height=\"full\" >}}\n\nHere we checked the collection status for `\"Ready\"`. Then, using the same `pinecone.create_index` method we usually use, we initialized a p2 pod index and specified `source_collection` to build it from our `oscar-minilm-collection` collection. The creation time is not instant. In this case, it took 23 minutes, a typical time for a collection of this size with the p2 pod type. p1 and s1 index creation is faster (~5 minutes).\n\n---\n\n## p2 Pods\n\nWe've already seen some of the p2 pod when we initialized a new index from our collection. p2 pods are a new index type that enables up to 10x faster search speeds by utilizing a graph-based index. There are both pros and cons, **Q**ueries **P**er **S**econd (QPS) is faster, but the vector ingestion rate is much slower.\n\n|                                      | s1            | p1            | p2           |\n| ------------------------------------ | ------------- | ------------- | ------------ |\n| Capacity (768-d vectors)             | ~5M           | ~1M           | ~1M          |\n| Query latency at full capacity (p95) | <200ms        | <50ms         | <10ms        |\n| QPS at full capacity / replica       | ~5 QPS        | ~20 QPS       | ~200 QPS     |\n| Ingestion rate / pod                 | 10k vectors/s | 10k vectors/s | 50 vectors/s |\n\nThe decision between p1, s1, and p2 relies on your application priorities. p2 is ideal for minimal latency, high-throughput, and indexes with relatively low update rates.\n\n{{< notebook file=\"aug-22-query\" height=\"full\" >}}\n\nIf we test our two indexes, the p1 index and the p2 index, with a random query, p2 cuts latency from ~35ms to just ~15ms (including network latency).\n\n---\n\nThat's a quick rundown of the latest features in Pinecone. All of these are currently in public preview and are not yet covered by Pinecone's standard SLAs. Therefore we do not recommend them for production use just yet.\n\nThese are the three key features, but there are other changes too. Both p1 and s1 pods now (on average) have 50% lower latency and 50% higher throughput. s1 pods have been added to the free Standard plan, meaning standard users can store and query up to 5M 768-dimensional vectors *for free*.\n\nWith all of these new features, there's plenty to be excited about. As we all move towards an increasingly vector-centric future, there's no better time to [get started with vector search](https://app.pinecone.io/) than today.\n\nLearn more about these new features in our [announcement](https://pinecone.io/learn/faster-easier-scalable/). \n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbde"
  },
  "filename": "composite-indexes.md",
  "title": "ebook-post",
  "category": "\"Composite Indexes and the Faiss Index Factory\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Composite Indexes and the Faiss Index Factory\"\nheadline: \"Facebook AI and the Index Factory\"\ncategories:\n  - \"Faiss: The Missing Manual\"\ntoc: >-\nweight: 7\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: Complete guide to composite search indexes and Faiss Index Factory.\n# Open graph\nimages: ['/images/composite-indexes-1.jpg']\n---\n\n![Facebook AI and the Index Factory](/images/composite-indexes-1.jpg)\n\nIn the world of [vector search](/learn/what-is-similarity-search/), there are many indexing methods and vector processing techniques that allow us to prioritize between recall, latency, and memory usage.\n\nUsing specific methods such as IVF, [PQ](/learn/product-quantization/), or [HNSW](/learn/hnsw/), we can often return good results. But for *best performance* we will usually want to use *composite indexes*.\n\n---\n\n*Note: [Pinecone](/) lets you build scalable, high-performance vector search into your applications without knowing anything about composite indexes. However, we know you like seeing how things work, so enjoy learning about composite indexes and the Faiss Index Factory!*\n\n---\n\nWe can view a composite index as a step-by-step process of vector transformations and one or more indexing methods. Allowing us to place multiple indexes and/or processing steps together to create our 'ideal' index.\n\nFor example, we can use an inverted file (IVF) index to reduce the scope of our search (increasing search speed), and then add a compression technique such as [product quantization (PQ)](/learn/product-quantization/) to keep larger indexes within a reasonable size limit.\n\nWhere there is the ability to customize indexes, there is the risk of producing indexes with unnecessarily poor recall, latency, or memory usage.\n\nWe must know how composite indexes work if we want to build robust and high-performance vector similarity search applications. It is essential to understand where different indexes or vector transformations can be used — and when they are not needed.\n\nIn this article, we will learn how to build high-performance composite indexes using [Facebook AI Similarity Search (Faiss)](/learn/faiss/) — a powerful library used by many for building fast and accurate vector similarity search indexes. We will also introduce the Faiss `index_factory` which allows us to build composite indexes with clearer, more elegant code.\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/GEhmmcx1lvM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n## What are Composite Indexes\n\nComposite indexes are akin to *lego blocks*; we place one on top of another. We will find that most blocks fit together — but different combinations can produce anything from an artistic masterpiece to an unrecognizable mess.\n\nThe same applies to Faiss. Most components *can* be placed together — but that does not mean they *should* be placed together.\n\nA composite index is built from any combination of:\n\n* **Vector transform** — a pre-processing step applied to vectors before indexing (PCA, OPQ).\n\n* **Coarse quantizer** — *rough* organization of vectors to sub-domains (for restricting search scope, includes IVF, IMI, and HNSW).\n* **Fine quantizer** — a *finer* compression of vectors into smaller domains (for compressing index size, such as PQ).\n* **Refinement** — a final step at search-time which re-orders results using distance calculations on the original flat vectors. Alternatively, another index (non-flat) index can be used.\n\nNote that coarse quantization refers to the 'clustering' of vectors (such as inverted indexing with IVF). By using coarse quantization, we enable *non-exhaustive* search by limiting the search scope.\n\nFine quantization describes the compression of vectors into *codes* (as with PQ) <sup>\\[1\\]\\[2\\]\\[3\\]</sup>. The purpose of this is to reduce the memory usage of the index.\n\n### Index Components\n\nWe can build a composite index using the following components:\n\n| Vector transform | Coarse quantizer | Fine quantizer | Refinement |\n| ------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | -------------------- |\n| `PCA`, `OPQ`, `RR`, `L2norm`, `ITQ`, `Pad` | `IVF,Flat`, `IMI`, `IVF,HNSW`, `IVF,PQ`, `IVF,RCQ`, `HNSW,Flat`, `HNSW,SQ`, `HNSW,PQ` | *`Flat*`*, `PQ`, `SQ`, *`Residual*`*, `RQ`, `LSQ`, `ZnLattice`, `LSH` | `RFlat`, *`Refine*`* |\n\nFor example, we could build an index where we:\n\n* Transform incoming vectors using `OPQ`.\n* Perform coarse quantization of vectors by storing them in an inverted file list `IVF`, enabling non-exhaustive search.\n* Compress vectors, reducing memory usage with `PQ` within each IVF cell *(the vectors are quantized, but their cell assignment does not change)*.\n* After the search, re-order results based on their original flat vectors `RFlat`.\n\nWhen building these indexes, it can get messy to use a list of the different Faiss classes — so it is often clearer to build our indexes using the Faiss `index_factory`.\n\n![Example composite index](/images/composite-indexes-8.jpg)\n<small>We can merge IVF and PQ indexes to store quantized PQ vectors in an IVF structure.</small>\n\n## Faiss Index Factory\n\nThe Faiss `index_factory` function allows us to build composite indexes using little more than a string. It allows us to switch:\n\n<p>```python\nquantizer = faiss.IndexFlatL2(128)\nindex = faiss.IndexIVFFlat(quantizer, 128, 256)\n```</p>\n\nFor this:\n\n<p>```python\nindex_f = faiss.index_factory(128, \"IVF256,Flat\")\n```</p>\n\n*We haven't specified the L2 distance in our `index_factory` example because the `index_factory` uses L2 by default. If we'd like to use `IndexFlatIP` we add `faiss.METRIC_INNER_PRODUCT` to our `index_factory` parameters.*\n\nWe can confirm that both methods produce the same composite index by comparing their performance. First, do they return the same nearest neighbors?\n\n<p>{{< notebook file=\"index-factory-nn-check\" height=\"full\" >}}</p>\n\n\nIdentical results, and how do they compare for search speed and memory usage?\n\n<p>{{< notebook file=\"speed-and-memory\" height=\"full\" >}}</p>\n\nThe `get_memory` function returns an exact match for memory usage. Search speeds are incredibly close, with the `index_factory` version 5µs faster — a negligible difference.\n\n*We calculate recall as the percentage of matches from the top-`k` between a flat L2 index and the tested index.*\n\n*The more commonly used metric in literature is recall@k; this is **not** the recall calculated here. Recall@k is the percentage of queries that returned its nearest neighbor in the top `k` returned records.*\n\n*If we returned the ground-truth nearest neighbor 50% of the time when using a `k` value of `100`, we would say the recall@100 performance is 0.5.*\n\n### Why Use the Index Factory\n\nJudging from our tests, we can be confident that these two index-building methods are nothing more than separate paths to the same destination.\n\nWith that in mind — why should we care to learn how we use `index_factory`? First, it can depend on personal preference. If you prefer the class-based index building approach, stick with it.\n\nHowever, through using the `index_factory` we can greatly improve the elegance and clarity of our code. We will see that five lines of complicated code can be represented in a single — more readable — line of code when using the `index_factory`.\n\nLet's put together a composite index where we pre-process vectors with OPQ, cluster with IVF, quantize using PQ, then re-order with a flat index.\n\n<p>```python\nd = xb.shape[1]\nm = 32\nnbits = 8\nnlist = 256\n\n# we initialize our OPQ and coarse+fine quantizer steps separately\nopq = faiss.OPQMatrix(d, m)\n# d now refers to shape of rotated vectors from OPQ (which are equal)\nvecs = faiss.IndexFlatL2(d)\nsub_index = faiss.IndexIVFPQ(vecs, d, nlist, m, nbits)\n# now we merge the preprocessing, coarse, and fine quantization steps\nindex = faiss.IndexPreTransform(opq, sub_index)\n# we will add all of the previous steps to our final refinement step\nindex = faiss.IndexRefineFlat(q)\n# train the index, and index vectors\nindex.train(xb)\nindex.add(xb)\n```</p>\n\nThis code demonstrates the complexity that adding several components to our index can create. If we rewrite this using the `index_factory`, we get much simpler code:\n\n<p>```python\nd = xb.shape[1]\n# in index string, m==32, nlist==256, nbits is 8 by default\n\nindex = faiss.index_factory(d, \"OPQ32,IVF256,PQ32,RFlat\")\n# train and index vectors\nindex.train(xb)\nindex.add(xb)\n```</p>\n\nBoth approaches produce the exact same index. The performance for each:\n\n| | Recall | Search Time | Memory Usage |\n| ----------------------- | ------ | ----------- | ------------ |\n| Without `index_factory` | 31% | 181µs | 552MB |\n| *With* `index_factory` | 31% | 174µs | 552MB |\n\nSearch time does tend to be slightly faster when using the `index_factory` — but otherwise, there are no performance differences between equivalent indexes built with or without the `index_factory`.\n\n## Popular Composite Indexes\n\nNow that we know how to quickly build composite indexes using the `index_factory`, let's explore a few popular and high-performance combinations.\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/3Wqh4iUupbM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n### IVFADC\n\nWe have covered a modified **IVFADC** index above — the `IVF256,PQ32` portion of our previous examples make up the core of IVFADC. Let's dive into it in a little more detail.\n\nThe index was introduced alongside product quantization in 2010 <sup>[4]</sup>. Since then, it has remained one of the most popular indexes — thanks to being an easy-to-use index that produces reasonable recall, fast speeds, and *incredible* memory usage.\n\nIVFADC is ideal when our main priority is to minimize memory usage while maintaining fast search speeds. This comes at the cost of *okay* — but not *good* recall performance.\n\nThere are two steps to indexing with IVFADC:\n\n1. Vectors are assigned to different lists (*or* Voronoi cells) in the IVF structure.\n2. The vectors are compressed using PQ.\n\n![Indexing process for IVFADC](/images/composite-indexes-21.jpg)\n<small>Indexing process for IVFADC, adapted from [4].</small>\n\nAfter indexing vectors, an **A**symmetric **D**istance **C**omputation (ADC) is performed between query vectors `xq` and our indexed, quantized vectors.\n\nThe search is referred to as being *asymmetric* because it compares `xq` — which is not compressed, against compressed PQ vectors (that we previously indexed).\n\n![Symmetric Distance Computation (SDC)](/images/composite-indexes-9.jpg)\n<small>With **s**ymmetric **d**istance **c**omputation (SDC, left) we quantize `xq` before comparing it to our previously quantized `xb` vectors. ADC (right) skips the quantization of `xq` and compares it directly to the quantized `xb` vectors.</small>\n\nTo implement the index using the `index_factory` we can write:\n\n<p>{{< notebook file=\"ivfadc\" height=\"full\" >}}</p>\n\nWith this, we create an IVFADC index with `256` IVF cells; each vector is compressed with PQ using `m` and `nbits` values of `32` and `8`, respectively. PQ uses `nbits == 8` by default so we can also write `\"IVF256,PQ32\"`.\n\n*`m`: number of subvectors that original vectors are split into*\n\n*`nbits`: number of bits used by each subquantizer, we can calculate the number of centroids used by each subquantizer as `2**nbits`*\n\nWe can decrease `nbits` to reduce index memory usage or increase to improve recall and search speed. However, the current version of Faiss does restrict `nbits` to `>= 8` for `IVF,PQ`.\n\nIt is also possible to increase the `index.nprobe` value to search more IVF cells — by default, this value is `1`.\n\n<p>{{< notebook file=\"nprobe\" height=\"full\" >}}</p>\n\nHere we have our index performance for various `nbits` and `nprobe` values:\n\n| Index | nprobe | Recall | Search Time | Memory |\n| --------------- | ------ | ------ | ----------- | ------ |\n| `IVF256,PQ32x4` | `1` | 27% | 329µs | 25MB |\n| `IVF256,PQ32x4` | `6` | 45% | 975µs | 25MB |\n| `IVF256,PQ32x8` | `1` | 30% | 136µs | 40MB |\n| `IVF256,PQ32x8` | `8` | 74% | 729µs | 40MB |\n\n#### Optimized Product Quantization\n\n**IVFADC** and other indexes using PQ can benefit from **O**ptimized **P**roduct **Q**uantization (OPQ).\n\nOPQ works by rotating vectors to flatten the distribution of values across the subvectors used in PQ. This is particularly beneficial for unbalanced vectors with uneven data distributions.\n\nIn Faiss, we add OPQ as a pre-processing step. For IVFADC, the OPQ index string looks like `\" OPQ32,IVF256,PQ32\"` where the `32` in `OPQ32` *and* `PQ32` refers to the number of bytes `m` in the PQ generated codes.\n\n*The OPQ matrix in Faiss is **not** the whole rotation and PQ process. It is only the rotation. A PQ step must be included downstream for OPQ to be implemented.*\n\nAs before, we will need to `train` the index on initialization.\n\n<p>{{< notebook file=\"opq\" height=\"full\" >}}</p>\n\nThe data distribution of the Sift1M dataset is already well balanced, so OPQ gives us only a minor increase in recall performance. With an `nprobe == 1` we have increased recall from 30% -> 31%.\n\nWe can increase our `nprobe` value to improve recall (at the cost of speed). However, because we added a pre-processing step to our index, we cannot access `nprobe` directly with `index.nprobe` as this `index` no longer refers to the IVF portion of our index.\n\nInstead, we must *extract* the IVF index before modifying the `nprobe` value — we can do this using the `extract_index_ivf` function.\n\n<p>{{< notebook file=\"extract-index-ivf\" height=\"full\" >}}</p>\n\nWith a higher `nprobe` value of `14` — we return a recall of 74%. A similar recall result to PQ alone, alongside an increased search time from 729µs -> 1060µs.\n\n| Index | nprobe | Recall | Speed | Memory |\n| ------------------- | ------ | ------ | ------ | ------ |\n| `IVF256,PQ32` | `1` | 30% | 136µs | 40.2MB |\n| `OPQ32,IVF256,PQ32` | `1` | 31% | 143µs | 40.3MB |\n| `IVF256,PQ32` | `8` | 74% | 729µs | 40.2MB |\n| `OPQ32,IVF256,PQ32` | `13` | 74% | 1060µs | 40.3MB |\n\nWe will see later in the article that OPQ can be used to improve performance, but as we can see here, that is not *always* the case.\n\n![Performance of composite indexes](/images/composite-indexes-19.jpg)\n<small>Search time (top) and recall (bottom) for various `nprobe` values. We have included `\"IVF256,Flat\"` for comparison. The *flat* index has much higher memory usage at 520MB.</small>\n\nOPQ can also be used to reduce the dimensionality of our vectors in this pre-processing step. This dimensionality `D` must be a multiple of `M`, preferably `D == 4M`. To reduce dimensionality to `64`, we could use `\"OPQ16_64,IVF256,PQ16\"`.\n\n### Multi-D-ADC\n\nMulti-D-ADC refers to **multi-d**imensional indexing, alongside a PQ step which produces an **a**symmetric **d**istance **c**omputation at search time (as we discussed previously) <sup>[5]</sup>.\n\nThe **multi-D-ADC** index is based on the inverted multi-index (IMI), an extension of IVF.\n\nIMI can outperform IVF in both recall and search speed but does increase memory usage <sup>[7]</sup>. This makes IMI indexes (such as multi-D-ADC) ideal in cases where IVFADC doesn’t quite reach the recall and speed required, and you can spare more memory usage.\nThe IMI index works in a very similar way to IVF, but Voronoi cells are split across vector dimensions. What this produces is akin to a multi-level Voronoi cell structure.\n\n![IMI Index](/images/composite-indexes-6.jpg)\n<small>Voronoi cells split across multiple vector subspaces. Given a query vector `xq`, we would compare each `xq` subvector to its respective subspace cells.</small>\n\nWhen we add a vector compression to IMI using PQ, we produce the **multi-D-ADC** index. Where ADC refers to the asymmetric distance computation that is made when comparing query vectors to PQ vectors.\n\nPutting all of this together, we can create a multi-D-ADC index using the index factory string `\" IMI2x8,PQ32\"`.\n\n<p>{{< notebook file=\"multi-d-adc\" height=\"full\" >}}</p>\n\nTo return a similar recall to our IVFADC equivalent, we increased search time to 1.3ms, which is very slow. However, if we add OPQ to our index, we will return much better results.\n\n<p>{{< notebook file=\"o-multi-d-adc\" height=\"full\" >}}</p>\n\nFor a recall of 74%, our OPQ multi-D-ADC index is fastest at an average search time of just 461µs.\n\n| Index | Recall | Search Time | Memory |\n| ------------------- | ------ | ----------- | ------ |\n| `IVF256,PQ32` | 74% | 729µs | 40.2MB |\n| `IMI2x8,PQ32` | 72% | 1350µs | 40.8MB |\n| `OPQ32,IMI2x8,PQ32` | 74% | 461µs | 40.7MB |\n\nAs before, we can fine-tune the index to prioritize recall or speed using `nprobe`.\n\n![Performance of OPQ composte index](/images/composite-indexes-5.jpg)\n<small>Search time (top) and recall (bottom) for various `nprobe` values. We have included `\"IMI2x8,Flat\"` for comparison. The *flat* index has much higher memory usage at 520MB.</small>\n\n`\"OPQ32,IMI2x8,PQ32\"` is one of our best indexes in terms of recall and speed at low memory. However, we'll see that we can improve these metrics even further with the following index.\n\n### HNSW Indexes\n\nIVF with **H**ierarchical **N**avigable **S**mall-**W**orld (HNSW) graphs is our final composite index. This index splits our indexed vectors into cells as per usual with IVF, but this time we will optimize the process using HNSW.\n\nCompared to our previous two indexes, IVF with HNSW produces comparable or better speed and significantly higher recall — at the cost of *much* higher memory usage.\n\nAt a high level, HNSW is based on the *small-world graph theory* that all vertices (*nodes*) in a network — no matter how large — can be traversed in a small number of steps.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/composite-indexes-11.mp4\" type=\"video/mp4\">\n</video>\n\n<small class=\"video\">Example of a navigable small-world graph, all nodes within the graph are connected by a small number of edge traversals. Small world graph theory assumes the same to be true even for huge networks with billions of vertices.</small>\n\nIn this small world graph, we see both short-range and long-range links. When traversing across long-range links, we move more quickly across the graph.\n\nHNSW takes advantage of this by splitting graph links into multiple layers. At the higher entry layers, we find only long-range links. As we move down the layers, shorter-range links are added.\n\nWhen searching, we start at these higher layers with long-range links. Meaning our first traversals are across long-range links. As we move down the layers, our search becomes finer as we traverse across more short-range links.\n\n![HNSW graph layers](/images/composite-indexes-3.jpg)\n<small>HNSW graphs break the typical graph containing both long-range and short-range links into multiple layers (hierarchies). During the search, we begin at the highest layer, which consists of long-range links. As we move down through each layer, the links become more granular.</small>\n\nThis approach should minimize the number of traversals (speeding up search) while still performing a very fine search in the lower layers (maintaining high recall).\n\nThat is HNSW, but how can we merge HNSW with IVF?\n\nUsing vanilla IVF, we introduce our query vector and compare it to every cell centroid, identifying the nearest centroids for restricting our search scope.\n\nTo pair this process with HNSW, we produce an HNSW graph of all of these cell centroids, making the exhaustive centroid search *approximate*.\n\n![HNSW graph with IVF](/images/composite-indexes-4.jpg)\n<small>HNSW can be used to quickly find the approximate nearest neighbor using IVF cell centroids.</small>\n\nPreviously, we have been using IVF indexes with 256 cell centroids. An exhaustive search of 256 is fast, and there is no reason to use an approximate search with so few centroids.\n\nAnd because we have so few cells, each cell must contain many vectors - which will still be searched using an exhaustive search. In this case, IVF+HNSW on the cell centroids does not help.\n\nWith IVF+HNSW indexes, we need to swap *' few centroids and large cells'* for *' many centroids and small cells'*.\n\nFor our 1M index, an `nlist` value of `65536` is recommended <sup>[8]</sup>. However, we should provide *at least* `30*nlist == 1.97M` vectors to `index.train`, which we do not have. So a smaller `nlist` of `16384` or less is more suitable. For this dataset, `nlist == 4096` returned the highest recall (at slower speeds).\n\nUsing IVF+HNSW, we quickly identify the approximate nearest cell centroids using HNSW, then restrict our *exhaustive* search to those nearest cells.\n\nThe standard IVF+HNSW index can be built with ` \"IVF4096_HNSW32,Flat\"`. Using this, we have:\n\n* `4096` IVF cells.\n* Cell centroids are stored in an HNSW graph. Each centroid is linked to `32` other centroids.\n* The vectors themselves have not been changed. They are `Flat` vectors.\n\n<p>{{< notebook file=\"ivf-hnsw\" height=\"full\" >}}</p>\n\nWith this index, we can produce incredible performance ranging from 25% -> 100% recall at search times of 58.9µs -> 916µs.\n\n![Performance of HNSW IVF composite index](/images/composite-indexes-20.jpg)\n<small>Search time (top) and recall (bottom) for various `nprobe` values. At the cost of longer search times, we can increase recall by decreasing `nlist`.</small>\n\nHowever, the IVF+HNSW index is not without its flaws. Although we have incredible recall and fast search speeds, the memory usage of this index is *huge*. Our 1M 128-dimensional vectors produce an index size of 523MB+.\n\nAs we have done before, we can reduce this using PQ and OPQ, but this will reduce recall and increase search times.\n\n| Index | Recall | Search Time | Memory |\n| ------------------------------- | ------ | ----------- | ------ |\n| `IVF4096_HNSW,Flat` | 90% | 550µs | 523MB |\n| `IVF4096_HNSW,PQ32` (PQ) | 69% | 550µs | 43MB |\n| `OPQ32,IVF4096_HNSW,PQ32` (OPQ) | 74% | 364µs | 43MB |\n\nIf a lower recall is acceptable for minimizing search time and memory usage, the IVF+HNSW index with OPQ is ideal. On the other hand, IVF+HNSW with PQ offers no benefit over our previous *IVFADC* and *Multi-D-ADC* indexes.\n\n| Name | Index String | Recall | Search Time | Memory |\n| ----------- | ------------------- | ------ | ----------- | ------ |\n| IVFADC | `IVF256,PQ32` | 74% | 729µs | 40MB |\n| Multi-D-ADC | `OPQ32,IMI2x8,PQ32` | 74% | 461µs | 41MB |\n\n---\n\nThat's it for this article! We introduced composite indexes and how to build them using the Faiss `index_factory`. We explored several of the most popular composite indexes, including:\n\n* IVFADC\n* Multi-D-ADC\n* IVF-HNSW\n\nBy indexing and searching the Sift1M dataset, we learned how to modify each index's parameters to prioritize recall, speed, and memory usage.\n\nWith what we have covered here, you will be able to design and test a variety of composite indexes and better decide on an index structure that best suits your needs.\n\n{{< newsletter text=\"Subscribe for the latest in Faiss and similarity search!\" inputText=\"Email address...\" buttonText=\"Submit\">}}\n\n## References\n\n[1] Y.Chen, et al., [Approximate Nearest Neighbor Search by Residual Vector Quantization](https://www.researchgate.net/publication/51873001_Approximate_Nearest_Neighbor_Search_by_Residual_Vector_Quantization) (2010), *Sensors*\n\n[2] Y. Matsui, et al., [A Survey of Product Quantization](https://www.jstage.jst.go.jp/article/mta/6/1/6_2/_pdf) (2018), *ITE Trans. on MTA*\n\n[3] T. Ge, et. al., [Optimized Product Quantization](http://kaiminghe.com/publications/pami13opq.pdf) (2014), *TPAMI*\n\n[4] H. Jégou, et al., [Product quantization for nearest neighbor search](https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf) (2010), *TPAMI*\n\n[5] A. Babenko, V. Lempitsky, [The Inverted Multi-Index](http://sites.skoltech.ru/app/data/uploads/sites/25/2014/12/TPAMI14.pdf) (2012), *CVPR*\n\n[6] H. Jégou, et al., [Searching in One Billion Vectors: Re-rank with Source Coding](https://arxiv.org/pdf/1102.3828.pdf) (2011), *ICASSP*\n\n[7] D. Baranchuk, et al., [Revisiting the Inverted Indices for Billion-Scale Approximate Nearest Neighbors](https://arxiv.org/pdf/1802.02422.pdf) (2018), *ECCV*\n\n[8] [Guidelines to choose an index](https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index), Faiss wiki\n\n[9] [The Index Factory](https://github.com/facebookresearch/faiss/wiki/The-index-factory), Faiss wiki\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbdf"
  },
  "filename": "hybrid-search.md",
  "title": "post",
  "category": "Introducing the hybrid index to enable keyword-aware semantic search",
  "content": "---\nlayout: post\ntitle: Introducing the hybrid index to enable keyword-aware semantic search\nheadline: Introducing the hybrid index to enable keyword-aware semantic search\ncategories:\n  - Company News\ntoc: >-\nauthor:\n  name: Gibbs Cullen\n  position: Senior Product Marketing Manager\n  src: /images/gibbs-cullen.jpg\n  href: https://www.linkedin.com/in/gibbscullen/\ndate: \"2022-10-31\"\n# Open Graph\ndescription: New approach to hybrid search leads to more relevant results\nthumbnail: \"/images/hybrid-search-thumbnail.jpg\"\nimages: ['https://www.pinecone.io/images/hybrid-search.png']\n---\n\n![Keyword-Aware Semantic Search](/images/hybrid-search.png)\n\n## New approach to hybrid search leads to more relevant results \n\nGrowing expectations around search (that our applications should automatically understand our intent) have led to important advancements — like semantic search — that go beyond keyword search capabilities. However, both keyword and semantic search have important tradeoffs to consider:\n\n- **Keyword search can miss important context.** Keyword methods can struggle even with simple synonyms without manual tuning. \n- **Semantic search can miss important keywords.** Semantic search may overlook details searching for certain keywords (such as names, industry-specific jargon, or rare words), providing related but not the best results.\n\nIn fact, for text-search use cases, a hybrid approach — combining keyword and semantic search — provides more relevant results than either one alone. But running two separate search solutions plus a third system for combining the results is an engineering nightmare.\n\nThat’s why we’re excited to announce the **hybrid vector index**, a first-of-its-kind solution that lets engineers easily build keyword-aware semantic search into their applications. Continue reading to learn more, and [request early access today](/hybrid-search-early-access/).\n\n## Level-up your search with keyword-aware semantic search, powered by the Pinecone hybrid vector index\n\nCompanies are turning to hybrid search techniques to help users get more relevant search results. The ability to search based on both what users say and what they mean leads to better results and happier users.\n\n[Our research](https://arxiv.org/abs/2210.11934) shows the impact of hybrid search on relevance compared to standalone keyword and semantic search: Whether searching in-domain or out-of-domain from the original training data, the hybrid results are better across the board. \n\n![Evaluating lexical, semantic, and hybrid retrieval](/images/hybrid-search-1.png)\n<small>Figure: Evaluating lexical, semantic, and hybrid retrieval, NDCG@1000</small>\n\nWe also know there’s a growing area of research around using hybrid vectors for use cases outside of text (e.g. creating sparse vectors from a learned sparse model (like [SPLADE](https://dl.acm.org/doi/10.1145/3404835.3463098)) instead of BM25). However, existing solutions make doing this no easy feat. Not only do you need to run multiple solutions — keyword and vector search systems alongside a reranking system — but you also have to know which levers to pull in order to transform your vectors to work with these existing solutions.  \n\nWith the new hybrid vector index, you don’t need to be an ML expert to build hybrid search for these use cases. We’ve designed it to be: \n\n- **Simple**: No need to manage multiple solutions. It’s not a vector index, an inverted index, and a re-ranker duct-taped together. It’s one hybrid index.\n- **Flexible**: A first-of-its-kind hybrid index to store and search across both dense and sparse representations of any kind of data, not just text.\n- **Scalable**: Support for billions of vectors with low latency and zero-downtime scaling.\n\nAnd since text is the predominant use case for hybrid search, we’re adding a hybrid endpoint to the Pinecone API. This endpoint accepts vector embeddings (dense vectors) and term frequencies (sparse vectors) for uploading or querying the hybrid index. This new, hybrid API endpoint provides:\n\n- **Convenience**: Saves you time and pre-processing steps to normalize and combine vectors. Accepts dense vectors from any language model such as SentenceBERT (SBERT), and sparse vectors from any tokenization library such as Hugging Face Tokenizers.\n- **Consistency**: Gives you the benefits of tried-and-true BM25 scoring for the keyword part of hybrid search.\n- **Control**: Adjust the weight of keyword and semantic relevance when querying. Gives you control of importance for keyword vs. semantic.\n\nHybrid search is a powerful capability that we believe should be accessible to all. As [Nils Reimers](https://www.nils-reimers.de/), the creator of Sentence Transformers, put it:\n\n> Semantic search can largely improve search performance, but there are still some shortcomings, especially when it comes to keyword-specific queries. Combining semantic search capabilities with traditional BM25 solves many of these issues, but so far the available solutions are not practical to deploy as you need to use two different systems. This is why I am so excited that Pinecone is adding keyword semantic search functionality to their managed vector database. It will give even better search results for many use-cases.\n\n### How it works\n\nBefore diving into how our hybrid search solution works, let’s define some key terms:\n\n- A **dense vector** is a vector of fixed dimensions, typically between 100-1000, where every entry is almost always non-zero. They represent the learned semantic meaning of texts by ML models like [SBERT](https://www.sbert.net/).\n- A **sparse vector** is a vector of a very large dimension (e.g. 1,000,000), where only a small fraction of its entries are non-zero. They represent important keywords inside documents.\n\nWe designed our hybrid search to be easy to use and scale, which we’ll demonstrate with the following example.\n\nImagine you need to build a feature to let users browse and analyze employee survey responses. You want to support searches for both general concepts (e.g. company offsite in Greece) and company-specific terms (e.g. Pinecone).\n\nHere’s how to do it with Pinecone’s hybrid index:\n\n1. [Sign in](https://app.pinecone.io) to Pinecone to get an API key and create a hybrid index (`s1h`).\n\n   ```python\n   headers = {\"Api-Key\": APIKEY}\n   config = {\n       \"name\": \"my-index\",\n       \"dimension\": 328,\n       \"metric\": \"dotproduct\",\n       \"pods\": 1,\n       \"pod_type\": \"s1h\",\n   }\n   requests.post('https://controller.<env>.pinecone.io/databases', headers=headers, json=config)\n   ```\n\n2. Generate dense vectors for the survey responses through a dense embedding model such as [SBERT](/learn/sentence-embeddings/). Use a tokenizer or analyzer tool (such as those from [spaCy](https://spacy.io/) or [HuggingFace](https://huggingface.co/docs/tokenizers/index)) to generate sparse vectors (based on term frequency) for the same survey responses.\n\n   ```python\n   from sentence_transformers import SentenceTransformer\n   from transformers import AutoTokenizer\n   from collections import Counter\n   import requests\n\n   tokenizer = AutoTokenizer.from_pretrained('transfo-xl-wt103')\n   model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n\n   doc = \"Visiting the parthenon during the Pinecone offsite was an awesome experience.\"\n   vector = model.encode([doc]).tolist()  # [0.1, -0.1, 0.2, ...]\n   tokens = dict(Counter(tokenizer.encode(doc)))  # {5:1, 10500:1, 7:1, ... }\n   ```\n\n3. Upload both dense and sparse vectors into a Pinecone hybrid index using the hybrid API. Your sparse vectors will be automatically normalized and transformed to provide search results equivalent to [BM25](https://www.pinecone.io/learn/semantic-search/#bm25).\n\n   ```python\n   upsert = {\n       \"vectors\": [{\n           \"id\": \"example-id-1\",\n           \"values\": vector,  # Dense Vector\n           \"sparse_values\": tokens,  # Sparse Vector\n           \"metadata\": {'text': doc}\n       }],\n   }\n   requests.post('https://<index-name>-<project-id>.svc.<env>.pinecone.io/hybrid/vectors/upsert', json=payload, headers=headers)\n   ```\n\n4. Now you can query the index, providing the sparse and dense vectors (which are combined into sparse-dense hybrid vectors using linear-combination fusion) along with a weight for keyword relevance (“alpha”). `Alpha=1` will provide a purely semantic-based search result and `alpha=0` will provide a purely keyword-based result equivalent to BM25. The default value is `0.5`.\n\n   ```python\n   question = \"pinecone athens offsite\"\n\n   query = {\n       \"topK\": 2,\n       \"vector\": model.encode([question]).tolist(),\n       \"sparseVector\": dict(Counter(tokenizer.encode(question))),\n       \"alpha\": 0.5  # Weight\n   }\n   resp = requests.post('https://<index-name>-<project-id>.svc.<env>.pinecone.io/hybrid/query', json=query, headers=headers)\n   ```\n\n   Note: The below diagrams show the effects of `alpha` values on sample datasets.. When using a model that is not trained for the corpus (out-of-domain), you should downweight the semantic score with lower values of alpha (e.g. 0.3-0.6). When using a model that is fine-tuned (in-domain), use values closer to 1.\n\n   ![Diagram](/images/hybrid-search-2.png)\n\n5. Query results are then retrieved (scored by max dot product), and you’re able to see the top results for survey responses related to “Greece offsite”, specifically those about “Pinecone”.\n\n   ```python\n   # Matches\n   resp.json()['matches']\n   [{'id': '3706692',\n     'score': 0.763926864,\n     'values': [],\n     'sparseValues': {},\n     'metadata': {'text': 'Visiting the parthenon during the Pinecone offsite was an awesome experience.'}},\n   {'id': '3393693',\n     'score': 0.582026243,\n     'values': [],\n     'sparseValues': {},\n     'metadata': {'context': “Last time i visited greece was on my own.”}}]\n   ```\n\nJust like that, you can build keyword-aware semantic search into your applications, and provide great results without tuning models or indexes, or managing multiple systems.\n\nThe below diagram displays both the upsert and query paths.\n\n![Hybrid API](/images/hybrid-search-3.png)\n\nPinecone is built for high-performance vector search at massive scale, and this new hybrid index is no exception. You can expect the same capacity (around 5 million 768-dimension vectors per pod), throughput, and latency as our storage-optimized [s1 pods](https://www.pinecone.io/docs/indexes/#s1-pods). As always, actual capacity and performance may vary based on use case and datasets, so we encourage you to experiment and [contact us](/contact/) for help if needed. All index types in Pinecone come with metadata filtering, vertical and horizontal scaling, snapshots, expert support, [and more](/pricing/). \n\n## Try it today\n\nOur hybrid search solution is currently in private preview. [Request early access](/hybrid-search-early-access/) to try it, read the docs, and stay tuned for more updates and technical deep dives (including [how to get started with hybrid search](/learn/hybrid-search-intro/)).\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbe0"
  },
  "filename": "funding-search-ai-age.md",
  "title": "post",
  "category": "\"$28M to Bring Search into the AI Age\"",
  "content": "---\nlayout: post\ntitle: \"$28M to Bring Search into the AI Age\"\nheadline: \"$28M to Bring Search into the AI Age\"\ncategories:\n  - Company News\ntoc: >-\nauthor:\n  name: Edo Liberty\n  position: Founder and CEO\n  src: /images/company-edo.png\n  href: https://edoliberty.github.io/\ndate: \"2022-03-29\"\n# Date: March 29, 2022\n# Open graph\ndescription: Pinecone raised $28M in Series A funding to bring search into the AI age.\nimages: [\"/images/series-a-rectangle.png\"]\nthumbnail: \"/images/28m-thumbnail.png\"\n---\n\nAs demand grows for better search results and recommendations, the path to better search applications is through AI, specifically vector search. Last year, we launched [Pinecone](/) to make it easy for developers to build high-performance vector search applications — at any scale and without infrastructure hassles.\n\nToday I’m excited to announce **we raised $28M in Series A funding**. This investment, along with our rapidly growing number of users and customers, is an undeniable testament to what we believed from day one: The future of search is [vector search](/learn/vector-search-basics/). And the future of vector search is Pinecone.\n\nI’d like to share how we got here and where we’re headed. It all started in the year 1200...\n\n## Search from 1200 AD to Today\n\nIn the 13th century, close to 200 years before the invention of the printing press, the cardinal Hugh of Saint-Cher created the first concordance of the Latin Bible by listing important keywords along with the page or passage numbers where they appear.\n\nIt would seem that search technology has changed a lot since then — we have modern indexing and ranking methods, with databases that can store and search through billions of records in milliseconds. And yet the core idea hasn’t changed: Eight centuries after the _Concordantiae Sancti Jacobi_ was penned, search technology still revolves around keywords.\n\nIn recent years, advancements in AI/ML have made it possible to capture the meaning of any data in a machine-readable format called [vector embeddings](/learn/vector-embeddings/). That opened the door to vector search, a revolutionary information-retrieval method that searches through data using meaning and not only keywords.\n\n![Traditional vs. Vector Search](/images/traditional-vs-vector-search.png)\n\nThe biggest tech companies have already adopted this technology. When you search on Google, get recommended products on Amazon, and read relevant stories on your Facebook feed, you see vector search in action.\n\nIt’s no coincidence this revolution started at the largest, most advanced tech companies: Leveraging vector search inside large-scale and high-performance applications requires a new kind of infrastructure to be built and maintained, along with extensive engineering and data science work. In other words: It’s _hard_.\n\n## Enter Pinecone\n\nWe founded Pinecone to make it easy for engineers to build vector search applications. That meant creating a completely new kind of infrastructure and indexing algorithm, standing it up as a managed service, and exposing it through a simple API. We needed to call it something, so we came up with “[vector database](/learn/vector-database/).”\n\n![The vector database is part of the new search infrastructure](/images/new-search-infrastructure.png)\n\nSince launching in 2021, we:\n\n- Released new features such as real-time index updates and [single-stage filtering](/docs/metadata-filtering/).\n- Released a new REST API built using the OpenAPI standard.\n- Made significant performance and scalability improvements to the core engine.\n- Launched a [vector-search community](/community/) and the Pinecone Pioneers program.\n- Invested in educational materials about vector search, including a free online course on [NLP for semantic search](/learn/nlp/).\n- Tripled the size of our team, including Ram Sriharsha who joined as VP of Engineering after holding the same position at Splunk.\n- Released a [free plan](https://app.pinecone.io/) for experimentation and small applications, and scalable usage-based plans for everyone else.\n- Onboarded thousands of users, ranging from fast-growing startups to Fortune 500 companies.\n\nThe impact has been astounding. Engineering teams of all sizes and machine-learning skill levels — are already running vector search in production thanks to Pinecone.\n\nTo give a few of my favorite examples: Pinecone powers the semantic search inside [Mem](https://mem.ai/) to help people stay organized, the alert management at [Expel](https://expel.io/) to protect cloud infrastructure, the file search at [Searchable](https://www.searchable.ai/) to make teams more productive, and the feed ranking inside a major social app to bring people together.\n\nWe have now crossed another milestone by raising $28M in Series A financing, led by Menlo Ventures with participation from new investor Tiger Global and previous investors, including Wing Venture Capital. Tim Tully, Partner at Menlo Ventures and former CTO of Splunk, will join the board of directors.\n\nI first met Tim when we worked at Yahoo, where where he led the data organization and media properties. Since then he has led multiple engineering organizations (including Splunk) and advised some of the fastest-growing cloud infrastructure startups. He is the most technically experienced investor I know, with an exceptional understanding of the AI and data infrastructure space. I couldn’t have asked for a better partner to join us on this journey.\n\n## The Future of Search\n\nWe’re on a mission to build search and database technology for the AI age. In the near term, that means two things: First, we must make it incredibly fast and easy for developers to use vector search applications, regardless of their experience with machine learning. Second, we must push the boundaries of vector search to provide faster and more relevant results at any scale.\n\nTo that end, we’re investing even more in product and engineering, developer advocacy and customer success, and core research into machine learning, natural language processing, and information retrieval. That includes growing an incredibly talented and ambitious team in those areas. If this sounds exciting to you, [join us](/careers/)!\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbe1"
  },
  "filename": "weight-initialization.md",
  "title": "post",
  "category": "Weight Initialization Techniques in Neural Networks",
  "content": "---\nlayout: post\ntitle: Weight Initialization Techniques in Neural Networks\nheadline: Weight Initialization Techniques in Neural Networks\ncategories:\n  - Algorithms & Libraries\ntoc: >-\nweight: 12\nauthor:\n  name: Bala Priya C\n  position: Technical Writer\n  src: /images/bala-priya.jpg\n  href: https://www.linkedin.com/in/bala-priya/\ndescription: How to optimize weight initialization of neural networks for faster convergence and better overall performance.\nimages: [\"/images/weight-initialization-1.jpg\"]\n---\n\n![Weight Initialization](/images/weight-initialization-1.jpg)\n\nYou can build better deep learning models that train _much_ faster by using the correct weight initialization techniques. A neural network _learns_ the weights during the training process. But how much do the initial weights of the network benefit or _hinder_ the optimization process?\n\nThough the neural network “learns” the optimal values for the weights, the initial values of the weights play a significant role in how quickly and to _which_ local optimum the weights converge.\n\nInitial weights have this impact because the loss surface of a deep neural network is a complex, high-dimensional, and [non-convex landscape](https://www.cs.cornell.edu/courses/cs6787/2021fa/lectures/Lecture7.pdf) with many local minima. So the point where the weights start on this loss surface determines the local minimum to which they converge; the better the initialization, the better the model.\n\nThis tutorial will discuss the early approaches to weight initialization and the limitations of zero, constant, and random initializations. We’ll then learn better weight initialization strategies based on the number of neurons in each layer, choice of activation functions, and more.\n\nLet’s begin!\n\n## Early Approaches to Weight Initialization\n\nWhen training deep neural networks, finding the optimal initial value for weights was one of the earliest challenges faced by deep learning researchers. In 2006, Geoffrey Hinton and Salakhutdinov introduced a weight initialization strategy called _Greedy Layerwise Unsupervised Pretraining_ [1]. To parse the algorithm’s definition, let’s understand how it works.\n\nGiven the input layer and the first hidden layer, an unsupervised learning model, such as an autoencoder, is used to learn the weights between the input and the first hidden layer.\n\n![Pretraining the first hidden layer](/images/weight-initialization-2.png)\n<small>Learning the initial weights for the first hidden layer (Image by the author)</small>\n\nThese weights are frozen and are used as inputs to learn the weights that flow into the next hidden layer.\n\n![Pretraining the second hidden layer](/images/weight-initialization-3.png)\n<small>Learning the initial weights for the second hidden layer (Image by the author)</small>\n\nThe process continues until all the layers in the neural network have been traversed. The weights learned this way are fine-tuned and used as the initial weights to train the neural network.\n\n![Pretraining the third hidden layer](/images/weight-initialization-4.png)\n<small>Learning the initial weights for the third hidden layer (Image by the author)</small>\n\nWe can parse the terms now that we understand how this algorithm works. This approach is **greedy** because it does not optimize the initial weights across all layers in the network but only focuses on the current layer. The weights are learned **layerwise** in an **unsupervised** setting. The term **pretraining** signifies that this process occurs ahead of the actual training process.\n\nThis approach to weight initialization was widely used in the deep learning research community before the advent of newer weight initialization techniques that do not require pretraining.\n\n## Zero or Constant Initialization\n\nThe need for a complex algorithm like the _greedy layerwise unsupervised pretraining_ for weight initialization suggests that trivial initializations don't necessarily work.\n\nThis section will explain why initializing all the weights to a zero or constant value is suboptimal. Let’s consider a neural network with two inputs and one hidden layer with two neurons, and initialize the weights and biases to zero, as shown.\n\n![Nueral network](/images/weight-initialization-5.png)\n<small>A simple neural network with one hidden layer, with the biases set to zero (Image by the author)</small>\n\nFor this neural network, $a_{11}$ and $a_{12}$ are given by the following equations:\n\n\\begin{align}\na_{11} = w_{11}x_1 + w_{12}x_2\\\\\\\\\na_{12} = w_{21}x_1 + w_{22}x_2\n\\end{align}\n\n\n\\begin{align}\nSetting \\text{ }w_{11}, w_{12}, w_{21}, and \\text{ }w_{22} \\text{ }to\\text{ } 0,\\\\\\\\\na_{11} = a_{12} = 0\\\\\\\\\n\\implies h_{11} = \\sigma{(a_{11})} = 0\\\\\\\\\nand\\text{ } h_{12} = \\sigma{(a_{12})} = 0\n\\end{align}\n\nLet $\\sigma$ denote the activation function.\n\nGiven a loss function $\\mathcal{L}$, the updates that each of the weights in the neural network receives during backpropagation are computed as follows:\n\n\\begin{align}\n\\nabla w_{11} = \\frac{\\partial{\\mathcal{L}}}{\\partial{y}}.\\frac{\\partial{y}}{\\partial{h_{11}}}.\\frac{\\partial{h_{11}}}{\\partial{a_{11}}}.x_1\\\\\\\\\n\\nabla w_{21} = \\frac{\\partial{\\mathcal{L}}}{\\partial{y}}.\\frac{\\partial{y}}{\\partial{h_{12}}}.\\frac{\\partial{h_{12}}}{\\partial{a_{12}}}.x_1\\\\\\\\\nBut\\text{ } h_{11} = h_{12} \\text{ }(since \\text{ } a_{11} = a_{12})\\\\\\\\\n\\implies \\text{ }\\nabla w_{11} = \\nabla w_{21} \n\\end{align}\n\nAfter the first update, the weights $w_{11}$ and $w_{21}$ move away from zero but are equal.\n\nSimilarly, we see that the weights $w_{12}$ and $w_{22}$ are equal after the first update.\n\n\\begin{align}\n\\nabla w_{12} = \\frac{\\partial{\\mathcal{L}}}{\\partial{y}}.\\frac{\\partial{y}}{\\partial{h_{11}}}.\\frac{\\partial{h_{11}}}{\\partial{a_{11}}}.x_2\\\\\\\\\n\\nabla w_{22} = \\frac{\\partial{\\mathcal{L}}}{\\partial{y}}.\\frac{\\partial{y}}{\\partial{h_{12}}}.\\frac{\\partial{h_{12}}}{\\partial{a_{12}}}.x_2\\\\\\\\\nBut\\text{ }  h_{11} = h_{12} \\text{ }(since \\text{ }a_{11} = a_{12})\\\\\\\\\n\\implies \\text{ } \\nabla w_{12} = \\nabla w_{22} \n\\end{align}\n\nThe weights are initially equal and receive the same update at each step. The neurons, therefore, evolve _symmetrically_ as the training proceeds, and we will _not_ be able to **break this symmetry**. This is true even when the weights are initialized to any constant k. The weights are initially at k, then receive the same update, leading to the symmetry problem yet again!\n\n_But why is this a problem?_\n\nThe main advantage of using a neural network over traditional machine learning algorithms is its ability to learn a complex mapping from the input space onto the output. It is for this reason neural networks are called [universal function approximators](http://neuralnetworksanddeeplearning.com/chap4.html). The various parameters of the network (weights) enable the neurons in the different layers to learn other aspects of this mapping. However, so long as the weights flowing into a neuron stay equal, all the neurons in a layer learn the “_same_” thing. Such a model performs poorly in practice.\n\n**Key takeaway**: Under zero or constant weight initialization, the neurons in a layer change symmetrically throughout the training process.\n\n---\n\n📑 **Use of Regularization in Neural Networks**: When training deep neural networks, you can use regularization techniques such as **dropout** to avoid overfitting.\n\nIf you implement dropout, a specific fraction of neurons in each layer are randomly switched off during training. As a result, those neurons may not get updates as the training proceeds, and it **is** possible to break the symmetry.\n\nHowever, the scope of this tutorial is to explain how the weights in a neural network should be carefully initialized in the absence of other regularization techniques.\n\n---\n\n## Random Initialization\n\nGiven that we cannot initialize the weights to all zeros or any constant k, the next natural step is to initialize them to random values. But does random initialization work?\n\n### Initializing the Weights to Small Random Values\n\nLet’s try initializing the weights to small random values. We’ll take an example to understand what happens in this case. \n\n```python\nimport numpy as np\n```\n\nConsider a neural network with five hidden layers, each with 50 neurons. The input to the network is a vector of length 100. \n\n```python\n# x: input vector \nx = np.random.randn(1,100) \n# 5 hidden layers each with 50 neurons\nhidden_layers = [50]*5\nuse_activation = ['tanh']*len(hidden_layers)\n# available activations\nactivation_dict = {'tanh':lambda x:np.tanh(x),'sigmoid':lambda x:1/(1+np.exp(-x))}\nH_mat = {}\n```\n\nLet’s observe what happens during the forward pass through this network. The weights are drawn from a standard normal distribution with zero mean and unit variance, and they’re all scaled by a factor of 0.01.\n\n```python\nfor i in range(len(hidden_layers)):\n  if i == 0:\n    X = x\n  else:\n    X = H_mat[i-1]\n\n  # define fan_in and fan_out \n  fan_in = X.shape[1]\n  fan_out = hidden_layers[i]\n\n  # weights are small random values\n  W = np.random.randn(fan_in,fan_out)*0.01\n\n  H = np.dot(X,W)\n  H = activation_dict[use_activation[i]](H)\n  H_mat[i] = H\n```\n\nFor small random values of weights, we observe that the activations grow smaller as we go deeper into the neural network. \n\n![tanh activations across the hidden layers](/images/weight-initialization-6.png)\n<small>Vanishingly small activations in the deeper layers of the network</small>\n\nDuring backpropagation, the gradients that flow into a neuron are proportional to the activation they receive. When the magnitude of activations is small, the gradients are vanishingly small, and the neurons do not learn anything!\n\n### Initializing the Weights to Large Random Values\n\nLet’s try initializing the weights to larger random values. Replace the weight matrix with the following `W`, where the samples are drawn from a standard normal distribution.\n\n```python\nW = np.random.randn(fan_in,fan_out)\n```\n\nWhen the weights have a large magnitude, the sigmoid and tanh activation functions take on values very close to saturation, as shown below. When the activations become saturated, the gradients move close to zero during backpropagation.\n\n![sigmoid and tanh activations](/images/weight-initialization-7.png)\n<small>Saturation of sigmoid and tanh activations (Image by the author)</small>\n\n![tanh activations for large random weights](/images/weight-initialization-8.png)\n<small>Saturating tanh activations for large random weights</small>\n\nLet’s summarize the observations from the above experiments.\n\n1. In the first case, we sampled the initial weights from a standard normal distribution with zero mean and unit variance and scaled them by a factor of 0.01. This is equivalent to drawing samples from a standard normal distribution with zero mean and variance $(0.01)^2$, which is negligibly small. When the weight distribution has a small variance, both activations during forward pass and gradients during backprop vanish.\n\n2. In the second case, we sampled from a standard normal distribution, without scaling the samples. We faced the problem of saturating activations and vanishing gradients during backpropagation.\n\nHowever, suppose we pick the optimal scaling factor, or equivalently, find the optimal variance of the weight distribution. In that case, we can get the network to operate in the region between vanishing and saturating activations.\n\n## A Better Weight Initialization Strategy\n\nLet us assume that the inputs have been normalized to have zero mean and unit variance. The weights are drawn from a distribution with zero mean and a fixed variance. But what should that variance be? Let’s analyze!\n\nTo compute the optimal variance, we’ll use $a_{11}$ as the first input to the first neuron in the second hidden layer, instead of $h_{11} = \\sigma(a_{11})$. $h_{11}$ is proportional to $a_{11}$, so we ignore the explicit effect of activations to simplify the derivation.\n\n![Weights flowing into the first neuron](/images/weight-initialization-9.png)\n<small>Weights flowing into the first neuron in the first hidden layer (Image by the author)</small>\n\n\\begin{align}\na_{11} = \\sum_{i=1}^{n} w_{1i}x_i \\\\\\\\\nVar(a_{11}) = Var\\left(\\sum_{i=1}^{n} w_{1i}x_i\\right)\n\\end{align}\n\n**Assumption**: The weights and inputs are _[uncorrelated](https://www.stat.cmu.edu/~cshalizi/uADA/13/reminders/uncorrelated-vs-independent.pdf)_.\n\n\\begin{align}\nVar(a_{11}) = \\sum_{i=1}^{n} Var(w_{1i}x_i)\\\\\\\\\n\\implies Var(a_{11}) = \\sum_{i=1}^{n} \\{ (\\mathbb{E}[w_{1i}])^2 Var(x_i) + (\\mathbb{E}[x_i])^2 Var(w_{1i}) + Var(x_i)Var(w_{1i})\\}\n\\end{align}\n\n\nSubstituting $\\mathbb{E}[w_{1i}] = 0$ and $\\mathbb{E}[x_i] = 0$ in the above equation:\n\n\\begin{align}\nVar(a_{11}) = \\sum_{i=1}^{n} Var(x_i)Var(w_{1i})\\\\\\\\\nLet \\text{ } Var(x_i) = Var(x) \\text{ } and \\text{ } Var(w_{1i}) = Var(w_1)\\\\\\\\\n\\implies Var(a_{11}) = n.Var(w)Var(x)\n\\end{align}\n\nLet’s compute the variance of $a_{21}$ in the second hidden layer.\n\n![Variance in the second hidden layer](/images/weight-initialization-10.png)\n\n\\begin{align}\nVar(a_{21}) = \\sum_{i=1}^{n} Var(a_{1i})Var(w_{2i}) \\text{ }(1)\\\\\\\\\nSubstituting\\text{ } Var(a_{11}) = n.Var(w)Var(x) \\text{ }and \\\\\\\\\nVar(w_{1i}) = Var(w_2)\\\\\\\\\n\\implies Var(a_{21}) = n.Var(w_2)n.Var(w_1)Var(x)\n\\end{align}\n\n\\begin{align}\nVar(a_{21}) =  n^2[Var(w)]^2Var(x)\\\\\\\\\n\\end{align}\n\nBy induction, the variance of input to neuron `i` in the hidden layer `k` is given by:\n\n\\begin{align}\nVar(a_{ki}) = [n.Var(w)]^kVar(x)\\\\\\\\\n\\end{align}\n\nTo ensure that the quantity `n.Var(w)` neither vanishes nor grows exponentially (leading to instability in the training process), we need `n.Var(w) =1`.\n\n\\begin{align}\nn. Var(w) = 1\\\\\\\\\nVar(w) = \\frac{1}{n}\n\\end{align}\n\n\\begin{align}\nIf \\text{ }Var(X) = \\sigma^2, \\text{ }then \\text{ }Var(c.X) = c^2\\sigma^2\n\\end{align}\n\nTo achieve this, we can sample the weights from a standard normal distribution and scale them by a factor of $\\frac{1}{\\sqrt{n}}$.\n\n## Xavier or Glorot Initialization\n\nX. Glorot and Y. Bengio proposed an improved weight initialization strategy named the Xavier or Glorot initialization [2] (after the researcher Xavier Glorot). \n\nIn a neural network, the number of weights that flow into each neuron in a neural network is called $fan_{in}$, and the number of weights that flow out of the neuron is called $fan_{out}$.\n\n![Explaining fan_in and fan_out](/images/weight-initialization-11.png)\n<small>Explaining fan_in and fan_out (Image by the author)</small>\n\nWhen the  weight distribution's variance is set to $\\frac{1}{fan_{in}}$, the activations neither vanish nor saturate during the forward pass. \n\n\\begin{align}\nfan_{in}. Var(w) = 1\\\\\\\\\nVar(w) = \\frac{1}{fan_{in}}\n\\end{align}\n\nHowever, during backpropagation, the gradients flow backward through the full network from the output layer to the input layer. We know that the $fan_{out}$ of a neuron is the number of weights that flow out of the neuron into the next layer. But the $fan_{out}$ of a particular neuron is also the number of paths through which gradients flow *into* it during backpropagation. Therefore, having the variance of the weights equal to $\\frac{1}{fan_{out}}$ helps overcome the problem of vanishing gradients.\n\nTo account for both the forward pass and backprop, we do the following: When computing the variance, instead of $fan_{in}$ or $fan_{out}$, we consider the average of $fan_{in}$ and $fan_{out}$.\n\n\\begin{align}\n\\frac{fan_{in}+fan_{out}}{2}. Var(w) = 1\\\\\\\\\nVar(w) = \\frac{2}{fan_{in}+fan_{out}}\n\\end{align}\n\nA random variable that is uniformly distributed in an interval centered around zero has zero mean. So we can sample the weights from a uniform distribution with variance $\\frac{2}{fan_{in}+fan_{out}}$. But how do we find the endpoints of the interval?\n\nA continuous random variable `A` that is uniformly distributed in the interval [-a,a] has zero mean and variance of $\\frac{a^2}{3}$. \n\n![](/images/weight-initialization-12.png)\n\n\\begin{align}\nVar(A) = \\frac{(2a)^2}{12} = \\frac{a^2}{3}\n\\end{align}\n\nWe know that the variance should be equal to $\\frac{2}{fan_{in}+fan_{out}}$; we can work backward to find the endpoints of the interval.\n\n\\begin{align}\nVar(w) = \\frac{(2a)^2}{12} = \\frac{a^2}{3}\\\\\\\\\nWe\\text{ }have,\\text{ }\nVar(w) = \\frac{2}{fan_{in}+fan_{out}}\\\\\\\\\n\\implies \\frac{a^2}{3} =  \\frac{2}{fan_{in}+fan_{out}}\\\\\\\\\na^2 = \\frac{6}{fan_{in}+fan_{out}}\\\\\n\\implies a = \\sqrt{\\frac{6}{fan_{in}+fan_{out}}}\\\\\\\\\nw \\in \\mathcal{U}\\left[-\\sqrt{\\frac{6}{fan_{in}+fan_{out}}},\\sqrt{\\frac{6}{fan_{in}+fan_{out}}}\\right]\n\\end{align}\n\n---\n### 📑  Glorot Initialization in Keras\n\nTo implement Glorot initialization in your deep learning models, you can use either the `GlorotUniform` or `GlorotNormal` class in the Keras `initializers` module. If you do not specify a kernel initializer when defining the model, it defaults to `GlorotUniform`.\n\n- The `GlorotNormal` class initializes the weight tensors with samples from a truncated normal distribution with variance $\\frac{2}{fan_{in}+fan_{out}}$. When samples are drawn from a “truncated” normal distribution, samples that lie farther than two standard deviations away from the mean are discarded.\n\n- The `GlorotUniform` class initializes the weight tensors by sampling from a uniform distribution in the interval $\\left[-\\sqrt{\\frac{6}{fan_{in}+fan_{out}}},\\sqrt{\\frac{6}{fan_{in}+fan_{out}}}\\right]$.\n---\n\n## He Initialization\n\nIt was found that Glorot initialization did not work for networks that used ReLU activations as the backflow of gradients was impacted [3]. \n\nBut why does this happen?\n\nUnlike the sigmoid and tanh activations, the ReLU function, which maps all negative inputs to zero: `ReLU(x) = max(0,x)`, does not have a zero mean. \n\n![ReLU Activation](/images/weight-initialization-13.png)\n\nThe ReLU function, therefore, outputs `0` for one-half of the input spectrum, whereas tanh and sigmoid activations give non-zero outputs for all values in the input space. Kaiming He et al. introduced a new initialization technique that takes this into account by introducing a factor of 2 when computing the variance [4].\n\n\\begin{align}\n\\frac{fan_{in}}{2}. Var(w) = 1\\\\\\\\\nVar(w) = \\frac{2}{fan_{in}}\n\\end{align}\n\nAs with Glorot initialization, we can also draw the weights from a uniform distribution for He initialization.\n\n\\begin{align}\nVar(w) = \\frac{(2a)^2}{12} = \\frac{a^2}{3}\\\\\\\\\nWe\\text{ }have,\\text{ }\nVar(w) = \\frac{2}{fan_{in}}\\\\\\\\\n\\implies \\frac{a^2}{3} =  \\frac{2}{fan_{in}}\\\\\\\\\na^2 = \\frac{6}{fan_{in}}\\\\\\\\\n\\implies a = \\sqrt{\\frac{6}{fan_{in}}}\\\\\\\\\nw \\in \\mathcal{U}\\left[-\\sqrt{\\frac{6}{fan_{in}}},\\sqrt{\\frac{6}{fan_{in}}}\\right]\n\\end{align}\n\n---\n### 📑 He Initialization in Keras\n\nThe Keras `initializers` module provides the `HeNormal` and `HeUniform` for He initialization.\n\n- The `HeNormal` class initializes the weight tensors with samples drawn from a truncated normal distribution with zero mean and variance $\\frac{2}{fan_{in}}$.\n\n- The `HeUniform` class initializes the weight tensors with samples drawn from a uniform distribution in the interval $\\left[-\\sqrt{\\frac{6}{fan_{in}}},\\sqrt{\\frac{6}{fan_{in}}}\\right]$.\n---\n\nConsidering the $fan_{out}$ when initializing weights, you can draw the weights from normal distribution with the following variance:\n\n\\begin{align}\n\\frac{\\frac{fan_{in}+fan_{out}}{2}}{2}.Var(w) = 1\\\\\\\\\nVar(w) = \\frac{4}{fan_{in}+fan_{out}}\n\\end{align}\n\nEquivalently, you may sample the initial weights from a uniform distribution with variance $\\frac{4}{fan_{in}+fan_{out}}$.\n\n\\begin{align}\nVar(w) = \\frac{(2a)^2}{12}= \\frac{a^2}{3}\\\\\\\\\nWe\\ have,\\\nVar(w) = \\frac{4}{fan_{in}+fan_{out}}\\\\\\\\\n\\implies \\frac{a^2}{3} =  \\frac{4}{fan_{in}+fan_{out}}\\\\\\\\\na^2 = \\frac{12}{fan_{in}+fan_{out}}\\\\\\\\\n\\implies a = \\sqrt{\\frac{12}{fan_{in}+fan_{out}}}\\\\\\\\\nw \\in \\mathcal{U}\\left[-\\sqrt{\\frac{12}{fan_{in}+fan_{out}}},\\sqrt{\\frac{12}{fan_{in}+fan_{out}}}\\right]\n\\end{align}\n\n## Summing Up\n\nI hope this tutorial helped you understand the importance of weight initialization when training deep learning models.\n\n- Initializing the weights to **zero or a constant value** leads to the symmetry-breaking problem. This problem stems from the weights receiving the same updates at each step and _updating symmetrically_ as the training proceeds.\n\n- Initializing the weights to **small random values** leads to the problem of vanishing gradients. This is because the gradients flowing into a particular neuron are proportional to the activation that it receives. On the other hand, initializing the weights to **large random values** causes the activations to get saturated, resulting in vanishing gradients during backpropagation.\n\n- To prevent the weights from being drawn from a distribution whose variance is neither too large nor too small, the variance of the distribution must be approximately 1. \n\n- **Xavier or Glorot initialization** works well for networks using activations with zero mean, such as the sigmoid and tanh functions.\n\n- When using **ReLU** activation that does not have zero mean, it's recommended to use the **He initialization**.\n\nWhen using deep learning in practice, you can experiment with weight initialization, regularization, and techniques such as [batch normalization](/learn/batch-layer-normalization/) to improve the neural network’s training process. Happy learning and coding!\n\n## Resources\n\n[1]  G. E. Hinton, R. R. Salakhutdinov, [Reducing the Dimensionality of Data with Neural Networks](https://www.cs.toronto.edu/~hinton/science.pdf) (2006), Sciences\n\n[2] X. Glorot, Y. Bengio, [Understanding the difficulty of training deep feedforward neural networks](https://proceedings.mlr.press/v9/glorot10a.html) (2010), AISTATS 2010\n\n[3] K. Kumar, [On weight initialization in deep neural networks](https://arxiv.org/abs/1704.08863) (2017)\n\n[4] He et al., [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852) (2015), CVPR 2015\n\n[5] [Introduction to Statistical Signal Processing](https://ee.stanford.edu/~gray/sp.html), Gray and Davisson\n\n[6] [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/), Boyd and Vandenberghe\n\n[7] [Adaptive Methods and Non-convex Optimization](https://www.cs.cornell.edu/courses/cs6787/2021fa/lectures/Lecture7.pdf), Advanced Machine Learning Systems, Cornell University, Fall 2021\n\n[8] [Layer weight initializers](https://keras.io/api/layers/initializers/), keras.io\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbe2"
  },
  "filename": "public-collections.md",
  "title": "post",
  "category": "Explore the power of Pinecone with public collections",
  "content": "---\nlayout: post\ntitle: Explore the power of Pinecone with public collections\nheadline: Explore the power of Pinecone with public collections\ncategories:\n  - Company News\ntoc: >-\nauthor:\n  name: Gibbs Cullen\n  position: Senior Product Marketing Manager\n  src: /images/gibbs-cullen.jpg\n  href: https://www.linkedin.com/in/gibbscullen/\ndate: \"2022-09-16\"\nthumbnail: \"/images/public-collections-thumbnail.png\"\n# Open Graph\ndescription: Start your vector-database journey with a click.\nimages: [\"/images/public-collections-cover.png\"] \n---\n\n![Public collections](/images/public-collections-cover.png)\n\nLast month, we [announced](/learn/faster-easier-scalable/) a new feature in public preview: [collections](https://www.pinecone.io/docs/collections/). Collections allow users to save vectors and metadata from an index as a snapshot, and create new indexes from any collection. \n\nToday we are excited to announce the addition of **public collections** to help users quickly run a sample index pre-loaded with data and experience the power of the Pinecone [vector database](/learn/vector-database/).\n\n## What are public collections? \n\nFor users to run a query in Pinecone, they need to upload data to an index. This takes time. Public collections make it easier to explore Pinecone by providing public data from real-world data sources that can be used to create an index in one click.\n\nPinecone users can now create an index from pre-loaded vector embeddings in one of three example collections. Each collection features data from Pinecone partners:\n\n- Glue SSTB collection from OpenAI \n- Text REtrieval Conference (TREC) question classification collection from Cohere \n- Stanford Question Answering Dataset (SQuAD) collection from Stanford \n\nThese collections contain real-world data, load in less than a minute, and have matching guides to get started: \n\n- [Guide for OpenAI](https://www.pinecone.io/docs/integrations/openai/) \n- [Guide for Cohere](https://www.pinecone.io/docs/integrations/cohere/)\n- [Guide for SQuAD](https://www.pinecone.io/docs/examples/extractive-question-answering/)\n\n## How do they work? \n\nThe collections are available under **Public Collections** within the [Pinecone console](https://app.pinecone.io). You can create an index from the example collections and use the [guides](https://www.pinecone.io/docs/collections#public-collections-contain-real-world-data) to get started including code snippets in Python showing how to use the particular index. \n\n![Public collections](/images/public-collections.png)\n\nTo create an index from a [public collection](https://www.pinecone.io/docs/manage-indexes/#create-an-index-from-a-public-collection), follow these steps:\n\n1. Open the [Pinecone console](https://app.pinecone.com/).\n2. Click the name of the project in which you want to create the index.\n3. In the left menu, click **Public Collections**.\n4. Find the public collection from which you want to create an index. Next to that public collection, click **Create Index**.\n5. When index creation is complete, a message appears stating that the index is created and that vectors are successfully upserted. The **Click to View** button will take you to the new index.\n\n![Create index](/images/public-collections-create-index.png)\n\n## Get started today \n\nIf you don’t have an embedding model or ready-to-use data to start testing Pinecone, then public collections can help. All Pinecone users will have access to three example collections — Glue SSTB, TREC question classification, and SQuaD — starting today. We will add more public collections over time.\n\nTo learn more about public collections, check out the [guides](https://www.pinecone.io/docs/collections#public-collections) or [try them for yourself](https://app.pinecone.io) in the console. \n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbe3"
  },
  "filename": "openai-whisper.md",
  "title": "post",
  "category": "\"OpenAI Whisper",
  "content": "---\nlayout: post\ntitle: \"OpenAI Whisper: Introduction and Example Project\"\nheadline: \"Fixing YouTube Search with OpenAI's Whisper\"\ncategories:\n  - Projects\ntoc: >-\nweight: 3\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: How to use OpenAI's Whisper for better speech-enabled (audio) search.\n# Open graph\nimages: ['https://www.pinecone.io/images/openai-whisper-0.png']\n---\n\nOpenAI’s *Whisper* is a new state-of-the-art (SotA) model in speech-to-text. It is able to almost flawlessly transcribe speech across dozens of languages and even handle poor audio quality or excessive background noise.\n\nThe domain of spoken word has always been somewhat out of reach for ML use-cases. Whisper changes that for speech-centric use cases. We will demonstrate the power of Whisper alongside other technologies like transformers and vector search by building a new and improved YouTube search.\n\nSearch on YouTube is good but has its limitations, especially when it comes to answering questions. With trillions of hours of content, there should be an answer to almost every question. Yet, if we have a specific question like *\"what is OpenAI's CLIP?\"*, instead of a concise answer we get lots of very long videos that we must watch through.\n\nWhat if all we want is a short 20-second explanation? The current YouTube search has no solution for this. Maybe there's a good reason to encourage users to watch as much of a video as possible (more ads, anyone?).\n\nWhisper is the solution to this problem *and many others involving the spoken word*. In this article, we'll explore the idea behind a better speech-enabled search.\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/vpU_6x3jowg\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n---\n\n## The Idea\n\nWe want to get specific timestamps that answer our search queries. YouTube does support time-specific links in videos, so a more precise search with these links should be possible.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n <source src=\"./images/openai-whisper-1.mp4\" type=\"video/mp4\">\n</video>\n<small>Timestamp URLs can be copied directly from a video, we can use the same URL format in our search app.</small>\n\nTo build something like this, we first need to transcribe the audio in our videos to text. YouTube automatically captions every video, and the captions are okay — *but* OpenAI just open-sourced something called \"Whisper\".\n\nWhisper is best described as the GPT-3 or DALL-E 2 of speech-to-text. It's open source and can transcribe audio in real-time *or faster* with *unparalleled performance*. That seems like the most exciting option.\n\nOnce we have our transcribed text and the timestamps for each text snippet, we can move on to the [question-answering (QA)](https://www.pinecone.io/learn/question-answering) part. QA is a form of search where given a natural language query like *\"what is OpenAI's Whisper?\"* we can return accurate natural language answers.\n\nWe can think of QA as the most intuitive form of searching for information because it is how we ask other people for information. The only difference being we type the question into a search bar rather than verbally communicate it — for now.\n\nHow does all of this look?\n\n![whisper-architecture](./images/openai-whisper-2.png)\n<small>Overview of the process used in our demo. Covering OpenAI's Whisper, sentence transformers, the Pinecone vector database, and more.</small>\n\nNow let's color in the details and walk through the steps.\n\n## Video Data\n\nThe first step is to download our YouTube video data and extract the audio attached to each video. Fortunately, there's a Python library for exactly that called `pytube`.\n\nWith `pytube`, we provide a video ID (found in the URL bar or downloadable if you have a channel). I directly downloaded a summary of channel content, including IDs, titles, publication dates, etc., via YouTube. This same data is available via Hugging Face *Datasets* in a dataset called `jamescalam/channel-metadata`.\n\n{{< notebook file=\"whisper-yt-search-channel-meta\" height=\"full\" >}}\n\nWe're most interested in the `Title` and `Video ID` fields. With the video ID, we can begin downloading the videos and saving the audio files with `pytube`.\n\n```python\nfrom pytube import YouTube  # !pip install pytube\nfrom pytube.exceptions import RegexMatchError\nfrom tqdm.auto import tqdm  # !pip install tqdm\n\n# where to save\nsave_path = \"./mp3\"\n\nfor i, row in tqdm(videos_meta):\n    # url of video to be downloaded\n    url = f\"https://youtu.be/{row['Video ID']}\"\n\n    # try to create a YouTube vid object\n    try:\n        yt = YouTube(url)\n    except RegexMatchError:\n        print(f\"RegexMatchError for '{url}'\")\n        continue\n\n    itag = None\n    # we only want audio files\n    files = yt.streams.filter(only_audio=True)\n    for file in files:\n        # from audio files we grab the first audio for mp4 (eg mp3)\n        if file.mime_type == 'audio/mp4':\n            itag = file.itag\n            break\n    if itag is None:\n        # just incase no MP3 audio is found (shouldn't happen)\n        print(\"NO MP3 AUDIO FOUND\")\n        continue\n\n    # get the correct mp3 'stream'\n    stream = yt.streams.get_by_itag(itag)\n    # downloading the audio\n    stream.download(\n        output_path=save_path,\n        filename=f\"{row['Video ID']}.mp3\"\n    )\n```\n\nAfter this, we should find ~108 audio MP3 files stored in the `./mp3` directory.\n\n![mp3 files directory](./images/openai-whisper-3.png)\n<small>Downloaded MP3 files in the `./mp3` directory.</small>\n\nWith these, we can move on to transcription with OpenAI's Whisper.\n\n## Speech-to-Text with Whisper\n\nOpenAI’s Whisper speech-to-text-model is completely open source and available via [OpenAI's Whisper library](https://github.com/openai/whisper) available for `pip install` via GitHub:\n\n```\n!pip install git+https://github.com/openai/whisper.git\n```\n\nWhisper relies on another software called FFMPEG to convert video and audio files. The installation for this varies by OS [1]; the following cover the primary systems:\n\n```\n# on Ubuntu or Debian\nsudo apt update && sudo apt install ffmpeg\n\n# on Arch Linux\nsudo pacman -S ffmpeg\n\n# on MacOS using Homebrew (https://brew.sh/)\nbrew install ffmpeg\n\n# on Windows using Chocolatey (https://chocolatey.org/)\nchoco install ffmpeg\n\n# on Windows using Scoop (https://scoop.sh/)\nscoop install ffmpeg\n```\n\nAfter installation, we download and initialize the *large* model, moving it to GPU if CUDA is available.\n\n```python\nimport whisper\nimport torch  # install steps: pytorch.org\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = whisper.load_model(\"large\").to(device)\n```\n\nOther models are available, and given a smaller GPU (or even CPU) should be considered. We transcribe the audio like so:\n\n{{< notebook file=\"whisper-yt-search-transcribe\" height=\"full\" >}}\n\nFrom this, we have a list of ~27K transcribed audio segments, including text alongside start and end seconds. If you are waiting a long time for this to process, a pre-built version of the dataset is available. Download instructions are in the following section.\n\nThe last cell from above is missing the logic required to extract and add the metadata from our `videos_dict` that we initialized earlier. We add that like so:\n\n```python\ndata = []\n\nfor i, path in enumerate(tqdm(paths)):\n    _id = path.split('/')[-1][:-4]\n    # transcribe to get speech-to-text data\n    result = model.transcribe(path)\n    segments = result['segments']\n    # get the video metadata...\n    video_meta = videos_dict[_id]\n    for segment in segments:\n        # merge segments data and videos_meta data\n        meta = {\n            **video_meta,\n            **{\n                \"id\": f\"{_id}-t{segments[j]['start']}\",\n                \"text\": segment[\"text\"].strip(),\n                \"start\": segment['start'],\n                \"end\": segment['end']\n            }\n        }\n        data.append(meta)\n```\n\nAfter processing all of the segments, they are saved to file as a JSON lines file with:\n\n```python\nimport json\n\nwith open(\"youtube-transcriptions.jsonl\", \"w\", encoding=\"utf-8\") as fp:\n    for line in tqdm(data):\n        json.dump(line, fp)\n        fp.write('\\n')\n```\n\nWith that ready, let's build the QA embeddings and vector search component.\n\n## Question-Answering\n\nOn Hugging Face *Datasets*, you can find the data I scraped in a dataset called `jamescalam/youtube-transcriptions`:\n\n{{< notebook file=\"whisper-yt-search-get-transcriptions\" height=\"full\" >}}\n\nFor now, the dataset only contains videos from my personal channel, but I will add more videos from other ML-focused channels in the future.\n\nThe data includes a short chunk of text (the transcribed audio). Each chunk is relatively meaningless:\n\n{{< notebook file=\"whisper-yt-search-short-segments\" height=\"full\" >}}\n\nIdeally, we want chunks of text 4-6x larger than this to capture enough meaning to be helpful. We do this by simply iterating over the dataset and merging every *six* segments.\n\n{{< notebook file=\"whisper-yt-search-longer-segments\" height=\"full\" >}}\n\nA few things are happening here. First, we're merging every six segments, as explained before. However, doing this alone will likely cut a lot of meaning between related segments.\n\n![window-no-overlap](./images/openai-whisper-4.png)\n\n<small>Even when merging segments we're still left with a point where we must split the text (annotated with red cross-mark above). This can lead to us missing important information.</small>\n\nA common technique to avoid cutting related segments is adding some *overlap* between segments, where `stride` is used. For each step, we move *three* segments forward while merging *six* segments. By doing this, any meaningful segments cut in one step will be included in the next.\n\n![window-overlap](./images/openai-whisper-5.png)\n\n<small>We can avoid this loss of meaning by adding an overlap when merging segments. It returns more data but means we are much less likely to cut between meaning segments.</small>\n\nWith this, we have larger and more meaningful chunks of text. Now we need to encode them with a QA embedding model. Many high-performing, pretrained QA models are available via Hugging Face *Transformers* and the *Sentence Transformers* library. We will use one called [`multi-qa-mpnet-base-dot-v1`](https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1).\n\n{{< notebook file=\"whisper-yt-search-init-encoder\" height=\"full\" >}}\n\nUsing this model, we can encode a passage of text to a *meaningful* 768-dimensional vector with `model.encode(\"<some text>\")`. Encoding all of our segments at once or storing them locally would require too much compute or memory — so we first initialize the vector database where they will be stored:\n\n{{< notebook file=\"whisper-yt-search-init-pinecone\" height=\"full\" >}}\n\nWe should see that the index (vector database) is currently empty with a `total_vector_count` of `0`. Now we can begin encoding our segments and inserting the embeddings (and metadata) into our index.\n\n{{< notebook file=\"whisper-yt-search-index-vecs\" height=\"full\" >}}\n\nThat is everything we needed to prepare our data and add everything to the vector database. All that is left is querying and returning results.\n\n## Making Queries\n\nQueries are straightforward to make; we:\n\n1. Encode the query using the same embedding model we used to encode the segments.\n2. Pass to query to our index.\n\nWe do that with the following:\n\n{{< notebook file=\"whisper-yt-search-query\" height=\"full\" >}}\n\nThese results are relevant to the question; three, in particular, are from a similar location in the same video. We might want to improve the search interface to be more user-friendly than a Jupyter Notebook.\n\nOne of the easiest ways to get a web-based search UI up and running is with Hugging Face *Spaces* and Streamlit (or Gradio if preferred).\n\nWe won't go through the code here, but if you're familiar with Streamlit, you can build a search app quite easily within a few hours. Or you can use our example and do it in 5-10 minutes.\n\n<iframe src=\"https://hf.space/streamlit/jamescalam/ask-youtube/+\" data-src=\"https://hf.space/streamlit/jamescalam/ask-youtube/+\" data-sdk=\"streamlit\" title=\"Streamlit app\" style=\"width:100%;height:1000px;overflow: hidden;\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\" scrolling=\"yes\"></iframe>\n\nYou can test the app above or on the [homepage here](https://huggingface.co/spaces/jamescalam/ask-youtube). When querying again for `\"what is OpenAI's clip?\"` we can see that multiple results from a single video are merged. With this, we can jump to each segment by clicking on the part of the text that is most interesting to us.\n\nTry a few more queries like:\n\n```\nWhat is the best unsupervised method to train a sentence transformer?\n\nWhat is vector search?\n\nHow can I train a sentence transformer with little-to-no data?\n```\n\n---\n\nWe can build incredible speech-enabled search apps very quickly using Whisper alongside Hugging Face, sentence transformers, and Pinecone’s [vector database](https://www.pinecone.io/learn/vector-database).\n\nWhisper has unlocked a entire modality — the spoken word — and it’s only a matter of time before we see a significant increase in speech-enabled search and other speech-centric use cases.\n\nBoth machine learning and vector search have seen exponential growth in the past years. These technologies already seem like sci-fi. Yet, despite the incredible performance of everything we used here, it's only a matter of time before all of this gets *even better*.\n\n{{< newsletter text=\"Subscribe for more appled ML walkthroughs!\" inputText=\"Email address...\" buttonText=\"Submit\">}}\n\n## Resources\n\n[All Code Notebooks](https://github.com/jamescalam/ask-youtube/tree/main/youtube-search)\n\n* [MP3 Download](https://colab.research.google.com/github/jamescalam/ask-youtube/blob/main/youtube-search/00-download-videos.ipynb)\n* [Whisper Transcription](https://colab.research.google.com/github/jamescalam/ask-youtube/blob/main/youtube-search/01-openai-whisper.ipynb)\n* [Encode and Query](https://colab.research.google.com/github/jamescalam/ask-youtube/blob/main/youtube-search/02-build-embeddings.ipynb)\n\n[Demo App](https://huggingface.co/spaces/jamescalam/ask-youtube)\n\n[1] [OpenAI Whisper Repo](https://github.com/openai/whisper) (2022), GitHub\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbe4"
  },
  "filename": "locality-sensitive-hashing-random-projection.md",
  "title": "ebook-post",
  "category": "\"Random Projection for Locality Sensitive Hashing\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Random Projection for Locality Sensitive Hashing\"\nheadline: \"Random Projection for Locality Sensitive Hashing\"\ncategories:\n  - \"Faiss: The Missing Manual\"\ntoc: >-\nweight: 4\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: Practical implementation of Random Projection LSH for approximate nearest neighbors search.\n#Open Graph\nimages: ['/images/locality-sensitive-hashing-random-projection-8.jpeg']\n---\n\n![Similarity Search with Locality Sensitive Hashing with Random Projection](/images/locality-sensitive-hashing-random-projection-8.jpeg)\n\nLocality sensitive hashing (LSH) is a widely popular technique used in *approximate* similarity search. The solution to efficient similarity search is a profitable one — it is at the core of several billion (and even trillion) dollar companies.\n\nThe problem with similarity search is *scale*. Many companies deal with millions-to-billions of data points every single day. Given a billion data points, is it feasible to compare all of them with every search?\n\nFurther, many companies are not performing single searches — Google deals with more than 3.8 million searches every *minute*<sup>[1]</sup>.\n\nBillions of data points combined with high-frequency searches are problematic — and we haven’t considered the dimensionality nor the similarity function itself. Clearly, an exhaustive search across all data points is unrealistic for larger datasets.\n\nThe solution to searching impossibly huge datasets? *Approximate search.* Rather than *exhaustively* comparing every pair, we *approximate* — restricting the search scope only to high probability matches.\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/8bOrMqEdfiQ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n---\n\n## Locality Sensitive Hashing\n\n[Locality Sensitive Hashing (LSH)](/learn/locality-sensitive-hashing/) is one of the most popular approximate nearest neighbors search (ANNS) methods.\n\nAt its core, it is a hashing function that allows us to group similar items into the same hash buckets. So, given an impossibly huge dataset — we run all of our items through the hashing function, sorting items into buckets.\n\nUnlike most hashing functions, which aim to *minimize* hashing collisions — LSH algorithms aim to *maximize* hashing collisions.\n\n![Two hashing functions, the top (blue) **min**imizes hashing collisions. The bottom (magenta) maximizes hashing collisions — LSH aims to **max**imize collisions between similar items.](/images/locality-sensitive-hashing-random-projection-14.jpeg)<small>Two hashing functions, the top (blue) **min**imizes hashing collisions. The bottom (magenta) maximizes hashing collisions — LSH aims to **max**imize collisions between similar items.</small>\n\nThis result for LSH is that similar [vectors](/learn/vector-embeddings/) produce the same hash value and are bucketed together. In contrast, dissimilar vectors will *hopefully* not produce the same hash value — being placed in different buckets.\n\n### Search With LSH\n\nPerforming search with LSH consists of three steps:\n\n1. Index all of our vectors into their hashed vectors.\n\n1. Introduce our query vector (search term). It is hashed using the same LSH function.\n\n1. Compare our hashed query vector to all other hash buckets via Hamming distance — identifying the nearest.\n\nAt a very high level, that is the process of the LSH methodology we will be covering. However, we will explain all of this in much more detail further in the article.\n\n### Effects of Approximation\n\nBefore diving into the detail of LSH, we should note that grouping vectors into lower resolution *hashed* vectors also means that our search is not exhaustive (e.g., comparing *every* vector), so we expect a lower search quality.\n\n![We compress potentially huge dense vectors into highly compressed, grouped binary vectors.](/images/locality-sensitive-hashing-random-projection-12.jpeg)*We compress potentially huge dense vectors into highly compressed, grouped binary vectors.*\n\nBut, the reason for accepting this lower search quality is the potential for *significantly* faster search speeds.\n\n### Which Method?\n\nWe’ve been purposely lax on the details here because there are several versions of LSH — each utilizing different hash building and distance/similarity metrics. The two most popular approaches are:\n\n* Shingling, MinHashing, and banded LSH (traditional approach)\n\n* Random hyperplanes with dot-product and Hamming distance\n\nThis article will focus on the random hyperplanes method , which is more commonly used and implemented in various popular libraries such as [Faiss](/learn/faiss/).\n\n---\n\n## Random Hyperplanes\n\nThe random hyperplanes (also called random *projection*) approach is deceptively simple — although it can be hard to find details on the method.\n\nLet’s learn through an example — we will be using the Sift1M dataset throughout our examples, which you can download using [this script](https://gist.github.com/jamescalam/a09a16c17b677f2cf9c019114711f3bf).\n\nNow, given a single query vector `xq` we want to identify the top `k` nearest neighbors from the `xb` array.\n\n![Here we are returning the **three** nearest neighbors to our query vector **xq**.](/images/locality-sensitive-hashing-random-projection-2.png)<small>Here we are returning the **three** nearest neighbors to our query vector **xq**.</small>\n\nUsing the random projection method, we will reduce our highly-dimensional vectors into low-dimensionality binary vectors. Once we have these binary vectors, we can measure the distance between them using Hamming distance.\n\nLet’s work through that in a little more detail.\n\n### Creating Hyperplanes\n\nThe hyperplanes in this method are used to split our datapoints and assign a value of *0* for those data points that appear on the negative side of our hyperplane — and a value of *1* for those that appear on the positive side.\n\n![We assign a value of **1** to vectors on the +ve side of our hyperplane and a value of **0** to vectors on the -ve side of the hyperplane.](/images/locality-sensitive-hashing-random-projection-13.jpeg)<small>We assign a value of **1** to vectors on the +ve side of our hyperplane and a value of **0** to vectors on the -ve side of the hyperplane.</small>\n\nTo identify which side of the hyperplane our data point is located, all we need is the normal vector of the plane — e.g., a vector perpendicular to the plane. We feed this normal vector (alongside our datapoint vector) into a dot product function.\n\nIf two vectors share the same direction, the resultant dot product is positive. If they do *not* share the same direction, it is negative.\n\n![Where our hyperplane normal vector produces a +ve dot-product with another vector, we can view that vector as being in front of the hyperplane. The reverse is true for vectors that produce a -ve dot-product.](/images/locality-sensitive-hashing-random-projection-3.jpeg)<small>Where our hyperplane normal vector produces a +ve dot-product with another vector, we can view that vector as being in front of the hyperplane. The reverse is true for vectors that produce a -ve dot-product.</small>\n\n*In the unlikely case of both vectors being perfectly perpendicular (sitting on the hyperplane edge), the dot product is 0 — we will group this in with our negative direction vectors.*\n\nA single binary value doesn’t tell us much about the similarity of our vectors, but when we begin adding *more* hyperplanes — the amount of encoded information rapidly increases.\n\n![We add more hyperplanes to increase the amount of positional information stored in our binary vectors.](/images/locality-sensitive-hashing-random-projection-11.jpeg)<small>We add more hyperplanes to increase the amount of positional information stored in our binary vectors.</small>\n\nBy projecting our vectors into a lower-dimensional space using these hyperplanes, we produce our new *hashed* vectors.\n\nIn the image above, we have used two hyperplanes, and realistically we will need many more — a property that we define using the `nbits` parameter. We will discuss `nbits` in more detail later, but for now, we will use **four** hyperplanes by setting `nbits = 4`.\n\nNow, let’s create the normal vectors of our hyperplanes in Python.\n\n{{< notebook file=\"random-hyperplanes\" height=\"full\" >}}\n\nThrough `np.random.rand` we create a set of random values in the range *0* → *1*. We then add `-.5` to center our array values around the origin *(0, 0)*. Visualizing these vectors, we see:\n\n![The normal vectors that define the positions of our hyperplanes, which are all centered around the origin (0, 0).](/images/locality-sensitive-hashing-random-projection-10.jpeg)<small>The normal vectors that define the positions of our hyperplanes, which are all centered around the origin (0, 0).</small>\n\n### Hashing Vectors\n\nNow let’s add three vectors — **a**, **b**, and **c** — and work through building our hash values using our four *normal vectors and their hyperplanes*.\n\n{{< notebook file=\"creating-hashes\" height=\"full\" >}}\n\nVisualizing this again, we have our three vectors **a**, **b**, and **c** — alongside our four hyperplanes (perpendicular to their respective normal vectors). Taking the +ve and -ve dot-product values for each gives us:\n\n![A **zero** shows that the vector is behind the plane (-ve dot product), and a **one** shows that the vector is in front of the plane (+ve dot product). We combine these to create our binary vectors.](/images/locality-sensitive-hashing-random-projection-6.jpeg)<small>A **zero** shows that the vector is behind the plane (-ve dot product), and a **one** shows that the vector is in front of the plane (+ve dot product). We combine these to create our binary vectors.</small>\n\nWhich produces our hashed vectors. Now, LSH uses these values to create buckets — which will contain some reference back to our vectors (Eg their IDs). Note that we do not store the original vectors in the buckets — which would significantly increase the size of our LSH index.\n\nAs we will see with implementations such as [Faiss](/learn/faiss-tutorial/) — the position/order that we added the vector is usually stored. We will use this same approach in our examples.\n\n{{< notebook file=\"buckets\" height=\"full\" >}}\n\nNow that we have bucketed our three vectors, let’s consider the change in the complexity of our search. Let’s say we introduce a query vector that gets hashed as `0111`.\n\nWith this vector, we compare it to every bucket in our LSH index — which in this case is only two values — `1000` and `0110`. We then use Hamming distance to find the closest match, and this happens to be `0110`.\n\n![Hamming distance, there are **four** mismatches between the first two vectors — resulting in a Hamming distance of four. The next two contain just **one** mismatch, giving a Hamming distance of one.](/images/locality-sensitive-hashing-random-projection-9.jpeg)<small>Hamming distance, there are **four** mismatches between the first two vectors — resulting in a Hamming distance of four. The next two contain just **one** mismatch, giving a Hamming distance of one.</small>\n\nWe have taken a linear complexity function, which required us to compute the distance between our query vector and all of the previously indexed vectors — to a sub-linear complexity — as we no longer need to compute the distance for *every* vector — because they’re grouped into buckets.\n\nNow, at the same time, vectors **1** and **2** are both equal to `0110`. So we cannot possibly find which of those is closest to our query vector. This means there is a degree of search quality being lost — however, this is simply the cost of performing an approximate search. *We trade quality for speed.*\n\n---\n\n## Balancing Quality vs. Speed\n\nAs is often the case in [similarity search](/learn/what-is-similarity-search/), a good LSH index requires balancing search quality vs. search speed.\n\nWe saw in our mini-example that our vectors were not easy to differentiate, because out of a total of three vectors — random projection had hashed two of them to the same binary vector.\n\nNow imagine we scale this same ratio to a dataset containing one million vectors. We introduce our query vector `xq` — hash it and calculate the hamming distance between that (`0111`) and our *two* buckets (`1000` and `0110`).\n\nWow, a million sample dataset searched with just two distance calculations? That’s *fast*.\n\nUnfortunately, we return around 700K samples, all with the same binary vector value of `0110`. Fast yes, accurate — *not at all*.\n\nNow, in reality, using an `nbits` value of `4` would produce **16** buckets:\n\n{{< notebook file=\"binary\" height=\"full\" >}}\n\nWe stuck with the two buckets of `1000` and `0110` solely for dramatic effect. But even with *16* buckets — 1M vectors split into just 16 buckets still produces massively imprecise buckets.\n\nIn reality, we use many more hyperplanes — more hyperplanes mean higher resolution binary vectors — producing much more precise representations of our vectors.\n\nWe control this resolution through hyperplanes with the nbits value. A higher `nbits` value improves search quality by increasing the resolution of hashed vectors.\n\n![Increasing the **nbits** parameter increases the number of hyperplanes used to build the binary vector representations.](/images/locality-sensitive-hashing-random-projection-4.jpeg)<small>Increasing the **nbits** parameter increases the number of hyperplanes used to build the binary vector representations.</small>\n\nAdding more possible combinations of hash values increases the potential number of buckets , increasing the number of comparisons and, therefore, *search time*.\n\n{{< notebook file=\"nbits-and-buckets\" height=\"full\" >}}\n\nIt’s worth noting that not *all* buckets will necessarily be used — particularly with higher `nbits` values. We will see through our Faiss implementation that a `nbits` value of `128` or more is completely valid and still faster than using a [flat index](/learn/vector-indexes/).\n\nThere is also this [notebook that covers a simple implementation of LSH](https://github.com/pinecone-io/examples/blob/master/locality_sensitive_hashing_random_projection/random_projection.ipynb) in Python.\n\n---\n\n## LSH in Faiss\n\nWe have [discussed Faiss before](/learn/faiss-tutorial/), but let’s briefly recap. Faiss — or Facebook AI Similarity Search — is an open-source framework built for enabling similarity search.\n\n<p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/ZLfdQq_u7Eo\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>\n\nFaiss has many super-efficient implementations of [different indexes](/learn/vector-indexes/) that we can use in similarity search. That long list of indexes includes `IndexLSH` — an easy-to-use implementation of everything we have covered so far.\n\nWe initialize our LSH index and add our Sift1M dataset `wb` like so:\n\n{{< notebook file=\"lsh-init\" height=\"full\" >}}\n\nOnce our index is ready, we can begin searching using `index.search(xq, k)` — where `xq` is our one-or-more query vectors, and `k` is the number of nearest matches we’d like to return.\n\n{{< notebook file=\"search\" height=\"full\" >}}\n\nThe `search` method returns two arrays. Index positions `I` (e.g., row numbers from `wb`) of our `k` best matches. And the distances `D` between those best matches and our query vector `xq0`.\n\n### Measuring Performance\n\nBecause we have those index positions in `I`, we can retrieve the *original vectors* from our array `wb`.\n\n{{< notebook file=\"cosine-sim\" height=\"full\" >}}\n\nAnd from those original vectors, we see if our LSH index is returning relevant results. We do this by measuring the *cosine similarity* between our query vector `xq0` and the top `k` matches.\n\nThere are vectors in this index that should return similarity scores of around 0.8. We’re returning vectors with similarity scores of just 0.2 — why do we see such poor performance?\n\n### Diagnosing Performance Issues\n\nWe know that the `nbits` value controls the number of *potential* buckets in our index. When initializing this index, we set `nbits == 4` — so all of our vectors must be stored across 4-digit, low-resolution buckets. \n\nIf we try to cram 1M vectors into just 16 hash buckets, each of those buckets very likely contain 10–100K+ vectors.\n\nSo, when we hash our search query, it matches one of these 16 buckets perfectly — but the index cannot differentiate between the huge number of vectors crammed into that single bucket — they all have the same hashed vector!\n\nWe can confirm this by checking our distances `D`:\n\n{{< notebook file=\"get-D\" height=\"full\" >}}\n\nWe return a perfect distance score of *zero* for every item — but why? Well, we know that the Hamming distance can only be *zero* for *perfect* matches — meaning all of these hashed vectors must be identical.\n\nIf all of these vectors return a perfect match, they must all have the same hash value. Therefore, our index cannot differentiate between them — they all share the same position as far as our LSH index is concerned.\n\nNow, if we were to increase `k` until we return a non-zero distance value, we should be able to infer the number of vectors that have been bucketed with this same hash code. Let’s go ahead and try it.\n\n{{< notebook file=\"D\" height=\"full\" >}}\n\nA single bucket containing `172_039` vectors. That means that we are choosing our top `k` values at random from those 172K vectors. Clearly, we need to reduce our bucket size.\n\nWith 1M samples, which `nbits` value gives us enough buckets for a more sparse distribution of vectors? It’s not possible to calculate the exact distribution, but we can take an average:\n\n{{< notebook file=\"nbits-1m-avg\" height=\"full\" >}}\n\nWith an `nbits` value of 16 we’re still getting an average of `15.25` vectors within each bucket — which seems better than it is. We must consider that some buckets will be significantly larger than others, as different regions will contain more vectors than others.\n\nRealistically, the `nbits` values of `24` and `32` may be our tipping point towards genuinely effective bucket sizes. Let’s find the `mean` cosine similarity for each of these values.\n\n{{< notebook file=\"nbits-tipping-point\" height=\"full\" >}}\n\nIt looks like our estimate is correct — the overall similarity for our top 100 vectors experiences a sudden with each nbits increment before leveling off at the `nbits == 24` point. But what if we run the process with even larger `nbits` values?\n\n![As we increase vector resolution with **nbits**, our results will become more precise — here, we can see that a larger **nbits** value results in higher cosine similarity in our results.](/images/locality-sensitive-hashing-random-projection-7.jpeg)<small>As we increase vector resolution with **nbits**, our results will become more precise — here, we can see that a larger **nbits** value results in higher cosine similarity in our results.</small>\n\nHere, the results are apparent. A fast increase in similarity as we declutter the LSH buckets — followed by a slower increase in similarity.\n\nThe latter, slower increase in similarity is thanks to the increasing resolution of our hashed vectors. Our buckets are already very sparse — we have many more *potential* buckets than we have vectors , so we can find little to no performance increase there.\n\n*But* — we are increasing the resolution and, *therefore, the precision* of those buckets, so we pull the additional performance from here.\n\n### Extracting The Binary Vectors\n\nWhile investigating our index and the distribution of vectors across our buckets above, we inferred the problem of our bucket sizes. This is useful as we’re using what we have already learned about LSH and can see the effects of the indexes' properties.\n\nHowever, we can take a more direct approach. Faiss allows us to *indirectly* view our buckets by extracting the binary vectors representations of `wb`.\n\nLet’s revert to our `nbits` value of **4** and see what is stored in our LSH index.\n\n{{< notebook file=\"extract-binary\" height=\"full\" >}}\n\nFrom this, we can visualize the distribution of vectors across those 16 buckets — showing the most used buckets and a few that are empty.\n\n![Distribution of vectors in different buckets when **nbits == 4**.](/images/locality-sensitive-hashing-random-projection-1.jpeg)<small>Distribution of vectors in different buckets when **nbits == 4**.</small>\n\nThis and our previous logic are all we need to diagnose the aforementioned bucketing issues.\n\n---\n\n## Where to Use LSH\n\nWhile LSH can be a swift index, it is less accurate than a Flat index. Using increasingly larger portions of our Sift1M dataset, the best recall score was achieved using an `nbits` value of `768` (better recall is possible at excessive search times).\n\n![Recall against the number of indexed vectors. The recall is measured as the % of matches with exhaustive search results (using **IndexFlatL2**).](/images/locality-sensitive-hashing-random-projection-15.jpeg)<small>Recall against the number of indexed vectors. The recall is measured as the % of matches with exhaustive search results (using **IndexFlatL2**).</small>\n\nAlthough it’s worth noting that using an `nbits` value of `768` only returns marginally faster results than if using a flat index.\n\n![Search time as a factor of search time for IndexFlatL2 at different index sizes and using various **nbits** values.](/images/locality-sensitive-hashing-random-projection-5.jpeg)<small>Search time as a factor of search time for IndexFlatL2 at different index sizes and using various **nbits** values.</small>\n\nMore realistic recall rates — while maintaining a reasonable speed increase — are closer to 40%.\n\nHowever, varying dataset sizes and dimensionality can make a huge difference. An increase in dimensionality means a higher `nbits` value must be used to maintain accuracy, but this can still enable faster search speeds. It is simply a case of finding the right balance for each use-case and dataset.\n\nNow, of course, there are many options out there for vector similarity search. Flat indexes and LSH are just two of many options — and choosing the right index is a mix of experimentation and know-how.\n\nAs always, similarity search is a balancing act between different indexes and parameters to find the best solutions for our use-cases.\n\n---\n\nWe’ve covered *a lot* on LSH in this article. Hopefully, this has helped clear up any confusion on one of the biggest algorithms in the world of search.\n\nLSH is a complex topic, with [many different approaches](/learn/locality-sensitive-hashing/) — and even more implementations available across several libraries.\n\nBeyond LSH, we have many more algorithms suited for efficient similarity search, such as [HNSW](/learn/hnsw/), IVF, and [PQ](/learn/product-quantization). You can learn more in our [overview of vector search indexes](/learn/vector-indexes/).\n\n{{< newsletter text=\"Subscribe for updates on similarity search!\" inputText=\"Email address...\" buttonText=\"Submit\">}}\n\n## References\n\n* [Jupyter Notebooks](https://github.com/pinecone-io/examples/tree/master/locality_sensitive_hashing_random_projection)\n* [1] [Google Searches](https://skai.io/monday-morning-metrics-daily-searches-on-google-and-other-google-facts/), Skai.io Blog\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbe5"
  },
  "filename": "inside-the-pinecone.md",
  "title": "post",
  "category": "Inside the Pinecone",
  "content": "---\nlayout: post\ntitle: Inside the Pinecone\nheadline: Inside the Pinecone\ncategories:\n  - Company News\ntoc: >-\nauthor:\n  name: Edo Liberty\n  position: Founder and CEO\n  src: /images/company-edo.png\n  href: https://edoliberty.github.io/\ndate: \"2022-08-22\"\nthumbnail: \"/images/inside-the-pinecone.jpg\"\n# Open Graph\ndescription: Edo Liberty, Pinecone CEO, provides a glimpse into the journey behind building a database company, including some insights into the latest innovations around the product and vector search.\nimages: [\"/images/inside-the-pinecone.jpg\"] \n---\n\n![Inside the Pinecone](/images/inside-the-pinecone.jpg)\n\nLast week we [announced](/learn/faster-easier-scalable/) a major [update](https://www.pinecone.io/docs/release-notes/#august-16-2022). The incredible work that led to the launch and the reaction from our users — a combination of delight and curiosity — inspired me to write this post. This is a glimpse into the journey of building a database company up to this point, some of the internal debates and decisions we made along the way (bad and good alike), and on-going innovation at the core of Pinecone that I find exciting.\n\n## In-memory graph-based index\n\nIn May 2010, I wrote Professor [Yuval Rabani](https://scholar.google.com/citations?user=vTigGywAAAAJ&hl=en) an email. Yuval is one of the inventors of approximate nearest neighbor (ANN) search as a concept. I suggested we work together on answering a seemingly simple question: \n\n_“Why are graph based data structures so good at vector search?”_\n\nYou see, they were much better in practice than the theory predicted they should be. This hinted that we were lacking some fundamental understanding of either [vector search](/learn/vector-search-basics/) or graph based indexes (or both). We decided to analyze the behavior of vector search with graphs using mathematical machinery from a different domain. Specifically, those spearheaded by [Dan Speilman](http://www.cs.yale.edu/homes/spielman/) for bounding the running time of solving systems of linear equations (for the CS theoreticians, think about max-dot product search and its similarity to the simplex method, and it will make more sense). We made some progress with this approach, but never quite got a full answer. Graph based vector search still seemed to be “too good”.\n\nFast forward twelve years, graph based techniques now take center stage for vector search software. These include [HNSW](https://www.pinecone.io/learn/hnsw/) by [Yuri Malkov](https://www.linkedin.com/in/yury-malkov-b6597382/) (an advisor to Pinecone), implementations in [Faiss](https://www.pinecone.io/learn/faiss/), and a flurry of further research and software progress. It was therefore only natural for Pinecone to consider using graph indexes for its vector database.\n\nBringing graph based search into a production-grade database, however, was not going to be simple. As a managed database, we have to make sure our systems always work and without the user tweaking and experimenting with different configurations. We could not afford to have best-effort-heuristics and magic-constants floating around. We also knew we needed to efficiently combine metadata filtering and graph traversal, optimize updates and deletes, and other functionality which HNSW or Faiss do not provide.\n\nIn the end, we did it. It took many months of work by Pinecone’s technical leaders [Marek Galovic](https://www.linkedin.com/in/marek-galovic/?originalSubdomain=cz), [Roei Mutay](https://www.linkedin.com/in/roei-mutay-bb3743200/?originalSubdomain=il), and [Ram Sriharsha](https://www.linkedin.com/in/harsha340/) along with Yuri Malkov. We now have a high-performance, stable, fully dynamic, and filterable graph index! \n\nHowever, my curiosity from twelve years ago hasn’t diminished one bit. There is still so much we don’t know. Under what conditions can we guarantee the convergence of the search method? Can we check in advance what datasets are good for graph based indexes? Can we reliably prune graphs and keep their search effectiveness? I truly hope to be able to make some progress on these questions soon.\n\n## A new storage engine for vectors\n\nBecause Pinecone is a managed vector database and not just an index, we need to take care of storage and persistence of the data in every index. We also need to constantly update and fetch thousands of raw vectors in milliseconds. Think about batch updates (upserts), raw vector-metadata fetch operations, and most importantly, raw *exact* comparisons between queries and candidate matches suggested by the ANN vector search index. We need an object storage engine. \n\nWe originally chose [RocksDB](http://rocksdb.org/) for that. RocksDB is commonly used as an embedded layer in other database offerings. And, developing a completely new storage engine seemed like an insurmountable effort; the specter of data-loss looms big in our collective psyche. \n\nWe were very happy with our decision. We loved RocksDB. It is a great piece of software. It was 100% the right choice, until it wasn’t…\n\nWhen we tried to vertically scale our pods, we started hitting RocksDB limits. Write throughput became a big issue. Simultaneous fetches of thousands (sometimes tens of thousands) of objects started becoming inefficient, especially when fetching from collections of tens of millions of objects. The fetch latency variance also grew. Higher p99 latencies started leaking into our p50s when operating horizontally distributed indexes. Write amplification started really hurting our pods’ capacities… To make matters worse, we were also spending a ton of time and energy grappling with RocksDB trying to maximize our performance.\n\nLate last year, our VP of Engineering, [Ram Sriharsha](https://www.linkedin.com/in/harsha340/), came up with a brillant [bitcask-like](https://en.wikipedia.org/wiki/Bitcask) design for a completely new vector store that is optimized for random multigets and updates, low fetch variance, minimal storage overhead, and maximal operational simplicity.\n\n[Marek](https://www.linkedin.com/in/marek-galovic/?originalSubdomain=cz) and Ram took on the herculean challenge to build the initial new vector store (internally nicknamed memkey). The resulting object store is up to 10x faster than RocksDB on large datasets. It reduces our overall operating costs 30-50%. We passed these improvements on to our customers when we [updated p1 and s1 pods to use the new vector store](https://www.pinecone.io/docs/release-notes/#improved-p1-and-s1-performance). Customers will seamlessly get more capacity and lower latency without changing a single line of code on their end. How cool is that?\n\n![Multiget latency](/images/multiget-latency.png)\n<small>Memkey’s low (and flat!) multiget latency as collections grow compared to RocksDB. On the y-axis: Multiget times in milliseconds for fetching 1000 random objects (vectors) from local SSD. On the x-axis: Collection size being 5m, 10mm, 20m, and 40mm.</small>\n\n## Rust: A hard decision pays off\n\nAlthough this change was implemented inside Pinecone a while ago, this is the first time we’re talking about it publically. \n\nIn the beginning, Pinecone was built with a mix of C/C++ and Python. The argument for this went something like this: a C/C++ core will be highly efficient, and experienced C/C++ engineers are easy to find. Anything not at the core doesn’t have to be efficient. It can therefore be easily and quickly delivered in Python, which sacrifices running speed with easy development. It was, indeed, relatively easy to develop, and we got some impressive results pretty quickly. So, that seemed to be the winning formula. We went for it full tilt. \n\nNot until a few weeks before a big release last year did the number of issues we had to fix begin to pile up. We kept fixing one issue only to discover (or create) another. Every few hours we would find some major bug or oversight in our codebase; and each cycle to fix and redeploy took hours of work. To make matters worse, we would discover issues only after deploying (or in production!) due to Python’s run time nature. Sometimes these would be minor bugs that any compiler would have easily flagged, and sometimes they were complex runtime issues which were almost impossible to reproduce or isolate.\n\nThat’s when internal murmurs about a complete rewrite started brewing… \n\nI personally vehemently resisted the idea. Rewrites are notoriously dangerous. They feel exciting at first, but often turn sour. First, they always end up being much bigger projects than you plan for. This is especially dangerous in a startup where a significant delay in shipping new features could be your undoing. Second, rewrites often backfire in spectacular ways; the new codebase often produces completely new challenges, often much worse than those you originally had.\n\nNevertheless, we reached a tipping point. We decided to move our entire codebase to Rust (and Go for the k8s control plane). Rust seemed to give us all the capabilities we needed, however, there was still one *minor* problem -  no one on the team knew Rust. It was (and still is) a hard-to-master, young language with a much smaller community than either C++ or Python.\n\nWe started with a small team of senior engineers and managers learning Rust and developing the skeleton of the DB and dev environment (for others to build on). Then, slowly, others joined in rewriting and contributing different components until we eventually got rid of the old codebase altogether (I still remember the day my original C modules, from the first days of Pinecone, were taken out). Unbeknownst to most Pinecone customers, the new Rust core was deployed in March this year. And in the process of taking over running workloads, we managed not to drop a single API call!\n\nSo, what did we learn? We all expect performance and dev processes to improve. Those indeed happened. What we didn’t expect was the extent to which dev velocity increased and operational incidents decreased. Dev velocity, which was supposed to be the claim to fame of Python, improved dramatically with Rust. Built-in testing, CI/CD, benchmarking, and an overzealous compiler increased engineers’ confidence in pushing changes, and enabled them to work on the same code sections and contribute simultaneously without breaking the code base. Most impressively though, real time operational events dropped almost to zero overnight after the original release. Sure, there are still surprises here and there but, by and large, the core engine has been shockingly stable and predictable.\n\n---\n\n*Note: Interested in hearing more about our Rust rewrite? Don't miss [our talk at the NYC Rust meetup on August 31](https://www.meetup.com/rust-nyc/events/287821884/), and subscribe for updates at the bottom of this page to get notified when we post the full writeup.*\n\n---\n\n\n## Closing thoughts\n\nWhile I could keep rambling on about more exciting developments within the space like vertical scaling, data collections, more advanced monitoring, and others, those will have to wait until the next post.\n\nIf building the best vector database in the world sounds exciting, give me a ping. [We are hiring](https://www.pinecone.io/careers/) on all fronts. Also, if you are building on Pinecone and missing a feature or experiencing something other than expected, please don’t be shy and write to me directly (edo@pinecone.io).\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbe6"
  },
  "filename": "faster-easier-scalable.md",
  "title": "post",
  "category": "\"Vector search just got up to 10x faster, easier to set up, and vertically scalable\"",
  "content": "---\nlayout: post\ntitle: \"Vector search just got up to 10x faster, easier to set up, and vertically scalable\"\nheadline: \"Vector search just got up to 10x faster, easier to set up, and vertically scalable\"\ncategories:\n  - Company News\ntoc: >-\nauthor:\n  name: Gibbs Cullen\n  position: Senior Product Marketing Manager\n  src: /images/gibbs-cullen.jpg\n  href: https://www.linkedin.com/in/gibbscullen/\ndate: \"2022-08-16\"\n# Date: August 8, 2022\n# Open graph\ndescription: Announcing new features and performance improvements to make it easier and more cost-effective than ever for engineers to start and scale a vector database in production.\nimages: [\"/images/10x-faster-launch-resized.png\"]\nthumbnail: \"/images/10x-faster-launch-thumb.png\"\n---\n\n**Pinecone is paving the way for developers to easily start and scale with vector search.**\n\nWe created the first [vector database](/learn/vector-database/) to make it easy for engineers to build fast and scalable vector search into their cloud applications. In the past year, hundreds of companies like Gong, Clubhouse, and Expel added capabilities like semantic search, AI recommendations, image search, and AI threat detection to their applications using vector search with Pinecone.\n\nYet for teams who are new to [vector search](/learn/vector-search-basics/), some challenges remained:\n* It was hard to determine the type and size of index needed for their data and performance needs.\n* Supporting high throughput required a lot of hardware, which might’ve been cost-prohibitive.\n* Scaling up indexes meant re-uploading data to a new index and interrupting service for the switch-over.\n\nNot anymore. Today we’re excited to announce new features and performance improvements to Pinecone that make it easier and more cost-effective than ever for engineers to start and scale a vector database in production.\n\n## What’s new \n\nAs of today, these new features are available in Pinecone for all Standard, Enterprise, and Enterprise Dedicated users: \n\n* **Vertical Scaling**: Scale your vector database with zero downtime\n* **Collections**: Centrally store and reuse your vector embeddings and metadata to experiment with different index types and sizes.\n* **p2 pods**: Achieve up to 10x better performance for high-traffic applications.\n\nWe are also announcing:\n* Around **50% faster queries** on p1 and s1 pods (varies by use case).\n* **5x greater capacity** on the Starter (free) plan, with the storage-optimized s1 pod now available on the plan.\n* **Updated pricing** that will go into effect for new users starting September 1st, but not for existing customers.\n\nContinue reading for more details, then [get started today](https://app.pinecone.io). Also [register for our upcoming demo](https://pinecone-io.zoom.us/webinar/register/WN_mlCKGFF_QoOKWW1UNKZtjA) and [read the hands-on walkthrough](/learn/testing-p2-collections-scaling/) of these new features.\n\n## Vertical Scaling: Scale index capacity with zero downtime\n\n> *“Vertical scaling means no more migrating to bigger indexes or writing to an index already at storage capacity. This is going to be a huge timesaver for us.”*\n> *&mdash; Isabella Fulford, Software Engineer at Mem Labs*\n\nThe volume of data that a Pinecone index can hold is limited by the number of pods the index is running on. Previously, if your index grew beyond the available capacity you would need to create a new index with more pods, re-upload data to that index, then switch over traffic to the new index, or overprovision the number of pods and pay for unused capacity.\n\nEither way, valuable resources — spend and engineering time — are taken away from more impactful areas of your business. \n\nWith vertical scaling, pod capacities can be doubled for a live index with zero downtime. Pods are now available in different sizes —  x1, x2, x4, and x8 — so you can start with the exact capacity you need and easily scale your index. Your hourly cost for pods will change to match the new sizes, meaning you still only pay for what you use. See [documentation](https://www.pinecone.io/docs/manage-indexes/#changing-pod-sizes) for more detail.\n\n## Collections: Experiment with and store vector data in one place\n\n![Collections in Pinecone](/images/collections.png)\n\nUsers rely on Pinecone indexes to store their vector embeddings and associated metadata; they want it to be their source of truth. Before the addition of collections, actions such as temporarily shutting down or creating a new index would require re-uploading the original vector data from a different source. That meant users had to maintain an up-to-date copy of the data outside of Pinecone. Collections will alleviate this pain by providing users with a single source of truth for their vector data within Pinecone.\n\nToday, we are launching the public preview of collections in all Pinecone environments. Users can save data (i.e. vectors and metadata) from an index as a snapshot, and create new indexes from any collection. Whether using collections for backing up and restoring indexes, testing different index types with the same data, or moving data to a new index, users can now do it all within Pinecone. In the near future, collections will allow users to import and export data to and from S3 or GCS blob storage, and write streaming and bulk data directly to collections.\n\nStorage costs for collections will be $0.025/GB per month for all Standard, Enterprise, and Enterprise Dedicated users. Users on the Starter (free) plan can have one collection at no cost. See [documentation](https://www.pinecone.io/docs/collections/) for more detail. \n\n## p2 Pods: Purpose built for performance and high-throughput use cases \n\nWhile p1 pods provide low search latencies with uniquely fast ingestion speeds, high recall (accuracy of results), and fast filtering, users with high-throughput applications such as social media apps or streaming services require much higher throughput above all else.\n\nThe new p2 pod type provides blazing fast search speeds under 10ms and throughput up to 200 queries per second (QPS) per replica.* That’s up to 10x lower latencies and higher throughput than the p1 pod type. It achieves this with a new graph-based index that trades off ingestion speed, filter performance, and recall in exchange for higher throughput. It still supports filtering, live index updates, and all other index operations. \n\nToday, we are launching p2 pods as a public preview available in all Pinecone environments. If you currently use p1 pods with multiple replicas to achieve a high throughput, switching to p2 pods may dramatically lower your total costs. See [documentation](https://www.pinecone.io/docs/indexes/#p2-pods) for more detail.\n\n<em>\\* Your performance may vary and we encourage you to test with your own data and follow our [tips for performance tuning](https://www.pinecone.io/docs/performance-tuning/). Latencies are dependent on vector dimensionality, metadata size, metadata cardinality, network connection, cloud provider, and other factors.</em>\n\n## Other updates\n\n### Faster indexes on s1 and p1 pods\nQuery speed and performance are only becoming more and more critical for vector search applications, especially for those consumer facing features. That’s why we significantly improved performance for s1 and p1 pods. Users of these pods will now achieve on average 50% lower latency and 50% higher throughput per replica.\n\nThe specific performance gain depends on your workload, so you might see a higher or lower difference than this. This change is in effect for all new indexes starting today, and will be rolled out to existing indexes in the coming weeks. \n\n### Starter plan to include s1 pods\nGetting started with Pinecone is even easier with the addition of s1 pods to our Starter (free) plan. Previously, only p1 pods were available on the Starter plan.\n\nAs of today, users can choose between p1 and s1 pods and store 5x more vectors than before. This enhancement gives users the flexibility to more easily experiment with and fully realize the power of vector search with Pinecone.  \n\n### Pricing update\nThe features and enhancements announced today provide meaningful cost saving opportunities for all existing and new Pinecone customers, notably: \n\n* s1 and p1 pods now support 50% higher throughput for typical workloads, meaning fewer replicas are needed.\n* The new p2 pods, while being more expensive on a per pod-hour basis, provide up to 10x greater throughput, meaning even fewer replicas are needed.\n* Vertical scaling eliminates the need to overprovision at the start.\n* Collections allow users to delete (and not be billed for) indexes when they’re not being used without losing data.\n* Starter users can now store 5x more vectors for free with s1 pods.\n\nUpdated pricing for p1 and s1 pods will also go into effect for all new users as of September 1, 2022, starting at $0.096/hour and up depending on plan, pod size, and cloud environment.\n\nExisting users on a paid plan with a running index as of August 31, 2022 will not be affected, and will retain their current rates for s1 and p1.\n\nThis means anyone not yet on a paid plan can lock in current rates by upgrading and launching at least a one-pod index by August 31, 2022. See the [pricing page](/pricing/) for more details or [contact us](/contact/) with questions. \n\n## Get Started\n\nToday’s releases make it even easier for engineers to start, test, and scale vector databases with greater flexibility, lower cost, and better performance. Get started today by [launching your first vector database](https://app.pinecone.io), [contacting us](/contact/) for more information, or [registering for the upcoming demo](https://pinecone-io.zoom.us/webinar/register/WN_mlCKGFF_QoOKWW1UNKZtjA) of these new features.\n\n\n\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbe7"
  },
  "filename": "plagiarism-detection.md",
  "title": "post",
  "category": "\"Plagiarism Detection Using Transformers\"",
  "content": "---\nlayout: post\ntitle: \"Plagiarism Detection Using Transformers\"\nheadline: \"Plagiarism Detection Using Transformers\"\ncategories:\n  - Applications of Vector Search\ntoc: >-\nweight: 5\nauthor:\n  name: Zoumana Keita\n  position: Data Scientist\n  src: /images/zoumana-keita.jpg\n  href: \"https://www.linkedin.com/in/zoumana-keita/\"\ndescription: A complete guide to building a more robust plagiarism detector using transformer models.\n# Open graph\nimages: [\"/images/plagiarism-detection-0.jpg\"]\n---\n\nPlagiarism is one of the biggest issues in many industries, especially in academia. This phenomenon worsened with the rise of the internet and open information, where anyone can access any information at a click about a specific topic. \n\nBased on this observation, researchers have been trying to tackle the issue using different text analysis approaches. In this article, we will tackle two main limitations of plagiarism detection tools: (1) _content paraphrasing plagiarism_ and (2) _content translation plagiarism_. \n\n**_(1) Rephrased contents can be difficult to capture by traditional tools_** because they do not take into consideration synonyms and antonyms of the overall context.\n\n**_(2) Contents written in a language different from the original one_** are also a big issue faced by even the most advanced machine learning-based tools since the context is being completely shifted to another language.\n\nIn this conceptual blog, we will explain how to use transformer-based models to tackle these two challenges in an innovative way. First of all, we will walk you through the analytical approach describing the entire workflow, from data collection to performance analysis. Then, we will dive into the scientific/technical implementation of the solution before showing the final results.\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/_PqHRH55hV0\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n## Problem statement\n\nImagine you are interested in building a scholarly content management platform. You might want to only accept articles not shared on your platform. In this case, your goal will be to reject all new articles that are similar to existing ones at a certain threshold. \n\nTo illustrate this scenario, we will use the [cord-19 dataset](https://www.kaggle.com/datasets/allen-institute-for-ai/CORD-19-research-challenge), which is an open research challenge data made freely available on Kaggle by the [Allen Institute for AI](https://allenai.org/).\n\n## Analytical Approach\n\nBefore going further with the analysis, let’s clarify what we are trying to achieve here from the following question: \n\n_Problem: Can we find within our dataset one or more documents that are similar (at a certain threshold) to a new submitted document?_\n\nThe following workflow highlights all the main steps required to better answer this question.\n\n![Plagiarism detection workflow](/images/plagiarism-detection-1.png)\n\n_Let’s understand what is happening here 💡._\n\nAfter collecting the source data, we start by preprocessing the content, then create a vector index from BERT. \n\nThen, whenever we have a new incoming document, we check the language and perform plagiarism detection. More details are given later in the article.\n\n## Scientific Implementation \n\nThis section is focused on the technical implementation of each component in the approach.\n\n### Data preprocessing\n\nWe are only interested in the **_abstract_** column of the source data, and also, for simplicity’s sake, we will use only 100 observations to speed up the preprocessing.\n\n\n```python\nimport pandas as pd\n \ndef preprocess_data(data_path, sample_size):\n \n  # Read the data from specific path\n  data = pd.read_csv(data_path, low_memory=False)\n\n  # Drop articles without Abstract\n  data = data.dropna(subset = ['abstract']).reset_index(drop = True)\n\n  # Get \"sample_size\" random articles\n  data = data.sample(sample_size)[['abstract']]\n \n return data\n \n# Read data & preprocess it\ndata_path = \"./data/cord19_source_data.csv\"\n```\n\nBelow are the five random observations from the source data set.\n\n![Five random observations](/images/plagiarism-detection-2.png)\n\n### Document vectorizer \n\n![Document vectorizer](/images/plagiarism-detection-3.png)\n\nThe challenges observed in the introduction lead to respectively choosing the following two transformer-based models:\n\n**_(1) A BERT model_**: to solve the first limitation because it provides a better contextual representation of textual information. To do so, we will have:\n\n- `create_vector_from_text`: used to generate the vector representation of a single document.\n- `create_vector_index`: responsible for creating an index containing for each document the corresponding vector.\n\n```python\n# Useful libraries\nimport numpy as np\nimport torch\nfrom keras.preprocessing.sequence import pad_sequences\nfrom transformers import BertTokenizer,  AutoModelForSequenceClassification\n \n# Load bert model\nmodel_path = \"bert-base-uncased\"\ntokenizer = BertTokenizer.from_pretrained(model_path,\n                                         do_lower_case=True)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path,\n                                                         output_attentions=False,\n                                                         output_hidden_states=True)\n                                                        \ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n \ndef create_vector_from_text(tokenizer, model, text, MAX_LEN = 510):\n  \n    input_ids = tokenizer.encode(\n                        text,\n                        add_special_tokens = True,\n                        max_length = MAX_LEN,                          \n    )   \n    results = pad_sequences([input_ids], maxlen=MAX_LEN, dtype=\"long\",\n                              truncating=\"post\", padding=\"post\")\n    # Remove the outer list.\n    input_ids = results[0]\n    # Create attention masks   \n    attention_mask = [int(i>0) for i in input_ids]\n    # Convert to tensors.\n    input_ids = torch.tensor(input_ids)\n    attention_mask = torch.tensor(attention_mask)\n    # Add an extra dimension for the \"batch\" (even though there is only one\n    # input in this batch.)\n    input_ids = input_ids.unsqueeze(0)\n    attention_mask = attention_mask.unsqueeze(0)\n    # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n    model.eval()\n    # Run the text through BERT, and collect all of the hidden states produced\n    # from all 12 layers.\n    with torch.no_grad():       \n        logits, encoded_layers = model(\n                                    input_ids = input_ids,\n                                    token_type_ids = None,\n                                    attention_mask = attention_mask,\n                                    return_dict=False)\n\n    layer_i = 12 # The last BERT layer before the classifier.\n    batch_i = 0 # Only one input in the batch.\n    token_i = 0 # The first token, corresponding to [CLS]\n      \n    # Extract the vector.\n    vector = encoded_layers[layer_i][batch_i][token_i]\n    # Move to the CPU and convert to numpy ndarray.\n    vector = vector.detach().cpu().numpy()\n    return(vector)\n \ndef create_vector_index(data):\n  \n   # The list of all the vectors\n   vectors = []\n  \n   # Get overall text data\n   source_data = data.abstract.values\n  \n   # Loop over all the comment and get the embeddings\n   for text in tqdm(source_data):\n      \n       # Get the embedding\n       vector = create_vector_from_text(tokenizer, model, text)\n      \n       #add it to the list\n       vectors.append(vector)\n  \n   data[\"vectors\"] = vectors\n   data[\"vectors\"] = data[\"vectors\"].apply(lambda emb: np.array(emb))\n   data[\"vectors\"] = data[\"vectors\"].apply(lambda emb: emb.reshape(1, -1))\n   return data\n# Create the vector index\nvector_index = create_vector_index(source_data)\nvector_index.sample(5)\n```\n\nThe last line of the code shows the following five random observations from the vector index, with the new vectors column.\n\n![New column](/images/plagiarism-detection-4.png)\n\n**_2) A Machine Translation_** transformer model is used to translate the language of the incoming document into English because the source documents are in English in our case. The translation is performed only if the document’s language is one of the following five: _German, French, Japanese, Greek, and Russian_. Below is the helper function to implement this logic using the [MarianMT model](https://huggingface.co/docs/transformers/model_doc/marian).\n\n```python\nfrom langdetect import detect, DetectorFactory\nDetectorFactory.seed = 0\ndef translate_text(text, text_lang, target_lang='en'):\n  # Get the name of the model\n  model_name = f\"Helsinki-NLP/opus-mt-{text_lang}-{target_lang}\"\n  # Get the tokenizer\n  tokenizer = MarianTokenizer.from_pretrained(model_name)\n\n # Instantiate the model\n  model = MarianMTModel.from_pretrained(model_name)\n \n  # Translation of the text\n  formated_text = \">>{}<< {}\".format(text_lang, text)\n  translation = model.generate(**tokenizer([formated_text], \n                               return_tensors=\"pt\", padding=True))\n  translated_text = [tokenizer.decode(t, skip_special_tokens=True) for t in       translation][0]\n \n  return translated_text\n```\n\n## Plagiarism analyzer\n\nThere is plagiarism when the incoming document’s vector is similar to one of the index vectors at a certain threshold level.\n\n![Plagiarism analyzer](/images/plagiarism-detection-5.png)\n\n**_But, when are two vectors similar?_**\n\n**_→ When they have the same magnitude and direction._**\n\nThis definition requires our vectors to have the same magnitude, which can be an issue because the dimension of a document vector depends on the length of that document. Luckily, we have multiple similarity measure approaches that can be used to overcome this issue, and one of them is the cosine similarity, which will be used in our case. \n\n(If you are interested in other approaches, you can refer to this [semantic search overview](https://www.pinecone.io/learn/semantic-search/). It explains how each approach works and its benefits, with guides through their implementation.)\n\nThe plagiarism analysis is performed using the **_run_plagiarism_analysis_** function. We start by checking the document language using the check_incoming_document function to perform the right translation when required.\n\nThe final result is a dictionary with four main values: \n\n- `similarity_score`: the score between the incoming article and the most similar existing article in the index. \n- `is_plagiarism`: the value is true whether the similarity score is equal to or beyond the threshold. It is false otherwise.\n- `most_similar_article`: the textual information of the most similar article. \n- `article_submitted`: the article that was submitted for approval.\n\n```python\ndef process_document(text):\n  \"\"\"\n  Create a vector for given text and adjust it for cosine similarity search\n  \"\"\"\n  text_vect = create_vector_from_text(tokenizer, model, text)\n  text_vect = np.array(text_vect)\n  text_vect = text_vect.reshape(1, -1)\n  return text_vect\n\ndef is_plagiarism(similarity_score, plagiarism_threshold):\n  is_plagiarism = False\n  if(similarity_score >= plagiarism_threshold):\n      is_plagiarism = True\n  return is_plagiarism\n\ndef check_incoming_document(incoming_document):\n  text_lang = detect(incoming_document)\n  language_list = ['de', 'fr', 'el', 'ja', 'ru']\n  final_result = \"\"\n  if(text_lang == 'en'):\n    final_result = incoming_document\n  elif(text_lang not in language_list):\n    final_result = None\n  else:\n    # Translate in English\n    final_result = translate_text(incoming_document, text_lang)\n  return final_result\n \ndef run_plagiarism_analysis(query_text, data, plagiarism_threshold=0.8):\n  top_N=3\n  # Check the language of the query/incoming text and translate if required.\n  document_translation = check_incoming_document(query_text)\n  if(document_translation is None):\n    print(\"Only the following languages are supported: English, French, Russian, German, Greek and Japanese\")\n    exit(-1)\n  else:\n    # Preprocess the document to get the required vector for similarity analysis\n    query_vect = process_document(document_translation)\n\n    # Run similarity Search\n    data[\"similarity\"] = data[\"vectors\"].apply(lambda x:\n                                            cosine_similarity(query_vect, x))\n    data[\"similarity\"] = data[\"similarity\"].apply(lambda x: x[0][0])\n    similar_articles = data.sort_values(by='similarity',\n                                        ascending=False)[1:top_N+1]\n    formated_result = similar_articles[[\"abstract\", \"paper_id\",\n                                        \"similarity\"]].reset_index(drop = True)\n    similarity_score = formated_result.iloc[0][\"similarity\"]\n    most_similar_article = formated_result.iloc[0][\"abstract\"]\n    is_plagiarism_bool = is_plagiarism(similarity_score, plagiarism_threshold)\n    plagiarism_decision = {'similarity_score': similarity_score,\n                          'is_plagiarism': is_plagiarism_bool,\n                          'most_similar_article': most_similar_article,\n                          'article_submitted': query_text\n                          }\n    return plagiarism_decision\n```\n\n## Experimentation of the system\n\nWe have covered and implemented all the components of the workflow. Now, it is time to test our system using three of the languages accepted by our system: _German, French, Japanese, Greek, and Russian_.\n\n### Candidate articles and their submission evaluation\n\nThese are the abstract text of the articles we want to check whether the authors plagiarized or not. \n\n#### English article \n\nThis article is actually an example from the source data.\n\n```python\nenglish_article_to_check = \"The need for multidisciplinary research to address today's complex health and environmental challenges has never been greater. The One Health (OH) approach to research ensures that human, animal, and environmental health questions are evaluated in an integrated and holistic manner to provide a more comprehensive understanding of the problem and potential solutions than would be possible with siloed approaches. However, the OH approach is complex, and there is limited guidance available for investigators regarding the practical design and implementation of OH research. In this paper we provide a framework to guide researchers through conceptualizing and planning an OH study. We discuss key steps in designing an OH study, including conceptualization of hypotheses and study aims, identification of collaborators for a multi-disciplinary research team, study design options, data sources and collection methods, and analytical methods. We illustrate these concepts through the presentation of a case study of health impacts associated with land application of biosolids. Finally, we discuss opportunities for applying an OH approach to identify solutions to current global health issues, and the need for cross-disciplinary funding sources to foster an OH approach to research.\"\n\n# Select an existing article from the data\nnew_incoming_text = source_data.iloc[0]['abstract']\n \n# Run the plagiarism detection\nanalysis_result = run_plagiarism_analysis(new_incoming_text, \n                                         vector_index, plagiarism_threshold=0.8)\n```\n\n![English article similarity score](/images/plagiarism-detection-6.png)\n\nAfter running the system we get a similarity score of 1, which is a 100% match with an existing article. This is obvious because we took exactly the same article from the vector index.\n\n#### French article\n\nThis article is freely available from the [French agriculture website](https://agriculture.gouv.fr/quel-avenir-pour-les-reseaux-dinnovation-et-de-transfert-agricoles-et-les-systemes-recherche).\n\n```python\nfrench_article_to_check = \"\"\"Les Réseaux d'Innovation et de Transfert Agricole (RITA) ont été créés en 2011 pour mieux connecter la recherche et le développement agricole, intra et inter-DOM, avec un objectif d'accompagnement de la diversification des productions locales. Le CGAAER a été chargé d'analyser ce dispositif et de proposer des pistes d'action pour améliorer la chaine Recherche – Formation – Innovation – Développement – Transfert dans les outre-mer dans un contexte d'agriculture durable, au profit de l'accroissement de l'autonomie alimentaire.\"\"\"\n\nanalysis_result = run_plagiarism_analysis(french_article_to_check, \n                                         vector_index, plagiarism_threshold=0.8)\nanalysis_result\n```\n\n![French article similarity score](/images/plagiarism-detection-7.png)\n\nThere is no plagiarism in this situation because the similarity score is less than the threshold. \n\n#### German article\n\nLet’s imagine that some really liked the fifth article in the data, and decided to translate it into German. Now let’s see how the system will judge that article.\n\n```python\ngerman_article_to_check = \"\"\"Derzeit ist eine Reihe strukturell und funktionell unterschiedlicher temperaturempfindlicher Elemente wie RNA-Thermometer bekannt, die eine Vielzahl biologischer Prozesse in Bakterien, einschließlich der Virulenz, steuern. Auf der Grundlage einer Computer- und thermodynamischen Analyse der vollständig sequenzierten Genome von 25 Salmonella enterica-Isolaten wurden ein Algorithmus und Kriterien für die Suche nach potenziellen RNA-Thermometern entwickelt. Er wird es ermöglichen, die Suche nach potentiellen Riboschaltern im Genom anderer gesellschaftlich wichtiger Krankheitserreger durchzuführen. Für S. enterica wurden neben dem bekannten 4U-RNA-Thermometer vier Hairpin-Loop-Strukturen identifiziert, die wahrscheinlich als weitere RNA-Thermometer fungieren. Sie erfüllen die notwendigen und hinreichenden Bedingungen für die Bildung von RNA-Thermometern und sind hochkonservative nichtkanonische Strukturen, da diese hochkonservativen Strukturen im Genom aller 25 Isolate von S. enterica gefunden wurden. Die Hairpins, die eine kreuzförmige Struktur in der supergewickelten pUC8-DNA bilden, wurden mit Hilfe der Rasterkraftmikroskopie sichtbar gemacht.\"\"\"\n\nanalysis_result = run_plagiarism_analysis(german_article_to_check, \n                                         vector_index, plagiarism_threshold=0.8)\nanalysis_result\n\n```\n\n![German article similarity score](/images/plagiarism-detection-8.png)\n\n97% of similarity — this is what the model captured! The result is quite impressive. This article is definitely a plagiat.\n\n## Conclusion\n\nCongratulations, now you have all the tools to build a more robust plagiarism detection system, using BERT and Machine Translation models combined with Cosine Similarity.\n\nYou can also find more resources related to vector search and vector databases from our [learning center](/learn/). \n\n## Additional Resources\n\n[MarianMT model from HuggingFace](https://huggingface.co/docs/transformers/model_doc/marian)\n\n[Document classification with BERT](https://www.chrismccormick.ai/products/bert-document-classification-tutorial-with-code/categories/2108744/posts/7038793)\n\n[Source code of the article](https://colab.research.google.com/drive/17FQHRb45-ZlKDqGZ7O3bti-klCVyGEjb?usp=sharing)\n\n[Pinecone learning center](/learn/)\n\n[Allen Institute for AI](https://allenai.org/)\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbe8"
  },
  "filename": "similarity-search-benchmarks.md",
  "title": "post",
  "category": "\"Benchmarks of Similarity Search Algorithms",
  "content": "---\nlayout: post\ntitle: \"Benchmarks of Similarity Search Algorithms: Pinecone Vector Index vs Faiss vs NMSLIB\"\nheadline: \"Benchmarks of Similarity Search Algorithms: Pinecone Vector Index vs Faiss vs NMSLIB\"\ncategories:\n  - Algorithms & Libraries\ntoc: >-\nprettyTables: true\nweight: 1\ndraft: true\n---\n\n## Executive Summary\n\n[Pinecone](/) is a managed vector search solution. At its core is the Pinecone Vector Index, a proprietary set of algorithms and data structures that enable fast and accurate similarity search of vectorized objects.\n\nThis benchmark report compares the Pinecone Vector Index with two other state-of-the-art nearest-neighbor search libraries, [NMSLIB (HNSW)](https://github.com/nmslib/nmslib) and [Faiss-IVF](https://github.com/facebookresearch/faiss), across a range of quality and performance metrics using their production-grade library implementations.\n\nOur benchmarking is concerned with two aspects: accuracy and performance. We ran the benchmarks using a standard open-source benchmarking library and datasets.\n\n### Result highlights:\n\n- **Accuracy**: The Pinecone Vector Index achieves top accuracy results across datasets, and its theoretical guarantees assure this will be true with new, unseen data. Faiss is also reasonably accurate, while the accuracy of HNSW is insufficient for many machine learning applications.\n- **Performance (indexing time)**: The Pinecone Vector Index has the fastest indexing time while still having a low memory footprint. Faiss exhibits reasonably good indexing times. NMSLIB (HNSW) cannot build real-time indexes.\n- **Performance (throughput)**: NMSLIB (HNSW) has the highest throughput results, although it is less accurate than Faiss and the Pinecone Vector Index. (In practice, the Pinecone Vector Index can meet any throughput requirement thanks to the sharding and auto-scaling features of the managed Pinecone product.)\n\n## Benchmark Report\n\nThis is a comparison of a single-node deployment of the proprietary Pinecone Vector Index with other commonly used nearest-neighbor-search solutions.\n\nThe comparison is concerned primarily with accuracy and performance. For measuring accuracy we utilize a set of standard ranking metrics along with a new, more refined, metric. Performance metrics measure the total time for constructing an index, average throughput, and average query latency.\n\nA detailed description of the considered metrics, along with the full detailed results appear in Appendices A1 and A4.\n\n### Benchmarking Outline\n\nThe benchmarking is based on the standard open-source [ANNS-benchmarking library](https://github.com/erikbern/ann-benchmarks) running over a single machine (AWS EC2 machine, c4.5xlarge, with 16 vCPUs and 32 GB RAM), limited to use two CPUs. We focused on the cosine-similarity metric, and used datasets of various sizes ranging from 60k items to 10M items, and dimensions from 25 to 960. These datasets are part of the library benchmark data, and the corresponding results of various off-the-shelf open source solutions are published on the ANNS-benchmarking library website. Using these results, we chose the top performing solutions in terms of accuracy: _Faiss (IVF-Flat)_ and _NMSLIB (HNSW)_.\n\nWe compared the Pinecone Vector Index against the alternatives over the different datasets with a focus on accuracy and performance. For accuracy we used a set of standard ranking metrics including recall, precision, F-score, MRR, DCG, and a new metric called approximation-loss which measures how much the ranking metric scores of the approximated solution deviates from the optimal (exact search) scores. Performance is measured by the total time for constructing an index, average throughput, and average latency. The appendix of this report contains detailed descriptions of the metrics, as well as the full results over all datasets and numbers of desired results (i.e., k values defining k-top values).\n\nIn designing approximate nearest neighbor algorithms, one can always trade off accuracy and speed. As part of an enterprise solution, the Pinecone Vector Index is held to very high standards with respect to accuracy. This is not the case for other algorithms we tested. HNSW (_NMSLIB_) exhibits the highest throughput results on a single machine, whereas Faiss and the Pinecone Vector Index lag behind. Note, however, that we report the throughput and latency (Figure 1) over just a single machine. In practice, the Pinecone Vector Index can meet any throughput requirement thanks to sharding, replication, namespacing, and auto-scaling across many machines that comes with the Pinecone fully managed service.\n\n### Accuracy\n\nIn terms of accuracy, all algorithms can be configured to achieve near optimal accuracy (which is the exact search accuracy). For example, Faiss-IVF can be configured in such a way that it is essentially performing exact search (see second paragraph in the [Faiss FAQ](https://github.com/facebookresearch/faiss/wiki/FAQ#what-does-it-mean-when-a-search-returns--1-ids)). Alas, this hurts performance, so these algorithms require workload-specific tuning. The Pinecone Vector Index, in contrast, automatically tunes its configuration to achieve high accuracy while keeping good performance.\n\nFigure 1 below depicts a few typical results for the considered accuracy metrics. Here we depict the F1-score that balances between recall and precision. The optimal value for the figures is 1.0. All considered solution configurations achieve high accuracy results. The Pinecone Vector Index, depicted as a red cross marker, is a top performer, while Faiss-IVF is a runner-up, and HNSW lags behind. For some large-scale datasets, HNSW failed to run due to high memory consumption. This is typical for the majority of the datasets.\n\n![Benchmark figure 1](/images/benchmark-figure1.png)\n\n<small>\n\n_**Figure 1.** Depicting typical accuracy results over various datasets, and metrics tested. Here we present the F1 score metric results. Each marker corresponds to a single run of a specific library configuration. Note that y-axis values are log-scaled. Observe that the Pinecone Vector Index has a single marker because it  auto-tunes its configuration, while alternative libraries manually tune their configurations, thus, have a marker per test configuration. The optimal F-1 score metric value is 1.0. The Pinecone Vector Index is designed to be provably accurate and gains an accuracy advantage across different datasets. In contrast, both HNSW and Faiss get a performance boost by sacrificing accuracy._\n\n</small>\n\n### Index Build Time\n\nNear real-time retrieval applications require fast index creation and update times. Index creation time is a good indication for time overhead for ‘write’ operations: insert, update, delete. Figure 2 depicts the index creation time in seconds versus the index size. Here for each solution we chose the most accurate configuration. The x-axis is on a logarithmic scale. The Pinecone Vector Index has a clear advantage over alternatives, with Faiss coming in second, and NMSLIB (i.e., HNSW) being far behind.\n\nIndexing smaller than million-item datasets takes the Pinecone Vector Index a fraction of a second, a million items take a few seconds, and 10 million items take roughly one minute. Faiss gives good indexing time and can be treated as a real-time engine. NMSLIB lags behind, and even fails to run on large datasets due to lack of memory (given the AWS machine configuration we used).\n\n![Benchmark figure 2](/images/benchmark-figure2.png)\n\n<small>\n\n_**Figure 2.** Index creation time v.s. number of items times dimension. Note that the x and y axes scale is logarithmic. Each marker corresponds to a dataset/algorithm-configuration pair. Observe that the Pinecone Vector Index is a clear winner, Faiss is the runner-up, and HNSW (Nmslib) is far behind. (Recall that Nmslib failed to run on some datasets.)_\n\n</small>\n\n### Pinecone Vector Index Highlights\n\nThe Vector Index is our proprietary nearest-neighbor search indexing solution at the core of the Pinecone product. Its advantages seen in this benchmark report are as follows:\n\n- Able to run on all datasets and never produced empty results, unlike NMSLIB (HNSW).\n- Achieves the best accuracy results.\n- Automatically tunes its configuration (i.e., hyper parameters). Other solutions require manual tuning to balance accuracy and performance. Each dataset requires different configuration, and non-optimal choices degrade accuracy or speed.\n- The fastest indexing time. Faiss exhibits reasonably good indexing times. NMSLIB (HNSW) cannot build real-time indexes.\n- HNSW has the highest throughput results at the expense of accuracy. Among the high-accuracy algorithms, the Pinecone Vector Index and Faiss, the former is faster overall. Note the benchmarking focused on a single-node comparison only. In practice, Pinecone is a fully managed service that combines sharding and auto-scaling to meet any throughput requirement.\n\n---\n\n## Appendix: Metrics and Results\n\n### A1. Accuracy Metrics\n\nListing the set of accuracy metrics that have been utilized in the benchmarking:\n\n- Recall@k / precision@k: taking the k-rank score (+epsilon) as a threshold defining relevant/non-relevant items. Precision@k is the ratio of top-k items with score above the threshold value. Recall@k is the fraction of top-k items with score above the threshold from all relevant items in the database. We defined the threshold to be the average score differences between consecutive top-k items.\n- F-score: $F_β= (1+β^2) * \\frac{precision * recall}{(β^2*precision) + recall}$; the parameter $β$ indicates how much the recall is more important than precision. We can take $β$=1, 0.5, 2. We should take the corresponding “@k” measures.\n- Mean reciprocal rank: $\\frac{1}{|Q|} \\sum_{i=1}^{|Q|}= \\frac{1}{rank_i}$ wherein $rank_i$ is the first rank that is relevant (if none then we should use a high penalty value).\n- DCG: $\\sum_{i=1}^k \\frac{2^{rel_i}-1}{log_2(i+1)}$ wherein the relevancy score is its divergence from the exact-rank-i’th score.\n- Average Approximation Loss @ 3 / 10: For every query we retrieve the sorted top-k approximated items and their corresponding scores. We then compute the average difference between each of these scores and their corresponding exact search positional score. The resulting value is the Average-Approximation-Loss@k.\n- Distribution information of average-approximation-loss: mean / p50 / p75 / p90 / p99 of the average-approximation-loss over a test set of queries.\n\n### A2. Performance Metrics\n\n- Indexing time (build time): How fast does a system get fully operational.\n- Throughput: Queries per second (QPS).\n- Query latency: Distributional information, mean / p50 / p95 / p99.\n\n### A3. Missing Results\n\nThe benchmark results do not contain all metrics for all libraries and parameter configurations. Missing results indicate failure. There are two sources of failures:\n\n- **Out of memory**: NMSLIB HNSW uses graph-based indexes with large memory footprints. In fact, their memory consumption is super linear in the size of the data. As a result, larger datasets could not be indexed into a single machine’s memory. In those cases, the rows corresponding to NMSLIB HNSW are missing.\n- **Empty result set**: For some reason, libraries sometimes return empty result sets. This is clearly unintended behavior. Accuracy for empty result sets are meaningless. They are left blank in the tables below.\n\n### A4. Detailed Results\n\nBest results are appear in **bold**. Scroll sideways to see more columns.\n\n#### NYTimes-256-angular k=3\n\n| Algorithm                                                                              | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-Latency | QPS          |\n|----------------------------------------------------------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|\n| Pinecone Vector Index                                                                                 | **2.18E+00** | **1.99E-08** | **3.10E-10** | **7.40E-17** | **0.00E+00** | **1.00E+00** | **2.32E-08** | **7.28E-01** | **8.19E-01** | **6.54E-01** | 6.74E-03     | 1.48E+02     |\n| FaissIVF (n\\_list=256, n\\_probe=50)                                                    | 6.77E+00     | 9.20E-02     | 1.16E-02     | 9.93E-09     | **0.00E+00** | 9.68E-01     | 6.82E-03     | 6.57E-01     | 7.40E-01     | 5.91E-01     | 5.75E-03     | 1.74E+02     |\n| FaissIVF (n\\_list=256, n\\_probe=100)                                                   | 6.77E+00     | 4.14E-02     | 1.99E-08     | 3.33E-16     | **0.00E+00** | 9.88E-01     | 2.31E-03     | 6.97E-01     | 7.85E-01     | 6.27E-01     | 1.08E-02     | 9.29E+01     |\n| FaissIVF (n\\_list=512, n\\_probe=50)                                                    | 1.07E+01     | 1.27E-01     | 1.96E-02     | 9.93E-09     | **0.00E+00** | 9.53E-01     | 1.05E-02     | 6.37E-01     | 7.17E-01     | 5.72E-01     | 2.99E-03     | 3.34E+02     |\n| FaissIVF (n\\_list=512, n\\_probe=100)                                                   | 1.07E+01     | 7.63E-02     | 7.00E-03     | 1.55E-10     | **0.00E+00** | 9.74E-01     | 5.22E-03     | 6.73E-01     | 7.58E-01     | 6.05E-01     | 5.74E-03     | 1.74E+02     |\n| FaissIVF (n\\_list=1024, n\\_probe=100)                                                  | 2.14E+01     | 1.08E-01     | 1.27E-02     | 9.93E-09     | **0.00E+00** | 9.64E-01     | 8.24E-03     | 6.56E-01     | 7.39E-01     | 5.90E-01     | 3.09E-03     | 3.24E+02     |\n| FaissIVF (n\\_list=1024, n\\_probe=50)                                                   | 2.14E+01     | 1.53E-01     | 2.65E-02     | 1.99E-08     | **0.00E+00** | 9.42E-01     | 1.36E-02     | 6.26E-01     | 7.05E-01     | 5.63E-01     | 1.68E-03     | 5.95E+02     |\n| Nmslib (method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 5.78E+02     | 3.47E-02     | 9.93E-09     | 1.48E-16     | **0.00E+00** | 9.82E-01     | 1.95E-03     | 6.97E-01     | 7.85E-01     | 6.27E-01     | 1.05E-03     | 9.51E+02     |\n| Nmslib (method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 3.04E+02     | 3.31E-01     | 1.48E-02     | 3.88E-11     | **0.00E+00** | 9.25E-01     | 2.40E-02     | 6.42E-01     | 7.24E-01     | 5.78E-01     | **3.48E-04** | **2.88E+03** |\n| Nmslib (method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 3.04E+02     | 1.23E-01     | 1.99E-08     | 1.48E-16     | **0.00E+00** | 9.71E-01     | 1.09E-02     | 6.89E-01     | 7.76E-01     | 6.19E-01     | 1.12E-03     | 8.92E+02     |\n| Nmslib (method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 5.78E+02     | 4.77E-03     | 4.97E-09     | 7.40E-17     | **0.00E+00** | 9.97E-01     | 2.25E-04     | 7.21E-01     | 8.12E-01     | 6.48E-01     | 3.64E-03     | 2.74E+02     |\n\n#### NYTimes-256-angular qk=10\n\n| Algorithm                                                                             | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-Latency | QPS          |\n|---------------------------------------------------------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|\n| Pinecone Vector Index                                                                                | **2.18E+00** | **1.19E-08** | **5.96E-09** | **1.49E-09** | **0.00E+00** | **1.00E+00** | **5.69E-07** | **8.84E-01** | **9.24E-01** | **8.47E-01** | 6.74E-03     | 9.82E+01     |\n| FaissIVF(n\\_list=256, n\\_probe=50)                                                    | 6.92E+00     | 7.58E-02     | 1.94E-02     | 5.47E-03     | 5.96E-09     | 9.67E-01     | 1.88E-02     | 7.04E-01     | 7.36E-01     | 6.75E-01     | 5.75E-03     | 1.76E+02     |\n| FaissIVF(n\\_list=256, n\\_probe=100)                                                   | 6.92E+00     | 3.61E-02     | 6.61E-03     | 4.87E-04     | 1.49E-09     | 9.88E-01     | 6.82E-03     | 7.97E-01     | 8.34E-01     | 7.64E-01     | 1.08E-02     | 9.46E+01     |\n| FaissIVF(n\\_list=512, n\\_probe=50)                                                    | 1.12E+01     | 1.05E-01     | 2.70E-02     | 8.17E-03     | 5.96E-09     | 9.50E-01     | 2.70E-02     | 6.71E-01     | 7.01E-01     | 6.43E-01     | 2.99E-03     | 3.35E+02     |\n| FaissIVF(n\\_list=512, n\\_probe=100)                                                   | 1.12E+01     | 6.59E-02     | 1.40E-02     | 3.25E-03     | 2.98E-09     | 9.74E-01     | 1.42E-02     | 7.40E-01     | 7.73E-01     | 7.09E-01     | 5.74E-03     | 1.74E+02     |\n| FaissIVF(n\\_list=1024, n\\_probe=100)                                                  | 2.13E+01     | 8.24E-02     | 2.01E-02     | 4.86E-03     | 2.98E-09     | 9.62E-01     | 2.00E-02     | 7.11E-01     | 7.43E-01     | 6.81E-01     | 3.09E-03     | 3.19E+02     |\n| FaissIVF(n\\_list=1024, n\\_probe=50)                                                   | 2.13E+01     | 1.13E-01     | 3.18E-02     | 8.86E-03     | 5.96E-09     | 9.39E-01     | 3.14E-02     | 6.60E-01     | 6.90E-01     | 6.33E-01     | 1.68E-03     | 5.88E+02     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 5.80E+02     | 2.38E-02     | 2.87E-03     | 5.96E-09     | 2.22E-17     | 9.81E-01     | 4.21E-03     | 8.13E-01     | 8.50E-01     | 7.79E-01     | 1.05E-03     | 9.03E+02     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 3.03E+02     | 1.25E-01     | 1.53E-02     | 1.92E-04     | 1.33E-16     | 9.25E-01     | 2.60E-02     | 7.47E-01     | 7.81E-01     | 7.16E-01     | **3.48E-04** | **3.07E+03** |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 3.03E+02     | 2.91E-02     | 3.14E-03     | 5.96E-09     | 2.22E-17     | 9.77E-01     | 6.47E-03     | 8.10E-01     | 8.47E-01     | 7.76E-01     | 1.12E-03     | 9.59E+02     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 5.80E+02     | 6.22E-03     | 1.19E-08     | 2.98E-09     | **0.00E+00** | 9.97E-01     | 6.70E-04     | 8.61E-01     | 9.00E-01     | 8.25E-01     | 3.64E-03     | 2.72E+02     |\n\n#### Glove-25-angular k=3\n\n| Algorithm                                                                             | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-latency | QPS          |\n|---------------------------------------------------------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|\n| Pinecone Vector Index                                                                                | **8.94E+00** | **9.93E-09** | **4.97E-09** | **4.97E-09** | **2.48E-09** | **1.00E+00** | **3.55E-09** | **8.39E-01** | **9.29E-01** | **7.65E-01** | 1.37E-02     | 7.30E+01     |\n| FaissIVF(n\\_list=256, n\\_probe=50)                                                    | 1.02E+01     | 1.24E-08     | 7.45E-09     | **4.97E-09** | **2.48E-09** | **1.00E+00** | 1.81E-05     | 8.38E-01     | 9.28E-01     | 7.64E-01     | 3.88E-03     | 2.58E+02     |\n| FaissIVF(n\\_list=256, n\\_probe=100)                                                   | 1.02E+01     | 9.93E-09     | 7.45E-09     | **4.97E-09** | **2.48E-09** | **1.00E+00** | 4.51E-07     | **8.39E-01** | 9.29E-01     | **7.65E-01** | 6.85E-03     | 1.46E+02     |\n| FaissIVF(n\\_list=512, n\\_probe=50)                                                    | 1.42E+01     | 1.51E-03     | 7.45E-09     | **4.97E-09** | **2.48E-09** | 9.99E-01     | 9.69E-05     | 8.35E-01     | 9.24E-01     | 7.61E-01     | 2.03E-03     | 4.94E+02     |\n| FaissIVF(n\\_list=512, n\\_probe=100)                                                   | 1.42E+01     | 1.24E-08     | 7.45E-09     | **4.97E-09** | **2.48E-09** | **1.00E+00** | 6.38E-06     | **8.39E-01** | 9.28E-01     | **7.65E-01** | 3.92E-03     | 2.55E+02     |\n| FaissIVF(n\\_list=1024, n\\_probe=100)                                                  | 2.22E+01     | 1.49E-08     | 7.45E-09     | **4.97E-09** | **2.48E-09** | **1.00E+00** | 4.27E-05     | 8.38E-01     | 9.27E-01     | 7.64E-01     | 2.07E-03     | 4.83E+02     |\n| FaissIVF(n\\_list=1024, n\\_probe=50)                                                   | 2.22E+01     | 7.21E-03     | 7.45E-09     | **4.97E-09** | **2.48E-09** | 9.97E-01     | 3.02E-04     | 8.28E-01     | 9.17E-01     | 7.55E-01     | 1.09E-03     | 9.16E+02     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 6.68E+02     | **9.93E-09** | **4.97E-09** | **4.97E-09** | **2.48E-09** | **1.00E+00** | 7.01E-06     | **8.39E-01** | 9.28E-01     | **7.65E-01** | 3.36E-04     | 2.97E+03     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 4.34E+02     | 6.60E-03     | 7.45E-09     | **4.97E-09** | **2.48E-09** | 9.97E-01     | 2.87E-04     | 8.23E-01     | 9.11E-01     | 7.50E-01     | **1.12E-04** | **8.90E+03** |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 4.34E+02     | 9.93E-09     | **4.97E-09** | **4.97E-09** | **2.48E-09** | **1.00E+00** | 1.43E-05     | 8.38E-01     | 9.28E-01     | **7.65E-01** | 3.67E-04     | 2.73E+03     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 6.68E+02     | **9.93E-09** | **4.97E-09** | **4.97E-09** | **2.48E-09** | **1.00E+00** | **3.55E-09** | **8.39E-01** | **9.29E-01** | **7.65E-01** | 1.12E-03     | 8.95E+02     |\n\n#### Glove-25-angular k=10\n\n| Algorithm                                                                             | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-latency | QPS          |\n|---------------------------------------------------------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|\n| Pinecone Vector Index                                                                                | **8.94E+00** | **7.45E-09** | **4.47E-09** | **3.73E-09** | **2.24E-09** | **1.00E+00** | **8.11E-09** | **9.39E-01** | **9.75E-01** | **9.06E-01** | 1.40E-02     | 7.12E+01     |\n| FaissIVF(n\\_list=256, n\\_probe=50)                                                    | 1.07E+01     | 1.11E-03     | 5.96E-09     | 4.47E-09     | 2.98E-09     | **1.00E+00** | 8.60E-05     | 9.35E-01     | 9.71E-01     | 9.02E-01     | 3.88E-03     | 2.58E+02     |\n| FaissIVF(n\\_list=256, n\\_probe=100)                                                   | 1.07E+01     | 8.94E-09     | 5.96E-09     | 4.47E-09     | 2.98E-09     | **1.00E+00** | 3.07E-06     | 9.39E-01     | **9.75E-01** | **9.06E-01** | 6.84E-03     | 1.46E+02     |\n| FaissIVF(n\\_list=512, n\\_probe=50)                                                    | 1.42E+01     | 3.26E-03     | 7.45E-09     | 4.47E-09     | 2.98E-09     | 9.99E-01     | 3.09E-04     | 9.27E-01     | 9.62E-01     | 8.94E-01     | 2.01E-03     | 4.98E+02     |\n| FaissIVF(n\\_list=512, n\\_probe=100)                                                   | 1.42E+01     | 1.19E-08     | 5.96E-09     | 4.47E-09     | 2.98E-09     | **1.00E+00** | 3.44E-05     | 9.38E-01     | 9.73E-01     | 9.05E-01     | 3.91E-03     | 2.56E+02     |\n| FaissIVF(n\\_list=1024, n\\_probe=100)                                                  | 2.32E+01     | 1.93E-03     | 5.96E-09     | 4.47E-09     | 2.98E-09     | **1.00E+00** | 1.54E-04     | 9.33E-01     | 9.68E-01     | 9.00E-01     | 2.30E-03     | 4.35E+02     |\n| FaissIVF(n\\_list=1024, n\\_probe=50)                                                   | 2.32E+01     | 6.20E-03     | 3.24E-04     | 5.22E-09     | 2.98E-09     | 9.97E-01     | 8.05E-04     | 9.09E-01     | 9.43E-01     | 8.77E-01     | 1.21E-03     | 8.27E+02     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 6.57E+02     | 2.30E-04     | 5.22E-09     | **3.73E-09** | **2.24E-09** | **1.00E+00** | 3.15E-05     | 9.37E-01     | 9.73E-01     | 9.04E-01     | 3.40E-04     | 2.94E+03     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 4.21E+02     | 6.40E-03     | 1.39E-03     | 5.96E-09     | 2.98E-09     | 9.97E-01     | 1.14E-03     | 8.82E-01     | 9.15E-01     | 8.51E-01     | **1.16E-04** | **8.62E+03** |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 4.21E+02     | 5.19E-04     | 5.22E-09     | **3.73E-09** | **2.24E-09** | **1.00E+00** | 4.62E-05     | 9.37E-01     | 9.72E-01     | 9.04E-01     | 3.73E-04     | 2.68E+03     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 6.57E+02     | **7.45E-09** | **4.47E-09** | **3.73E-09** | **2.24E-09** | **1.00E+00** | **8.11E-09** | **9.39E-01** | **9.75E-01** | **9.06E-01** | 1.12E-03     | 8.91E+02     |\n\n##### Glove-100-angular k=3\n\n| Algorithm                                                                             | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-latency | QPS          |\n|---------------------------------------------------------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|\n| Pinecone Vector Index                                                                                | **8.29E+00** | **1.99E-08** | **9.93E-09** | **0.00E+00** | **0.00E+00** | **1.00E+00** | **3.16E-09** | **8.23E-01** | **9.21E-01** | **7.43E-01** | 1.45E-02     | 6.91E+01     |\n| FaissIVF(n\\_list=256, n\\_probe=50)                                                    | 1.17E+01     | 2.02E-02     | **9.93E-09** | 9.93E-09     | **0.00E+00** | 9.90E-01     | 1.10E-03     | 7.98E-01     | 8.93E-01     | 7.21E-01     | 9.06E-03     | 1.10E+02     |\n| FaissIVF(n\\_list=256, n\\_probe=100)                                                   | 1.17E+01     | 6.43E-03     | **9.93E-09** | 4.97E-09     | **0.00E+00** | 9.97E-01     | 3.01E-04     | 8.15E-01     | 9.13E-01     | 7.37E-01     | 1.72E-02     | 5.82E+01     |\n| FaissIVF(n\\_list=512, n\\_probe=50)                                                    | 1.57E+01     | 2.69E-02     | 1.47E-03     | 9.93E-09     | **0.00E+00** | 9.83E-01     | 1.80E-03     | 7.79E-01     | 8.72E-01     | 7.04E-01     | 5.05E-03     | 1.98E+02     |\n| FaissIVF(n\\_list=512, n\\_probe=100)                                                   | 1.57E+01     | 1.59E-02     | **9.93E-09** | 9.93E-09     | **0.00E+00** | 9.93E-01     | 7.21E-04     | 8.04E-01     | 9.00E-01     | 7.27E-01     | 9.15E-03     | 1.09E+02     |\n| FaissIVF(n\\_list=1024, n\\_probe=100)                                                  | 2.65E+01     | 2.27E-02     | 1.99E-08     | 9.93E-09     | **0.00E+00** | 9.86E-01     | 1.44E-03     | 7.88E-01     | 8.82E-01     | 7.12E-01     | 5.00E-03     | 2.00E+02     |\n| FaissIVF(n\\_list=1024, n\\_probe=50)                                                   | 2.65E+01     | 3.36E-02     | 5.53E-03     | 9.93E-09     | **0.00E+00** | 9.71E-01     | 2.97E-03     | 7.54E-01     | 8.44E-01     | 6.81E-01     | 2.58E-03     | 3.87E+02     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 1.59E+03     | 2.52E-02     | 3.49E-03     | 9.93E-09     | **0.00E+00** | 9.76E-01     | 2.07E-03     | 7.59E-01     | 8.49E-01     | 6.86E-01     | 6.28E-04     | 1.59E+03     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 8.61E+02     | 8.77E-02     | 2.19E-02     | 4.36E-03     | **0.00E+00** | 8.93E-01     | 1.04E-02     | 6.45E-01     | 7.22E-01     | 5.83E-01     | **1.92E-04** | **5.20E+03** |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 8.61E+02     | 3.48E-02     | 5.11E-03     | 9.93E-09     | **0.00E+00** | 9.65E-01     | 3.01E-03     | 7.47E-01     | 8.36E-01     | 6.75E-01     | 6.23E-04     | 1.60E+03     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 1.59E+03     | 5.85E-03     | **9.93E-09** | 4.97E-09     | **0.00E+00** | 9.97E-01     | 2.40E-04     | 8.13E-01     | 9.09E-01     | 7.34E-01     | 2.13E-03     | 4.69E+02     |\n\n#### Glove-100-angular k=10\n\n| Algorithm                                                                             | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-latency | QPS          |\n|---------------------------------------------------------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|\n| Pinecone Vector Index                                                                                | **8.29E+00** | **8.94E-09** | **5.96E-09** | **2.98E-09** | **2.98E-09** | **1.00E+00** | **2.60E-07** | **9.32E-01** | **9.72E-01** | **8.96E-01** | 1.56E-02     | 6.42E+01     |\n| FaissIVF(n\\_list=256, n\\_probe=50)                                                    | 1.19E+01     | 1.25E-02     | 2.27E-03     | 6.71E-09     | **2.98E-09** | 9.90E-01     | 2.38E-03     | 8.71E-01     | 9.08E-01     | 8.37E-01     | 9.04E-03     | 1.11E+02     |\n| FaissIVF(n\\_list=256, n\\_probe=100)                                                   | 1.19E+01     | 5.76E-03     | 8.94E-09     | 5.96E-09     | **2.98E-09** | 9.97E-01     | 6.71E-04     | 9.16E-01     | 9.54E-01     | 8.80E-01     | 1.71E-02     | 5.84E+01     |\n| FaissIVF(n\\_list=512, n\\_probe=50)                                                    | 1.62E+01     | 1.67E-02     | 4.26E-03     | 4.24E-04     | **2.98E-09** | 9.83E-01     | 3.96E-03     | 8.30E-01     | 8.65E-01     | 7.97E-01     | 5.04E-03     | 1.98E+02     |\n| FaissIVF(n\\_list=512, n\\_probe=100)                                                   | 1.62E+01     | 9.80E-03     | 1.18E-03     | 5.96E-09     | **2.98E-09** | 9.93E-01     | 1.60E-03     | 8.88E-01     | 9.26E-01     | 8.54E-01     | 9.26E-03     | 1.08E+02     |\n| FaissIVF(n\\_list=1024, n\\_probe=100)                                                  | 2.56E+01     | 1.47E-02     | 3.23E-03     | 1.19E-08     | **2.98E-09** | 9.86E-01     | 3.14E-03     | 8.49E-01     | 8.85E-01     | 8.16E-01     | 5.04E-03     | 1.98E+02     |\n| FaissIVF(n\\_list=1024, n\\_probe=50)                                                   | 2.56E+01     | 2.15E-02     | 7.06E-03     | 1.76E-03     | 4.47E-09     | 9.70E-01     | 6.43E-03     | 7.80E-01     | 8.13E-01     | 7.50E-01     | 2.59E-03     | 3.87E+02     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 1.58E+03     | 1.90E-02     | 6.18E-03     | 1.50E-03     | **2.98E-09** | 9.75E-01     | 5.34E-03     | 7.80E-01     | 8.13E-01     | 7.49E-01     | 6.32E-04     | 1.58E+03     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 8.61E+02     | 5.74E-02     | 2.20E-02     | 7.91E-03     | 8.32E-04     | 8.89E-01     | 2.16E-02     | 6.15E-01     | 6.41E-01     | 5.91E-01     | **1.99E-04** | **5.02E+03** |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 8.61E+02     | 2.39E-02     | 7.37E-03     | 1.67E-03     | **2.98E-09** | 9.65E-01     | 6.72E-03     | 7.70E-01     | 8.02E-01     | 7.39E-01     | 6.40E-04     | 1.56E+03     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 1.58E+03     | 5.40E-03     | 6.13E-04     | 5.96E-09     | **2.98E-09** | 9.97E-01     | 7.79E-04     | 8.97E-01     | 9.35E-01     | 8.62E-01     | 2.13E-03     | 4.69E+02     |\n\n#### Glove-200-angular k=3\n\n| Algorithm                                                                             | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-latency | QPS          |\n|---------------------------------------------------------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|\n| Pinecone Vector Index(k\\_bits=1024, p\\_factor=1)                                                     | **8.63E+00** | **1.99E-08** | **9.93E-09** | **0.00E+00** | **0.00E+00** | **1.00E+00** | **4.05E-07** | **8.20E-01** | **9.19E-01** | **7.40E-01** | 1.61E-02     | 6.20E+01     |\n| FaissIVF(n\\_list=256, n\\_probe=50)                                                    | 1.30E+01     | 3.39E-02     | 1.28E-03     | 4.97E-09     | **0.00E+00** | 9.81E-01     | 2.13E-03     | 7.78E-01     | 8.72E-01     | 7.02E-01     | 1.65E-02     | 6.06E+01     |\n| FaissIVF(n\\_list=256, n\\_probe=100)                                                   | 1.30E+01     | 1.63E-02     | 1.99E-08     | 0.00E+00     | **0.00E+00** | 9.93E-01     | 7.65E-04     | 8.05E-01     | 9.03E-01     | 7.27E-01     | 3.24E-02     | 3.09E+01     |\n| FaissIVF(n\\_list=512, n\\_probe=50)                                                    | 1.79E+01     | 4.32E-02     | 5.11E-03     | 9.93E-09     | **0.00E+00** | 9.71E-01     | 3.27E-03     | 7.54E-01     | 8.45E-01     | 6.81E-01     | 8.54E-03     | 1.17E+02     |\n| FaissIVF(n\\_list=512, n\\_probe=100)                                                   | 1.79E+01     | 2.76E-02     | 1.99E-08     | 0.00E+00     | **0.00E+00** | 9.86E-01     | 1.61E-03     | 7.87E-01     | 8.82E-01     | 7.10E-01     | 1.62E-02     | 6.16E+01     |\n| FaissIVF(n\\_list=1024, n\\_probe=100)                                                  | 2.82E+01     | 3.72E-02     | 3.16E-03     | 9.93E-09     | **0.00E+00** | 9.77E-01     | 2.64E-03     | 7.65E-01     | 8.58E-01     | 6.91E-01     | 8.57E-03     | 1.17E+02     |\n| FaissIVF(n\\_list=1024, n\\_probe=50)                                                   | 2.82E+01     | 5.33E-02     | 9.37E-03     | 1.99E-08     | **0.00E+00** | 9.57E-01     | 4.78E-03     | 7.26E-01     | 8.13E-01     | 6.55E-01     | 4.67E-03     | 2.14E+02     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 2.42E+03     | 5.11E-02     | 9.35E-03     | 9.93E-09     | **0.00E+00** | 9.46E-01     | 4.84E-03     | 7.14E-01     | 8.00E-01     | 6.44E-01     | 1.02E-03     | 9.76E+02     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 1.25E+03     | 1.45E-01     | 4.02E-02     | 1.03E-02     | **0.00E+00** | 8.34E-01     | 1.97E-02     | 5.87E-01     | 6.57E-01     | 5.29E-01     | **3.13E-04** | **3.20E+03** |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 1.25E+03     | 6.68E-02     | 1.13E-02     | 1.99E-08     | **0.00E+00** | 9.35E-01     | 6.26E-03     | 7.03E-01     | 7.88E-01     | 6.34E-01     | 9.91E-04     | 1.01E+03     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 2.42E+03     | 1.83E-02     | 2.42E-04     | **0.00E+00** | **0.00E+00** | 9.87E-01     | 1.19E-03     | 7.80E-01     | 8.75E-01     | 7.04E-01     | 3.48E-03     | 2.87E+02     |\n\n#### Glove-200-angular k=10\n\n| Algorithm                                                                             | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-latency | QPS          |\n|---------------------------------------------------------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|\n| Pinecone Vector Index(k\\_bits=1024, p\\_factor=1)                                                     | **8.63E+00** | **1.19E-08** | **5.96E-09** | **2.98E-09** | **0.00E+00** | **1.00E+00** | **1.84E-06** | **9.30E-01** | **9.71E-01** | **8.93E-01** | 1.85E-02     | 5.41E+01     |\n| FaissIVF(n\\_list=256, n\\_probe=50)                                                    | 1.34E+01     | 1.91E-02     | 3.75E-03     | 3.55E-04     | 2.98E-09     | 9.82E-01     | 4.10E-03     | 8.37E-01     | 8.73E-01     | 8.03E-01     | 1.65E-02     | 6.07E+01     |\n| FaissIVF(n\\_list=256, n\\_probe=100)                                                   | 1.34E+01     | 1.04E-02     | 5.74E-04     | 5.96E-09     | 7.45E-10     | 9.94E-01     | 1.48E-03     | 8.98E-01     | 9.37E-01     | 8.62E-01     | 3.23E-02     | 3.09E+01     |\n| FaissIVF(n\\_list=512, n\\_probe=50)                                                    | 1.78E+01     | 2.33E-02     | 6.44E-03     | 1.66E-03     | 5.96E-09     | 9.71E-01     | 6.52E-03     | 7.83E-01     | 8.17E-01     | 7.52E-01     | 8.52E-03     | 1.17E+02     |\n| FaissIVF(n\\_list=512, n\\_probe=100)                                                   | 1.78E+01     | 1.55E-02     | 2.80E-03     | 1.19E-08     | 2.98E-09     | 9.86E-01     | 3.15E-03     | 8.54E-01     | 8.91E-01     | 8.20E-01     | 1.63E-02     | 6.14E+01     |\n| FaissIVF(n\\_list=1024, n\\_probe=100)                                                  | 3.00E+01     | 2.13E-02     | 5.20E-03     | 9.60E-04     | 2.98E-09     | 9.77E-01     | 5.28E-03     | 8.06E-01     | 8.42E-01     | 7.74E-01     | 8.59E-03     | 1.16E+02     |\n| FaissIVF(n\\_list=1024, n\\_probe=50)                                                   | 3.00E+01     | 3.04E-02     | 9.64E-03     | 3.18E-03     | 5.96E-09     | 9.56E-01     | 9.56E-03     | 7.35E-01     | 7.67E-01     | 7.06E-01     | 4.70E-03     | 2.13E+02     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 2.41E+03     | 3.21E-02     | 1.12E-02     | 3.32E-03     | 5.96E-09     | 9.40E-01     | 1.05E-02     | 7.13E-01     | 7.44E-01     | 6.84E-01     | 1.01E-03     | 9.91E+02     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 1.25E+03     | 9.29E-02     | 3.28E-02     | 1.26E-02     | 1.78E-03     | 8.23E-01     | 3.60E-02     | 5.58E-01     | 5.82E-01     | 5.35E-01     | **3.14E-04** | **3.18E+03** |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 1.25E+03     | 4.04E-02     | 1.20E-02     | 3.53E-03     | 5.96E-09     | 9.30E-01     | 1.23E-02     | 7.09E-01     | 7.40E-01     | 6.81E-01     | 9.78E-04     | 1.02E+03     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 2.41E+03     | 1.31E-02     | 3.28E-03     | 7.75E-05     | 2.98E-09     | 9.86E-01     | 2.92E-03     | 8.30E-01     | 8.67E-01     | 7.97E-01     | 3.33E-03     | 3.01E+02     |\n\n#### Gist-960-angular k=3\n\n| Algorithm                            | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-latency | QPS          |\n|--------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|\n| Pinecone Vector Index                               | **9.64E+00** | **2.97E-16** | **0.00E+00** | **0.00E+00** | **0.00E+00** | **1.00E+00** | **5.62E-06** | 8.30E-01     | 9.22E-01     | 7.54E-01     | 4.93E-02     | 2.03E+01     |\n| FaissIVF(n\\_list=256, n\\_probe=50)   | 2.17E+01     | 2.29E-06     | 2.48E-09     | **0.00E+00** | **0.00E+00** | **1.00E+00** | 1.55E-05     | 8.29E-01     | 9.21E-01     | 7.53E-01     | 7.90E-02     | 1.27E+01     |\n| FaissIVF(n\\_list=256, n\\_probe=100)  | 2.17E+01     | 4.97E-09     | 2.48E-09     | **0.00E+00** | **0.00E+00** | **1.00E+00** | 7.20E-06     | **8.31E-01** | **9.23E-01** | **7.56E-01** | 1.50E-01     | 6.65E+00     |\n| FaissIVF(n\\_list=512, n\\_probe=50)   | 2.72E+01     | 1.28E-03     | 2.48E-09     | **0.00E+00** | **0.00E+00** | **1.00E+00** | 5.80E-05     | 8.18E-01     | 9.08E-01     | 7.43E-01     | 4.05E-02     | 2.47E+01     |\n| FaissIVF(n\\_list=512, n\\_probe=100)  | 2.72E+01     | 4.97E-09     | 2.48E-09     | **0.00E+00** | **0.00E+00** | **1.00E+00** | 4.16E-07     | 8.32E-01     | 9.24E-01     | 7.57E-01     | 7.98E-02     | 1.25E+01     |\n| FaissIVF(n\\_list=1024, n\\_probe=100) | 4.20E+01     | 9.78E-04     | 2.48E-09     | **0.00E+00** | **0.00E+00** | **1.00E+00** | 3.98E-05     | 8.23E-01     | 9.15E-01     | 7.49E-01     | 4.11E-02     | 2.43E+01     |\n| FaissIVF(n\\_list=1024, n\\_probe=50)  | 4.20E+01     | 2.50E-03     | 1.45E-04     | 2.48E-09     | **0.00E+00** | **1.00E+00** | 1.89E-04     | 7.92E-01     | 8.79E-01     | 7.20E-01     | **2.09E-02** | **4.77E+01** |\n\n#### Gist-960-angular k=10\n\n| Algorithm                            | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-latency | QPS          |\n|--------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|\n| Pinecone Vector Index                               | **9.62E+00** | 3.64E-04     | **0.00E+00** | **0.00E+00** | **0.00E+00** | **1.00E+00** | 3.14E-05     | 9.23E-01     | 9.61E-01     | 8.88E-01     | 5.07E-02     | 1.97E+01     |\n| FaissIVF(n\\_list=256, n\\_probe=50)   | 2.14E+01     | 5.59E-04     | 1.49E-09     | 7.45E-10     | **0.00E+00** | **1.00E+00** | 4.18E-05     | 9.25E-01     | 9.63E-01     | 8.90E-01     | 7.90E-02     | 1.27E+01     |\n| FaissIVF(n\\_list=256, n\\_probe=100)  | 2.14E+01     | **2.98E-09** | 1.49E-09     | 7.45E-10     | **0.00E+00** | **1.00E+00** | 1.14E-05     | **9.32E-01** | **9.71E-01** | **8.96E-01** | 1.51E-01     | 6.61E+00     |\n| FaissIVF(n\\_list=512, n\\_probe=50)   | 2.46E+01     | 9.32E-04     | 1.97E-04     | 1.49E-09     | 7.45E-10     | **1.00E+00** | 1.60E-04     | 8.96E-01     | 9.33E-01     | 8.62E-01     | 4.08E-02     | 2.45E+01     |\n| FaissIVF(n\\_list=512, n\\_probe=100)  | 2.46E+01     | 2.23E-04     | 1.49E-09     | 7.45E-10     | **0.00E+00** | **1.00E+00** | **1.12E-05** | 9.30E-01     | 9.69E-01     | 8.95E-01     | 8.00E-02     | 1.25E+01     |\n| FaissIVF(n\\_list=1024, n\\_probe=100) | 4.48E+01     | 7.84E-04     | 2.26E-05     | 7.45E-10     | 7.77E-17     | **1.00E+00** | 1.20E-04     | 9.12E-01     | 9.50E-01     | 8.77E-01     | 4.08E-02     | 2.45E+01     |\n| FaissIVF(n\\_list=1024, n\\_probe=50)  | 4.48E+01     | 1.70E-03     | 5.55E-04     | 1.20E-04     | 7.45E-10     | **1.00E+00** | 4.94E-04     | 8.37E-01     | 8.72E-01     | 8.05E-01     | **2.08E-02** | **4.80E+01** |\n\n#### Fashion-mnist-784-angular k=3\n\n| Algorithm                                                                             | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-latency | QPS          |\n|---------------------------------------------------------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|\n| Pinecone Vector Index                                                                                | **5.52E-01** | 2.45E-07     | **1.64E-07** | **1.26E-07** | 8.98E-08     | **1.00E+00** | 1.67E-06     | **8.39E-01** | **9.29E-01** | **7.66E-01** | 5.61E-03     | 1.78E+02     |\n| FaissIVF(n\\_list=256, n\\_probe=50)                                                    | 5.57E+00     | **2.43E-07** | **1.64E-07** | **1.26E-07** | 8.96E-08     | **1.00E+00** | 3.19E-06     | **8.39E-01** | **9.29E-01** | **7.66E-01** | 3.81E-03     | 2.63E+02     |\n| FaissIVF(n\\_list=256, n\\_probe=100)                                                   | 5.57E+00     | **2.43E-07** | **1.64E-07** | **1.26E-07** | **8.94E-08** | **1.00E+00** | **1.45E-07** | **8.39E-01** | **9.29E-01** | **7.66E-01** | 7.16E-03     | 1.40E+02     |\n| FaissIVF(n\\_list=512, n\\_probe=50)                                                    | 4.33E+00     | 2.45E-07     | **1.64E-07** | **1.26E-07** | 8.97E-08     | **1.00E+00** | 3.27E-06     | **8.39E-01** | **9.29E-01** | **7.66E-01** | 2.18E-03     | 4.59E+02     |\n| FaissIVF(n\\_list=512, n\\_probe=100)                                                   | 4.33E+00     | **2.43E-07** | **1.64E-07** | **1.26E-07** | **8.94E-08** | **1.00E+00** | 1.60E-06     | **8.39E-01** | **9.29E-01** | **7.66E-01** | 4.06E-03     | 2.46E+02     |\n| FaissIVF(n\\_list=1024, n\\_probe=100)                                                  | 4.69E+00     | 2.45E-07     | **1.64E-07** | **1.26E-07** | 8.96E-08     | **1.00E+00** | 4.53E-06     | **8.39E-01** | **9.29E-01** | **7.66E-01** | 2.39E-03     | 4.18E+02     |\n| FaissIVF(n\\_list=1024, n\\_probe=50)                                                   | 4.69E+00     | 2.48E-07     | **1.64E-07** | 1.27E-07     | 9.00E-08     | **1.00E+00** | 1.17E-05     | **8.39E-01** | **9.29E-01** | 7.65E-01     | 1.40E-03     | 7.14E+02     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 2.94E+01     | 2.56E-07     | **1.64E-07** | 1.27E-07     | 9.00E-08     | **1.00E+00** | 6.86E-05     | 8.38E-01     | 9.28E-01     | 7.65E-01     | 5.98E-04     | 1.67E+03     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 2.81E+01     | 2.77E-07     | 1.65E-07     | 1.27E-07     | 9.06E-08     | 9.99E-01     | 1.13E-04     | 8.37E-01     | 9.26E-01     | 7.63E-01     | **2.91E-04** | **3.44E+03** |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 2.81E+01     | 2.51E-07     | **1.64E-07** | 1.27E-07     | 9.00E-08     | **1.00E+00** | 5.62E-05     | **8.39E-01** | 9.28E-01     | 7.65E-01     | 7.46E-04     | 1.34E+03     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 2.94E+01     | 2.47E-07     | **1.64E-07** | **1.26E-07** | 9.00E-08     | **1.00E+00** | 4.21E-05     | **8.39E-01** | 9.28E-01     | 7.65E-01     | 1.45E-03     | 6.90E+02     |\n\n#### Fashion-mnist-784-angular k=10\n\n| Algorithm                                                                             | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-latency | QPS          |\n|---------------------------------------------------------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|\n| Pinecone Vector Index                                                                                | **5.50E-01** | 1.96E-07     | **1.39E-07** | **1.15E-07** | **9.24E-08** | **1.00E+00** | 3.08E-06     | **9.40E-01** | **9.75E-01** | **9.08E-01** | 7.01E-03     | 1.43E+02     |\n| FaissIVF(n\\_list=256, n\\_probe=50)                                                    | 5.07E+00     | 1.96E-07     | **1.39E-07** | **1.15E-07** | **9.24E-08** | **1.00E+00** | 6.31E-06     | **9.40E-01** | **9.75E-01** | **9.08E-01** | 3.91E-03     | 2.56E+02     |\n| FaissIVF(n\\_list=256, n\\_probe=100)                                                   | 5.07E+00     | **1.95E-07** | **1.39E-07** | **1.15E-07** | **9.24E-08** | **1.00E+00** | 1.36E-06     | **9.40E-01** | **9.75E-01** | **9.08E-01** | 7.16E-03     | 1.40E+02     |\n| FaissIVF(n\\_list=512, n\\_probe=50)                                                    | 5.31E+00     | 2.00E-07     | 1.40E-07     | **1.15E-07** | **9.24E-08** | **1.00E+00** | **1.04E-05** | **9.40E-01** | **9.75E-01** | 9.07E-01     | 2.22E-03     | 4.50E+02     |\n| FaissIVF(n\\_list=512, n\\_probe=100)                                                   | 5.31E+00     | 1.96E-07     | **1.39E-07** | **1.15E-07** | **9.24E-08** | **1.00E+00** | 4.07E-06     | **9.40E-01** | **9.75E-01** | **9.08E-01** | 4.07E-03     | 2.45E+02     |\n| FaissIVF(n\\_list=1024, n\\_probe=100)                                                  | 3.35E+00     | 1.99E-07     | 1.40E-07     | **1.15E-07** | **9.24E-08** | **1.00E+00** | 1.16E-05     | **9.40E-01** | **9.75E-01** | 9.07E-01     | 2.42E-03     | 4.13E+02     |\n| FaissIVF(n\\_list=1024, n\\_probe=50)                                                   | 3.35E+00     | 2.17E-07     | 1.41E-07     | **1.15E-07** | 9.26E-08     | **1.00E+00** | 3.08E-05     | 9.39E-01     | 9.74E-01     | 9.07E-01     | 1.42E-03     | 7.06E+02     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 2.93E+01     | 2.15E-07     | 1.41E-07     | **1.15E-07** | 9.28E-08     | **1.00E+00** | 9.01E-05     | 9.38E-01     | 9.73E-01     | 9.06E-01     | 5.88E-04     | 1.70E+03     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 2.81E+01     | 3.40E-04     | 1.43E-07     | 1.16E-07     | 9.31E-08     | 9.99E-01     | 1.80E-04     | 9.35E-01     | 9.70E-01     | 9.03E-01     | **2.89E-04** | **3.46E+03** |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=16'\\]) | 2.81E+01     | 2.02E-07     | 1.40E-07     | **1.15E-07** | **9.24E-08** | **1.00E+00** | 7.17E-05     | 9.39E-01     | 9.74E-01     | 9.07E-01     | 7.34E-04     | 1.36E+03     |\n| Nmslib(method\\_name=hnsw, index\\_param=\\[u'efConstruction=800', u'post=2', u'M=32'\\]) | 2.93E+01     | 1.99E-07     | 1.40E-07     | **1.15E-07** | **9.24E-08** | **1.00E+00** | 5.15E-05     | **9.40E-01** | 9.74E-01     | 9.07E-01     | 1.48E-03     | 6.74E+02     |\n\n#### Deep-image-96 k=3\n\n| Algorithm                            | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-latency | QPS          |\n|--------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|\n| Pinecone Vector Index                               | 7.00E+01     | **1.79E-07** | **1.22E-07** | **8.94E-08** | **5.96E-08** | **1.00E+00** | **9.90E-08** | **8.36E-01** | **9.27E-01** | **7.61E-01** | 6.74E-02     | 1.48E+01     |\n| FaissIVF(n\\_list=256, n\\_probe=50)   | **6.95E+01** | 1.89E-07     | 1.24E-07     | **8.94E-08** | **5.96E-08** | **1.00E+00** | 1.56E-05     | 8.35E-01     | 9.26E-01     | **7.61E-01** | 6.62E-02     | 1.51E+01     |\n| FaissIVF(n\\_list=256, n\\_probe=100)  | **6.95E+01** | 1.81E-07     | **1.22E-07** | **8.94E-08** | **5.96E-08** | **1.00E+00** | 4.30E-06     | **8.36E-01** | **9.27E-01** | **7.61E-01** | 1.29E-01     | 7.75E+00     |\n| FaissIVF(n\\_list=512, n\\_probe=50)   | 7.56E+01     | 1.94E-07     | 1.24E-07     | **8.94E-08** | **5.96E-08** | 9.99E-01     | 3.37E-05     | 8.35E-01     | 9.26E-01     | 7.60E-01     | 3.39E-02     | 2.95E+01     |\n| FaissIVF(n\\_list=512, n\\_probe=100)  | 7.56E+01     | 1.81E-07     | **1.22E-07** | **8.94E-08** | **5.96E-08** | **1.00E+00** | 6.41E-06     | **8.36E-01** | **9.27E-01** | **7.61E-01** | 6.52E-02     | 1.53E+01     |\n| FaissIVF(n\\_list=1024, n\\_probe=100) | 9.22E+01     | 1.89E-07     | 1.24E-07     | **8.94E-08** | **5.96E-08** | **1.00E+00** | 2.99E-05     | 8.35E-01     | 9.26E-01     | **7.61E-01** | **3.40E-02** | **2.94E+01** |\n| FaissIVF(n\\_list=1024, n\\_probe=50)  | 9.22E+01     | 8.27E-04     | 1.24E-07     | **8.94E-08** | **5.96E-08** | 9.99E-01     | 1.08E-04     | 8.32E-01     | 9.23E-01     | 7.58E-01     | 1.80E-02     | 5.56E+01     |\n\n#### Deep-image-96 k=10\n\n| Algorithm                            | Build time   | Aloss-p99    | Aloss-p90    | Aloss-p75    | Aloss-p50    | MRR          | Mean-DCG     | F-1          | F-0.5        | F-2          | Mean-latency | QPS          |\n|--------------------------------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|\n| Pinecone Vector Index                               | 7.00E+01     | **1.67E-07** | **1.15E-07** | **8.64E-08** | **5.66E-08** | **1.00E+00** | **2.08E-07** | **9.38E-01** | **9.74E-01** | **9.04E-01** | 6.88E-02     | 1.45E+01     |\n| FaissIVF(n\\_list=256, n\\_probe=50)   | **6.92E+01** | 2.04E-07     | 1.16E-07     | 8.72E-08     | **5.66E-08** | **1.00E+00** | 4.43E-05     | 9.36E-01     | 9.72E-01     | 9.02E-01     | 6.63E-02     | 1.51E+01     |\n| FaissIVF(n\\_list=256, n\\_probe=100)  | **6.92E+01** | 1.70E-07     | **1.15E-07** | **8.64E-08** | **5.66E-08** | **1.00E+00** | 8.33E-06     | 9.37E-01     | **9.74E-01** | **9.04E-01** | 1.29E-01     | 7.73E+00     |\n| FaissIVF(n\\_list=512, n\\_probe=50)   | 7.53E+01     | 9.99E-04     | 1.20E-07     | 8.87E-08     | 5.81E-08     | 9.99E-01     | 9.69E-05     | 9.34E-01     | 9.70E-01     | 9.00E-01     | 3.37E-02     | 2.97E+01     |\n| FaissIVF(n\\_list=512, n\\_probe=100)  | 7.53E+01     | 1.78E-07     | **1.15E-07** | **8.64E-08** | **5.66E-08** | **1.00E+00** | 1.69E-05     | 9.37E-01     | 9.73E-01     | 9.03E-01     | 6.48E-02     | 1.54E+01     |\n| FaissIVF(n\\_list=1024, n\\_probe=100) | 9.27E+01     | 3.85E-04     | 1.18E-07     | 8.79E-08     | 5.74E-08     | **1.00E+00** | 7.41E-05     | 9.35E-01     | 9.71E-01     | 9.02E-01     | 3.40E-02     | 2.94E+01     |\n| FaissIVF(n\\_list=1024, n\\_probe=50)  | 9.27E+01     | 3.02E-03     | 1.28E-07     | 9.09E-08     | 5.96E-08     | 9.99E-01     | 2.68E-04     | 9.28E-01     | 9.64E-01     | 8.95E-01     | **1.79E-02** | **5.59E+01** |\n\n---\n\nIf you would like to take advantage of the accuracy and performance of the Pinecone Vector Index, [learn more about Pinecone](/) and [try it yourself](/start).\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbe9"
  },
  "filename": "2022-highlights.md",
  "title": "post",
  "category": "\"History in the making",
  "content": "---\nlayout: post\ntitle: \"History in the making: A year of learning, growing, and connecting together\"\nheadline: \"History in the making: A year of learning, growing, and connecting together\"\ncategories:\n  - Company News\ntoc: >-\nauthor:\n  name: Gibbs Cullen\n  position: Senior Product Marketing Manager\n  src: /images/gibbs-cullen.jpg\n  href: https://www.linkedin.com/in/gibbscullen/\ndate: \"2022-12-23\"\n# Open Graph\ndescription: \"History in the making: A year of learning, growing, and connecting together\"\nimages: [\"/images/2022-highlights-1.jpg\"]\nthumbnail: \"/images/2022-highlights-thumbnail.jpg\"\n---\n\n![History in the making](/images/2022-highlights-1.jpg)\n\nThis has been an exciting year for vector search and vector databases. Embedding models got better and more accessible, the use cases for vector search expanded, and new applications powered by vector search generated a lot of buzz. While a lot happened over the past twelve months, we wanted to share a few highlights of our own.\n\nIn 2022...\n\n## Demand grew, and so did we\n\nWe [launched](https://techcrunch.com/2021/01/27/pinecone-lands-10m-seed-for-purpose-built-machine-learning-database/) Pinecone back in 2021 – introducing [the vector database](/learn/vector-database/) as a way to make it easy for developers to build high-performance vector search applications. The adoption of Pinecone exceeded even our own ambitious expectations.\n\nThat’s why at the beginning of 2022, we needed to make some changes to keep up with our quickly growing user base. And we did just that - starting with a $28M Series A funding [announcement](https://www.pinecone.io/learn/funding-search-ai-age/) in March. Since then, we’ve continued to observe the adoption of vector search in various applications and across new use cases. Pinecone is now being used by customers like Workday, Mem, Clubhouse, BambooHR, [Expel](/learn/expel-alert-similarity/), Course Hero, and many others for things like semantic search, anomaly detection, recommendation systems, mutli-modal search, fraud detection, and more.\n\nTo support all this growth, we also needed to grow the team. At the start of 2022, we were a company of 21 employees. Today, we more than doubled that number with employees (especially in engineering) spanning 25 cities and 6 countries and quickly growing hubs in New York City and Tel Aviv. We celebrated this amazing growth in person during [our offsite in Greece](https://www.linkedin.com/posts/pinecone-io_a-couple-of-weeks-ago-we-had-the-pleasure-activity-6984203237394366464-XVc7?utm_source=share&utm_medium=member_desktop)!\n\n![Team Pinecone](/images/2022-highlights-2.jpg)\n<small>Team Pinecone visited the Acropolis during our company offsite in Greece </small>\n\n## Research advancements kept us on our toes\n\nThis year was also focused a lot on learning - whether that be learning more about new use cases and the capabilities of vector search, or for creating learning opportunities for our users and the community.\n\nWe created many new learning resources around the latest technologies, including:\n\n- Learning how to train models in [low-resource scenarios with GPL](/learn/gpl/).\n- Exploring multi-modal ML with [Open AI’s CLIP](/learn/clip/) and in the [Embedding Methods for Image Search](/learn/image-search/) series.\n- Searching through [video and audio with Whisper](/learn/openai-whisper/).\n- Making [Stable Diffusion more accessible](https://www.youtube.com/watch?v=YMlzhnlSAww) with “vector caching”.\n- Collaborating with [Deepset AI](https://www.deepset.ai) to release the [HaystackDocumentStore](https://docs.pinecone.io/docs/haystack) for Haystack users to build better, faster, and feature-rich [semantic search tools](/learn/haystack-lfqa/).\n- Helping users get started with semantic search in the [NLP for Semantic Search](/learn/nlp/) series.\n\nIn 2022, we attended and sponsored several conferences, including [Southern Data Science Conference](https://www.southerndatascience.com/), [SIGIR](/learn/sigir-2022/), and the [AI Summit](https://newyork.theaisummit.com/). We also hosted many webinars with guest speakers from Pinecone partners and supporters like [Yury Malkov](https://twitter.com/malkovyury) (advisor to Pinecone), [Nils Reimers](https://www.nils-reimers.de/), and [Julien Simon](https://huggingface.co/juliensimon). Subscribe to our [mailing list](/community/) and join our [meetup group](https://www.meetup.com/vector-search/) for the latest news and upcoming events.\n\n![Pinecone Booth at AI Summit in NYC ](/images/2022-highlights-3.jpg)\n<small>Happy campers at the Pinecone booth during the AI Summit in NYC</small>\n\n## Our users inspired us to keep innovating\n\nSince launching Pinecone, we’ve worked hard to innovate and add features for our users. We had many product launches this year to help our users build bigger, better, and faster. Some highlights include:\n\n- [Hybrid vector index](/learn/hybrid-search/) for keyword-aware semantic search\n- 10x faster searches with proprietary [graph-based vector indexes](/learn/pods-for-performance/)\n- Partial and real-time index updates\n- Combined vector search with [single-stage metadata filtering](/learn/vector-search-filtering/)\n- Zero-downtime [vertical scaling](/learn/faster-easier-scalable/)\n\n[Sign-up](http://app.pinecone.io) for free, and experience the ease of building high-performance vector search applications with Pinecone.\n\n## What’s next?\n\nOverall, it’s been a year full of growth, learning, coming together, and pushing the boundaries of vector search. We are very proud of the company and culture we’ve built, and can’t wait to continue growing, learning, and improving in 2023. Want to join us and continue to shape the future of AI and ML applications? [We’re hiring](/careers/#open-roles) across all functions!\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbea"
  },
  "filename": "faiss.md",
  "title": "book",
  "category": "\"Faiss",
  "content": "---\nlayout: book\ntitle: \"Faiss: The Missing Manual\"\ndescription: Vector Search with Facebook AI's Similarity Search\nauthor: James Briggs\nintro: |\n  Facebook AI Similarity Search (Faiss) is one of the best open source options for similarity search. In this ebook, you will learn the essentials of vector search and how to apply them in Faiss to build powerful vector indexes.\nemailSubmit: true\nsocialShare: true\nimage: /images/faiss-ebook.png\nimages: ['/images/faiss-ebook.png']\nintroChapter: \n    title: Introduction\n    text: |\n      Vector search has been used by tech giants like Google and Amazon for decades. It has been claimed to be a significant driver in clicks, views, and sales across several platforms. Yet, it was only with Faiss that this technology became more accessible.\n      \n      In the past few years, vector search exploded in popularity. It has driven ecommerce sales, powered music and podcast search, and even recommended your next favorite shows on streaming platforms. Vector search is everywhere and in the following chapters you will discover why it has found such great success and how to apply it yourself using the Facebook AI Similarity Search (Faiss) library.\nchapters:\n  - title: Introduction to Facebook AI Similarity Search (Faiss)\n    text: An overview of the Faiss library and similarity search.\n    url: /learn/faiss-tutorial/\n  - title: Nearest Neighbor Indexes for Similarity Search\n    text: Learn how to choose the right index in Faiss.\n    url: /learn/vector-indexes/\n  - title: \"Locality Sensitive Hashing (LSH): The Illustrated Guide\"\n    text: Take your first steps to a deeper understanding of approximate nearest neighbor indexes with LSH.\n    url: /learn/locality-sensitive-hashing/\n  - title: Random Projection for Locality Sensitive Hashing\n    text: Apply LSH to modern dense vector representations using random projection.\n    url: /learn/locality-sensitive-hashing-random-projection/\n  - title: Product Quantization\n    text: Learn how Product Quantization (PQ) can be used to compress indexes by upto 97%.\n    url: /learn/product-quantization/\n  - title: Hierarchical Navigable Small Worlds (HNSW)\n    text: HNSW graphs are among the top performing indexes in similarity search.\n    url: /learn/hnsw/\n  - title: Composite Indexes and the Faiss Index Factory\n    url: /learn/composite-indexes/\n    text: Learn how to apply all we have learned so far to create multi-step composite indexes.\n  - title: EMAIL_SUBMIT #email submit form\n  - title: And more...\n---\n\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbeb"
  },
  "filename": "wild.md",
  "title": "book",
  "category": "\"Vector Search in the Wild\"",
  "content": "---\nlayout: book\ntitle: \"Vector Search in the Wild\"\ndescription: Vector Search in the Wild.\nauthor: James Briggs\nintro: |\n  Big tech has been using it for years, and now it is spreading like wildfire across industries everywhere. Without realizing it, we all use vector search every day. Here we take a look at the world of vector search and its incredible potential.\nemailSubmit: true\nsocialShare: true\nimage: /images/wild-ebook.png\nimages: ['/images/wild-ebook.png']\nintroChapter: \n    title: Introduction\n    text: |\n      Vector search has been big tech's tightly kept secret for years. When you perform a Google search, get recommended shows on Netflix, or find Amazon suggesting that product you never knew you wanted, that isn't magic; it's vector search.\n\n      It is ubiquitous in our daily lives, and every day new use cases appear, but now it’s not only big tech’s secret sauce. From startups to blue-chip corporations, vector search is finding success everywhere. Here, we will take the lid off the jar and reveal where vector search is used.\n\n      In the following chapters, you will learn how vector search can enhance existing applications and open doors to previously inaccessible opportunities.\nchapters:\n  - title: How Spotify Uses Semantic Search for Podcasts\n    text: How Spotify is reimagining podcast discovery with semantic search.\n    url: /learn/spotify-podcast-search/\n  - title: Building the Self-Organizing Workspace at Mem\n    text: How Mem uses language models and vector search to unlock humanity's collective intelligence.\n    url: /learn/mem-semantic-search/\n  - title: Detecting Similar Security Alerts at Expel\n    text: How Expel built an alert similarity detector to help security analysts detect and remediate threats faster.\n    url: /learn/expel-alert-similarity/\n  - title: How Nyckel Built An API for Semantic Image Search\n    text: How Nyckel automates ML workflows for image and text classification, object detection, and visual search with the help of vector search.\n    url: /learn/nyckel-ml-automation/\n    bonusSection: # Bonus content / further materials\n      title: More\n      links:\n        - title: \"Gong: Introducing Generation 3 Conversation Understanding\"\n          url: https://www.gong.io/blog/introducing-generation-3-conversation-understanding/\n  - title: EMAIL_SUBMIT #email submit form\n  - title: How Vector Search Powers Ecommerce\n    text: How giants like Amazon or Shein know what you want before you do.\n  - title: The Future of Document Discovery\n    text: How organizations are using Q&A to enable a more \"human\" search experience.\n  - title: AI-Powered Assistants are Everywhere\n    text: Learn how vector search is powering the next generation of chatbots.\n  - title: And more...\n---\n\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbec"
  },
  "filename": "cnn.md",
  "title": "ebook-post",
  "category": "\"Visual Guide to Applied Convolution Neural Networks\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Visual Guide to Applied Convolution Neural Networks\"\nheadline: \"Visual Guide to Applied Convolution Neural Networks\"\ncategories:\n - Embedding Methods for Image Search\ntoc: >-\nweight: 4\nauthors:\n - name: James Briggs\n   position: Developer Advocate\n   src: /images/james-briggs.jpeg\n   href: \"https://www.youtube.com/c/jamesbriggs\"\n - name: Laura Carnevali\n   position: Developer\n   src: /images/laura-carnevali.jpeg\n   href: \"https://www.linkedin.com/in/laura-carnevali-071a01b7/\"\ndescription: A visual explainer of everything you need to know about convolutional neural networks with Python implementation details\n# Open graph\nimages: ['https://www.pinecone.io/images/cnn-0.png']\n---\n\n**C**onvolutional **N**eural **N**etworks (CNNs) have been the undisputed champions of **C**omputer **V**ision (CV) for almost a decade. Their widespread adoption kickstarted the world of deep learning; without them, the field of AI would look very different today.\n\nBefore deep learning with CNNs, CV applications relied on brittle edge detection algorithms, color profiles, and a plethora of manually scripted processes. These could rarely be applied across different datasets or use cases.\n\nThe result is that every dataset and every use-case required significant manual intervention and domain-specific knowledge, rarely producing performance acceptable for broader use.\n\nDeep-layered CNNs changed this. Rather than manual feature extraction, CNNs proved capable of doing this automatically for a vast number of datasets and use cases. All they needed was training data.\n\nBig data with deep CNNs have remained the de-facto standard in computer vision. New models using vision transformers (ViT) and multi-modality may change this in the future, but for now CNNs still dominate state-of-the-art benchmarks in vision. In this hands-on article we will learn why.\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/ZBfpkepdZlw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n---\n\n## What Makes a CNN?\n\nCNNs are neural networks known for their performance on image datasets. They are characterized by something called a *convolutional layer* that can detect abstract features of an image. These images can be shifted, squashed, or rotated; if a human can still recognize the features, a CNN likely can too.\n\nBecause of their affinity to image-based applications, we find CNNs used for image classification, object detection, object recognition, and many more tasks within the realm of CV.\n\n![cnn](./images/cnn-1.png)\n<small>The typical architecture of a CNN.</small>\n\nDeep-layered CNNs are any neural network that satisfies two conditions; (1) they have many layers (deep), and (2) they contain convolutional layers. Beyond that, networks can have many features, including pooling, normalization, and linear layers.\n\n### Convolutional Layers\n\nAn image is a big array of pixel activation values. These arrays are followed by more arrays of (initially random) values – the weights – that we call a *\"filter\"* or *\"kernel\"*. A convolutional layer is an element-wise multiplication between these pixel values and the filter weights – *which are then summed*.\n\n![scalar-product](./images/cnn-2.png)\n\nThis element-wise operation **followed by the sum of the resulting values** is often referred to as the *\"scalar product\"* because it results in a single *scalar* value [2].\n\n![scalar-product-ex](./images/cnn-3.png)\n\nConsidering the above example, when the 3x3 filter is applied to the input image starting from the top-left, the resulting value from the element-wise multiplication is 3.\n\nWe don't return a single scalar value because we perform many of these operations on each layer. For each convolutional layer, the filter slides (or *\"convolves\"*) over the previous layer's matrix (or image) from left-to-right and top-to-bottom.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/cnn-4.mp4\" type=\"video/mp4\">\n</video>\n\nThe filter can detect specific *local* features such as edges, shapes, and textures by sliding over the image.\n\n---\n\n*This convolution has a parallel in signal processing. Given an input audio wave $f$, we \"convolve\" a filter $g$ over it to produce a modified audio wave. See the appendix for a more detailed explanation.*\n\n---\n\nThe convolution output is called a **\"feature map\"** or *\"activation map\"* thanks to the representation or *activations* of detected features from the input layer.\n\nAs the element-wise multiplication of the filter outputs a single value after processing multiple input values, we need to be mindful of excessive information loss via dimensionality reduction (i.e., compression).\n\nWe may want to increase or decrease the amount of compression our filters create. Compression is controlled using the filter *size* and how quickly it moves across the image (the `stride`).\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/cnn-5.mp4\" type=\"video/mp4\">\n</video>\n<small>Larger filters create more compression.</small>\n\nThe `stride` defines the number of *pixels* a filter moves after every calculation. By increasing the stride, the filter will travel across the entire input image in fewer steps, outputting fewer values and producing a more compressed feature map.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/cnn-6.mp4\" type=\"video/mp4\">\n</video>\n<small>A greater `stride` produces a more compressed feature map.</small>\n\nThere are some surprising effects of image compression, and one that we must be careful of is the filter's interaction with the border areas of an input layer.\n\n![border](./images/cnn-7.png)\n\nThe border effects *on small images* can result in a rapid loss of information for images containing too little information. To avoid this, we either reduce compression using the previously discussed techniques (filter size and `stride`) or add `padding`.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/cnn-8.mp4\" type=\"video/mp4\">\n</video>\n<small>Adding padding to images/layers can help limit compression.</small>\n\nAdding a set of zero-value pixels around the image allows us to limit or prevent compression between layers. In the example above, the operation *without* padding causes compression of *10x10* pixels into *8x8* pixels. With padding, no compression occurs (a *10x10* feature map is output). TK 10x10??\n\nFor larger images, compression is less likely to cause problems. However, when working with smaller images, `padding` acts as an easy and effective remedy to border effects,\n\n### Depth\n\nWe mentioned that *deep-layered* CNNs were what took CV to new heights. The AlexNet authors found the *depth* of networks to be a key component for higher performance [1]. Each successive layer extracts features from the previous layer (and previously extracted features). This chain of consecutive extractions produces ever more *\"abstract\"* features that can better represent images in more *\"human\"* terms.\n\nA shallow network may recognize that an image contains an animal, but as we add another layer, it may identify a dog. Adding another may result in identifying specific breeds like Staffordshire Bull Terrier or Husky. More layers generally result in recognition of more abstract and specific concepts.\n\n### Activation Functions\n\nActivation functions are a common feature in every type of neural network. They add *non-linearity* to networks, enabling the representation of more complex patterns.\n\n![act-functions](./images/cnn-9.png)\n<small>Some of the most popular activation functions; ReLU, tanh, and sigmoid.</small>\n\nIn the past, CNNs often used hidden layer activation functions like sigmoid or tanh. However, in 2012 a new activation function called a **Re**ctified **L**inear **U**nit was popularized through its use in AlexNet, the best-performing CNN of its time.\n\n![reluvstanh](./images/cnn-10.png)\n<small>The authors of AlexNet noted that results from a four-layer CNN with ReLU activation functions reached a 25% error rate on the CIFAR-10 dataset six times faster than the equivalent with Tanh [1].</small>\n\nNowadays, ReLU is still a popular choice. It is simpler than tanh and sigmoid and does not require normalization to avoid saturation (i.e., activations congregating towards the min/max values).\n\n### Pooling Layers\n\nOutput feature maps are highly sensitive to small changes in the location of input features [3]. To some degree, this can be useful, as it can tell us the difference between a cat's face and a dog's face. However, if an eye is two pixels further to the left than expected, the model should still be able to identify a face.\n\nCNNs use pooling layers to handle this. Pooling layers are a downsampling method that compresses information from one layer into a smaller space in the next layer.\n\nAn effect of pooling is that information across several pixels is compressed into a single activation, essentially \"smoothing out\" variations across groups (or patches) of pixels.\n\nThe two most common pooling methods are average pooling and max pooling. Average pooling takes the average of activations in the window, whereas max pooling takes their maximum value.\n\n![avg-vs-max-pooling](./images/cnn-11.png)\n\n### Fully-Connected Layers\n\nFully connected linear layers are another common feature of CNNs. They are neural networks in their most stripped-down form; the dot product between inputs $X$ and layer weights $W$ with a bias term $b$ and activation function.\n\n![fully-connected](./images/cnn-12.png)\n\nThese layers are usually found towards the end of a CNN and handle the transformation of CNN embeddings from 3D tensors to more understandable outputs like class predictions.\n\nOften within these final layers, we will find the most *information-rich* vector representations of the data being input to the model. In the next chapter of the ebook, we will explore these in more depth for use in content-based image retrieval (CBIR).\n\nFor now, let's focus on the task of classification. Classifiers tend to apply a *softmax* activation function that creates a probability distribution across the final output nodes — where each node represents a specific class.\n\n![softmax](./images/cnn-13.png)\n\nAfter these final fully-connected layers, we have our predictions.\n\nThese are a few of the most common components of CNNs, but with time many different types of CNNs with different network architectures were designed. So there is no \"specific\" architecture, just a set of guideposts in the form of high-performing networks.\n\n## Popular Architectures\n\nThroughout the years, there have been several hugely successful CNN architectures. We will take a high-level look at a few of the most relevant.\n\n### LeNet\n\nLeNet is the earliest example of a \"deep\" CNN, developed in 1998 by Yann LeCun, et. al. [4]. Many of us have likely interacted with LeNet as Bell Labs licensed it to banks around the globe for reading the digits on handwritten cheques.\n\n![lenet](./images/cnn-14.png)\n<small>LeNet model architecture [4].</small>\n\nThis first example of a commercially successful deep CNN was surprisingly the only such example of a successful deep CNN for another 14 years.\n\n### AlexNet\n\nOctober 2012 is widely regarded as ground zero for the birth of deep learning. The catalyst was AlexNet winning the ImageNet ILSVRC challenge [1]. AlexNet can be seen as a continuation of LeNet, using a similar architecture but adding more layers, training data, and safeguards against overfitting.\n\n![alexnet](./images/cnn-15.png)\n<small>AlexNet model architecture [1].</small>\n\nAfter AlexNet, the broader community of CV researchers began focusing on training deeper models with larger datasets. The following years saw variations of AlexNet continue winning ILSVRC and reaching ever more impressive performance.\n\n### VGGNet\n\n![veggnet16](./images/cnn-16.png)\n<small>VGGNet model architecture [5].</small>\n\nAlexNet was dethroned as the winner of ILSVRC in 2014 with the introduction of VGGNet, developed at Oxford University [5]. Many variants of VGGNet were developed, characterized by the number of layers they contained, such as 16 total layers (13 convolutional) for VGGNet-16, and 19 total layers for VGGNet-19.\n\n### ResNet\n\nResNet became the new champion of CV in 2015 [6]. The ResNet variants were much deeper than before, the first containing 34 layers. Since then, 50+ layer ResNet models have been developed and hold state-of-the-art results on many benchmarks.\n\n![resnet](./images/cnn-17.png)\n<small>ResNet model architecture [6].</small>\n\nResNet was inspired by VGGNet but added smaller filters and a less complex network architecture. Shortcut connections between different layers were also added, giving the name of **Res**idual **Net**work (ResNet).\n\nWithout these shortcuts, the greater depth of ResNet results in information loss over the many layers of transformations. Adding the shortcuts enabled information to be maintained across these greater distances.\n\n## Classification with CNNs\n\nWe've understood the standard components of CNNs and how these work together to extract abstract — but meaningful — features from images.\n\nWe also looked at a few of the most popular architectures. Let's now put all of this together and work through an application of a CNN for image classification.\n\n### Data Preprocessing\n\nAs usual, our first task is data preparation and preprocessing. We will use a popular image classification dataset called CIFAR-10 hosted on Hugging Face *datasets*.\n\n{{< notebook file=\"cnn-get-data\" height=\"full\" >}}\n\nHere we have specified that we want the *training* split of the dataset with `split='train'`. We return *50K* images split across ten classes from this.\n\nMost CNNs can only accept images of a fixed size. To handle this, we will reshape all images to 32x32 pixels using `torchvision.transforms`; a pipeline built for image preprocessing.\n\n```python\nimport torchvision.transforms as transforms\n\n# image size\nimg_size = 32\n\n# preprocess variable, to be used ahead\npreprocess = transforms.Compose([\n    transforms.Resize((img_size,img_size)),\n    transforms.ToTensor()\n])\n```\n\nThe `preprocess` pipeline handles the height and width of our images but not the *depth*. Every image has a number of \"color channels\" that define its depth. For RGB images, there are three color channels; **r**ed, **g**reen, and **b**lue, whereas grayscale images have just one.\n\nDatasets commonly contain images with different color profiles, so we must convert them into a set format. We will use RGB, and as our images are all Python PIL objects, the color format is stored in an attribute called `mode`. The mode will be `RGB` for RGB images and `L` for grayscale images.\n\nWe perform the conversion to RGB and also perform the `preprocess` transformations like so:\n\n{{< notebook file=\"cnn-process-train-data\" height=\"full\" >}}\n\nLeaving us with *50,000* training examples, each a *3x32x32*-dimensional tensor. The tensors are normalized to a $[0, 1]$ range by the `transforms.ToTensor()` step.\n\nRight now, this normalization does not consider the pixel values of our overall set of images. Ideally, we should normalize by the mean and standard deviation values specific to this dataset. For this dataset, these are:\n\n```python\nmean = [0.4670, 0.4735, 0.4662]\nstd = [0.2496, 0.2489, 0.2521]\n```\n\nThis normalization step is applied by another `transformers.Compose` step like so:\n\n```python\npreprocess = transforms.Compose([\n    transforms.Normalize(mean=mean, std=std)\n])\n\nfor i in tqdm(range(len(inputs_train))):\n    # prepocessing\n    input_tensor = preprocess(inputs_train[i][0])\n    # replace with normalized tensor\n    inputs_train[i][0] = input_tensor\n```\n\nWe repeat the steps above for a test set that we will use for validating our CNN classifier performance. The validation set is also downloaded from Hugging Face datasets via `load_dataset` by switching the earlier `split` parameter to `'test'`:\n\n```python\ndataset_val = load_dataset(\n    'cifar10',\n    split='test',  # test set (used as validation set)\n    ignore_verifications=False  # set to True if seeing splits Error\n)\n```\n\n*The validation set must also be preprocessed, [find the code for it here](https://github.com/pinecone-io/examples/blob/master/learn/image-retrieval/cnn/cifar10.ipynb).*\n\nBoth train and validation splits are added into `DataLoader` objects. The data loaders shuffle, batch, and load data into the model during training or inference (validation).\n\n```python\nbatch_size = 64\n\n# add to dataloaders\ndloader_train = torch.utils.data.DataLoader(\n  \tinputs_train,\n    batch_size=batch_size,\n    shuffle=True\n)\n\ndloader_val = torch.utils.data.DataLoader(\n  \tinputs_val,\n    batch_size=batch_size,\n    shuffle=False\n)\n```\n\nWith that, our data is ready, and we can move on to building and then training our CNN.\n\n### CNN Construction\n\nWe can start building our CNN by creating a `ConvNeuralNet` class that will contain all our network layers and define the order of transformations through the network. The network will look like this:\n\n![code-cnn](./images/cnn-18.png)\n\n```python\n# creating a CNN class\nclass ConvNeuralNet(nn.Module):\n\t#  determine what layers and their order in CNN object \n    def __init__(self, num_classes):\n        super(ConvNeuralNet, self).__init__()\n        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, padding=1)\n        self.relu1 = nn.ReLU()\n        self.max_pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n\n        self.conv_layer2 = nn.Conv2d(in_channels=64, out_channels=192, kernel_size=4, padding=1)\n        self.relu2 = nn.ReLU()\n        self.max_pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n\n        self.conv_layer3 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, padding=1)\n        self.relu3 = nn.ReLU()\n        \n        self.conv_layer4 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1)\n        self.relu4 = nn.ReLU()\n\n        self.conv_layer5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n        self.relu5 = nn.ReLU()\n        self.max_pool5 = nn.MaxPool2d(kernel_size=3, stride=2)\n        \n        self.dropout6 = nn.Dropout(p=0.5)\n        self.fc6 = nn.Linear(1024, 512)\n        self.relu6 = nn.ReLU()\n        self.dropout7 = nn.Dropout(p=0.5)\n        self.fc7 = nn.Linear(512, 256)\n        self.relu7 = nn.ReLU()\n        self.fc8 = nn.Linear(256, num_classes)\n    \n    # progresses data across layers    \n    def forward(self, x):\n        out = self.conv_layer1(x)\n        out = self.relu1(out)\n        out = self.max_pool1(out)\n        \n        out = self.conv_layer2(out)\n        out = self.relu2(out)\n        out = self.max_pool2(out)\n\n        out = self.conv_layer3(out)\n        out = self.relu3(out)\n\n        out = self.conv_layer4(out)\n        out = self.relu4(out)\n\n        out = self.conv_layer5(out)\n        out = self.relu5(out)\n        out = self.max_pool5(out)\n        \n        out = out.reshape(out.size(0), -1)\n        \n        out = self.dropout6(out)\n        out = self.fc6(out)\n        out = self.relu6(out)\n\n        out = self.dropout7(out)\n        out = self.fc7(out)\n        out = self.relu7(out)\n\n        out = self.fc8(out)  # final logits\n        return out\n```\n\nAfter designing the network architecture, we initialize it — and *if* we have access to hardware acceleration (through CUDA or MPS), we move the model to that device.\n\n```python\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# set the model to device\nmodel = ConvNeuralNet(num_classes).to(device)\n```\n\nNext, we set the loss and optimizer functions used during training.\n\n```python\n# set loss function\nloss_func = nn.CrossEntropyLoss()\n# set learning rate \nlr = 0.008\n# set optimizer as SGD\noptimizer = torch.optim.SGD(\n    model.parameters(), lr=lr\n) \n```\n\nWe will train the model for 50 epochs. To ensure we're not overfitting to the training set, we pass the validation set through the model for *inference only* at the end of each epoch. If we see validation set performance suddenly degrade while train set performance improves, we are likely overfitting.\n\nThe training and validation loops are written like so:\n\n```python\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    model.train()\n\t\t# load in the data in batches\n    for i, (images, labels) in enumerate(dloader_train):  \n        # move tensors to the configured device\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # forward propagation\n        outputs = model(images)\n        loss = loss_func(outputs, labels)\n        \n        # backward propagation and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n    # at end of epoch check validation loss and acc\n    with torch.no_grad():\n      \t# switch model to eval (not train) model\n        model.eval()\n        correct = 0\n        total = 0\n        all_val_loss = []\n        for images, labels in dloader_val:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            total += labels.size(0)\n            # calculate predictions\n            predicted = torch.argmax(outputs, dim=1)\n            # calculate actual values\n            correct += (predicted == labels).sum().item()\n            # calculate the loss\n            all_val_loss.append(loss_func(outputs, labels).item())\n        # calculate val-loss\n        mean_val_loss = sum(all_val_loss) / len(all_val_loss)\n        # calculate val-accuracy\n        mean_val_acc = 100 * (correct / total)\n    print(\n        'Epoch [{}/{}], Loss: {:.4f}, Val-loss: {:.4f}, Val-acc: {:.1f}%'.format(\n            epoch+1, num_epochs, loss.item(), mean_val_loss, mean_val_acc\n        )\n    )\n```\n\nAfter training for 50 epochs, we should reach a validation accuracy of ~80%. We can save the model to file and load it again with the following:\n\n```python\n# save to file\ntorch.save(model, 'cnn.pt')\n# load from file and switch to inference mode\nmodel = torch.load('cnn.pt')\nmodel.eval()\n```\n\n### Inference\n\nNow that we have a fine-tuned CNN model let's look at how we can do image classification. We will use the same test set of CIFAR-10 for this step (ideally, we would not use the same data in our validation and test sets).\n\nFirst, we preprocess the images using the same `preprocess` pipeline as before and stack every processed image tensor into a single tensor batch.\n\n{{< notebook file=\"cnn-preprocess-testing\" height=\"full\" >}}\n\nWe process the tensors through the `model` and use an `argmax` function to retrieve the predictions. The predictions are all integer values, so we retrieve the textual names within the dataset `features`.\n\n{{< notebook file=\"cnn-predict\" height=\"full\" >}}\n\nNow let's loop through the predictions and view our results:\n\n{{< notebook file=\"cnn-results\" height=\"full\" >}}\n\nAlmost all predictions are correct, despite being very low-resolution images that many people might struggle to classify.\n\n---\n\nThat's it for this introduction to the long-reigning champions of computer vision; **C**onvolutional **N**eural **N**etworks (CNNs). We've worked through the intuition of convolutions, defined the typical network components, and saw how they were used to construct several of the best-performing CNNs.\n\nFrom there, we applied CNNs in practice. Building and training a network from scratch before testing it on the CIFAR-10 test set.\n\nGoing into the following chapters of the ebook, we will learn how CNNs are used in image retrieval and what may be the [successor of these models](/learn/vision-transformers/).\n\n{{< newsletter text=\"Subscribe for more on computer vision!\" inputText=\"Email address...\" buttonText=\"Submit\">}}\n\n## Resources\n\n[All Code Notebooks](https://github.com/pinecone-io/examples/tree/master/learn/image-retrieval/cnn)\n\n[1] A. Krizhevsky et al., [ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) (2012), NeurIPS\n\n[2] J. Brownlee, [How Do Convolutional Layers Work in Deep Learning Neural Networks?](https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/) (2019), Deep Learning for Computer Vision\n\n[3] J. Brownlee, [A Gentle Introduction to Pooling Layers for Convolutional Neural Networks](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/) (2019), Deep Learning for Computer Vision.\n\n[4] Y. LeCun, et. at., [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) (1998), Proc. of the IEEE.\n\n[5] K. Simonyan et al., [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556) (2014), CVPT\n\n[6] K. He et al., [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) (2015), CVPR."
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbed"
  },
  "filename": "cool-vendor-2021.md",
  "title": "post",
  "category": "\"Pinecone Recognized as a 2021 Gartner Cool Vendor\"",
  "content": "---\nlayout: post\ntitle: \"Pinecone Recognized as a 2021 Gartner Cool Vendor\"\nheadline: >-\n  Pinecone Recognized as a 2021 Gartner<sup><small>&reg;</small></sup> Cool Vendor\ncategories:\n  - Company News\ntoc: >-\nauthor:\n  name: Bryan Turriff\n  position: Director of Product Marketing\n  src: /images/company-bryan.jpeg\n  href: https://www.linkedin.com/in/bryanturriff/\ndate: \"2021-10-28\"\n# Date: October 28, 2021\n# Open Graph\ndescription: Pinecone created the category of Vector Databases to bring the power of vector similarity search to all companies. We are excited today to announce that Pinecone has been named a Gartner Cool Vendor in the October 2021 Gartner Cool Vendors&trade; in Data for Artificial Intelligence and Machine Learning.\n# No image in article, default will be used\nthumbnail: \"/images/gartner-thumbnail.jpg\"\n---\n\n[Pinecone](/) created the category of [Vector Databases](/learn/vector-database/) to bring the power of vector similarity search to all companies. We are excited today to announce that Pinecone has been named a Gartner Cool Vendor in the October 2021 [Gartner Cool Vendors&trade; in Data for Artificial Intelligence and Machine Learning](https://www.gartner.com/en/documents/4006842/cool-vendors-in-data-for-artificial-intelligence-and-machine-learning)&ast;.\n\nAccording to the report, “As AI and ML techniques become common in the enterprise, data is coming to the foreground. Data is what makes a difference in AI now. Data and analytics leaders want to improve the delivery of AI results with data innovations.” The report also noted that “AI teams are expanding their focus from model development to data that makes these models effective. Many of them are unaware of the proven data management solutions and are looking for AI-specific data offerings to improve and simplify their data-related efforts.”\n\nVector search can be more accurate and intuitive than traditional keyword search methods, which require the user to make guesses about how data is structured. Before Pinecone, only a few tech giants had the engineering resources and budgets to build their own vector databases. Pinecone’s fully-managed vector database enables organizations of any size to quickly move similarity search and recommendation engines into production without tasking a large group of ML and database engineers to build and maintain one of their own.\n\nVector databases often require expensive infrastructures to operate and are notoriously difficult to manage. Pinecone solves both of these challenges with a solution that was built to efficiently store and query vector data within a platform that is easy to use.\n\n“We are honored to be recognized as a 2021 Gartner Cool Vendor which we believe is a powerful recognition of the value of vector databases and our work to expand AI-based search technology,” said Edo Liberty, Founder & CEO of Pinecone. “We introduced the vector database and we continue to work with our customers to ensure it powers the best search and recommendation experiences available.”\n\nGartner clients can [access the full report](https://www.gartner.com/en/documents/4006842-cool-vendors-in-data-for-artificial-intelligence-and-machine-learning).\n\n*&ast; Gartner, “Cool Vendors in Data for Artificial Intelligence and Machine Learning,” Svetlana Sicular, Chirag Dekate, Anthony Mullen, Arun Chandrasekaran, Afraz Jaffri, 13 October 13, 2021*\n\n### Gartner Disclaimer\n\n<small>GARTNER and COOL VENDORS are registered trademarks and service marks of Gartner, Inc. and/or its affiliates in the U.S. and internationally and are used herein with permission. All rights reserved.</small>\n\n<small>Gartner does not endorse any vendor, product or service depicted in its research publications, and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of Gartner’s Research & Advisory organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose.</small>"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbee"
  },
  "filename": "announcing-vector-database.md",
  "title": "post",
  "category": "\"Announcing the Pinecone Vector Database and $10M in Seed Funding\"",
  "content": "---\nlayout: post\ntitle: \"Announcing the Pinecone Vector Database and $10M in Seed Funding\"\nheadline: \"Announcing the Pinecone Vector Database and $10M in Seed Funding\"\ncategories:\n  - Company News\ntoc: >-\nauthor:\n  name: Edo Liberty\n  position: Founder and CEO\n  src: /images/company-edo.png\n  href: https://edoliberty.github.io/\ndate: \"2021-01-27\"\n# Date: January 27, 2021\n# Open graph\ndescription: Machine Learning (ML) represents everything as vectors, from documents, to videos, to user behaviors. This representation makes it possible to accurately search..\nimages: ['/images/pinecone-product-overview.png']\nthumbnail: \"/images/vector-database-thumbnail.jpg\"\n---\n\nToday we are launching the [Pinecone vector database](/) as a public beta, and announcing $10M in seed funding led by Wing Venture Capital.\n\n## The Problems and Promises of Vectors\n\nMachine Learning (ML) represents everything as [vectors](/learn/vector-embeddings/), from documents, to videos, to user behaviors. This representation makes it possible to accurately search, retrieve, rank, and classify different items by similarity and relevance. This is useful in many applications such as product recommendations, semantic search, image search, anomaly detection, fraud detection, face recognition, and many more.\n\nEdo Liberty led the creation of Amazon SageMaker at AWS, when he realized the main difficulty companies were facing in leveraging machine learning wasn’t in training or deploying models. The main difficulty was in working with large amounts of vector data in real-time.\n\nWhat’s so difficult about working with vector data? For starters, the vectors need to be stored and indexed somewhere. Also, the index needs to be updated every time the data is changed. Next, there needs to be a way to search the index and retrieve the most similar items. This is computationally intensive — especially if the results are needed in real-time — so it needs to run on a distributed compute system. Finally, this entire system needs to be operational which means it needs to be monitored and maintained.\n\nThere are many solutions that do this for columnar, JSON, document, and other kinds of data, but not for the dense, high-dimensional vectors used in ML and especially in Deep Learning. As a result, companies have been forced to either compromise on accuracy and speed of the application, or to build and maintain their own complex infrastructure for supporting vector data.\n\nIt was obvious to Edo this challenge would become widespread as companies launch or expand their AI/ML initiatives, so in 2019 he founded Pinecone and built the vector index &mdash; the core of the vector database.\n\n\n## Introducing the Vector Database\n\nPinecone is a managed database for working with vectors. It provides the infrastructure for ML applications that need to search and rank results based on similarity. With Pinecone, engineers and data scientists can build vector-based applications that are accurate, fast, and scalable, all with a simple API and zero maintenance.\n\n![Pinecone product overview](/images/pinecone-product-overview.png)\n\nThere are four components of the vector database:\n\n* The **vector index** provides blazing-fast [indexing](/learn/what-is-a-vector-index/) and efficient storage for high-dimensional vectors. It uses a proprietary nearest-neighbor search algorithm that is faster and more accurate than any open-source library. (Benchmarks will be published soon.)\n* **Container distribution** ensures exceptional performance regardless of scale, with dynamic load balancing, replication, name-spacing, sharding, and more.\n* The **API** enables updating and querying vector indexes from anywhere, including Jupyter notebooks. It is also used for managing artifacts such as models, indexes, and services.\n* **Managed operations** provide hands-free (for users) resource allocation, observability, SLA guarantees, security, and more.\n\nSince Pinecone is a fully managed service, there is no need to configure open-source software or set up and maintain any infrastructure.\n\nSee the [product overview](/product/) for a complete list of features.\n\n## Funding for Growth and Development\n\nIn addition to the product launch, we are also announcing that we raised $10M in seed funding led by [Wing Venture Capital](https://www.wing.vc/), whose founding partner Peter Wagner has joined our board. Peter is a visionary in the cloud, data, and machine learning spaces, as evidenced by his early investment in Snowflake. We can’t imagine a better partner for us than Peter, and we are beyond excited to have him onboard.\n\n## Try Pinecone or Join Us\n\nPinecone is available as a public beta starting today. [Try it free for 30 days.](https://www.pinecone.io/start/)\n\nFollowing the free trial, Pinecone comes with transparent, consumption-based pricing. Companies that require additional operational control, tighter security and governance, guaranteed performance and resilience, and 24/7 on-call operational support can [contact us](/contact/) to learn more and to see a demo.\n\nCompanies are only beginning to see the potential of machine learning, and we are excited to help them achieve that potential sooner. For any engineers also excited by this: [We are hiring!](/careers/)\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbef"
  },
  "filename": "dimensionality-reduction.md",
  "title": "post",
  "category": "Straightforward Guide to Dimensionality Reduction",
  "content": "---\nlayout: post\ntitle: Straightforward Guide to Dimensionality Reduction\nheadline: Straightforward Guide to Dimensionality Reduction\ncategories:\n  - Algorithms & Libraries\ntoc: >-\nweight: 12\nauthor:\n  name: Diego Lopez Yse\n  position: Data Scientist\n  src: /images/diego-lopez-yse.jpeg\n  href: \"https://lopezyse.medium.com/\"\n# Open Graph\ndescription: A look at dimensionality reduction and the key techniques like PCA, t-SNE, and UMAP.\nimages: ['/images/dimensionality-reduction-1.png']\n---\n\nThere is a golden rule in Machine Learning that states: **the more data, the better**. This rule is a double-edged sword. An indiscriminate addition of data might introduce noise, reduce model performance, and slow down its training process. In this case, more data can hurt model performance, so it’s essential to understand how to deal with it.\n\nIn Machine Learning, “**dimensionality**” refers to the number of features (i.e. input variables) in a dataset. These features (represented as columns in a tabular dataset) fed into a Machine Learning model are called predictors or “*p*”, and the samples (represented as rows) “*n*“. [Most Machine Learning algorithms assume](https://machinelearningmastery.com/how-to-handle-big-p-little-n-p-n-in-machine-learning/) that there are many more samples than predictors. Still, sometimes the scenario is exactly the opposite, a situation referred to as “**big-p, little-n**” (“p” for predictors, and “n” for samples).\n\nIn industries like [Life Sciences](/learn/ml-life-sciences/), data tends to behave exactly in this way. For example, by using [high throughput screening technologies](https://www.dominodatalab.com/blog/analyzing-large-p-small-n-data-examples-from-microbiome), you can measure thousands or millions of data points for a single sample (e.g., the entire genome, the amounts of metabolites, the composition of the microbiome). Why is this a problem? Because in high dimensions, the data assumptions needed for statistical testing are not met, and [several problems](https://www.dominodatalab.com/blog/the-curse-of-dimensionality) arise:\n\n- Data points move far away from each other in high dimensions.\n- Data points move far away from the center in high dimensions.\n- The distances between all pairs of data points become the same.\n- The accuracy of any predictive model approaches 100%.\n\nThis situation is referred to as “**the Curse of Dimensionality**”, which states that as data dimensionality increases, we can suffer from a significant impact on the implementation time of certain algorithms, make visualization extremely challenging, and make some Machine Learning models useless. A large number of dimensions in the feature space can mean that the [volume of that space is very large](https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/), and in turn, the points that we have in that space often represent a small and non-representative sample.\n\n![Dimensionality reduction diagram](/images/dimensionality-reduction-1.png)\n<small>With one dimension (top left), there are only ten possible positions. Therefore ten datum are required to create a representative sample that ‘covers’ the problem space. With two dimensions, there are 10² = 100 possible positions; therefore 100 datum are required to create a representative sample that ‘covers’ the problem space. With three dimensions, there are now 10³ = 1000 possible positions; therefore 1000 datum are required to create a representative sample that ‘covers’ the problem space. This exponential growth in the required number of datum continues to grow exponentially indefinitely. Source: [Turing Finance](http://www.turingfinance.com/artificial-intelligence-and-statistics-principal-component-analysis-and-self-organizing-maps/)</small>\n\nThe good news is that we can reduce data dimensionality to overcome these problems. The concept behind [dimensionality reduction](https://towardsdatascience.com/the-basics-of-data-prep-7bb5f3af77ac) is that high-dimensional data are dominated by a small number of simple variables. This way, we can find a subset of variables to represent the same level of information in the data or transform the variables into a new set of variables without losing much information.\n\n# Main Algorithms\n\nWhen facing high-dimensional data, it is helpful to reduce dimensionality by projecting the data to a lower-dimensional subspace that captures the data’s “essence.” There are several different ways in which you can achieve this algorithmically: PCA, t-SNE and UMAP.\n\n## Principal Component Analysis (PCA)\n\nPCA is a **linear** dimensionality reduction algorithm that helps us extract a new set of variables from an existing high-dimensional dataset. The idea is to reduce the dimensionality of a dataset while retaining as much variance as possible.\n\nPCA is also an **unsupervised** algorithm that creates linear combinations of the original features, called principal components. **Principal components** are learned in such a way that the first principal component explains maximum variance in the dataset, the second principal component tries to explain the remaining variance in the dataset while being uncorrelated to the first one, and so on.\n\nInstead of simply choosing useful features and discarding others, PCA uses a linear combination of the existing features in the dataset and constructs new features that are an alternative representation of the original data.\n\n![Dimensionality reduction diagram](/images/dimensionality-reduction-2.png)\n<small>The three original variables (genes) are reduced to a lower number of two new variables termed principal components (PCs). Left: Using PCA, we can identify the two-dimensional plane that optimally describes the highest variance of the data. This two-dimensional subspace can then be rotated and presented as a two-dimensional component space (right). Source: [nlpca.org](http://www.nlpca.org/pca_principal_component_analysis.html)</small>\n\nThis way, you might keep only as many principal components as needed to reach a cumulative explained variance of 85%. But why not keep all components? What happens is that [each additional component expresses less variance and more noise](https://books.google.com.ar/books?id=Nud4EAAAQBAJ&pg=PA35&lpg=PA35&dq=each+additional+component+expresses+less+variance+and+more+noise,+so+representing+the+data+with+a+smaller+subset+of+principal+components+preserves+the+signal+and+discards+the+noise.&source=bl&ots=RhXUvO099b&sig=ACfU3U29_6RH6GZNhwWDC44mTla7vv14zw&hl=en&sa=X&ved=2ahUKEwjf7sK0rvH6AhXvpZUCHQGaCyoQ6AF6BAgbEAM#v=onepage&q=each%20additional%20component%20expresses%20less%20variance%20and%20more%20noise%2C%20so%20representing%20the%20data%20with%20a%20smaller%20subset%20of%20principal%20components%20preserves%20the%20signal%20and%20discards%20the%20noise.&f=false), so representing the data with a smaller subset of principal components preserves the signal and discards the noise.\n\nPCA increases interpretability while minimizing information loss. It can be used to find the most significant features in a dataset and allows data visualization in 2D and 3D. At the same time, PCA is most suitable when variables have a linear relationship among them (it won’t be able to capture more complex relationships) and is susceptible to significant outliers.\n\n![Dimensionality reduction example](/images/dimensionality-reduction-3.png)\n<small>Example of dimensionality reduction of linear and nonlinear data by PCA. The same 2D points can be mapped onto a 3D space using a linear transformation (rotation) or a nonlinear transformation (spiral). When PCA is applied to the 3D datasets, the resulting 2D visualizations are strikingly different. For the linearly-transformed data, PCA is able to entirely recover the structure of the original data. However, for the nonlinear dataset, the limitation of PCA to rigid rotation of the axes causes the loss of salient information about the original data. For the linear case, PC1 and PC2 cumulatively account for 100% of the total variation, whereas they account for only 75% of the total variation in the nonlinear case. Source: [ResearchGate](https://www.researchgate.net/publication/329160047_Dimensionality_Reduction_Techniques_for_Visualizing_Morphometric_Data_Comparing_Principal_Component_Analysis_to_Nonlinear_Methods)</small>\n\nYou can find a tutorial on calculating PCA in [this link](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf) and an example coded in Python [here](http://ethen8181.github.io/machine-learning/dim_reduct/PCA.html#PCA).\n\n## t-Distributed Stochastic Neighbour Embedding (t-SNE)\n\nCreated for high-dimensional [visualization purposes](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf), t-SNE is a **non-linear** dimensionality reduction algorithm. This algorithm tries to [maximize the probability](https://www.sciencedirect.com/science/article/pii/S2468502X22000201) that similar points are positioned near each other in a low-dimensional map while preserving longer-distance relationships as a secondary priority. It attracts data points that are nearest neighbors to each other and simultaneously pushes all points away from each other.\n\nContrary to PCA, t-SNE it’s not a deterministic technique but a **probabilistic** one. The idea behind it is to [minimize the divergence between two distributions](https://builtin.com/data-science/tsne-python): a distribution that measures pairwise similarities of the input objects, and a distribution that measures pairwise similarities of the corresponding low-dimensional points in the embedding. It looks to match both distributions to determine how to best represent the original dataset using fewer dimensions.\n\nMore specifically, [t-SNE uses](http://theprofessionalspoint.blogspot.com/2019/03/what-is-t-sne-how-does-it-work-using-t.html) a **normal distribution** (in the higher dimension) and a **t-Distribution** (in the lower dimension) to reduce dimensionality. t-Distribution is a lot like a normal distribution, with the difference that it is not as tall as a normal distribution in the middle, but its tails are taller at the ends. The idea is to cluster data points in the lower dimension in a more sparse way to generate better visualizations.\n\n![t-distribution vs normal distribution](/images/dimensionality-reduction-4.jpg)\n<small>Why is t-Distribution used instead of normal distribution in lower dimensions? Because without it data clusters would clump up in the middle and will be harder to visualize. Source: [The Professionals Point](http://theprofessionalspoint.blogspot.com/2019/03/what-is-t-sne-how-does-it-work-using-t.html)</small>\n\nThere are several tuneable [hyperparameters](https://www.sciencedirect.com/science/article/pii/S2468502X22000201) to optimize t-SNE, like:\n\n- **Perplexity**, which controls the size of the neighborhood used for attracting data points.\n- **Exaggeration**, which controls the magnitude of attraction.\n- **Learning rate**, which controls the step size for the gradient descent that seeks to minimize the error. \n\nChanging these hyperparameters can deeply affect the accuracy and quality of t-SNE results.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/dimensionality-reduction-5.mp4\" type=\"video/mp4\">\n</video>\n\n<small class=\"video\">t-SNE can give us a fascinating projection of the latent space. In this example from the MNIST handwritten digits dataset, the 3D projection shows dense clusters where the same digits are close to one another. Source: [Data Science Dojo](https://online.datasciencedojo.com/blogs/curse-of-dimensionality-python)</small>\n\nt-SNE is an incredibly flexible algorithm that can find structure where others cannot. Unfortunately, it can be hard to interpret: after processing, the input features are no longer identifiable, and you cannot make any inference based only on the outputs.\n\nWhile being stochastic, multiple executions with different initializations will yield different results, and whatsmore, both its computational and memory [complexity](https://towardsdatascience.com/essential-programming-time-complexity-a95bb2608cac) are [O(n2 )](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf), which can make it **quite heavy on system resources**.\n\nFind [here](https://distill.pub/2016/misread-tsne/) an interactive explanation of t-SNE.\n\n## Uniform Manifold Approximation and Projection (UMAP)\n\n[UMAP](https://umap-learn.readthedocs.io/en/latest/index.html) is another **nonlinear** dimension-reduction algorithm that overcomes some of the limitations of t-SNE. It works similarly to t-SNE in that it tries to find a low-dimensional representation that preserves relationships between neighbors in high-dimensional space, but with an increased speed and better preservation of the data’s global structure.\n\nUMAP is a **non-parametric** algorithm that consists of two steps: (1) compute a fuzzy topological representation of a dataset, and (2) optimize the low dimensional representation to have as close a fuzzy topological representation as possible as measured by cross entropy.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/dimensionality-reduction-6.mp4\" type=\"video/mp4\">\n</video>\n\n<small class=\"video\">Dimensionality reduction applied to the Fashion MNIST dataset. 28x28 images of clothing items in 10 categories are encoded as 784-dimensional vectors and then projected to 3 using UMAP and t-SNE. Source: [Understanding UMAP](https://pair-code.github.io/understanding-umap/)</small>\n\nThere are [two main hyperparameters](https://pair-code.github.io/understanding-umap/) in UMAP that are used to control the balance between local and global structure in the final projection:\n\n- **The number of nearest neighbors**: which controls how UMAP balances local versus global structure - low values will push UMAP to focus more on the local structure by constraining the number of neighboring points considered when analyzing the data in high dimensions. In contrast, high values will push UMAP towards representing the big-picture structure, hence losing fine detail.\n- **The minimum distance between points in low-dimensional space**: which controls how tightly UMAP clumps data points together, with low values leading to more tightly packed embeddings. Larger values will make UMAP pack points together more loosely, focusing instead on the preservation of the broad topological structure.\n\nFine-tuning these hyperparameters can be challenging, and this is where UMAP's speed is a big advantage: by running it multiple times with a variety of hyperparameter values, you can get a better sense of how the projection is affected.\n\nCompared to t-SNE, UMAP presents several [advantages](https://www.learndatasci.com/tutorials/applied-dimensionality-reduction-techniques-using-python/) since it:\n\n- Achieves **comparable visualization performance** with t-SNE.\n- Preserves more of the **global data structure**. While the distance between the clusters formed in t-SNE does not have significant meaning, in UMAP the distance between clusters matters.\n- UMAP is **fast** and can **scale to Big Data**.\n- UMAP is not restricted for visualization-only purposes like t-SNE. It can serve as a general-purpose Dimensionality Reduction algorithm.\n\nYou can find an interactive explanation of UMAP [here](https://pair-code.github.io/understanding-umap/) and try different algorithms and datasets in the [TensorBoard Embedding Projector](https://projector.tensorflow.org/).\n\n# Why Reduce Dimensionality?\n\nThere’s one thing we can be certain about in Machine Learning: the future will bring more data. Whatsmore, Machine Learning models will continue to evolve to highly complex architectures. In that scenario, dimensionality reduction algorithms can bring huge benefits like:\n\n- Reducing storage needs for massive datasets.\n- Facilitating data visualizations by compressing information in fewer features.\n- Making Machine Learning models more computationally efficient.\n\nIn turn, these benefits translate into better model performance, increased interpretability, and improved data scalability. But of course, dimensionality reduction comes with data loss. No dimensionality reduction technique is perfect :  by definition, we’re [distorting the data to fit it into lower dimensions](https://pair-code.github.io/understanding-umap/). This is why it’s so critical to understand how these algorithms work and how they are used to effectively visualize and understand high-dimensional datasets.\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbf0"
  },
  "filename": "vector-search-basics.md",
  "title": "post",
  "category": "\"Vector Search for Developers",
  "content": "---\nlayout: post\ntitle: \"Vector Search for Developers: A Gentle Introduction\"\nheadline: \"Introduction to <span>Vector Search for Developers</span>\"\ncategories:\n  - Vector Search 101\ntoc: >-\nweight: 4\nauthor:\n    name: Greg Kogan\n    position: VP Marketing\n    src: /images/company-greg.png\n    href: https://www.linkedin.com/in/gkogan/\n\ndescription: A very gentle introduction to vector search and vector databases for developers.\n# Open Graph\nimages: ['/images/rise-of-vector-data-5.png']\n---\n\nThere’s a lot of excitement and opportunity presented by machine learning, but it can often feel like more trouble than it’s worth. Machine learning and, by extension, data science are incredibly complicated fields. As a developer, you may only have the bandwidth to explore these fields to the extent they bring solutions to your present problems. For many developers, the present problem is vector similarity search. The solution is Pinecone.\n\n[Pinecone](/) is a managed vector database that provides vector search (or \"similarity search\") for developers with a straightforward API and usage-based pricing. (And it's [free to try](https://app.pinecone.io).)\n\nWhile it may be encouraging to hear that a SaaS solution exists for your data science needs, you still might feel lost. What is vector search? How or why would you use it? In this article, we’ll walk through everything you need to get started with vector search using Pinecone. Let’s go!\n\n## Vector search… what’s that?\n\nIf you’ve ever used a `LIKE` query in SQL, then you’ve performed a very simple form of [vector search](/learn/what-is-similarity-search/), though it’s unlikely that any ML theorist or practitioner would say as much. In reality though, vector search is fundamentally the same exercise.\n\nWhen searching for similar text, there must be some kind of metric to check for overlap in the data. For text, this is simple: are the characters in the strings you’re searching over close to the ones you have in your search string? Depending on your background, you may have even heard technical terms like [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance) or [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance), which are precise metrics for describing the similarity (or dissimilarity) of strings.\n\nFor more complicated data sets, it’s also possible to make use of metrics like these. This is where vector search shines. In order to measure the distance between items in a particular data set, we need a programmatic way to quantify these things and their differences. Regardless of the types of objects we’re searching through, we use \"vectors\" or \"vector embeddings\" to convert the data we’re analyzing into simpler representations. Then, we check for similarity on those representations while still maintaining the deeper meaning of the objects themselves.\n\n## Vector embeddings?\n\nAt this point, we’ve defined an unfamiliar concept with an even more unfamiliar concept, but don’t be concerned. [Vector embeddings](/learn/vector-embeddings/) are really just a simplified numerical representation of complex data, used to make it easier to run generic machine-learning algorithms on sets of that data. By taking real-world objects and translating them to vector embeddings — numerical representations — those numbers can be fed into machine learning algorithms to determine [semantic similarity](learn/semantic-search/).\n\nFor example, let's consider the phrase \"one in a million.\" Is this phrase more similar to \"once in a lifetime\" or more similar to \"a million to one\"? You might have some intuition about which pairing is more similar. By creating a vector embedding for each of these phrases, machine learning goes beyond human intuition to generate actual metrics to quantify that similarity.\n\nIf you're thinking about how to get vector embeddings but coming from an SQL background, what might come to mind are [aggregate functions](https://www.postgresql.org/docs/9.5/functions-aggregate.html) which — if used correctly in cleverly crafted queries — could yield some sort of \"all of this data boils down to a single number\" result. At best, though, an SQL query can only perform really simple aggregations to distill out a final numerical representation.\n\nObtaining a vector embedding of *real* value requires some machine-learning techniques and a good understanding of the problem space.\n\nImage data, for example, is represented numerically based on the pixel values of each image. It already has the form of a vector embedding. But what about cases where obtaining a numerical representation isn't so straightforward? Let's consider some examples.\n\nIf we wanted to determine [similarity across movies](/docs/examples/movie-recommender/), we might look at people who had watched and rated the same movies, along with what other movies they had watched and rated. To find [product similarity](/docs/examples/product-recommendation-engine/), we could look at customer purchases and see who has bought other items in the same purchase.\n\nThese examples are too complex and cumbersome to be handled manually. This data needs to be fed into some kind of neural network to reduce the number of dimensions of these vectors (length of the list of numbers). The following diagram shows how this reduction might work for simple object examples:\n\n![Vector embedding examples](/images/rise-of-vector-data-5.png)\n\nDepending on the data you’re working with, there may be existing models you can use. But if you’re working with a unique dataset or use case, you’ll need to put some time into ensuring your model will capture your data well. For more generic situations — such as text data — we can use a more widely available model like [Word2vec](https://jalammar.github.io/illustrated-word2vec/). That model is trained against a wide collection of text data to determine similarities and differences between the concepts that real words represent.\n\nRegardless of the model you’re using, though, Pinecone can help you search through the generated vector embeddings to find similar items.\n\n## So what do I do with my vector embedding?\n\nOnce you have a vector embedding, you need to be able to run queries against it. This is where Pinecone comes in. Rather than requiring you to learn all kinds of techniques for searching through your data, Pinecone provides managed vector search. You store vector embeddings with IDs that tie your data back to the objects they represent, allowing you to search through that data with a straightforward API and client. A store of vector embeddings and their IDs is called a \"vector index.\"\n\nCreating a vector index is quite simple with Pinecone’s [Python client](/docs/quickstart-python/):\n\n```python\nimport pinecone\npinecone.init(YOUR_API_KEY)\nindex = pinecone.create_index(\"my-new-index\")\n```\n\n(Java and Go clients are coming soon.)\n\nYou’ll need to [get an API key](/start/) and choose a name for your index. There are other parameters — such as the metric measuring similarity between vectors searching your data — but when you’re just starting, the defaults should be good enough.\n\nOnce you have an index created, you can insert your data as a tuple containing the id and vector representation of each object in your data set. Pinecone is blazing fast, able to index 10,000 tuples or more in just a second.\n\nQuerying an index is very simple. You can perform a unary query with a single vector, or you can query with a list of vectors too. All you need is a vector or a list of vectors you want to find similarities for and how many results (`integer`) you want Pinecone to return:\n\n```python\nindex.query(queries=vectors, top_k=integer)\n```\n\nIn the above line, `top_k` is a reference to the [k-nearest neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) to any given vector. If you’re not familiar with that algorithm, Pinecone's usage is analogous to a `LIMIT` clause from SQL. All of the k-nearest neighbors analysis is taken care of by the platform! Pinecone will return IDs that match. It also returns a score that shows its confidence in the match.\n\n## Your turn!\n\nBy now, you should have a better understanding of what vector search is and how Pinecone can help you integrate vector search into your application. If you’re interested in learning more, try out some techniques from [our examples](/docs/examples/) and start playing around with Pinecone yourself.\n\nIf you find you’re ready to get going with Pinecone on your own product, don’t hesitate to [sign up](/start/) or [get in touch with us](/contact/). We’ll be happy to help you get started!\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbf1"
  },
  "filename": "data-augmentation.md",
  "title": "ebook-post",
  "category": "\"Data Augmentation with BERT\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Data Augmentation with BERT\"\nheadline: \"Making the Most of Data: Augmentation with BERT\"\ncategories:\n  - NLP for Semantic Search\ntoc: >-\nweight: 10\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: In-domain data augmentation using AugSBERT.\n# Open graph\nimages: ['/images/in-domain-augsbert-00.jpg']\n---\n\nMany of the most significant breakthroughs of AI and ML in the 2010s were theorized and described many decades ago. A few of the greatest ingredients to the AI gold-rush of the past decade are the perceptron (1958), backpropagation (1975), and (for NLP) recurrent neural networks (1986) [1, 2, 3].\n\nHow do seemingly obscure innovations from 1958, 1975, and 1986 become critical to a science and tech revolution in the 21st century?\n\nThey were not 'rediscovered'; nothing was ever *lost*. Instead, the world at the time was not ready.\n\nThe many innovations that spurred AI forward over the last decade were ahead of their time, and their success required a few missing ingredients. The AI age was missing compute and data.\n\n![flops_over_time](/images/in-domain-augsbert-01.jpg)\n<small>Compute power (FLOPS) over time using a logarithmic scale. Source [4].</small>\n\nNeural networks need many parameters to be effective. Look at some of the latest transformer models from the likes of OpenAI and Microsoft:\n\n![model_params](/images/in-domain-augsbert-02.jpg)\n<small>Model parameters over time, note the y-axis is using a log-scale.</small>\n\nModels have been getting bigger and bigger. With good reason, they perform *better*. Just how big they will get is anyone's guess, but size matters, and,until very recently, there was not enough compute to train even the smallest models.\n\n![data_vol_over_time](/images/in-domain-augsbert-03.jpg)\n<small>Estimated data volume (worldwide) measured in *zeta*bytes. Estimated values for 2021 are 79ZB compared to just 2ZB in 2010. Source [5]</small>\n\nThe second missing ingredient was data. ML models are *data-hungry*. They consume massive amounts of data to identify generalized patterns and apply those learned patterns to new data.\n\nAs models get bigger, so do datasets. And although we have seen an explosion of data in the past decade, it is often not accessible or in an ML-friendly format, especially in niche domains (such as [climate-claim data](https://huggingface.co/datasets/climate_fever)) or low resource languages (Dhivehi, Navajo, etc).\n\nSemantic search mostly requires the use of [sentence transformers](/learn/sentence-embeddings/). Thanks to the improvements to computing power, finding the compute to train these models is not usually an issue. But, they’re big models and finding enough of the right data? That is a problem.\n\nFor many niche, low-resource domains, finding or annotating a substantial dataset manually is practically impossible.\n\nWe can try [training without labeled data](/learn/unsupervised-training-sentence-transformers/) — but this only works for straight-forward semantic similarity tasks, and cannot produce as high-performing models as other supervised training methods (with labeled data).\n\nFortunately, we don't need to label (or even find) this new data. Instead, we can automatically generate or label data using one or more *data augmentation* techniques.\n\nIn this article, we will introduce data augmentation and its application to the field of NLP. We will focus on the 'in-domain' flavor of a particular data-augmentation strategy named augmented SBERT (AugSBERT).\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/3IPCEeh4xTg\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n## Data Augmentation\n\nData augmentation has been applied across the ML landscape and is not exclusive to NLP or sentence transformers. However, using data augmentation in NLP has proven to be a difficult task.\n\nAugmentation is better developed within the field of computer vision (CV). It is relatively straightforward to apply many transformations that retain the 'essence' of an image while modifying pixel values. Images can be rotated, color graded, lightened/darkened, morphed, and more.\n\nLanguage is more complex. It is more abstract and nuanced. Meaning is too easily corrupted by switching, replacing, deleting, or adding words. It is  easy to end up with nonsensical gibberish.\n\nThe AugSBERT training strategy requires that we generate new sentence pairs that are:\n\n1. Sensible and on-topic\n2. Semantically and grammatically correct.\n\nThere are several ways we can build these new pairs, and we can mix several techniques. By far, the simplest is to randomly sample new sentence pairs.\n\nGiven just three sentence pairs where each sentence is unique we can generate six new pairs:\n\n{{< notebook file=\"3-to-9-pairs\" height=\"full\" >}}\n\nAs the number of original *source* or *gold* pairs increases, so does the number of generated *silver* pairs. Given 1,000 pairs we can generate 1,000,000 pairs. We will see later in the article that this approach is all we need to create a dataset large enough for training a sentence transformer.\n\n*(We refer to gold data as the high-quality original data, whereas silver is artificially generated, and therefore lower quality data.)*\n\nThere are alternative techniques, and one of the more common options is the insertion or substitution of words. As mentioned, it is *hard* to do this without changing the meaning of a sentence, so we must approach insertion/substitution tactfully.\n\nThe [`nlpaug`](https://github.com/makcedward/nlpaug) library covers many data augmentation techniques for NLP, including insertion and substitution using word embeddings using embedding methods like *word2vec* and *fastText* to ensure we insert/substitute relevant words:\n\n![word_insert_sub](/images/in-domain-augsbert-04.jpg)\n<small>The original text (top) followed by a word2vec augmentation with insertion and substitution. Example  from nlpaug [6].</small>\n\nWe can even use *context* aware words embeddings with transformer models, including BERT, DistilBERT, RoBERTa, and XLNet.\n\n![bert_insert_sub](/images/in-domain-augsbert-05.jpg)\n<small>The original text (top) followed by a BERT augmentation with insertion and substitution. Example  from nlpaug [6].</small>\n\n`nlpaug` covers many other augmentation techniques, which are reviewed in-depth in the nlpaug GitHub repository [6]. However, we will be sticking with the random sampling strategy.\n\n\n\n## Augmented SBERT\n\nFine-tuning sentence transformer models require pairs of labeled data and lots of them.\n\nThe original SBERT was trained on 1M **n**atural **l**anguage **i**nference (NLI) pairs, that is, 1M sentence pairs labeled as being highly related (entailment), contradictory, or neutral [7]. More recent models like `flax-sentence-embeddings/all_datasets_v4_mpnet-base` were trained on more than 1B sentence pairs [8].\n\nWhen fine-tuning for a specific use case, it's unlikely that we'll find an existing and relevant dataset. That leaves us with two options:\n\n1. We manually annotate 100K+ sentence pairs\n2. We take 1-5K of existing (or manually annotated) sentence pairs and augment them with more data.\n\nUnder the assumption that you choose the latter option, how can we augment a dataset and generate realistic sentence pair labels? There are two steps: We use random sampling to create new pairs and then label them using a *cross-encoder*.\n\nWhen comparing the semantic similarity of sentence pairs, we are not limited to sentence transformers (*bi-encoders*). We can use cross-encoder models too.\n\n![cross_and_bi_encoder](/images/in-domain-augsbert-06.jpg)\n<small>A cross-encoder (left) is a single BERT inference step that takes both sentences as a single input and outputs a similarity score. Bi-encoders (right) perform an inference step for *each* sentence and output sentence vectors.</small>\n\nCross-encoders are more accurate than sentence transformers and require less data to train. However, this greater accuracy comes at a cost: Cross-encoders are *much slower*.\n\nWe must pass both sentence pairs to the cross-encoder, which outputs a similarity score.\n\nThe similarity score is often more accurate, but we had to perform a full cross-encoder (let's assume we're using BERT) inference step to get that single pairwise similarity.\n\nIf we need to search across a *'small'* dataset containing 1M sentences? We need to perform 1M full-inference computations. That is *slow*.\n\nClustering with cross-encoders is an even more inefficient quadratic complexity [7]. For clustering, we must compare every single pair. For all but the tiniest datasets, this is too slow to be usable in most use-cases.\n\nOn the other hand, sentence transformers require us to encode each sentence to produce a *sentence vector*. We will need to run 1M full inference computations on our first run to create these vectors, but once we have them, we store them in a database/index for quicker lookups.\n\nGiven a new sentence, performing a search across that same small dataset of 1M sentence *vectors* means we encode the new sentence (one inference computation) and then calculate the Euclidean/cosine similarity between that one vector and the 1M already indexed sentence vectors.\n\n1M cosine similarity computations are faster than 1M full BERT inference computations. Additionally, with sentence vectors, we can use **a**pproximate **n**earest **n**eighbors **s**earch (ANNS) to speed up the process even further.\n\nA BERT cross-encoder can take 65 hours to cluster 10K sentences. The equivalent process with SBERT takes *five seconds* [7].\n\n### When to Use AugSBERT\n\nWe will assume one of two scenarios:\n\n1. We have found an existing labeled dataset, but it is tiny,maybe just 1-5K pairs or\n2. We have an unlabeled dataset, but it is within reason to manually label (annotate) at least 1-5K pairs.\n\nWe have a small but annotated dataset. We could try fine-tuning a sentence transformer, but it is unlikely to perform well. Instead, we can turn to augmentation to enhance our dataset and improve the potential sentence transformer performance.\n\nThere are now the two steps we mentioned earlier to create this data. First, we generate more pairs, for which we will use random sampling. Second, we label that new data with a cross-encoder fine-tuned on the original (smaller) dataset.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/in-domain-augsbert-07.mp4\" type=\"video/mp4\">\n</video>\n\n<small class=\"video\">Random sampling is used to enlarge the number of sentence pairs in our dataset. After producing this larger dataset, we use the cross-encoder to label the new pairs.</small>\n\nBecause cross-encoders require fewer data and produce high-accuracy similarity scores, they're great for annotating our unlabeled data. With more labeled data, we can fine-tune better-performing sentence transformers.\n\nThe Augmented SBERT (AugSBERT) fine-tuning strategy is ideal for training with tiny datasets. Evaluation of the strategy showed improvements of up to 6% for in-domain tasks and up to 37% for domain adaption tasks [9].\n\nThe training process always begins with a *gold dataset*. Gold is our already labeled and (hopefully) high-quality data. If you can't get gold, the next best thing is silver. Likewise, the next best *'augmented data'* is named the *silver dataset*.\n\nWe feed the gold and unlabeled data into a BERT cross-encoder, producing our silver data. The SBERT bi-encoder is fine-tuned with this gold and silver data.\n\nAt a high level, this is how in-domain AugSBERT training works. Let's flesh out the details and work through an example.\n\n\n\n## In-Domain Walkthrough\n\nTo implement the in-domain AugSBERT training strategy, we need to have a small amount of labeled data within *the same* domain that we'd like to fine-tune our sentence transformer. We can then use this to generate more *in-domain* data.\n\n![in_domain_flow](/images/in-domain-augsbert-08.jpg)\n<small>Step-by-step process for fine-tuning a sentence transformer (bi-encoder) with the AugSBERT in-domain strategy.</small>\n\nWe can use this gold data to fine-tune the sentence transformer. The *problem* is that we do *not have enough data*. So, we must generate *more* data, or if there is already unlabeled data available, we can label that directly.\n\nWe will use the **S**emantic **T**extual **S**imilarity **b**enchmark (STSb) dataset. It is available via the 🤗 Datasets library. It contains just 5,749 pairs, very little for fine-tuning a sentence transformer model.\n\n{{< notebook file=\"stsb-download\" height=\"full\" >}}\n\nAfter normalizing the `label` feature to between 0 -> 1, each row in our dataset will look something like this:\n\n```json\n{\n'sentence1': 'A plane is taking off.',\n'sentence2': 'An air plane is taking off.',\n'label': 1.0, # this value will range from 0.0 -> 1.0\n'idx': 0\n}\n```\n\nWe have 5,749 pairs in the train set and 1,500 pairs in the validation (or *dev*) set. Fine-tuning with this core *gold* dataset produces a model that scores 0.506 using Spearman's correlation with those *dev set* labels, where 0.0 means no correlation and 1.0 is an exact match or perfect correlation.\n\nWe can improve this score using an in-domain AugSBERT training strategy, which begins by training a cross-encoder using this small *gold* dataset.\n\n### Fine-Tune Cross-Encoder\n\nBefore training, we must reformat our training to a list of `InputExamples` and use them to initialize a `DataLoader`.\n\n{{< notebook file=\"setup-train-set\" height=\"full\" >}}\n\nWe then initialize and fine-tune our cross encoder.\n\n{{< notebook file=\"bert-cross-encoder-fit\" height=\"full\" >}}\n\nThe number of warmup steps is 40% of the total training steps. It is high but helps prevent overfitting. The same could likely be achieved using a lower learning rate (the default is `2e-5`).\n\nEvaluation of the cross-encoder model on the dev set returns a correlation score of *0.578*.\n\n### Create Unlabeled Data\n\nThe cross-encoder is one half of the recipe for building a silver dataset, and the other half are the unlabeled sentence pairs. There are different strategies for generating this data, but one of the simplest and most effective is to randomly sample pairs from the gold data, creating new sentence pairs.\n\nFor this, we can transform the pairs from dataset objects to Pandas DataFrames, as these provide easy-to-use sampling methods.\n\n{{< notebook file=\"gold-load\" height=\"full\" >}}\n\nWe can then initialize a new `pairs` dataframe, loop through each unique sentence from the `sentence1` column and find new pairs from the `sentence2` column.\n\n{{< notebook file=\"random-sample\" height=\"full\" >}}\n\nFinally, we should drop any duplicates from the new `pairs` data.\n\n{{< notebook file=\"drop-dupes\" height=\"full\" >}}\n\nWith that, we have 27,180 unlabeled sentence pairs; the second half needed to build a *fully labeled* silver dataset.\n\n### Labeling the Silver Dataset\n\nWe generate label predictions for the unlabeled data using the cross-encoder that we trained. It is this cross-encoder-labeled data that we refer to as the *silver dataset*.\n\n![in_domain_training](/images/in-domain-augsbert-09.jpg)\n<small>In-domain training strategy with AugSBERT, source [9]. We feed **unlabeled pairs** into the fine-tuned cross-encoder to create the **silver dataset**.</small>\n\nEarlier we saved the cross-encoder to file in the local `bert-stsb-cross-encoder` directory. To load it from file we use:\n\n{{< notebook file=\"load-cross-encoder\" height=\"full\" >}}\n\nThen we `predict` new labels for our unlabeled data.\n\n{{< notebook file=\"create-labels\" height=\"full\" >}}\n\nWe now have both gold *and* silver datasets to fine-tune the sentence transformer, a total of `5_749 + 27_180 == 32_929` pairs. Now, we can  fine-tune the sentence transformer.\n\n### Fine-Tune Sentence Transformer\n\nBefore training, we need to merge the *silver* and *gold* datasets; with both as Pandas DataFrame objects, we use `append`. As before, we also transform our data into a list of `InputExample` objects and use them to initialize a `DataLoader`.\n\n```python\nall_data = gold.append(pairs, ignore_index=True)\n\n# format into input examples\ntrain = []\nfor _, row in all_data.iterrows():\n    train.append(\n        InputExample(\n            texts=[row['sentence1'], row['sentence2']],\n            label=float(row['label'])\n        )\n    )\n\n# initialize dataloader\nloader = DataLoader(\n    train, shuffle=True, batch_size=batch_size\n)\n```\n\nOur data is ready for training, so we initialize our model. The model consists of a core transformer model, in this case, `bert-base-uncased` from 🤗 Transformers. Following this, we have a *pooling layer*, which transforms the 512 token-level vectors into single sentence vectors. We will use the *mean* pooling method.\n\n```python\nfrom sentence_transformers import models, SentenceTransformer\n\n# initialize model\nbert = models.Transformer('bert-base-uncased')\npooler = models.Pooling(\n    bert.get_word_embedding_dimension(),\n    pooling_mode_mean_tokens=True\n)\nmodel = SentenceTransformer(modules=[bert, pooler])\n```\n\nWe must define a loss function to optimize on. We will use a cosine similarity loss function as we have similarity scores in the dataset.\n\n```python\nfrom sentence_transformers import losses\n\nloss = losses.CosineSimilarityLoss(model=model)\n```\n\nThen we begin training. We use the default learning rate and warmup for the first 15% of steps.\n\n```python\n# and training\nepochs = 1\n# warmup for first 15% of training steps\nwarmup_steps = int(len(loader) * epochs * 0.15)\n\nmodel.fit(\n    train_objectives=[(loader, loss)],\n    epochs=epochs,\n    warmup_steps=warmup_steps,\n    output_path='bert-stsb-aug'\n)\n```\n\nWith that, we have fine-tuned our STSb sentence transformer using the in-domain AugSBERT training strategy. To evaluate the model, we run a [small evaluation script](https://gist.github.com/jamescalam/133a83c32642ea27a9f648bcc9297003), which returns the correlation score with pairs from the STSb dev set.\n\n| Model | Score | Note |\n| ------------------------- | ----- | ------------------------------------------------------------ |\n| `bert-stsb-aug` | 0.691 | BERT-base uncased model fine-tuned on the gold *and* silver STSb data. |\n| `bert-stsb-gold` | 0.506 | BERT-base uncased model fine-tuned on the gold STSb data only (no dataset augmentation). |\n| `bert-stsb-cross-encoder` | 0.692 | BERT-base uncased cross-encoder model fine-tuned on the gold STSb data. Used to create the silver data. |\n\nWe return some incredible results for the sentence transformer fine-tuned using the AugSBERT strategy, returning almost 19% better performance than the model fine-tuned on the gold dataset only.\n\nThe paper introducing AugSBERT demonstrated performance increases of *up to* six points for in-domain tasks. So, it's worth assuming that the 19-point improvement here is *very high* and an atypical improvement. However, it shows just how good an AugSBERT training strategy can be.\n\n\n\nThat's it for fine-tuning using the in-domain Augmented SBERT strategy. The renaissance of ML we are currently witnessing may have been ignited by 50-70s research and enabled through massive advances in compute availability, but without the right data, we’re stuck.\n\nIt is with new techniques like AugSBERT that we are finally able to traverse the last mile and bridge those gaps in data.\n\nWe've introduced the idea of data augmentation in the field of NLP and how we can use insertion/substitution or simple random sampling techniques to generate new sentence pairs (e.g., the *silver dataset*).\n\nWe learned about cross encoders, bi-encoders, and how we can label our silver data using cross encoders.\n\nFinally, we fine-tuned our bi-encoder *or 'sentence transformer'* using gold and silver datasets and evaluated its performance against another bi-encoder trained without the AugSBERT strategy.\n\nWith this strategy, we can apply semantic similarity models to niche domains that have not been caught in the swell of data from the past decade.Demonstrating that AugSBERT can be a convenient approach to enhancing model performance in these domains.\n\n\n\n## References\n\n[1] F. Rosenblatt, [The Perceptron: A Probabilistic Model For Information Storage and Organization in the Brain](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf) (1958), PsycINFO\n\n[2] P. Werbos, [Beyond Regression: New Tools For Prediction and Analysis in the Behavioral Sciences](https://www.researchgate.net/publication/279233597_Beyond_Regression_New_Tools_for_Prediction_and_Analysis_in_the_Behavioral_Science_Thesis_Ph_D_Appl_Math_Harvard_University) (1974), Harvard University\n\n[3] D. Rumelhard, et al., [Learning Representations by Back-Propagating Errors](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf) (1986), Nature\n\n[4] [Top500 Supercomputer Leaderboards](https://www.top500.org)\n\n[5] A. Holst, [Volume of data/information created, copied, and consumed worldwide from 2010 to 2025](https://www.statista.com/statistics/871513/worldwide-data-created/) (2021), Statistica\n\n[6] E. Ma, [nlpaug](https://github.com/makcedward/nlpaug), GitHub\n\n[7] N. Reimers, I. Gurevych, [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084) (2019), EMNLP 2019\n\n[8] [Model Card for `all_datasets_v4_mpnet-base`](https://huggingface.co/flax-sentence-embeddings/all_datasets_v4_mpnet-base), HuggingFace Models\n\n[9] N. Thakur, et al., [Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks](https://arxiv.org/pdf/2010.08240.pdf) (2021), NAACL"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbf2"
  },
  "filename": "vector-embeddings-for-developers.md",
  "title": "post",
  "category": "\"Vector Embeddings for Developers",
  "content": "---\nlayout: post\ntitle: \"Vector Embeddings for Developers: The Basics\"\nheadline: \"<span>Vector Embeddings for Developers:</span> The Basics\"\ncategories:\n  - Vector Search 101\ntoc: >-\nweight: 4\nauthor:\n    name: Roie Schwaber-Cohen\n    position: Developer Advocate\n    src: /images/company-roie-s-c.jpeg\n    href: https://www.linkedin.com/in/roiecohen/\n\ndescription: A basic introduction to vector embeddings for developers.\n# Open Graph\nimages: ['/images/rise-of-vector-data-5.png']\n---\n\nYou might not know it yet, but vector embeddings are everywhere. They are the building blocks of many machine learning and deep learning algorithms used by applications ranging from search to AI assistants. If you’re considering building your own application in this space, you will likely run into vector embeddings at some point. In this post, we’ll try to get a basic intuition for what vector embeddings are and how they can be used.\n\n# What problem are we trying to solve?\n\nWhen you build a traditional application, your data structures are represented as objects that probably come from a database. These objects have properties (or columns in a database) that are relevant to the application you’re building.\n\nOver time, the number of properties of these objects grows — to the point where you may need to be more intentional about which properties you need to complete a given task. You may even end up creating specialized representations of these objects to solve particular tasks without paying the overhead of having to process very “fat” objects. This process is known as feature engineering — you optimize your application by picking only the essential features relevant to the task at hand.\n\nWhen you deal with **unstructured** data, you will have to go through this same feature engineering process. However, unstructured data is likely to have many more pertinent features, and performing manual feature engineering is bound to be untenable.\n\nIn those cases, we can use **vector embeddings** as a form of automatic feature engineering. Instead of manually picking the required features from our data, we apply a pre-trained machine learning model that will produce a representation of this data that is more compact while preserving what’s meaningful about the data.\n\n\n# What are vector embeddings?\n\nBefore we delve into what vector embeddings are, let’s talk about vectors. A **vector** is a mathematical structure with a size and a direction. For example, we can think of the vector as a point in space, with the “direction” being an arrow from (0,0,0) to that point in the **vector space.**\n\n<img src=\"/images/vector.png\" width=\"200\">\n\nAs developers, it might be easier to think of a vector as an array containing numerical values. For example:\n\n\n```\nvector = [0,-2,...4]\n```\n\nWhen we look at a bunch of vectors in one space, we can say that some are closer to one another, while others are far apart. Some vectors can seem to cluster together, while others could be sparsely distributed in the space.\n\n<img src=\"/images/multiple-vectors.png\" width=\"250\">\n\nWe’ll soon explore how these relationships between vectors can be useful.\n\nVectors are an ideal data structure for machine learning algorithms — modern CPUs and GPUs are optimized to perform the mathematical operations needed to process them. But our data is rarely represented as vectors. This is where vector embedding comes into play. It’s a technique that allows us to take virtually any data type and represent it as vectors.\n\nBut it isn’t as simple as just turning data into vectors. We want to ensure that we can perform tasks on this transformed data without losing the data’s original meaning. For example, if we want to compare two sentences — we don’t want just to compare the words they contain but rather whether or not they mean the same thing. To preserve the data’s **meaning**, we need to understand how to produce vectors where **relationships** between the vectors **make sense.**\n\nTo do this, we need what’s known as an **embedding model**. Many modern embedding models are built by passing a large amount of **labeled data** to a neural network. You might have heard of neural networks before — they are also a popular tool used to solve all sorts of complex problems. In very simple terms, neural networks are made of layers of nodes connected by functions. We then train these neural networks to perform all sorts of tasks.\n\nWe train neural networks by applying supervised learning —  feeding the network a large set of training data made of pairs of inputs and labeled outputs. Alternatively, we can apply self-supervised or unsupervised learning either of which doesn’t require labeled outputs. These values are transformed with each layer of network activations and operations. With every iteration of training, the neural network modifies the activations in each layer. Eventually, it can predict what an output label should be for a given input — even if it hasn’t seen that particular input before.\n\nThe embedding model is basically this neural network with the last layer removed. Instead of getting a specific labeled value for an input, we get a vector embedding.\n\nA great example of an embedding model is the popular [word2vec](https://en.wikipedia.org/wiki/Word2vec), which is regularly used for a wide variety of text-based tasks. Let’s take a look at a visualization produced by TensorFlow’s [projector](https://projector.tensorflow.org) tool, which makes it easy to visualize embeddings.\n\n![Embedding Visualization](/images/embedding-visualization.png)\n\n\nWhile this visualization represents only three dimensions of the embeddings, it can help us understand how the embedding model works. There are multiple data points highlighted in the visualization, each representing a vector embedding for a word. As the name suggests, word2vec embeds words. Words that appear close to one another are semantically similar, while far-apart words have different semantic meanings.\n\nOnce trained, an embedding model can transform our raw data into vector embeddings. That means it knows where to place new data points in the vector space.\n\n![Embedding Process](/images/embedding-process.png)\n\nAs we saw with word2vec, within the context of the model, vectors that are close together have a contextual similarity, whereas far-apart vectors are different from one another. That’s what gives our vector **meaning** — its relationship with other vectors in the vector space depends on how the embedding model “understands” the domain it was trained on.\n\n\n# What can I do with vector embeddings?\n\nVector embeddings are an incredibly versatile tool and can be applied in many domains. Generally speaking, an application would use a vector embedding as its query and produce other vector embeddings which are similar to it, with their corresponding values. The difference between applications of each domain is the significance of this similarity.\n\nHere are some examples:\n\n* Semantic Search - search engines traditionally work by searching for overlaps of keywords. By leveraging vector embeddings, [semantic search](https://www.pinecone.io/docs/examples/semantic-text-search/) can go beyond keyword matching and deliver based on the query's semantic meaning.\n* Question-answering applications - by training an embedding model with pairs of questions and corresponding answers, we can create an application that would [answer questions](https://www.pinecone.io/docs/examples/extractive-question-answering/) that have not been seen before.\n* Image search - vector embeddings are perfectly suited to serve as the basis for image retrieval tasks. There are multiple off-the-shelf models, such as [CLIP](https://www.pinecone.io/learn/clip/), ResNet, and more. Different models handle different types of tasks like [image similarity](https://www.pinecone.io/docs/examples/image-similarity-search/), object detection, and many more.\n* Audio search - by converting the audio into a set of activations (an audio spectrogram), we produce vector embeddings that can be used for [audio similarity search](https://www.pinecone.io/docs/examples/audio-search/).\n* Recommender Systems - we can create embeddings out of structured data that correlate to different entities such as [products](https://www.pinecone.io/docs/examples/product-recommendation-engine/), articles, etc. In most cases, you’d have to create your own embedding model since it would be specific to your particular application. Sometimes this can be combined with unstructured embedding methods when images or text descriptions are found.\n* Anomaly detection - We can create embeddings for anomaly detection using large data sets of labeled sensor information that [identify anomalous occurrences](https://www.pinecone.io/docs/examples/it-threat-detection/).\n\nVector embeddings are incredibly powerful, and this is by no means an exhaustive list — head to our [example apps section](https://www.pinecone.io/docs/examples/) to go deeper. You can also read more about the basics of [vector search](https://www.pinecone.io/learn/vector-search-basics/) to see how [Pinecone](https://www.pinecone.io/) can help you wrangle vector embeddings.\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbf3"
  },
  "filename": "genq.md",
  "title": "ebook-post",
  "category": "\"The Art of Asking Questions with GenQ\"",
  "content": "---\nlayout: ebook-post\ntitle: \"The Art of Asking Questions with GenQ\"\nheadline: \"Unsupervised Training of Retrievers Using GenQ\"\ncategories:\n  - NLP for Semantic Search\ntoc: >-\nweight: 12\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: Asymmetric semantic search with limited data using GenQ\n# Open graph\nthumbnail: \"/images/genq-0.jpg\"\n---\n\nFine-tuning effective dense retrieval models is challenging. Bi-encoders (sentence transformers) are the current best models for dense retrieval in semantic search. Unfortunately, they're also notoriously data-hungry models that typically require a particular type of labeled training data.\n\nHard problems like this attract attention. As expected, there is plenty of attention on building ever better techniques for training retrievers.\n\nOne of the most impressive is GenQ. This approach to building bi-encoder retrievers uses the latest text generation techniques to synthetically generate training data. In short, all we need are passages of text. The generation model then augments these passages with synthetic queries, giving us the exact format we need to train an effective bi-encoder model.\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/J0cntjLKpmU\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n## GenQ Method\n\nLet's work through the details of this training method. At a high level, there are two key steps.\n\n1. Generate queries for pre-existing but unlabeled passages: Creating (query, passage) pairs.\n2. Fine-tune a bi-encoder model using these (query, passage) pairs and **M**ultiple **N**egatives **R**anking (MNR) loss.\n\n![genq_overview](/images/genq-1.jpg)<small>High-level view of the *GenQ* training process.</small>\n\nDon't worry if any (or even all) of the above doesn't make sense. We'll detail everything from start to finish.\n\n### Unlabeled Passages\n\nWe can describe data as either being *in-domain* or belonging to another domain. The *domain* here refers to the target data and use-case where we apply the eventual fine-tuned bi-encoder model.\n\nFor example, we may want to build a retriever model that encodes sentences (passages) for financial documents in German. In that case, any text from German financial documents is *in-domain*, and everything else is out-of-domain.\n\n![in_and_out_domain](/images/genq-2.jpg)<small>For our target domain of *German financial documents*, anything that fits the topic and we would expect our model to encounter is *in-domain*. Anything else is out-of-domain.</small>\n\nTo achieve good performance with a language model (LM), we need to train (fine-tune) it on in-domain data. We would typically need *a lot* of *labeled* in-domain data to fine-tune a bi-encoder.\n\nFor most domains, we can either have *a lot* of ***un**labeled* data or *a little labeled* data. It's hard to get both, and most bi-encoder training needs both.\n\nGenQ aims to break the reliance on requiring labeled data by synthetically generating queries for otherwise unlabeled passages of text. Producing *(query, passage) pairs* from an unlabeled dataset. That means that given a large, in-domain, but unlabeled dataset, we can train with GenQ.\n\nThe task that GenQ is designed for is referred to as *asymmetric semantic search* [3]. That means the query is much shorter than the passage we would aim to retrieve. A typical query may consist of (for example) six words *\"How do I tie my shoelaces?\"*, and the relevant passage can be much longer:\n\n*\"To tie your shoelaces, take both laces and place one over the other, pulling them tightly together...\"*\n\n![search_asymmetry](/images/genq-3.jpg)\n<small>Asymmetric semantic search is where the length of queries are typically much smaller than that of the passages/contexts being searched.</small>\n\n\nIt is this task, with asymmetry between queries and passages, where GenQ can be applied.\n\n### Generation of Queries\n\nWe need passages and a query generation model to generate the (query, passage) pairs. The model used by GenQ is the **T**ext-to-**T**ext **T**ransfer **T**ransformer (T5).\n\nThe T5 model philosophy is that all NLP tasks can be defined as a *text-to-text* problem, so they are pretrained on many different tasks with vast amounts of data.\n\n![T5](/images/genq-4.jpg)\n<small>T5 views every task as a text-to-text problem. Here are a few examples adapted from the paper that introduced T5 [4].</small>\n\nOne of these tasks is query generation. In this case, the input text, or *passage*, is fed into a special query generation T5 model that generates questions that the passage may answer [2].\n\nGiven a large corpus of passages, such as paragraphs scraped from documentation, web pages, etc. We use T5 to generate several queries for each passage.\n\n![t5_query_gen](/images/genq-5.jpg)\n\n<small>Using a T5 model fine-tuned for query generation (like [BeIR/query-gen-msmarco-t5-large-v1](https://huggingface.co/BeIR/query-gen-msmarco-t5-large-v1)) we can generate sets of queries using passages of text.</small>\n\nIt's important to note that query generation is not perfect. We're using a general-purpose T5 model. The queries it generates can be noisy with plenty of randomness and nonsensical queries. Because of that, GenQ is prone to poor performance where the synthetic data is *too noisy* [1].\n\nWe have what should be a very large dataset of (query, passage) pairs. With this data, we can move on to fine-tuning the bi-encoder model.\n\n### Fine-Tuning the Bi-Encoder\n\nTo fine-tune the bi-encoder (sentence transformer) we use [**M**ultiple **N**egatives **R**anking (MNR) loss](/learn/fine-tune-sentence-transformers-mnr/). MNR loss is ideal for training where our dataset consists of pairs of related sentences.\n\nFor example, when training a QA retriever model, we can train with MNR loss if we have sets of (question, answer) pairs. If we have a **N**atural **L**anguage **I**nference (NLI) dataset, we can use MNR loss to train on (anchor, positive) pairs. In this case, we fine-tune on (query, passage) pairs.\n\nMNR loss works by placing all of these pairs into batches. For each batch, the model is optimized so that pair **(Q<sub>i</sub>, P<sub>j=i</sub>)** has the highest similarity. Meaning that within a batch of 32, the similarity score between **Q<sub>i=3</sub>** and **P<sub>j=3</sub>** must be higher than the similarity between **Q<sub>i=3</sub>** and any other passage **P<sub>j≠3</sub>**.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/genq-6.mp4\" type=\"video/mp4\">\n</video>\n\n<small class=\"video\">Similarity scores using five (query, passage) pairs. MNR loss optimizes so that **(Qi, Pi)** scores higher than any other pair **(Qi, Pj≠i)**</small>\n\nAt the end of this training process, we have a new bi-encoder fine-tuned to a specific domain. The model's performance can vary depending on the models being used, source and target domains, and many other variables. However, GenQ can sometimes achieve performances approaching models trained with supervised methods [1].\n\nLet's move on to the implementation of GenQ.\n\n## Implementation Walkthrough\n\nFirst, we need a dataset to train on. We will take the *context* paragraphs from the **S**tanford **Q**uestion and **A**nswering **D**ataset (SQuAD) dataset, which we will download from HuggingFace *Datasets*.\n\n{{< notebook file=\"genq-squad\" height=\"full\" >}}\n\nIn this dataset, we already have query `'question'` and passage `'context'` pairs. However, we want to emulate the scenario in which we do *not* have queries. We will remove all but the `'context'` data to do that.\n\n{{< notebook file=\"genq-passages\" height=\"full\" >}}\n\nNow that we have our `passages`, we can begin **generating queries**. For this, we need a query generation model. We will use a T5 model fine-tuned for query generation as part of the BeIR project, named `BeIR/query-gen-msmarco-t5-large-v1`.\n\n{{< notebook file=\"genq-init-model\" height=\"full\" >}}\n\nSome layers in the model behave differently during training and inference. To ensure the model is running in \"inference mode\", we call `model.eval()`.\n\n{{< notebook file=\"genq-generate\" height=\"full\" >}}\n\nWith this, the model will generate three queries for each passage. In this case, we generate *56,673* pairs from *18,891* passages and save them as TSV files.\n\nWe can see that the queries are generally *much smaller* than the passages; this is where the ***asymmetric** in asymmetric similarity search* comes from.\n\n{{< notebook file=\"genq-asymmetry\" height=\"full\" >}}\n\n<small>Example of a few generated queries given a paragraph about Python.</small>\n\nThe next step is to fine-tune a model using MNR loss. We do this easily with the `sentence-transformers` library.\n\nWe start by loading the pairs dataset we created into a list of `InputExample` objects.\n\n{{< notebook file=\"genq-input-examples\" height=\"full\" >}}\n\nNext, we load the pairs into a `NoDuplicatesDataLoader`. We use the *no duplicates* data loader to avoid placing duplicate passages in the same batch, as this will confuse the ranking mechanism of MNR loss.\n\n{{< notebook file=\"genq-loader\" height=\"full\" >}}\n\nNow we initialize the bi-encoder that we will be fine-tuning. We create the transformer-to-pooler architecture using *modules*.\n\n{{< notebook file=\"genq-biencoder\" height=\"full\" >}}\n\nHere we are initializing from a pretrained MPNet model, which by default outputs 512 embeddings. The second module is a mean pooling layer that takes the average activations across all of these embeddings to create a single sentence embedding.\n\nWith this, our bi-encoder is initialized. We now need to fine-tune the model, which we do using MNR loss.\n\n{{< notebook file=\"genq-loss\" height=\"full\" >}}\n\nEverything is now in place, and we fine-tune the model by calling the `fit` method.\n\n{{< notebook file=\"genq-finetune\" height=\"full\" >}}\n\nWe now have a fine-tuned bi-encoder that we can use for asymmetric semantic search. Let's move on to setting up a search index and testing a few searches to see what we return.\n\n## Evaluation\n\nFor evaluation, we will work through a simple qualitative test. We take a few example questions from the SQuAD validation set, and we will (hopefully) see that we are returning relevant contexts.\n\nWe can use Pinecone as an ultra-fast way to store our vectors. All we need is an [API key](https://app.pinecone.io/) and to install the Pinecone client with `pip install pinecone-client`. To initialize our connection to Pinecone and create an index to store the vectors we write:\n\n{{< notebook file=\"genq-init-index\" height=\"full\" >}}\n\nThe vector database will store all encoded contexts from the SQuAD validation set, so let's download, encode, and upsert our contexts.\n\nTo **download**, we use HuggingFace *Datasets* as before.\n\n{{< notebook file=\"genq-load-val\" height=\"full\" >}}\n\nWe can now **encode** using our newly trained `mpnet-genq-squad` model.\n\n{{< notebook file=\"genq-encode\" height=\"full\" >}}\n\nAnd finally **upsert** to Pinecone.\n\n{{< notebook file=\"genq-upsert\" height=\"full\" >}}\n\nWe're now ready to begin querying; we can take a few example queries from SQuAD.\n\n{{< notebook file=\"genq-query1\" height=\"full\" >}}\n\nWe immediately return the best possible answer as the highest rated passage. Let's try with some more SQuAD queries.\n\n{{< notebook file=\"genq-query2\" height=\"full\" >}}\n\nAnother great result; let's try one final query.\n\n{{< notebook file=\"genq-query3\" height=\"full\" >}}\n\nAll of these great results show that our model fine-tuned with GenQ has fit well to the SQuAD domain.\n\n\n\nThat's it for this chapter covering the *GenQ* training method, a clearly powerful approach to fine-tuning models where we have limited datasets.\n\nUsing this approach, we can take passages of text, generate (query, passage) pairs, and use these pairs to train effective bi-encoder models ideal for asymmetric semantic search.\n\nGenQ is an excellent, low-effort technique enabling projects that focus or rely on retrieving passages of text from natural language queries. Using GenQ you can begin fine-tuning models with limited data, unlocking previously inaccessible domains.\n\n## References\n\n[1] J. Ma, et al., [Zero-shot Neural Passage Retrieval via Domain-targeted Synthetic Question Generation](https://arxiv.org/abs/2004.14503) (2021), ACL\n\n[2] N. Reimers, [GenQ Page](https://www.sbert.net/examples/unsupervised_learning/query_generation/README.html), SBERT.net\n\n[3] N. Reimers, et. al., [Semantic Search Page](https://www.sbert.net/examples/applications/semantic-search/#symmetric-vs-asymmetric-semantic-search), SBERT.net\n\n[4] C. Raffel, et. al., [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) (2020), JMLR\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbf4"
  },
  "filename": "zero-shot-image-classification-clip.md",
  "title": "ebook-post",
  "category": "\"Zero-shot Image Classification with OpenAI's CLIP\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Zero-shot Image Classification with OpenAI's CLIP\"\nheadline: \"Zero-shot Image Classification with OpenAI's CLIP\"\ncategories:\n  - Embedding Methods for Image Search\ntoc: >-\nweight: 10\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: A deep dive on OpenAI's multi-modal CLIP for zero-shot image classification\n# Open graph\nimages: ['https://www.pinecone.io/images/zero-shot-image-classification-clip-0.png']\n---\n\nState-of-the-art (SotA) computer vision (CV) models are characterized by a *restricted* understanding of the visual world based on their training data [1].\n\nThese models can perform *very well* on specific tasks and datasets, but they do not generalize well. They cannot handle new classes or images beyond the domain they have been trained with.\n\nThis brittleness can be a problem for building niche image classification use-cases, such as defect detection in agriculture to identifying false banknotes to fight fraud. It can be extraordinarily hard to gather labeled datasets that are large enough to fine-tune CV models with traditional methods for those specialized use cases.\n\nIdeally, a CV model should learn the contents of images without excessive focus on the specific labels it is initially trained to understand. With an image of a dog, the model should understand that a dog is in the image. But it would be a lot more useful if it could also understand that there are trees in the background, that it is daytime, and that the dog is on a grassy field.\n\nUnfortunately, the result of classification training is the opposite. Models learn to push their internal representations of dogs into the same \"dog vector space\" and cats into the same \"cat vector space\". All that matters is the binary yes/no as to whether an image aligns with a class.\n\n![vector-space](./images/zero-shot-image-classification-clip-1.png)\n\nRetraining classification models is an option, but training requires significant time and capital investment for gathering a classification dataset *and* the act of model training itself.\n\nFortunately, OpenAI's CLIP has proved itself as an incredibly flexible classification model that often requires *zero* retraining. In this chapter, we will explore zero-shot image classification using CLIP.\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n   <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/98POYg2HZqQ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n## N-Shot Learning\n\nBefore diving into CLIP, let's take a moment to understand what exactly \"zero-shot\" is and its significance in ML.\n\nThe concept derives from N-shot learning. Here we define *N* as the number of samples required to train a model to begin making predictions in a new domain or on a new task.\n\nMany SotA models today are pretrained on vast amounts of data like ResNet or BERT. These pretrained models are then *fine-tuned* for a specific task and domain. For example, a ResNet model can be pretrained with [ImageNet](https://www.pinecone.io/learn/imagenet) and then fine-tuned for clothing categorization.\n\nModels like ResNet and BERT are called *\"many-shot\"* learners because we need *many* training samples to reach acceptable performance during that final fine-tuning step.\n\nMany-shot learning is only possible when we have compute, time, and data to allow us to fine-tune our models. Ideally, we want to maximize model performance while minimizing N-shot requirements.\n\nZero-shot is the natural *best-case scenario* for a model as it means we require *zero* training samples before shifting it to a new domain or task.\n\nCLIP may not be breaking SotA performance benchmarks on specific datasets. Still, it is proving to be a massive leap forward in zero-shot performance across various tasks in both image and text modalities.\n\n---\n\n*The point of CLIP is not SotA performance. However, it's worth noting that CLIP did beat the previous SotA results on the STL10 benchmark despite never being trained on that dataset.*\n\n---\n\nThe zero-shot adaptability of CLIP was found to work across many domains and different tasks. We will be talking about image classification in this article, but it can also be used in [multi-modal search/recommendation](https://www.pinecone.io/learn/clip), object detection, and likely many more as of yet unknown tasks.\n\n## How CLIP Makes Zero-Shot So Effective?\n\n**C**ontrastive **L**anguage-**I**mage **P**retraining (CLIP) is a primarily transformer-based model released by OpenAI in 2021 [1].\n\nCLIP consists of two models, as discussed in more depth in the [previous chapter](https://www.pinecone.io/learn/clip). The version of CLIP we use here consists of a text transformer for encoding text embeddings and a vision transformer (ViT) for encoding image embeddings.\n\n![clip-architecture-2](./images/zero-shot-image-classification-clip-2.png)\n\nBoth CLIP models are optimized during pretraining to align similar text and images in vector space. It does this by taking image-text pairs and pushing their output vectors nearer in vector space while separating the vectors of non-pairs.\n\n<center><div> <img src=\"./images/zero-shot-image-classification-clip-3.png\" alt=\"CLIP pretraining\" style=\"width:500px;\"/></div> </center>\n\nIt distinguishes itself from typical classification models for several reasons. First, OpenAI trained it on a **huge dataset of 400M text-image pairs** that were scraped from across the internet.\n\nThere are three primary benefits here:\n\n1. CLIP requires just **image-text pairs** rather than specific class labels thanks to the *contrastive* rather than *classification* focused training function. This type of data is abundant in today's social-media-centric world.\n2. The large dataset size means CLIP can build a strong understanding of general textual concepts displayed within images.\n3. Text descriptors often describe various features of an image, not just one. Meaning a more holistic representation of images (and text) can be built.\n\nThese benefits of CLIP are the primary factors that have led to its outstanding zero-shot performance.\n\nThe authors of CLIP draw a great example of this by comparing a ResNet-101 model trained specifically on ImageNet to CLIP when both are applied to other datasets derived from ImageNet.\n\n![clip-imagenet-comparison](./images/zero-shot-image-classification-clip-4.png)\n<small>ResNet-101 fine-tuned on ImageNet vs. zero-shot CLIP, source [1].</small>\n\nIn this comparison, we can see that despite ResNet-101 training for ImageNet, its performance on *similar* datasets is much worse than CLIP on the same tasks. CLIP outperforms a SotA model trained for ImageNet on slightly modified ImageNet tasks.\n\nWhen applying a ResNet model to other domains, a standard approach is to use a \"linear probe\". That is where the ResNet learned features (from the last few layers) are fed into a linear classifier that is then fine-tuned for a specific dataset. We would regard this as few to many-shot learning.\n\nIn the CLIP paper, linear probe ResNet-50 was compared to zero-shot CLIP. In one scenario, zero-shot CLIP outperforms linear probing across many tasks.\n\n<center><div> <img src=\"images/zero-shot-image-classification-clip-5.png\" alt=\"CLIP performance\" style=\"width:300px;\"/></div> </center>\n<small>Zero-shot CLIP performance compared to ResNet with linear probe, source [1].</small>\n\nDespite CLIP not being trained for these specific tasks, it outperforms a ResNet-50 with a linear probe. However, it's worth noting that zero-shot did not outperform linear probing when given more training samples.\n\n## Zero-Shot Classification\n\nHow exactly does CLIP do zero-shot classification? We know that the image and text encoder creates a 512-dimensional image and text vector that map to the same vector space.\n\nConsidering this vector space alignment, what if we wrote the dataset classes as text sentences?\n\nGiven a task where we must identify whether a photo contains a car, bird, or cat: we could create and encode three text classes:\n\n```\n\"a photo of a car\" -> T_1\n\"a photo of a bird\" -> T_2\n\"a photo of a cat\" -> T_3\n```\n\nEach of these *\"classes\"* are output from the text encoder as vectors $T_1$, $T_2$, and $T_3$ respectively. Given an photo of a cat, we encode it with the ViT model to create vector $I_1$. When we calculate the similarity of these vectors with cosine similarity, we expect $sim(T_3, I_1)$ to return the highest score.\n\n<center><div> <img src=\"images/zero-shot-image-classification-clip-6.png\" alt=\"Drawing\" style=\"width:500px;\"/></div> </center>\n<small>We reformat the one-word classes into sentences and encode them into label-based text vectors. When we encode and compare an image of a cat, we expect to share the highest similarity with *\"a photo of a cat\"*.</small>\n\nWe format the one-word classes into sentences because we expect CLIP saw more *sentence-like* text during pretraining. For ImageNet it was reported that a 1.3 percentage point improvement in accuracy was achieved using the same prompt template of `\"a photo of a {label}\"` \\[1\\].\n\nPrompt templates don't always improve performance and they should be tested for each dataset. For the dataset we use below, we found it to *reduce* performance by 1.5 percentage points.\n\n### Python Implementation\n\nLet's move on to an applied example of CLIP for zero-shot classification. We will use the `frgfm/imagenette` dataset via Hugging Face *Datasets*.\n\n{{< notebook file=\"clip-classification-dataset\" height=\"full\" >}}\n\nThe dataset contains *10* labels, all stored as integer values. To perform classification with CLIP we need the *text content* of these labels. Most Hugging Face datasets include the mapping to text labels inside the the dataset `info`:\n\n{{< notebook file=\"clip-classification-labels\" height=\"full\" >}}\n\nBefore we can compare labels and photos, we need to initialize CLIP. We will use the CLIP implementation found via Hugging Face transformers.\n\n{{< notebook file=\"clip-classification-init-model\" height=\"full\" >}}\n\nText transformers cannot read text directly. Instead, they need a set of integer values known as token IDs (or `input_ids`), where each unique integer represents a word or sub-word (known as a *token*).\n\nWe create these token IDs alongside another tensor called the *attention mask* (used by the transformer's attention mechanism) using the `processor` we just initialized.\n\n{{< notebook file=\"clip-classification-label-tokens\" height=\"full\" >}}\n\nUsing these transformer-readable tensors, we create the label text embeddings like so:\n\n{{< notebook file=\"clip-classification-get-text-features\" height=\"full\" >}}\n\nThe vectors that CLIP outputs *are not normalized*, meaning dot product similarity will give inaccurate results *unless* the vectors are normalized beforehand. We do that like so:\n\n{{< notebook file=\"clip-classification-normalization\" height=\"full\" >}}\n\n*(Alternatively, you can use cosine similarity)*\n\nAll we have left is to work through the same process with the images from our dataset. We will test this with a single image first.\n\n{{< notebook file=\"clip-classification-image-process\" height=\"full\" >}}\n\nAfter processing the image, we return a single (`1`) three-color channel (`3`) image width of `224` pixels and a height of `224` pixels. We must process incoming images to normalize and resize them to fit the input size requirements of the ViT model.\n\nWe can create the image embedding with:\n\n{{< notebook file=\"clip-classification-get-image-features\" height=\"full\" >}}\n\nFrom here, all we need to do is calculate the dot product similarity between our image embedding and the ten label text embeddings. The highest score is our predicted class.\n\n{{< notebook file=\"clip-classification-one-example\" height=\"full\" >}}\n\nLabel *2*, i.e., *\"cassette player\"* is our correctly predicted winner. We can repeat this logic over the entire `frgfm/imagenette` dataset to get the classification accuracy of CLIP.\n\n{{< notebook file=\"clip-classification-dataset-perf\" height=\"full\" >}}\n\nThat gives us an impressive zero-shot accuracy of *98.7%*. CLIP proved to be able to accurately predict image classes with little more than some minor reformating of text labels to create sentences.\n\n---\n\nZero-shot image classification with CLIP is a fascinating use case for high-performance image classification with minimal effort and zero fine-tuning required.\n\nBefore CLIP, this was not possible. Now that we have CLIP, it is almost *too* easy. The multi-modality and contrastive pretraining techniques have enabled a technological leap forward.\n\nFrom multi-modal search, zero-shot image classification, and object detection to industry-changing tools like OpenAI's Dall-E and Stable Diffusion, CLIP has opened the floodgates to many new use-cases that were previously blocked by insufficient data or compute.\n\n## Resources\n\n\\[1\\] A. Radford, et. al., [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) (2021)\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbf5"
  },
  "filename": "expel-alert-similarity.md",
  "title": "ebook-post",
  "category": "\"Detecting Similar Security Alerts at Expel\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Detecting Similar Security Alerts at Expel\"\nheadline: \"Detecting Similar Security Alerts at Expel\"\ncategories:\n  - Vector Search in the Wild\ntoc: >-\nweight: 2\nauthors:\n  - name: Dan Whalen\n    position: Principal Researcher, Expel\n    src: /images/dan-whalen.png\n    href: \"https://www.linkedin.com/in/dan-whalen/\"\n  - name: Peter Silberman\n    position: CTO, Expel\n    src: /images/peter-silberman.jpg\n    href: \"https://www.linkedin.com/in/petersilberman/\"\ndescription: \"How Expel built an alert similarity feature to help security analysts detect and remediate threats faster.\"\n# Open graph\nimages: [\"/images/expel-1.png\"]\nthumbnail: \"/images/expel-1.png\"\n---\n\n*Written by [Dan Whalen](https://www.linkedin.com/in/dan-whalen/) and [Peter Silberman](https://www.linkedin.com/in/petersilberman/) for [the Expel blog](https://expel.com/blog/how-we-built-it-alert-similarity/). Reposted with permission.*\n\nSince the beginning of our journey here at Expel, we’ve invested in creating processes and tech that set us up for success as we grow – meaning we keep our analysts engaged (and help them avoid burnout as best we can) while maintaining the level of service our customers have come to expect from us. \n\nOne of the features we recently built and released helps us do all of this: Alert Similarity. Why did we build it and how does it benefit our analysts and customers?\n\nHere’s a detailed look at how we approached the creation of Alert Similarity. If you’re interested in trying to develop a similar feature for your own security operations center (SOC), or learning about how to bring research to production, then read on for tips and advice.\n\n## Getting started \n\nIn our experience, it’s best to kick off with some research and experimentation – this is an easy way to get going and start identifying low-hanging fruit, as well as to find opportunities to make an impact without it being a massive undertaking.\n\nWe began our Alert Similarity journey by using one of our favorite research tools: a [Jupyter notebook](https://expel.com/blog/our-journey-jupyterhub-beyond/). The first task was to validate our hypothesis: we had a strong suspicion that new security alerts are similar to others we’ve seen in the past.\n\n![Expel Analysts Triage ~1000 Alerts/Day](/images/expel-1.png)\n\nTo test the theory, we designed an experiment in a Jupyter notebook where we:\n\n1. Gathered a representative sample set of alerts;\n2. Created vector embeddings for these alerts;\n3. Generated an n:n similarity matrix comparing all alerts; and\n4. Examined the results to see if our hypothesis held up.\n\nWe then gathered a sample of alerts over a few months (approximately 40,000 in total). This was a relatively easy task, as our platform stores security alerts and we have simple mechanisms in place to retrieve them regularly. \n\nNext, we needed to decide how to create vector embeddings. For the purposes of testing our hypothesis, we decided we didn’t need to spend a ton of time perfecting how we did it. If you’re familiar with generating embeddings, you’ll know this usually turns into a never-ending process of improvement. To start, we just needed a baseline to measure our efforts against. To that end, we chose MinHash as a quick and easy way to turn our selected alerts into vector embeddings.\n\n### What is MinHash and how does it work?\n\n[MinHash](https://en.wikipedia.org/wiki/MinHash) is an efficient way to approximate the [Jaccard Index](https://en.wikipedia.org/wiki/Jaccard_index) between documents. The basic principle is that the more data shared between two documents, the more similar they are. Makes sense, right?\n\nCalculating the true Jaccard index between two documents is a simple process that looks like this:\n\n```python\nJaccard Index = (Intersection of tokens between both documents) / (Union of tokens between both documents)\n```\n\nFor example, if we have two documents:\n\n1. The lazy dog jumped over the quick brown fox\n2. The quick hare jumped over the lazy dog\n\nWe could calculate the Jaccard index like this:\n\n```python\n(the, dog, jumped, over, quick) / (the, lazy, dog, jumped, over, quick, brown, fox, hare)\n→ 5 / 6\n→ 0.8333\n```\n\nThis is simple and intuitive, but at scale it presents a problem: You have to store all tokens for all documents to calculate this distance metric. In order to calculate the result, you inevitably end up using lots of storage space, memory, and CPU. \n\nThat’s where MinHash comes in. It solves the problem by approximating Jaccard similarity, yet only requires that you store a vector embedding of length K for each document. The larger K, the more accurate your approximation will be.\n\nBy transforming our input documents (alerts) into MinHash vector embeddings, we’re able to efficiently store and query against millions of alerts. This approach allows us to take any alert and ask, “What other alerts look similar to this one?” Similar documents are likely good candidates for further inspection.\n\n## Validating our hypothesis\n\nOnce we settled on our vectorization approach (thanks, MinHash!), we tested our hypothesis. By calculating the similarity between all alerts for a specific time period, we confirmed that 5-6% of alerts had similar neighbors (Fig 1.). Taking that even further, our metrics allowed us to estimate actual time savings for our analysts (Fig 2.).\n\n![Percentage of alerts with similar neighbors](/images/expel-fig1.png)\n<small>Fig. 1: Percentage of alerts with similar neighbors</small>\n\n![Estimated analyst hours saved](/images/expel-fig2.png)\n<small>Fig. 2: Estimated analyst hours saved (extrapolated)</small>\n\nThese metrics proved that we were onto something. Based on these results, we chose building an Alert Suggestion capability off the back of Alert Similarity as our first use case to target. This use case would allow us to improve efficiencies in our SOC and, in turn, enhance the level of service we provide to our customers.\n\n## Our journey to production\n\n### Step 1: Getting buy-in across the organization\n\nBefore moving full speed ahead into our project, we communicated our research idea and its potential benefits across the business. The TL;DR here? You can’t get your colleagues to buy into a new idea unless they understand it. Our R&D groups pride themselves on never creating “Tad-dah! It’s in production!” moments for Engineering or Product Management without them having the background on new projects first. \n\nWe created a presentation that outlined the opportunity and our research, and allowed Expletives (anyone from Product Management to Customer Success to Engineering) to review our proof of concept. In this case, we used a heavily documented notebook to walk viewers through what we did. We discussed our go-forward plan and made sure our peers across the organization understood the opportunity and were invested in our vision.\n\n### Step 2: Reviewing the design\n\nNext, we created a design review document outlining a high-level design of what we wanted to build. This is a standard process at Expel and is an important part of any new project. This document doesn’t need to be a perfect representation of what you’ll end up building, nor does it need to include every last implementation detail, but it does need to give the audience an idea of the problem you’re aiming to solve and the general architectural design of the solution you’re proposing.\n\nHere’s a quick look at the design we mocked up to guide our project:\n\n![Project Mock Up Design](/images/expel-3.png)\n\nAs part of this planning process, we identified the following goals and let those inform our design:\n\n- Build new similarity-powered features with little friction\n- Monitor the performance and accuracy of the system\n- Limit complexity wherever possible (don’t reinvent the wheel)\n- Avoid making the feature availability mission critical (so we can move quickly without risk)\n\nAs a result of this planning exercise, we concluded that we needed to build the following components:\n\n- Arnie (Artifact Normalization and Intelligent Encoding): A shared library to turn documents at Expel into vector embeddings\n- Vectorizor consumer: A worker that consumes raw documents and produces vector embeddings\n- Similarity API: A grpc service that provides an interface to search for similar documents\n\nWe also decided that we wouldn’t build our own vector search database and instead decided to use [Pinecone.io](/) to meet this need. This was a crucial decision that saved us a great deal of time and effort. (Remember how we said we wouldn’t reinvent the wheel?) \n\nWhy Pinecone? At this stage, we had a good sense for our technical requirements. We wanted sub-second vector search across millions of alerts, an API interface that abstracts away the complexity, and we didn’t want to have to worry about database architecture or maintenance. As we examined our options, Pinecone quicky became our preferred partner. We were really impressed by the performance we were able to achieve and how quick and easy their service was to set up and use.\n\n### Step 3: Implementing our Alert Similarity feature\n\t\nWe’re lucky to have an extremely talented core platform team here at Expel  infrastructure capabilities we can reliably build on. Implementing our feature was as simple as using these building blocks and best practices for our use case.\n\n### Release day\n\nOnce the system components were built and running in staging, we needed to coordinate a release in production that didn’t introduce risk into our usual business operations. Alert Suggestion would produce suggestions in Expel Workbench like this, which could inform decisions made by our SOC analysts. \n\n![Release Day](/images/expel-4.png)\n\nHowever, if our feature didn’t work as expected – or worse, created incorrect suggestions – we could cause confusion or defects in our process.\n\nTo mitigate these risks when moving to production, it was important to gather metrics on the performance and accuracy of our feature before we started putting suggestions in front of our analysts. We used [LaunchDarkly](https://launchdarkly.com/) and [Datadog](https://www.datadoghq.com/) to accomplish this. LaunchDarkly feature flags allowed us to deploy to production silently – meaning it runs behind the scenes and is invisible to end users. This allowed us to build a Datadog dashboard with all kinds of useful metrics like:\n\n- How quickly we’re able to produce a suggestion\n- The percentage of alerts we can create suggestions for\n- How often our suggestions are correct (we did this by comparing what the analyst did with the alert versus what we suggested)\n- Model performance (accuracy, recall, F1 score)\n- The time it takes analysts to handle alerts with and without suggestions\n\nTo say these metrics were invaluable would be an understatement. Deploying our feature silently for a period of time allowed us to identify several bugs and correct them without having any impact on our customers. This boosted confidence in Alert Similarity before we flipped the switch. When the time came, deployment was as simple as updating a single feature flag in LaunchDarkly.\n\n## What we’ve learned so far\n\nWe launched Alert Similarity in February 2022, and throughout the building process we learned (or in many cases, reaffirmed) several important things:\n\n### Communication is key. \n\nYou can’t move an organization forward with code alone. The time we spent sharing research, reviewing design documents, and gathering feedback was crucial to the success of this project.\n\n### There’s nothing like real production data. \n\nA silent release with feature flags and metrics allowed us to identify and fix bugs without affecting our analysts or customers. This approach also gave us data to feel confident that we were ready to release the feature. We’ll look to reuse this process in the future.\n\n### If you can’t measure it, you don’t understand it. \n\nThis whole journey from beginning to end was driven by data, allowing us to move forward based on a validated hypothesis and realistic goals versus intuition. This is how we knew our investment was worth the time and how we were able to prove the value of Alert Similarity \nonce it was live.\n\n## What’s next?\n\nAlthough we targeted suggestions powered by Alert Similarity as our first feature, we anticipate an exciting road ahead filled with additional features and use cases. We’re interested in exploring other types of documents that are crucial to our success and how similarity search could unlock new value and efficiencies. \n\nAdditionally, as we alluded to above, there’s always room for improvement when transforming documents into vector embeddings. We’re already exploring new ways to represent security alerts that improve our ability to find similar neighbors for alerts. We see a whole world of opportunities where similarity search can help us, and we’ll continue experimenting, building and sharing what we learn along the way.\n\nInterested in more engineering tips and tricks, and ideas for building your own features to enhance your service (and make your analysts’ lives easier?) [Subscribe to our blog](https://expel.com/blog/) to get the latest posts sent right to your inbox. \n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbf6"
  },
  "filename": "imagenet.md",
  "title": "ebook-post",
  "category": "\"AlexNet and ImageNet",
  "content": "---\nlayout: ebook-post\ntitle: \"AlexNet and ImageNet: The Birth of Deep Learning\"\nheadline: \"AlexNet and ImageNet: The Birth of Deep Learning\"\ncategories:\n - Embedding Methods for Image Search\ntoc: >-\nweight: 3\nauthors:\n - name: James Briggs\n   position: Developer Advocate\n   src: /images/james-briggs.jpeg\n   href: \"https://www.youtube.com/c/jamesbriggs\"\n - name: Laura Carnevali\n   position: Developer\n   src: /images/laura-carnevali.jpeg\n   href: \"https://www.linkedin.com/in/laura-carnevali-071a01b7/\"\ndescription: How ImageNet and AlexNet triggered the deep learning renaissance\n# Open graph\nimages: ['https://www.pinecone.io/images/imagenet-0.png']\n---\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n    <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/c_u4AHNjOpk\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\nToday's deep learning revolution traces back to the 30th of September, 2012. On this day, a **C**onvolutional **N**eural **N**etwork (CNN) called AlexNet won the ImageNet 2012 challenge [1]. AlexNet didn’t just win; it *dominated*.\n\nAlexNet was unlike the other competitors. This new model demonstrated unparalleled performance on the largest image dataset of the time, ImageNet. This event made AlexNet the first widely acknowledged, successful application of *deep learning*. It caught people's attention with a 9.8 percentage point advantage over the nearest competitor [2].\n\n<img src=\"./images/imagenet-11.png\" style=\"width:80%;display:block;margin-left:auto;margin-right:auto;\" />\n<small>The best ImageNet challenge results in 2010 and 2011, compared against all results in 2012, including AlexNet [2].</small>\n\nUntil this point, deep learning was a nice idea that most deemed as impractical. AlexNet showed that deep learning was more than a pipedream, and the authors showed the world *how* to make it practical. Yet, the surge of deep learning that followed was not fueled solely by AlexNet. Indeed, without the huge *ImageNet* dataset, there would have been no AlexNet.\n\n![ml-arxiv-papers](./images/imagenet-1.png)\n<small>Number of \"ML\" papers in ArXiv per year [3].</small>\n\nThe future of AI was to be built on the foundations set by the ImageNet challenge and the novel solutions that enabled the synergy between ImageNet and AlexNet.\n\n---\n\n## Fei-Fei Li, WordNet, and Mechanical Turks\n\nIn 2006, the world of computer vision was an underfunded discipline with little attention. Yet, many researchers were focused on building better models. Year after year saw progress, but it was slow.\n\nFei-Fei Li had just completed her Ph.D. in Computer Vision at Caltech [4] and started as a computer science professor at the University of Illinois Urbana-Champaign. During this time, Li noticed this focus on models and subsequent *lack of focus* on data.\n\nLi thought that the key to better model performance could be bigger datasets that reflected the diversity of the real world.\n\nDuring Li's research into datasets, she learned about professor Christiane Felbaum, a co-developer of a dataset from the 1980s called WordNet. WordNet consisted of many English-language terms organized into an ontological structure [5].\n\n<img src=\"./images/imagenet-2.png\" style=\"width:60%;display:block;margin-left: auto;margin-right: auto;\" />\n<small>Example of the ontological structure of WordNet [5].</small>\n\nIn 2007, Li and Felbaum met. Felbaum discussed her current work on adding a reference image to each word in WordNet. This inspired an idea that would shift the world of computer vision into hyperdrive. Soon after, Li put together a team to build what would become the largest image dataset of its time: ImageNet [6].\n\nThe idea behind ImageNet is that a large ontology of images – based on WordNet – could be the key to developing advanced, content-based image retrieval and understanding [7].\n\nTwo years later, the first version of ImageNet was released with 12 million images structured and labeled in line with the WordNet ontology. If one person had annotated one image/minute and did nothing else in those two years (including sleeping or eating), it would have taken 22 years and 10 months.\n\nTo do this in under two years, Li turned to Amazon Mechanical Turk, a crowdsourcing platform where anyone can hire people from around the globe to perform tasks cost-effectively.\n\nThe ImageNet team instructed \"Turkers\" to decide whether an image represents a given word (from the WordNet ontology) or not. Several measures were implemented to ensure accurate annotation, including having multiple Turker scores for each image-word pair [7].\n\nOn its release, ImageNet was the world's largest labeled dataset of images publically available. Yet, there was very little interest in the dataset. After being presented as a poster at the CVPR conference, they needed to find another way to stir interest.\n\n---\n\n## ImageNet\n\nWhen the paper detailing ImageNet was released in 2009, the dataset comprised 12 million images across 22,000 categories.\n\n![imagenet-paper](./images/imagenet-3.png)\n<small>Example ontologies from WordNet used by ImageNet [7].</small>\n\nAs it used WordNet's ontological structure, these images rolled up into evermore general categories.\n\nAt the time, a few other image datasets also used an ontological structure like ImageNet's. One of the better known of these was the **E**xtra **S**ensory **P**erception (ESP) dataset, which used a similar \"crowdsourcing\" approach but via the \"ESP game\". In this game, partners would try to match words to images, creating labels [8].\n\n![esp-imagenet](./images/imagenet-4.png)\n<small>The subtree for many terms were much larger and denser for ImageNet than the public subset of ESP [7].</small>\n\nDespite collecting a large amount of data, most of the dataset was not made public [8]. Of the 60K images that were, ImageNet offered much larger and denser coverage [7]. Additionally, ESP was found to be fundamentally flawed [9]. Beyond a relicensed version used for Google Image search [10], it did not impact the field of AI.\n\nThere was initially little interest in ImageNet or other similar datasets like ESP. At the time, very few people believed that the performance of models could be improved through more data.\n\nMost researchers dismissed the dataset as being too large and complex. In hindsight, this seems surprising. However, at the time, models struggled on datasets with 12 categories, so ImageNet's 22,000 categories must have seemed absurd.\n\n### ImageNet Challenge\n\nBy the following year, the ImageNet team managed to organize the **I**mageNet **L**arge **S**cale **V**isual **R**ecognition **C**hallenge (ILSVRC). Competitors had to correctly classify images and detect different objects and scenes across a *trimmed* list of 1,000 ImageNet categories. Every year, the team that produced the model with the lowest error rate won [2].\n\nThe scale of the dataset and competition resulted in ILSVRC becoming the primary benchmark in computer vision. Researchers realized that more data could be a good thing.\n\n2012 was not like the previous years. On September 30, 2012, the latest ILSVRC results were released. One model called AlexNet was clearly distinguished from the others [11].\n\n<img src=\"./images/imagenet-5.png\" style=\"width:70%;display:block;margin-left:auto;margin-right:auto;\" />\n<small>Results of ILSVRC 2012 [11].</small>\n\nAlexNet was the first model to score a sub-25% error rate. The nearest competitor scored 9.8 percentage points behind [1]. AlexNet dominated the competition, and they did it with a deep-layered **C**onvolutional **N**eural **N**etwork (CNN), an architecture dismissed by most as impractical.\n\n### Convolutional Neural Networks\n\nA CNN is a neural network model architecture that uses convolutional layers. These models are known today for their high performance on image data and minimal preprocessing or manual feature extraction requirements.\n\n![cnn](./images/imagenet-10.png)\n<small>Typical architecture of a CNN.</small>\n\nCNNs use several convolutional layers stacked on top of one another. The first layers can recognize simple features, like edges, shapes, and textures. As the network gets deeper, it produces more \"abstract\" representations, eventually identifying concepts from mammals to dogs and even Siberian huskies.\n\n---\n\n*Convolutional Neural Networks will be explained in more detail in the next chapter of [Embedding Methods for Image Search](https://www.pinecone.io/learn/image-search/).*\n\n---\n\nThese networks generally work best with many layers and large amounts of data, so they were overlooked. Shallow implementations lacked benefits over other networks, and deeper implementations were computationally unrealistic; the odds were stacked against these networks.\n\nDespite these potential challenges, the authors of AlexNet won ILSVRC by a 9.8 percentage point margin with one of these models. It turns out they were the right people in the right place at the right time.\n\nSeveral pieces came together for this to work. ImageNet provided the massive amounts of data required to train a deep CNN. A few years earlier, Nvidia had released CUDA, an API that enabled software access to highly-parallel GPU processing [12][13]. GPU power had reached a point where training AlexNet's 60 million parameters became practical with the use of multiple GPUs.\n\n## AlexNet\n\nAlexNet was by no means small. To make it work, the authors had to solve many problems. The model consisted of eight layers: five convolutional layers followed by three fully-connected linear layers. To produce the 1000-label classification needed for ImageNet, the final layer used a 1000-node softmax, creating a probability distribution over the 1000 classes.\n\n![AlexNet architecture](./images/imagenet-6.png)\n<small>Network architecture of AlexNet [1].</small>\n\nA key conclusion from AlexNet was that the depth of the network had been instrumental to its performance. That depth produced *a lot* of parameters, making training either impractically slow or simply impossible; if training on CPU. By training on GPU, training time could become practical. Still, high-end GPUs of the time were limited to ~3GB of memory, not enough to train AlexNet.\n\nTo make this work, AlexNet was distributed across two GPUs. Each GPU handled one-half of AlexNet. The two halves would communicate in specific layers to ensure they were not training two separate models.\n\n<img src=\"./images/imagenet-7.png\" style=\"width:50%;display:block;margin-left:auto;margin-right:auto;\" />\n<small>ReLU activation function.</small>\n\nTraining time was reduced further by swapping the standard sigmoid or tanh activation functions of the time for **Re**ctified **L**inear **U**nit (ReLU) activation functions.\n\n<img src=\"./images/imagenet-8.png\" style=\"width:70%;display:block;margin-left:auto;margin-right:auto;\" />\n<small>Results from a four-layer CNN with ReLU activation functions reached a 25% error rate on the CIFAR-10 dataset six times faster than the equivalent with Tanh activation functions [1].</small>\n\nReLU is a simpler operation and does not require normalization like other functions to avoid activations congregating towards min/max values (saturation). Nonetheless, another type of normalization called **L**ocal **R**esponse **N**ormalization (LRN) was included. Adding LRN reduced top-1 and top-5 error rates by 1.4% and 1.2% respectively [1].\n\nAnother critical component of AlexNet was the use of overlapping pooling. Pooling was already used by CNNs to summarize a group of activations in one layer to a single activation in the following layer.\n\n![overlapping-pooling](./images/imagenet-9.png)\n\nOverlapping pooling performs the same operation, but, as the pooling window moves across the preceding layer, it overlaps with the previous window. AlexNet found this to improve top-1 and top-5 error rates by 0.4% and 0.3%, respectively, and reduce overfitting.\n\n## AlexNet in Action\n\nWhile it's great to talk about all of this, it's even better to see it implemented in code. You can find the Colab notebook here, TK, if you'd like to follow along.\n\n### Data Preprocessing\n\nLet's start by downloading and preprocessing our dataset. We will use a small sample from ImageNet hosted on HuggingFace.\n\n{{< notebook file=\"imagenet-load-dataset\" height=\"full\" >}}\n\nThe `Maysee/tiny-imagenet` dataset contains 100K and 10K labeled images in the train and validation sets, respectively. All images are stored as Python PIL objects. Preprocessing of these images consists of several steps:\n\n* Convert all images to RGB format.\n* Resize to fit AlexNet's expected input dimensions.\n* Convert to tensor format.\n* Normalize values.\n* Stack this set of tensors into a single batch.\n\nWe start with RGB; AlexNet assumes all images will have three color channels (**R**ed, **G**reen, and **B**lue). But many other formats are supported by PIL, such as *L* (grayscale), RGBA, and CMYK. We must convert any non-RGB PIL objects into RGB format.\n\n{{< notebook file=\"imagenet-convert-rgb\" height=\"full\" >}}\n\nAlexNet, and many other pretrained models, expect input images to be tensors of dimensions (3 x H x W), where *3* represents the three color channels. H and W are expected to have a dimensionality of at least 224 [14]. We must resize our images; this is done easily using `torchvision.transforms`.\n\n{{< notebook file=\"imagenet-resize\" height=\"full\" >}}\n\nFinally, we must normalize the image tensors to a range of [0, 1] using `mean = [0.485, 0.456, 0.406]` and `std = [0.229, 0.224, 0.225]` as per the implementation notes on PyTorch docs [14].\n\n```python\n# resize and crop to 224x224\npreprocess = transforms.Compose([\n    transforms.ToTensor(),  # convert from PIL image to tensor before norm to avoid error\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# final result is normalized tensor of shape (3, 224, 224)\nnew_img = preprocess(new_img)\n```\n\nWe can combine all of this into a batch of `50` images. Rather than running preprocessing on our entire dataset and keeping everything in memory, we must process it in batches. For this example, we will only test the first 50 images as we know these should all be labeled as \"goldfish\".\n\n{{< notebook file=\"imagenet-preprocess-batch\" height=\"full\" >}}\n\nWe have preprocessed our first batch and produced tensors containing *50* input tensors of shape *(3, 224, 244)*, ready for inference with AlexNet.\n\n### Inference\n\nTo perform inference (e.g., make predictions) with AlexNet, we first need to download the model. We will download the pretrained AlexNet hosted by PyTorch.\n\n{{< notebook file=\"imagenet-alexnet-download\" height=\"full\" >}}\n\nWe can see the network architecture of AlexNet here with five convolutional layers followed by three feed-forward linear layers. This represented a more efficient modification of the original AlexNet and was proposed by Krizhevsky in a later paper [15].\n\nBy default, the model is loaded to the CPU. We can run it here, but running on a CUDA-enabled GPU or MPS on Apple Silicon is more efficient. We do this by setting the device like so:\n\n```python\n# move to device if available\ndevice = torch.device(\n    'cuda' if torch.cuda.is_available() else (\n        'mps' if torch.backends.mps.is_available() else 'cpu'\n    )\n)\n```\n\nFrom this, we must always move the input tensors and model to the device *before* performing inference. Once moved, we run inference with `model(inputs)`.\n\n{{< notebook file=\"imagenet-alexnet-inference\" height=\"full\" >}}\n\nThe model will output a set of logits (output activations) for each possible class. There are 1000 of these for every image we feed into the model. The highest activation represents the class that the model predicts for each image. We convert these logits into class predictions with an `argmax` function.\n\nMost of the predicted values belong to class `1`. That has no meaning for us, so we cross check this with the [PyTorch AlexNet classes](https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt) like so:\n\n{{< notebook file=\"imagenet-alexnet-classes\" height=\"full\" >}}\n\nClearly, the AlexNet model is predicting goldfish correctly. We can calculate the accuracy with:\n\n```python\nsum(preds == 1) / len(preds)\n```\n\nThis returns an accuracy of 72% for the goldfish class. A top-1 error rate of 28% beats the reported average error rate of 37.5% from the original AlexNet paper. However, this is only for a single class, and the model performance varies from class to class.\n\n---\n\nThat's our overview of one of the most significant events in computer vision and machine learning. The ImageNet Challenge was hosted annually until 2017. By then, 29 of 38 contestants had an error rate of less than 5% [16], demonstrating the massive progress made in computer vision during ImageNet's active years.\n\nAlexNet was superseded by even more powerful CNNs. Microsoft Research Asia dethroned AlexNet as the winner of ILSVRC in 2015 [17]. Since then, many more CNN architectures have come and gone. Recently, the use of another network architecture known as a [transformer](https://www.pinecone.io/learn/transformers/) has begun to disrupt CNNs domination of computer vision.\n\nThe final paragraph of the AlexNet paper proved almost prophetical for the future of AI and computer vision. They noted that they:\n\n<div style=\"padding: 1rem;\">\n    <em>\n    \"did not use any unsupervised pre-training even though we expect it will help\",\n    </em>\n    and\n    <em>\n    \"our results have improved as we have made our network larger... we still have many orders of magnitude to go in order to match the infero-temporal pathway of the human visual system\"\n    </em>\n</div>\n\nUnsupervised pre-training and ever larger models would later become the hallmark of ever better models.\n\n## Resources\n\n[1] A. Krizhevsky, I. Sutskever, G. Hinton, [ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) (2012), NeurIPS\n\n[2] [ImageNet Large Scale Visual Recognition Challenge (ILSVRC)](https://image-net.org/challenges/LSVRC/index.php), ImageNet\n\n[3] J. Dean, [Machine Learning for Systems and Systems for Machine Learning](http://learningsys.org/nips17/assets/slides/dean-nips17.pdf) (2017), NeurIPS 2017\n\n[4] F. Li, [Visual Recognition: Computational Models and Human Psychophysics](https://thesis.library.caltech.edu/2390/) (2005), Caltech\n\n[5] G. Miller, R. Beckwith, C. Felbaum, D. Gross, K. Miller, [Introduction to WordNet: An On-line Lexical Database](https://wordnetcode.princeton.edu/5papers.pdf) (1993)\n\n[6] D. Gershgorn, [The data that transformed AI research — and possibly the world](https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/) (2017), Quartz\n\n[7] J. Deng, W. Dong, R. Socher, L. Li, K. Li, L. Fei-Fei, [ImageNet: A large-scale hierarchical image database](https://ieeexplore.ieee.org/document/5206848) (2009), CVPR\n\n[8] L. Ahn, L. Dabbish, [Labeling images with a computer game](https://dl.acm.org/doi/10.1145/985692.985733) (2004), Proc. SIGCHI\n\n[9] I. Weber, S. Robertson, M. Vojnovic, [Rethinking the ESP Game](https://www.microsoft.com/en-us/research/publication/rethinking-the-esp-game/) (2009), ACM\n\n[10] A. Saini, [Solving the web's image problem](http://news.bbc.co.uk/1/hi/technology/7395751.stm) (2008), BBC News\n\n[11] O. Russakovsky, J. Deng, et. al., [ImageNet Large Scale Visual Recognition Challenge](https://arxiv.org/abs/1409.0575) (2015), IJCV\n\n[12] F. Abi-Chahla, [Nvidia's CUDA: The End of the CPU?](https://www.tomshardware.com/reviews/nvidia-cuda-gpu,1954.html) (2008), Tom's Hardware\n\n[13] A. Krizhevsky, [cuda-convnet](https://code.google.com/archive/p/cuda-convnet/) (2011), Google Code Archive\n\n[14] [AlexNet Implementation in PyTorch](https://pytorch.org/hub/pytorch_vision_alexnet/), PyTorch Resources\n\n[15] A. Krizhevsky, [One weird trick for parallelizing convolutional neural networks](https://arxiv.org/abs/1404.5997) (2014)\n\n[16] [ILSVRC2017 Results](https://image-net.org/challenges/LSVRC/2017/results) (2017)\n\n[17] K. He, X. Zhang, S. Ren, J. Sun, [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) (2015)"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbf7"
  },
  "filename": "batch-layer-normalization.md",
  "title": "post",
  "category": "\"Batch and Layer Normalization\"",
  "content": "---\nlayout: post\ntitle: \"Batch and Layer Normalization\"\nheadline: \"Build Better Deep Learning Models with Batch and Layer Normalization\"\ncategories:\n  - Algorithms & Libraries\ntoc: >-\nweight: 1\nauthor:\n  name: Bala Priya C\n  position: Technical Writer\n  src: /images/bala-priya.jpg\n  href: https://www.linkedin.com/in/bala-priya/\ndescription: \"Batch and layer normalization are two strategies for training neural networks faster, without having to be overly cautious with initialization and other techniques.\"\n# Open graph\nimages: [\"/images/batch-and-layer-normalization-strategies.png\"]\n---\n\n![Batch and layer normalization](/images/batch-and-layer-normalization-strategies.png)\n\nRecent advances in deep learning research have revolutionized fields like medical imaging, machine vision, and [natural language processing](https://www.pinecone.io/learn/nlp/). However, it's still challenging for data scientists to choose the optimal model architecture and to tune hyperparameters for best results.\n\nEven with the optimal model architecture, how the model is trained can make the difference between a phenomenal success or a scorching failure.\n\nFor example, take [weight initialization](https://cs231n.github.io/neural-networks-2/#init): In the process of training a neural network, we initialize the weights which are then updated as the training proceeds. For a certain random initialization, the outputs from one or more of the intermediate layers can be abnormally large. This leads to instability in the training process, which means the network will not learn anything useful during training.\n\nBatch and layer normalization are two strategies for training neural networks faster, without having to be overly cautious with initialization and other [regularization](https://cs231n.github.io/neural-networks-2/#reg) techniques.\n\nIn this tutorial, we’ll go over the need for normalizing inputs to the neural network and then proceed to learn the techniques of batch and layer normalization.\n\nLet's get started!\n\n## Why Should You Normalize Inputs in a Neural Network?\n\nWhen you train a neural network on a dataset, the numeric input features could take on values in potentially different ranges. For example, if you’re working with a dataset of student loans with the age of the student and the tuition as two input features, the two values are on totally _different_ scales. While the age of a student will have a median value in the range 18 to 25 years, the tuition could take on values in the range \\\\$20K - \\\\$50K for a given academic year.\n\nIf you proceed to train your model on such datasets with input features on different scales, you’ll notice that the neural network takes significantly longer to train because the gradient descent algorithm takes longer to converge when the input features are not all on the same scale. Additionally, such high values can also propagate through the layers of the network leading to the accumulation of large error gradients that make the training process unstable, called the problem of _exploding gradients_.\n\nTo overcome the above-mentioned issues of longer training time and instability, you should consider preprocessing your input data ahead of training. Preprocessing techniques such as normalization and standardization transform the input data to be on the same scale.\n\n### Normalization vs Standardization\n\nNormalization works by mapping all values of a feature to be in the range [0,1] using the transformation:\n\n$$x_{norm} = \\frac{x-x_{min}}{x_{max}-x_{min}}$$\n\nSuppose a particular input feature `x` has values in the range `[x_min, x_max]`. When `x` is equal to `x_min`, `x_norm` is equal to 0 and when `x` is equal to `x_max`, `x_norm` is equal to 1. So for all values of `x` between `x_min` and `x_max`, `x_norm` maps to a value between 0 and 1.\n\nStandardization, on the other hand, transforms the input values such that they follow a normal distribution with zero mean and unit variance (unit Gaussian). Mathematically, the transformation on the data points in a distrbution with mean μ and standard deviation σ is given by:\n\n$$x_{std} = \\frac{x-\\mu}{\\sigma}$$\n\nIn practice, this process of _standardization_ is also referred to as _normalization_ (not to be confused with the normalization process discussed above). As part of the preprocessing step, you can add a layer that applies this transform to the input features so that they all have a similar distribution. In Keras, you can add a [normalization layer](https://keras.io/api/layers/preprocessing_layers/numerical/normalization/) that applies this transform to the input features.\n\n## Need for Batch Normalization\n\nIn the previous section, we learned how we can normalize the input to the neural network in order to speed up training. If you look at the neural network architecture, the input layer is not the only input layer. For a network with hidden layers, the output of layer `k-1` serves as the input to layer `k`. If the inputs to a particular layer change drastically, we can again run into the problem of unstable gradients.\n\nWhen working with large datasets, you’ll split the dataset into multiple batches and run the mini-batch gradient descent. The [mini-batch gradient descent](https://d2l.ai/chapter_optimization/minibatch-sgd.html) algorithm optimizes the parameters of the neural network by batchwise processing of the dataset, one batch at a time.\n\nIt’s also possible that the input distribution at a particular layer keeps changing across batches. The seminal paper titled [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167) by Sergey Ioffe and Christian Szegedy refers to this change in distribution of the input to a particular layer across batches as _internal covariate shift_. For instance, if the distribution of data at the input of layer K keeps changing across batches, the network will take longer to train.\n\n_But why does this hamper the training process?_\n\nFor each batch in the input dataset, the mini-batch gradient descent algorithm runs its updates. It updates the weights and biases (parameters) of the neural network so as to fit to the distribution seen at the input to the specific layer for the current batch.\n\nNow that the network has learned to fit to the current distribution, if the distribution changes substantially for the next batch, it now has to update the parameters to fit to the new distribution. This slows down the training process.\n\nHowever, if we transpose the idea of _normalizing the inputs_ to the _hidden_ layers in the network, we can potentially overcome the limitations imposed by exploding activations and fluctuating distributions at the layer’s input. Batch normalization helps us achieve this, one mini-batch at a time, to accelerate the training process.\n\n## What is Batch Normalization?\n\nFor any hidden layer `h`, we pass the inputs through a non-linear activation to get the output. For every neuron (activation) in a particular layer, we can force the pre-activations to have zero mean and unit standard deviation. This can be achieved by subtracting the mean from each of the input features across the mini-batch and dividing by the standard deviation.\n\nFollowing the output of the layer `k-1`, we can add a layer that performs this normalization operation across the mini-batch so that the pre-activations at layer `k` are unit Gaussians. The figure below illustrates this.\n\n![Neural Network with Batch Normalization Layer](/images/batch-normalization-layer.png)\n<small>Section of a Neural Network with Batch Normalization Layer (Image by the author)</small>\n\nAs an example, let's consider a mini-batch with 3 input samples, each input vector being four features long. Here's a simple illustration of how the mean and standard deviation are computed in this case. Once we compute the mean and standard deviation, we can subtract the mean and divide by the standard deviation.\n\n![Batch Normalization Example](/images/batch-normalization-example.png)\n<small>How Batch Normalization Works - An Example (Image by the author)</small>\n\nHowever, forcing all the pre-activations to be zero and unit standard deviation across all batches can be too restrictive. It may be the case that the fluctuant distributions are necessary for the network to learn certain classes better.\n\nTo address this, batch normalization introduces two parameters: a scaling factor `gamma` (γ) and an offset `beta` (β). These are learnable parameters, so if the fluctuation in input distribution is necessary for the neural network to learn a certain class better, then the network learns the optimal values of `gamma` and `beta` for each mini-batch. The `gamma` and `beta` are learnable such that it’s possible to go back from the normalized pre-activations to the actual distributions that the pre-activations follow.\n\nPutting it all together, we have the following steps for batch normalization. If `x(k)` is the pre-activation corresponding to the k-th neuron in a layer, we denote it by `x` to simplify notation.\n\n\\begin{align}\n\\mu_b = \\frac{1}{B}\\sum_{i=1}^{B}x_i \\text{}\\text{ } (1)\\\\\\\\\n\\sigma_b^2 = \\frac{1}{B}\\sum_{i=1}^{B}(x_i - \\mu_b)^2 \\text{}\\text{ } (2)\\\\\\\\\n\\hat{x_i} = \\frac{x_i - \\mu_b}{\\sqrt{\\sigma_b^2}} \\text{}\\text{} (3)\\\\\\\\\nor\\text{ }\\hat{x_i} = \\frac{x_i - \\mu_b}{\\sqrt{\\sigma_b^2 + \\epsilon}} \\text{}\\text{ } (3) \\\\\\\\\nAdding\\text{ }\\epsilon\\text{ }helps\\text{ }when\\text{ }\\sigma_b^2\\text{ }is\\text{ }small\\\\\\\\\ny_i = \\mathcal{BN}(x_i) = \\gamma.x_i + \\beta \\text{}\\text{ }(4)\n\\end{align}\n\n### Limitations of Batch Normalization\n\nTwo limitations of batch normalization can arise:\n\n- In batch normalization, we use the *batch statistics*: the mean and standard deviation corresponding to the current mini-batch. However, when the batch size is small, the sample mean and sample standard deviation are not representative enough of the actual distribution and the network cannot learn anything meaningful. \n\n- As batch normalization depends on batch statistics for normalization, it is less suited for sequence models. This is because, in sequence models, we may have sequences of potentially different lengths and smaller batch sizes corresponding to longer sequences. \n\nLater, we’ll examine layer normalization, another technique that can be used for sequence models. For convolutional neural networks (ConvNets), batch normalization is still recommended for faster training.\n\n### How to Add a Batch Normalization Layer in Keras\n\nKeras provides a `BatchNormalization` class that lets you add a batch normalization layer wherever needed in the model architecture. For a complete review of the different parameters you can use to customize the batch normalization layer, refer to the [Keras docs for BatchNormalization](https://keras.io/api/layers/normalization_layers/batch_normalization/).\n\nThe code snippet below shows how you can add batch normalization layers to an arbitrary sequential model in Keras. You can choose to selectively apply batch normalization to specific layers in the network.\n\n```python\nimport keras\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, BatchNormalization\n\nmodel = Sequential([\n    Dense(units=10, input_shape=(1,4), activation='relu'),\n    # add batchnorm layer after activations in the previous layer\n    BatchNormalization(axis=1),\n    # pre-activations at the dense layer below are Gaussians\n    Dense(units=16, activation='relu'),\n    BatchNormalization(axis=1),\n    Dense(units=4, activation='softmax')\n])\n```\n\nIt’s important to understand how batch normalization works under the hood during training and testing. During training, batch normalization computes the mean and standard deviation corresponding to the mini-batch. \n\nHowever, at test time (inference time), we may not necessarily have a batch to compute the batch mean and variance. To overcome this limitation, the model works by maintaining a [moving average](https://mathworld.wolfram.com/MovingAverage.html) of the mean and variance at training time, called the moving mean and moving variance. These values are accumulated across batches at training time and used as mean and variance at inference time. \n\n## What is Layer Normalization?\n\n[Layer Normalization](https://arxiv.org/abs/1607.06450) was proposed by researchers Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. In layer normalization, all neurons in a particular layer effectively have the same distribution across all features for a given input. \n\nFor example, if each input has `d` features, it's a d-dimensional vector. If there are `B` elements in a batch, the normalization is done along the length of the d-dimensional vector and not across the batch of size `B`.\n\nNormalizing *across all features* but for each of the inputs to a specific layer removes the dependence on batches. This makes layer normalization well suited for sequence models such as [transformers](https://www.pinecone.io/learn/sentence-embeddings/) and [recurrent neural networks (RNNs)](https://www.ibm.com/cloud/learn/recurrent-neural-networks) that were popular in the pre-transformer era. \n\nHere’s an example showing the computation of the mean and variance for layer normalization. We consider the example of a mini-batch containing three input samples, each with four features.\n\n![Layer Normalization](/images/layer-normalization.png)\n<small>How  Layer Normalization Works - An Example (Image by the author)</small>\n\n\\begin{align}\n\\mu_l = \\frac{1}{d}\\sum_{i=1}^{d}x_i \\text{}\\text{ } (1)\\\\\\\\\n\\sigma_l^2 = \\frac{1}{d}\\sum_{i=1}^{d}(x_i - \\mu_l)^2 \\text{}\\text{ } (2)\\\\\\\\\n\\hat{x_i} = \\frac{x_i - \\mu_l}{\\sqrt{\\sigma_l^2}} \\text{}\\text{ } (3)\\\\\\\\\nor\\text{ }\\hat{x_i} = \\frac{x_i - \\mu_l}{\\sqrt{\\sigma_l^2 + \\epsilon}} \\text{}\\text{ } (3) \\\\\\\\\nAdding\\text{ }\\epsilon\\text{ }helps\\text{ }when\\text{ }\\sigma_l^2\\text{ }is\\text{ }small\\\\\\\\\ny_i = \\mathcal{LN}(x_i) = \\gamma.x_i + \\beta \\text{}\\text{ }(4)\n\\end{align}\n\nFrom these steps, we see that they’re similar to the steps we had in batch normalization. However, instead of the batch statistics, we use the mean and variance corresponding to specific input to the neurons in a particular layer, say `k`. This is equivalent to normalizing the output vector from the layer `k-1`.\n\n### How to Add a Layer Normalization in Keras\n\nSimilar to batch normalization, Keras also provides a `LayerNormalization` class that you can use to add layer normalization to the inputs of specific layers. The code cell below shows how you can add [layer normalization](https://keras.io/api/layers/normalization_layers/layer_normalization/) in a simple sequential model. The parameter `axis` specifies the axis along which the normalization should be done.\n\n```python\nimport keras\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, LayerNormalization\n\nmodel = Sequential([\n    Dense(units=16, input_shape=(1,10), activation='relu'),\n    LayerNormalization(axis=1),\n    Dense(units=10, activation='relu'),\n    LayerNormalization(axis=1),\n    Dense(units=3, activation='softmax')\n])\n```\n\nTo understand how layer normalization is used in transformers, consider reading this TensorFlow tutorial on [transformer models for language understanding](https://www.tensorflow.org/text/tutorials/transformer).\n\n## Batch Normalization vs Layer Normalization\n\nSo far, we learned how batch and layer normalization work. Let's summarize the key differences between the two techniques.\n\n- Batch normalization normalizes each feature independently across the mini-batch. Layer normalization normalizes each of the inputs in the batch independently across all features.\n- As batch normalization is dependent on batch size, it’s not effective for small batch sizes. Layer normalization is independent of the batch size, so it can be applied to batches with smaller sizes as well.\n- Batch normalization requires different processing at training and inference times. As layer normalization is done along the length of input to a specific layer, the same set of operations can be used at both training and inference times.\n\n## Final Thoughts\n\nIn this tutorial, you learned the basics of and differences between batch and layer normalization techniques and how to implement them in Keras.\n\nOver the past several years, batch normalization and layer normalization have emerged as the go-to normalization techniques in computer vision and natural language processing, respectively. In certain computer vision tasks, [group and instance normalization](https://www.tensorflow.org/addons/tutorials/layers_normalizations) are also used. For further reading, consider checking out the recommended resources in the section below. Happy learning!\n\n{{< newsletter text=\"Subscribe for more deep learning walkthroughs!\" inputText=\"Email address...\" buttonText=\"Submit\">}}\n\n## 📚 Recommended Reading\n\n[1] [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167), Sergey Ioffe and Christian Szegedy, 2015.\n\n[2] [Layer Normalization](https://arxiv.org/abs/1607.06450), Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton, 2016.\n\n[3] [How Does Batch Normalization Help Optimization?](https://arxiv.org/abs/1805.11604), Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry, NeurIPS 2018.\n\n[4] [PowerNorm: Rethinking Batch Normalization in Transformers](https://arxiv.org/abs/2003.07845), Sheng Shen, Zhewei Yao, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer, ICML 2020.\n\n[5] [Batch Normalization Layer in Keras](https://keras.io/api/layers/normalization_layers/batch_normalization/)\n\n[6] [Layer Normalization Layer in Keras](https://keras.io/api/layers/normalization_layers/layer_normalization/)\n\n[7] [Preprocessing: Normalization Layer in Keras](https://keras.io/api/layers/preprocessing_layers/numerical/normalization/)"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbf8"
  },
  "filename": "_index.md",
  "title": "\"Learning Center\"",
  "category": "learn",
  "content": "---\ntitle: \"Learning Center\"\nlayout: learn\nheadline: \"Learn to Love Working with <span>Vector Embeddings</span>\"\ndescription: \"Unlock the power of machine learning. Our guides will help you conquer vector embeddings and build better applications.\"\nseriesHeadline: Our Series\nseriesList:\n  - title: Natural Language Processing for Semantic Search\n    description: Learn how to build semantic search systems. From machine translation to question-answering.\n    image: \"/images/series-nlp-for-semantic-search.png\"\n    link: \"/learn/nlp/\"\n\n  - title: \"Faiss: The Missing Manual\"\n    description: Learn the essentials of vector search and how to apply them in Faiss.\n    image: \"/images/series-faiss-the-missing-manual.png\"\n    link: \"/learn/faiss/\"\n\n  - title: Vector Search in the Wild\n    description: Take a look at the hidden world of vector search and its incredible potential.\n    image: \"/images/series-vector-search-wild.png\"\n    link: \"/learn/wild/\"\n\n  - title: Embedding Methods for Image Search\n    description: Learn about the past, present, and future of image search, text-to-image, and more.\n    image: \"/images/series-image-search.png\"\n    link: \"/learn/image-search\"\n---\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbf9"
  },
  "filename": "vector-search-filtering.md",
  "title": "post",
  "category": "\"Filtering",
  "content": "---\nlayout: post\ntitle: \"Filtering: The Missing WHERE Clause in Vector Search\"\nheadline: \"The Missing WHERE Clause in Vector Search\"\ncategories:\n  - Algorithms & Libraries\ntoc: >-\nweight: 1\nauthor:\n  name: James Briggs\n  position: Developer Advocate\n  src: /images/james-briggs.jpeg\n  href: \"https://www.youtube.com/c/jamesbriggs\"\ndescription: Overview of pre-filtering, post-filtering, and single-stage filtering.\n#Open Graph\nimages: ['https://www.pinecone.io/images/vector-search-filtering-10.jpg']\n---\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/vector-search-filtering-1.mp4\" type=\"video/mp4\">\n</video>\n\nVector similarity search makes massive datasets searchable in fractions of a second. Yet despite the brilliance and utility of this technology, often what seem to be the most straightforward problems are the most difficult to solve. Such as filtering.\n\nFiltering takes the top place in being seemingly simple — but actually incredibly complex. Applying fast-but-accurate filters when performing a vector search (ie, nearest-neighbor search) on massive datasets is a surprisingly stubborn problem.\n\nThis article explains the two common methods for adding filters to vector search, and their serious limitations. Then we will explore [Pinecone’s solution](/) to filtering in vector search.\n\n<p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/H_kJDHvu-v8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>\n\n---\n\n## The Filter Problem\n\nIn [vector similarity search](https://www.pinecone.io/learn/what-is-similarity-search/) we build vector representations of some data (images, text, cooking recipes, etc), storing it in an *index* (a database for vectors), and then searching through that index with another *query vector*.\n\nIf you found this article through Google, what brought you here was a semantic search identifying that the [vector representation](https://www.pinecone.io/learn/vector-embeddings/) of your search query aligned with Google’s vector representation of this page.\n\nNetflix, Amazon, Spotify, and many others use vector similarity search to power their recommendation systems, using it to identify similar users and base their new show, product, or music recommendations on what these similar groups of people seem to like.\n\nIn search and recommender systems there is almost always a need to apply filters: Google can filter searches by category (like news or shopping) , date, or even language and region; Netflix, Amazon, and Spotify may want to focus on comparing users in similar geographies. Restricting the search scope to relevant vectors is — for many use-cases — an absolute *necessity* for providing a better customer experience.\n\nDespite the clear need, there has been no good approach for metadata filtering — restricting the scope of a vector search based on a set of metadata conditions — that’s both accurate *and* fast.\n\nWe may want to restrict our search to a category of records or a specific date range.\n\nFor example, say we want to build a semantic search application for a large corporation to search through internal documents. Users will often want to filter their search to a specific department. It *should* be as simple as writing this pseudo-code:\n\n```python\ntop-k where department == 'engineering'\nOR\ntop-k where department != 'hr'\n\ntop-k where {“department”: {\"$eq\": \"engineering\"}}\nOR\ntop-k where {“department”: {\"$ne\": \"hr\"}}\n```\n\nAnd what if we only want the last few weeks of data? Or need to see how the documents have changed over time — we would want to restrict the search to old records only!\n\n```python\ntop-k where date >= 14 days ago\nOR\ntop-k where date < 3 years ago\n\ntop-k where {“date”: {\"$gte\": 14}}\nOR\ntop-k where {“date”: {\"$lt\": 1095}}\n```\n\nThere are countless variations and combinations of queries just like these.\n\n```python\ntop-k where date >= 14 days ago AND department == 'finance'\n\ntop-k where {\"date\": {\"$gte\": 14}, “department”: {\"$eq\": \"finance\"}}\n```\n\nWithout filters, we restrict our ability to build powerful tools for search. Imagine Excel without data filters, SQL without a `WHERE` clause, or Python without `if...else` statements. The capabilities of these tools become massively diminished, and the same applies to vector search without filters.\n\nBut implementing such filters into a semantic/vector search application isn’t *easy*. Let’s first look at the two most common approaches — along with their advantages and disadvantages. They are **pre**-filtering and **post**-filtering.\n\n![Post and pre-filtering — note that for post-filtering, we search then apply the metadata filter. For pre-filtering, we apply the metadata filter then search.](/images/vector-search-filtering-10.jpg)\n\nThese approaches require *two* indexes: The *[vector index](https://www.pinecone.io/learn/vector-indexes/)* as per usual, and a *metadata index*. It is through this metadata index that we identify those vectors which satisfy our filter conditions.\n\n### Pre-Filtering\n\nThe first approach we could take is to pre-filter our vectors. This consists of taking our *metadata index* and applying our set of conditions. From here, we return a list of all records in our *vector index* that satisfies our filter conditions.\n\nNow when we search, we've restricted our scope — so our *vector similarity search* will be faster, and we'll only return relevant results!\n\nThe problem is that our pre-filter disrupts how ANN engines work (ANN requires the full index), leaving us with two options:\n\nBuild an ANN index for every possible filter.\nPerform a brute-force kNN search across remaining vectors.\n\nOption (1) is simply not practical, there are far too many possible filter outputs in a typical index.\n\nThat leaves us with option (2), creating the additional overhead of brute-force checking *every single* vector embedding remaining after the metadata filter.\n\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/vector-search-filtering-4.mp4\" type=\"video/mp4\">\n</video>\n\nPre-filtering is excellent for returning relevant results, but significantly slows-down our search due to the exhaustive, brute-force search that’s required.\n\nWhile slow for smaller datasets — the problem is exacerbated for larger datasets. This brute-force search is simply *not manageable* for datasets in the millions or billions.\n\nMaybe **post**-filtering will work better?\n\n### Post-Filtering\n\nWe can't rely on a brute-force search every time we apply a filter, so what if we applied the filter *after* our vector search?\n\nWorking through it, we have our vector index and a query — we perform a similarity search as usual. We return `k` of the top nearest matches. We'll set `k == 10`.\n\nWe've now got our top `10` best matches, but there's plenty of vectors in here that would *not* satisfy our filter conditions.\n\nWe go ahead and apply our post-search filter to remove those irrelevant results. That filter has removed *six* of our results, leaving us with just *four* results. We wanted ten results, and we’ve returned four. That's not so great.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/vector-search-filtering-6.mp4\" type=\"video/mp4\">\n</video>\n\nUnfortunately, it gets worse... What if we didn't return *any results* in the top `k` that satisfy our filter conditions? That leaves us with no results *at all*.\n\nBy applying our metadata filter post-search, we avoid the speed issues of an initial brute-force search. Still, we risk returning very few or even *zero* results, even though there may be relevant records in the dataset. So much for improving customer experience...\n\nWe can eliminate the risk of returning too few (or zero) results by increasing `k` to a high number. But now we have slower search times thanks to our excessively high `k` searches, and so we’re back to the slow search times of *pre*-filtering.\n\nSlow search with pre-filtering, or unreliable results with post-filtering. Neither of these approaches sounds particularly attractive, so what can we do?\n\n## Single-Stage Filtering\n\nPinecone's single-stage filtering produces the accurate results of pre-filtering at *even faster* speeds than post-filtering.\n\nIt works by *merging the vector and metadata indexes into a single index* — resulting in a single-stage filter as opposed to the two-stage filter-and-search method of pre- and post-filtering.\n\nThis gives us pre-filtering accuracy benefits *without being restricted* to small datasets. It can even increase search speeds whether you have a dataset of 1 million or 100 *billion*.\n\nExactly how this works will be covered in a later article. For now, let's explore some of the new single-stage filtering features and test its performance.\n\n### Testing the New Filtering\n\nFiltering in Pinecone is effortless to use. All we need are some vectors, metadata, and an [API key](https://www.pinecone.io/start/).\n\nIn this example we use the [Stanford Question and Answering Dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer/) for both English and Italian languages.\n\nThe SQuAD datasets are a set of paragraphs, questions, and answers. Each of those paragraphs is sourced from different Wikipedia pages, and the SQuAD data includes the topic title:\n\n```python\n{\n 'id': '573387acd058e614000b5cb5',\n 'title': 'University_of_Notre_Dame',\n 'context': 'One of the main driving forces in the growth of the University was its football team, the ... against the New York Giants in New York City.',\n 'question': 'In what year did the team lead by Knute Rockne win the Rose Bowl?',\n 'answers': {\n 'text': '1925',\n 'answer_start': 354\n }\n}\n```\n\nFrom this, we have *two* metadata tags:\n\n* `lang` — the source text's language (either `en` or `it`).\n* `title` — the topic title of the source text.\n\nWe pre-process the datasets to create a new format that looks like:\n\n```python\n{\n 'id': '573387acd058e614000b5cb5en',\n 'context': 'One of the main driving forces in the growth of the University was its football team, the ... against the New York Giants in New York City.',\n 'metadata': {\n 'lang': 'en',\n 'title': 'University_of_Notre_Dame'\n }\n}\n```\n\nFor Pinecone, we need three items: `id`, `vector`, and `metadata`. We have two of those, but we're missing our `vector` field.\n\nThe `vector` field will contain the dense vector embedding of the `context`. We can build these embeddings using models from `sentence-transformers`. \n\nWe will search in either English or Italian in our use case and return the most similar English/Italian paragraphs. For this, we will need to use a [multi-lingual vector embedding model](/learn/multilingual-transformers/). We can use the `stsb-xlm-r-multilingual` transformer model from `sentence-transformers` to do this.\n\nLet's go ahead and encode our `context` values:\n\n\n{{< notebook file=\"encode\" height=\"full\" >}}\n\nThe new format we produce includes our sample `id`, `text`, `vector`, and `metadata`, which we can go ahead and `upsert` to Pinecone:\n\n```python\n{\n 'id': '573387acd058e614000b5cb5en',\n 'vector': [0.1, 0.8, 0.2, ..., 0.7],\n 'metadata': {\n 'lang': 'en',\n 'title': 'University_of_Notre_Dame'\n }\n}\n```\n\nWe can add as many `metadata` tags as we'd like. They can even contain numerical values.\n\n#### Creating the Index\n\nNow, all we need to do is build our Pinecone index. We can use the Python client — which we install using `pip install pinecone`.\n\nUsing an [API key](https://www.pinecone.io/start/) we can initialize a new index for our SQuAD data with:\n\n\n{{< notebook file=\"create-index-1\" height=\"full\" >}}\n\nAnd we're ready to `upsert` our data! We'll perform the upload in batches of `100`:\n\n\n{{< notebook file=\"upsert-batch\" height=\"full\" >}}\n\nThat's it — our vectors and metadata are ready to go!\n\n#### Searching\n\nLet's start with an unfiltered search. We'll search for paragraphs that are similar to `\"Early engineering courses provided by American Universities in the 1870s\"`. We encode this using the `sentence-transformer` model, then query our index with `index.query`.\n\n{{< notebook file=\"encode-and-query\" height=\"full\" >}}\n\nWe're returning the three `top_k` (most similar) results — Pinecone returns our IDs and their respective similarity scores. As we have our contexts stored locally, we can map these IDs back to our contexts using a dictionary (`get_sample`) like so:\n\n\n{{< notebook file=\"get-sample\" height=\"full\" >}}\n\nThe first Italian paragraph reads:\n\n```\nThe College of Engineering was established in the 1920s, however, the first courses in civil and mechanical engineering have been a part of the College of Sciences since the 1870s. Today the college, housed in the Fitzpatrick, Cushing and Stinson-Remick Halls of Engineering, comprises five study departments - aerospace and mechanical engineering, chemical and biomolecular engineering, civil engineering and geological sciences, computer science and engineering, electronic engineering - with eight B. S bachelor's offers. In addition, the college offers five years of dual degree programs with the College of Arts and Letters and an additional B.\n```\n\nSo we're getting great results for both English *and* Italian paragraphs. However, what if we'd prefer to return English results *only*?\n\nWe can do that using Pinecone's single-stage filtering. Filters can be built using a set of operators that can be applied to *strings* and/or *numbers*:\n\n| Operator | Function | String | Number |\n| -------- | ------------------------ | ------ | ------ |\n| `$eq` | Equal to | ✅ | ✅ |\n| `$ne` | Not equal to | ✅ | ✅ |\n| `$gt` | Greater than | | ✅ |\n| `$gte` | Greater than or equal to | | ✅ |\n| `$lt` | Less than | | ✅ |\n| `$lte` | Less than or equal to | | ✅ |\n| `$in` | In array | ✅ | |\n| `$nin` | Not in array | ✅ | |\n\nWe're looking to filter based on if `lang` is *equal to* (`$eq`) `en`. All we do is add this filter condition to our `query`:\n\n{{< notebook file=\"lang-filter\" height=\"full\" >}}\n\nAnd we're now returning English-only paragraphs, with a search-time equal to (or even *faster*) than our unfiltered query.\n\nAnother cool filter feature is the ability to add multiple conditions. We could exclude the titles *'University_of_Kansas'* and *'University_of_Notre_Dame'* while retaining the `lang` filter:\n\n{{< notebook file=\"nin-filter\" height=\"full\" >}}\n\nNow we're returning English-only results that are *not* from the Universities of Kansas or Notre Dame topics.\n\nThere are also numeric filters. The SQuAD data doesn’t contain any numerical metadata — so we'll generate a set of random date values and `upsert` those to our existing vectors.\n\nFirst, we use a `date_maker` function to randomly generate our dates (we could use POSIX format too!), then we'll use the `Dataset.map` method to add `date` to our metadata. After this, we `upsert` as we did before.\n\n{{< notebook file=\"dates-metadata\" height=\"full\" >}}\n\nBefore we filter by date, let's see the dates we generated for our previous set of results:\n\n{{< notebook file=\"view-dates\" height=\"full\" >}}\n\nWe have dates from *2006* and *2016*. Let's swap this out for a specific date range of *2020* only.\n\n{{< notebook file=\"multi-filter\" height=\"full\" >}}\n\nWe've built some detailed metadata filters with multiple conditions and even numeric ranges. All with relentlessly fast search speeds. But we’re using a small dataset of 40K records, which means we’re missing out on one of the best features of single-stage filtering — let’s experiment with something bigger.\n\n#### Filtering and Search Speeds\n\nEarlier, we mentioned that applying the single-stage filter can speed up our search while still maintaining the accuracy of a pre-filter. This speedup exists with a small 40K dataset, but it's masked by the network latency.\n\nSo we're going to use a new dataset. The vectors have been randomly generated using `np.random.rand`. We're still using a vector size of `768`, but our index contains 1.2M vectors this time.\n\nWe will test the metadata filtering through a single tag, `tag1`, consisting of an integer value between `0` and `100`.\n\nWithout any filter, we start with a search time of 79.2ms:\n\n{{< notebook file=\"speedtest-no-filter\" height=\"full\" >}}\n\nUsing a greater than `$gt` condition and steadily increasing the filter scope — as expected, we return faster search times the more we restrict our search scope.\n\n{{< notebook file=\"speedtests\" height=\"full\" >}}\n\n![Filter speed test](/images/vector-search-filtering-9.jpg)\n*<small>As we increase our `tag1` `$gt` value, the search scope is reduced, making our search with the single-stage filter *even faster*.</small>*\n\nAnd that's it! We explored the flexible filter queries and tested the impressive search speeds of the single-stage filter!\n\n---\n\nThanks for following with us through the three approaches to metadata filtering in vector similarity search; pre-filtering, post-filtering, and Pinecone’s single-stage filtering.\n\nWe hope the article has been helpful! Now you can [try our single-stage filtering](https://www.pinecone.io/start/) for your own vector search application.\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbfa"
  },
  "filename": "cross-entropy-loss.md",
  "title": "post",
  "category": "\"Cross-Entropy Loss",
  "content": "---\nlayout: post\ntitle: \"Cross-Entropy Loss: Everything You Need to Know\"\nheadline: \"Cross-Entropy Loss: Make Predictions with Confidence\"\ncategories:\n  - Algorithms & Libraries\ntoc: >-\nweight: 1\nauthor:\n  name: Bala Priya C\n  position: Technical Writer\n  src: /images/bala-priya.jpg\n  href: https://www.linkedin.com/in/bala-priya/\ndescription: \"Cross-Entropy Loss Function: Everything You Need to Know\"\nimages: ['/images/cross-entropy-loss.png']\n---\n\n![Cross-Entropy Loss](/images/cross-entropy-loss.png)\n\nHave you ever wondered what happens under the hood when you train a neural network? You’ll run the [gradient descent optimization](https://cs231n.github.io/optimization-1/#opt3) algorithm to find the optimal parameters (weights and biases) of the network. In this process, there’s a *loss* function that tells the network how good or bad its current prediction is. The goal of optimization is to find those parameters that *minimize* the loss function: the lower the loss, the better the model. \n\nIn classification problems, the model predicts the class label of an input. In such problems, you need metrics beyond accuracy. While accuracy tells the model whether or not a particular prediction is correct, **cross-entropy loss** gives information on *how* correct a particular prediction is. When training a classifier neural network, minimizing the cross-entropy loss during training is equivalent to helping the model learn to predict the correct labels with higher confidence.\n\nIn this tutorial, we'll go over binary and categorical cross-entropy losses, used for binary and multiclass classification, respectively. We’ll learn how to interpret cross-entropy loss and implement it in Python. As the loss function’s derivative drives the gradient descent algorithm, we’ll learn to compute the derivative of the cross-entropy loss function.\n\nLet’s begin!\n\n## What is Cross Entropy?\n\nBefore we proceed to learn about cross-entropy loss, it'd be helpful to review the definition of cross entropy. In the context of [information theory](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html), the cross entropy between two discrete probability distributions is related to [KL divergence](http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf), a metric that captures how close the two distributions are.\n\nGiven a true distribution **t** and a predicted distribution **p**, the cross entropy between them is given by the following equation.\n\n$$H(\\textbf{t},\\textbf{p}) = - \\sum_{s \\in S} \\textbf{t}(s).log(\\textbf{p}(s))$$ \n\nHere, both **t** and **p** are distributed on the same support S, but could take potentially different values. For a three-element support S, if **t** = [t1, t2, t3] and **p** = [p1, p2, p3], it’s not necessary that `t_i = p_i` for i in {1,2,3}.\n\n**Note:** `log(x)` refers to log to the base `e` (or natural logarithm), also written as `ln(x)`.\n\n*So how is cross entropy relevant in neural networks?*\n\nRecall that in binary classification, the [sigmoid activation](https://www.pinecone.io/learn/softmax-activation/#can-you-use-sigmoid-or-argmax-activations-instead) is used in the output layer, and the neural network outputs a probability score (p) between 0 and 1; the true label (t) being one of {0, 1}.\n\nIn case of multiclass classification, we use the [softmax activation](https://www.pinecone.io/learn/softmax-activation/#the-softmax-activation-function-explained) at the output layer to get a vector of predicted probabilities **p**. The true distribution **t** contains all of the probability mass (1)  at the index of the correct class, and 0 everywhere else. For example, in a classification problem with N classes, the true distribution corresponding to class `i` is a vector that's `N` classes long, with 1 at the index of the class label `i` and 0 at all other indices.\n\nWe’ll discuss this in greater detail in the coming sections.\n\n## Cross-Entropy Loss for Binary Classification\n\nLet's start this section by reviewing the `log` function in the interval (0,1].\n\n▶️ Run the following code snippet to plot the values of `log(x)` and `-log(x)` in the range 0 to 1. As `log(0)` is -∞, we add a small offset, and start with 0.001 as the smallest value in the interval.\n\n```python\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nsns.set() \nx_arr = np.linspace(0.001,1)\nlog_x = np.log(x_arr)\nfig, axes = plt.subplots(1, 2,figsize=(8,4))\nsns.lineplot(ax=axes[0],x=x_arr,y=log_x)\naxes[0].set_title('Plot of log(x) in the interval (0,1]')\naxes[0].set(xlabel='x', ylabel='log(x)')\nsns.lineplot(ax=axes[1],x=x_arr,y=-log_x)\naxes[1].set_title('Plot of -log(x) in the interval (0,1]')\naxes[1].set(xlabel='x', ylabel='-log(x)')\n```\n\n![Sigmoid Function Equation](/images/log-x-plot.png)\n<small>Plot of log x and -log x in the interval (0,1]</small>\n\nAs seen in the plots above, in the interval (0,1], `log(x)` and `-log(x)` are negative and positive, respectively. Observe how `-log(x)` approaches 0 as `x` approaches 1. This observation will be helpful when we parse the expression for cross-entropy loss.\n\nIn binary classification, the raw output of the neural network is passed through the sigmoid function, which outputs a probability score `p = σ(z)`, as shown below.\n\n![Binary Classification](/images/binary-classification.png)\n<small>A Simple Binary Classification Model (Image by the author)</small>\n\nThe true value, or the true label, is one of {0, 1} and we’ll call it `t`. The binary cross-entropy loss, also called the log loss, is given by:\n\n$$\\mathcal{L}(t,p) = -(t.log(p) + (1-t).log(1-p))$$\n\nAs the true label is either 0 or 1, we can rewrite the above equation as two separate equations.\n\nWhen `t = 1`, the second term in the above equation goes to zero, and the equation reduces to the following:\n\n$$When \\text{ } t=1,\\mathcal{L}(t,p) = -log(p)$$\n\nTherefore, when `t =1`, the binary cross-entropy loss is equal to the negative logarithm of the predicted probability `p`.\n\nSimilarly, when the true label `t=0`, the term `t.log(p)` vanishes, and the expression for binary cross-entropy loss reduces to:\n\n$$When \\text{ } t=0,\\mathcal{L}(t,p) = -log(1-p)$$\n\nNow, let's plot the binary cross-entropy loss for different values of the predicted probability `p`.\n\n```python\nbce_1 = -np.log(p)\nbce_0 = -np.log(1-p)\nplot1 = sns.lineplot(x=p,y=bce_1,label='True value:1').set(ylim=(0,4))\nplot2 = sns.lineplot(x=p,y=bce_0,label='True value:0').set(ylim=(0,4))\nplt.xlabel('p')\nplt.ylabel('Binary Cross-Entropy Loss')\n```\n\n![Binary Cross-Entropy Loss](/images/binary-cross-entropy-loss.png)\n\nFrom the plots above, we can make the following observations:\n\n- When the true label `t` is 1, the cross-entropy loss approaches 0 as the predicted probability `p` approaches 1 and\n- When the true label `t` is 0, the cross-entropy loss approaches 0 as the predicted probability `p` approaches 0.\n\nIn essence, the cross-entropy loss attains its *minimum* when the predicted probability `p` is close to the true value and is substantially *higher* when the predicted probability is far away from the true label.\n\n*But which predictions does cross-entropy loss penalize the most?*\n\nRecall that `log(0)` → -∞; so `-log(0)` →  ∞. As seen from the plots of the binary cross-entropy loss, this happens when the network outputs `p=1` or a value close to 1 when the true class label is 0, and outputs `p=0` or a value close to 0 when the true label is 1.\n\nPutting it all together, cross-entropy loss increases drastically when the network makes *incorrect* predictions with *high* confidence.\n\nIf there are `S` samples in the dataset, then the total cross-entropy loss is the sum of the loss values over all the samples in the dataset.\n\n$$\\mathcal{L}(t,p) = -\\sum_{i=1}^{S}(t_i.log(p_i) + (1-t_i).log(1-p_i))$$\n\n### Binary Cross-Entropy Loss in Python\n\nLet's define a Python function to compute the binary cross-entropy loss.\n\n```python\ndef binary_cross_entropy(t,p):\n    t = np.float_(t)\n    p = np.float_(p)\n    # binary cross-entropy loss\n    return -np.sum(t * np.log(p) + (1 - t) * np.log(1 - p))\n```\n\nNext, let’s call the function `binary_cross_entropy`with arrays of true and predicted values as the arguments.\n\n```python    \nt = [0,1,1,0,0,1,1]\np = [0.07,0.91,0.74,0.23,0.85,0.17,0.94]\n\nbinary_cross_entropy(t,p)\n4.460303459760249\n```\n\nTo get a better idea of how the loss varies with `p`, let's modify the function definition to print out the values of the loss for each of the samples, as shown below.\n\n```python\ndef binary_cross_entropy(t,p):\n    t = np.float_(t)\n    p = np.float_(p)\n    for tt, pp in zip(t,p):\n      print(f'true_val = {tt}, predicted_val = {pp}, loss = {-(tt * np.log(pp) + (1 - tt) * np.log(1 - pp))}')\n    return -np.sum(t * np.log(p) + (1 - t) * np.log(1 - p))\n```\n\nNow that we’ve modified the function, let’s call the function yet again to check the outputs.\n\n```python\nbinary_cross_entropy(t,p)\n\n# Output\ntrue_val = 0.0, predicted_val = 0.07, loss = 0.0725706928348355\ntrue_val = 1.0, predicted_val = 0.91, loss = 0.09431067947124129\ntrue_val = 1.0, predicted_val = 0.74, loss = 0.3011050927839216\ntrue_val = 0.0, predicted_val = 0.23, loss = 0.2613647641344075\ntrue_val = 0.0, predicted_val = 0.85, loss = 1.897119984885881\ntrue_val = 1.0, predicted_val = 0.17, loss = 1.7719568419318752\ntrue_val = 1.0, predicted_val = 0.94, loss = 0.06187540371808753\n4.460303459760249\n```\n\nIn the above output, when the true and predicted values are closer, the cross-entropy loss is lower; the loss increases when the true and predicted values are different. \n\nThe highest value of the loss, 1.897, occurs when the network predicts a probability score of 0.85 corresponding to a true value of 0. Suppose the problem is to classify whether the given image is that of a seal or not. The model, in this case, is 85% confident that the image is a seal when it actually isn't.\n\nSimilarly, the second highest value of the binary cross-entropy loss, 1.771 occurs when the network predicts a score of 0.17, significantly lower than the true value of 1. In the image classification example, this means that the model is only about 17% confident of the input image being a seal, when it actually is a seal.\n\nThis validates our earlier observation that the loss is higher when the predictions are away from the true values.\n\nIn the next section, let's explore an extension of cross-entropy loss to the multiclass classification case.\n\n## Categorical Cross-Entropy Loss for Multiclass Classification\n\nLet's formalize the setting we'll consider. In a multiclass classification problem over `N` classes, the class labels are 0, 1, 2 through N - 1. The labels are one-hot encoded with 1 at the index of the correct label, and 0 everywhere else.\n\nFor example, in an image classification problem where the input image is one of {panda, seal, duck}, the class labels and the corresponding one-hot vectors are shown below.\n\n![Multiclass Classification Encoding](/images/multiclass-classification-encoding.png)\n<small>One-hot encoding of class labels in multiclass classification (Image by the author)</small>\n\nIn multiclass classification, the raw outputs of the neural network are passed through the [softmax activation](https://www.pinecone.io/learn/softmax-activation/), which then outputs a vector of predicted probabilities over the input classes.\n\n![Multiclass Classification Model](/images/multiclass-classification-model.png)\n<small>A Simple Multiclass Classification Model (Image by the author)</small>\n\nThe categorical cross-entropy loss between the true distribution **t** and the predicted distribution **p** in a multiclass classification problem with N classes is given by:\n\n$$\\mathcal{L}(\\textbf{t},\\textbf{p}) = - \\sum_{j=1}^{N}t_j log(p_j)$$\n\nThis expression may seem daunting, but we’ll parse this and arrive at a much simpler expression.\n\nRecall that the true distribution **t** is a one-hot vector that has 1 at one of the indices and zero everywhere else. If a given image belongs to the class `k`, in the true distribution vector, `t_k = 1`, and all other indices are zero.\n\nSubstituting as follows,\n$$t_k=1 \\text{ } and \\text{ } t_j = 0 \\text{ } for \\text{ } j \\neq k$$\nWe see that `N - 1` terms in the summation go to zero, and you’ll have the following simplified expression:\n\n$$\\mathcal{L}(\\textbf{t},\\textbf{p}) = - t_k log(p_k) = -log(p_k)\\\\\\\\\nfor \\text{ } an\\text{ } input \\text{ } \\in \\text{ } Class \\text{ }k$$\n\nThe loss, therefore, reduces to the negative logarithm of the predicted probability for the correct class. The loss approaches zero, as `p_k` → 1.\n\nIn the figure below, we present some examples of true and predicted distributions. In our image classification example, if the target class is *seal*, the categorical cross-entropy loss is minimized when the network predicts a probability score close to 1 for the correct class (*seal*). This works similarly for the other target classes, *panda* and *duck*.\n\n![Predicted Loss Probability](/images/predicted-loss-probability.png)\n<small>Cross-entropy loss decreases as the predicted probability for the target class approaches 1 (Image by the author)</small>\n\nFor a dataset with `S` samples in all, the categorical cross-entropy loss is given by:\n\n$$\\mathcal{L}(\\textbf{t},\\textbf{p}) = - \\sum_{i=1}^{S}\\sum_{j=1}^{N}t_{ij} log(p_{ij})$$\n\nIn practice, you could also use the average cross-entropy loss across all samples in the dataset. Next, let's code the categorical cross-entropy loss in Python.\n\n### Categorical Cross-Entropy Loss in Python\n\nThe code snippet below contains the definition of the function `categorical_cross_entropy`.The function accepts two lists as arguments: `t_list` and `p_list` containing lists of true and predicted distributions, respectively. It then computes the cross-entropy loss over each set of predicted and true values.\n\n```python\ndef categorical_cross_entropy(t_list,p_list):\n    t_list = np.float_(t_list)\n    p_list = np.float_(p_list)\n    losses = []\n    for t,p in zip(t_list,p_list):\n      loss = -np.sum(t * np.log(p))\n      losses.append(loss)\n      print(f't:{t}, p:{p},loss:{loss}\\n')\n    return np.sum(losses)\n```\n\nNow, let’s make a call to the function with the lists of true and predicted distribution vectors as the arguments and check the outputs.\n\n```python   \nt_list = [[1,0,0],[0,1,0],[0,0,1],[1,0,0]]\np_list = [[0.91,0.04,0.05],[0.11,0.8,0.09],[0.3,0.1,0.6],[0.25,0.4,0.35]]\ncategorical_cross_entropy(t_list,p_list)\n\nt:[1. 0. 0.], p:[0.91 0.04 0.05],loss:0.09431067947124129\n\nt:[0. 1. 0.], p:[0.11 0.8  0.09],loss:0.2231435513142097\n\nt:[0. 0. 1.], p:[0.3 0.1 0.6],loss:0.5108256237659907\n\nt:[1. 0. 0.], p:[0.25 0.4  0.35],loss:1.3862943611198906\n\n2.214574215671332\n```\n\nFrom the output above, we see that the loss is lower when the model predicts a higher probability corresponding to the correct class label.\n\n## Derivative of the Softmax Cross-Entropy Loss Function \n\nOne of the [limitations of the argmax function](https://www.pinecone.io/learn/softmax-activation/#limitations-of-the-argmax-function) as the output layer activation is that it doesn’t support the backpropagation of gradients through the layers of the neural network. However, when using the softmax function as the output layer activation, along with cross-entropy loss, you can compute gradients that facilitate backpropagation. The gradient evaluates to a simple expression, easy to interpret and intuitive.\n\nIf you’d like to know how softmax activation and cross-entropy loss yield a gradient that can be used in backpropagation, please read ahead. The preceding sections do not necessitate the use of the following section but this will provide interesting and potentially helpful insight. \n\nThe following subsections assume you have some familiarity with [differential calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr). To follow along, you should be able to apply the chain rule for differentiation and compute partial derivatives. \n\n### Derivative of the Softmax Function\n\nRecall that if **z** is the output of the neural network, then `softmax(z)` outputs a vector **p** of probabilities. Let's start with the expression for softmax activation. \n\n<div>\n$$softmax(\\textbf{z})_i = p_i = \\frac{e^{z_i}}{\\sum_{j = 1}^{N} e^{z_j}}\\\\\\\\\nLet \\text{ } \\sum_{j = 1}^{N} e^{z_j} = \\Sigma N \\\\\\\\\nsoftmax(\\textbf{z})_i = \\frac{e^{z_i}}{ \\Sigma N}$$\n</div>\n\nNow, let’s compute the derivative of the softmax output `p_i` with respect to the raw output `z_i` of the neural network.\n\nHere, the computation of derivatives can be handled under two different cases, as shown below.\n\n\\begin{align}\nCase\\text{ }1: \\text{ }When\\text{ } i = j: \\\\\\\\\n\\frac{\\partial p_i}{\\partial z_i} = \\frac{\\partial \\frac{e^{z_i}}{\\Sigma N}}{\\partial z_i}\\\\\\\\\n= \\frac{\\Sigma N.\\frac{\\partial e^{z_i}}{\\partial z_i} - e^{z_i}.\\frac{\\partial \\Sigma N}{\\partial z_i}}{(\\Sigma N)^{2}}\\\\\\\\\n\\frac{\\partial \\Sigma N}{\\partial z_i} = \\frac{\\partial \\sum_{j \\neq i} e^{z_j}}{\\partial z_i} + \\frac{\\partial e^{z_i}}{\\partial z_i} = e^{z_i}\\\\\\\\\n\\frac{\\partial e^{z_i}}{\\partial z_i} = e^{z_i}\\\\\\\\\n\\frac{\\partial p_i}{\\partial z_i} = \\frac{\\Sigma N.e^{z_i} - e^{z_i}.e^{z_i}}{(\\Sigma N)^{2}} = \\frac{e^{z_i}}{\\Sigma N}\\left[1- \\frac{e^{z_i}}{\\Sigma N}\\right] = p_i(1-p_i)\n\\end{align}\n\n\\begin{align}\nCase\\text{ }2: \\text{ }When\\text{ } i \\neq j:\\\\\\\\\n\\frac{\\partial p_i}{\\partial z_j} = \\frac{\\partial \\frac{e^{z_i}}{\\Sigma N}}{\\partial z_j}\\\\\\\\\n= \\frac{\\Sigma N.\\frac{\\partial e^{z_i}}{\\partial z_j} - e^{z_i}.\\frac{\\partial \\Sigma N}{\\partial z_j}}{(\\Sigma N)^{2}}\\\\\\\\\n= 0 -  \\frac{e^{z_i}.e^{z_j}}{(\\Sigma N)^{2}}\\\\\\\\\n= 0 -  \\frac{e^{z_i}}{\\Sigma N}.\\frac{e^{z_j}}{\\Sigma N}\\\\\\\\\n\\implies \\frac{\\partial p_i}{\\partial z_j} = -p_ip_j\n\\end{align}\n\n### Derivative of the Cross-Entropy Loss Function\n\nNext, let's compute the derivative of the cross-entropy loss function with respect to the output of the neural network. We’ll apply the [chain rule](https://cs231n.github.io/optimization-2/#backprop) and substitute the derivatives of the softmax activation function.\n\n\n\\begin{align}\\frac{\\partial{L}}{\\partial{z_i}} = - \\sum_{j = 1}^{N} \\frac{\\partial{(t_j log p_j)}}{\\partial{z_i}}\\\\\\\\\n= - \\sum_{j = 1}^{N} t_j\\frac{\\partial{(log p_j)}}{\\partial{z_i}}\\\\\\\\\n= - \\sum_{j = 1}^{N} t_j\\frac{1}{p_j}\\frac{\\partial{p_j}}{\\partial{z_i}}\\\\\\\\\n= - \\frac{t_i}{p_i}\\frac{\\partial{p_i}}{\\partial{z_i}} - \\sum_{j \\neq i} \\frac{t_j}{p_j} \\frac{\\partial{p_j}}{\\partial{z_i}}\\\\\\\\\n= - \\frac{t_i}{p_i}p_i(1-p_i) - \\sum_{j \\neq i} \\frac{t_j}{p_j} (-p_jp_i)\\\\\\\\\n= -t_j + t_ip_i + \\sum_{j \\neq i} t_jp_i \\\\\\\\\n= -t_i + \\sum_{j = 1}^{N} t_jp_i\\\\\\\\\n= -t_i + p_i\\sum_{j = 1}^{N} t_j\\\\\\\\\n\\implies \\frac{\\partial{L}}{\\partial{z_i}} = p_i - t_i\\end{align}\n\n\nAs seen above, the gradient works out to the difference between the predicted and true probability values.\n\n## Wrapping Up\n\nIn this tutorial, you’ve learned how binary and categorical cross-entropy losses work. They impose a penalty on *predictions* that are significantly different from the *true* value. You’ve learned to implement both the binary and categorical cross-entropy losses from scratch in Python. In addition, we covered how using the cross-entropy loss, in conjunction with the softmax activation, yields a simple gradient expression in backpropagation. \n\nAs a next step, you may try spinning up a simple [image classification model](https://keras.io/examples/vision/mnist_convnet/) using softmax activation and cross-entropy loss function. Until the next tutorial!\n\n{{< newsletter text=\"Subscribe for more ML tutorials!\" inputText=\"Email address...\" buttonText=\"Submit\">}}\n\n## 📚 Resources\n\n[1] [Softmax Activation Function](https://www.pinecone.io/learn/softmax-activation/), pinecone.io\n\n[2] [Essence of Calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr), YouTube playlist by Grant Sanderson of 3Blue1Brown\n\n[3] [Gradient Descent Optimization](https://cs231n.github.io/optimization-1/), CS231n\n\n[4] [Backpropagation](https://cs231n.github.io/optimization-2/), CS231n\n\n[5] [Simple MNIST Convnet](https://keras.io/examples/vision/mnist_convnet/), keras.io\n\n[6] [Information Theory](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html), Dive into Deep Learning\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbfb"
  },
  "filename": "enterprise-plan.md",
  "title": "post",
  "category": "\"Introducing the Enterprise Plan\"",
  "content": "---\nlayout: post\ntitle: \"Introducing the Enterprise Plan\"\nheadline: \"Introducing the Enterprise Plan for Mission-Critical Applications\"\ncategories:\n  - Company News\ntoc: >-\nauthor:\n  name: Greg Kogan\n  position: VP Marketing\n  src: /images/company-greg.png\n  href: https://www.linkedin.com/in/gkogan/\ndate: \"2022-04-06\"\n# Date: April 6, 2022\n# Open graph\ndescription: Introducing the Enterprise plan for mission-critical applications.\nimages: [\"/images/enterprise-plan.png\"]\nthumbnail: \"/images/enterprise-thumbnail.png\"\n---\n\nThe most enjoyable thing about building a product in the cutting-edge [vector search](/learn/vector-search-basics/) space is seeing what users do with it. The second most enjoyable thing is using that information to make improvements that help them do it easier, better, and faster.\n\nOne thing we’re seeing more often is customers using Pinecone in mission-critical applications without needing it to run in a dedicated, single-tenant cloud environment. Those customers want the enterprise-grade support, availability, and observability that comes with our single-tenant Dedicated plan. Yet they also want the fast deployment, lower overhead, and lower cost of our self-serve, multi-tenant Standard plan.\n\nToday, we’re introducing a new plan that offers the best of both: Enterprise-grade features and support, with self-serve convenience.\n\n![Introducing the new Enterprise plan](/images/enterprise-plan.png)\n\nWith this update we also:\n\n- Made our support commitments more clear, so you can rest easy knowing we have your back.\n- Renamed the “Free” plan to “Starter” (it’s still free to use), and the “Dedicated” plan to “Enterprise Dedicated.”\n- Made the usage estimator more accurate by letting you specify the size of your metadata.\n\nContinue reading or jump to the [pricing page](/pricing/) for full details.\n\n## The New Enterprise plan\n\nThe new Enterprise plan gives you the features and support you need for mission-critical applications, in our fully managed cloud with self-service and usage-based billing. Previously, the maximum level of features and support was only available on the Dedicated plan, which required more time to deploy and cost more to run.\n\nIf you have a mission-critical application that does not _need_ to be in a private environment, you can now get the availability, reliability, observability, and support you need without having to wait or pay for a dedicated deployment. It’s now easy to start and scale with Pinecone whether you’re just experimenting or launching a large enterprise application into production.\n\nThe Enterprise plan has all the features of the Standard plan, plus:\n\n- Multiple availability zones for greater availability: When you [add replicas](/docs/api/operation/scale_index/#tag/Index-Operations/operation/scale_index) to your index, they are automatically distributed across availability zones within the same cloud region.\n- Promethesus metrics for greater observability: [Monitor](/docs/monitoring/) your Pinecone indexes by ingesting performance metrics into your own Prometheus instances, or into Prometheus- and OpenMetrics-compatible monitoring tools.\n- _And more coming soon to give you operational visibility and controls._\n\nIt also comes with Premium-level support for peace of mind — with availability SLAs, response-time SLAs, and 24/7 access to support by email or Slack.\n\nThe new plan is available today. Learn more and see complete details on our [pricing page](/pricing/), or [contact us](/contact/) with questions. To upgrade to the new plan, [go to your account](https://app.pinecone.io) and then to Billing.\n\n![Manage plans and billing in the Pinecone console](/images/enterprise-plan-screenshot.png)\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbfc"
  },
  "filename": "zero-shot-object-detection-clip.md",
  "title": "ebook-post",
  "category": "\"Zero Shot Object Detection with OpenAI's CLIP\"",
  "content": "---\nlayout: ebook-post\ntitle: \"Zero Shot Object Detection with OpenAI's CLIP\"\nheadline: \"Zero Shot Object Detection with OpenAI's CLIP\"\ncategories:\n  - Embedding Methods for Image Search\ntoc: >-\nweight: 11\nauthors:\n - name: James Briggs\n   position: Developer Advocate\n   src: /images/james-briggs.jpeg\n   href: \"https://www.youtube.com/c/jamesbriggs\"\n - name: Laura Carnevali\n   position: Developer\n   src: /images/laura-carnevali.jpeg\n   href: \"https://www.linkedin.com/in/laura-carnevali-071a01b7/\"\ndescription: Zero shot object detection and localization using OpenAI's CLIP\n# Open graph\nimages: ['https://www.pinecone.io/images/zero-shot-object-detection-clip-0.png']\n---\n\nThe **I**magenet **L**arge **S**cale **V**isual **R**ecognition **C**hallenge (ILSVRC)<sup>[1]</sup> was a world-changing competition hosted annually from 2010 until 2017. During this time, the competition acted as the catalyst for the explosion of deep learning<sup>[2]</sup> and was the place to find state-of-the-art image classification, object localization, and object detection.\n\nResearchers fine-tuned better-performance computer vision (CV) models to achieve ever more impressive results year-after-year. But there was an unquestioned assumption causing problems.\n\nWe assumed that every new task required model fine-tuning, this required *a lot* of data, and this needed both time and capital.\n\nIt wasn't until very recently that this assumption was questioned and proven wrong.\n\nThe astonishing rise of multi-modal models has made the impossible possible across various domains and tasks. One of those is zero-shot object detection and localization.\n\n\"Zero-shot\" means applying a model without the need for fine-tuning. Meaning we take a multi-modal model and use it to detect images in one domain, then switch to another entirely different domain *without* the model seeing a single training example from the new domain. \n\nNot needing a single training example means we completely skip the hard part of data annotation and model training. We can focus solely on application of our models.\n\nIn this chapter, we will explore how to apply OpenAI's CLIP to this task—using CLIP for localization and detection across domains with *zero* fine-tuning.\n\n<div style=\"left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;\">\n   <iframe style=\"border: 1; top: 0; left: 0; width: 100%; height: 100%; position: absolute;\" src=\"https://www.youtube-nocookie.com/embed/i3OYlaoj-BM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>\n\n---\n\n## Classification, Localization, and Detection\n\n[**Image classification**](https://www.pinecone.io/learn/zero-shot-image-classification-clip/) is one of the most straightforward tasks in visual recognition and the first step on the way to object detection. It consists of assigning a categorical label to an image.\n\n<img src=\"./images/zero-shot-object-detection-clip-1.png\" alt=\"image classification of dog image\" style=\"width:100%;\" center />\n<small>Image classified as (1) \"dog\".</small>\n\nWe could have an image classification model that identifies animals and could classify images of dogs, cats, mice, etc. If we pass the above image into this model, we'd expect it to return the class *\"dog\"*.\n\n**Object localization** takes this one step further by *\"localizing\"* the identified object.\n\n<img src=\"./images/zero-shot-object-detection-clip-2.png\" alt=\"Drawing\" style=\"width:100%;\"/>\n<small>(1) Image classified as \"dog\" and (2) object localized.</small>\n\nWhen we *localize* the object, we identify the object's coordinates on the image. That typically includes a set of patches where the object is located or a bounding box defined by $(x, y)$ coordinates, box width, and box height.\n\n**Object detection** can be thought of as the next step. With detection, we are *localizing* multiple object instances within the same image.\n\n<img src=\"./images/zero-shot-object-detection-clip-3.png\" alt=\"dog image passed through object localization model/algorithm\" style=\"width:100%;\"/>\n<small>(1) Object localized and classified as \"cat\" and (2) object localized and classified as \"dog\".</small>\n\nIn the example above, we are detecting two different objects within the image, a cat *and* a dog. Both objects are localized, and the results are returned.\n\nObject detection can also identify multiple instances of the *same* object in a single image. If we added another dog to the previous image, an object detection algorithm could detect two dogs and a single cat.\n\n## Zero Shot CLIP\n\n[OpenAI's CLIP](/learn/clip/) is a *multi-modal* model pretrained on a massive dataset of text-image pairs <sup>[3]</sup>. It can identify text and images with similar meanings by encoding both modalities into a shared vector space.\n\n![multi-modal-similarity](./images/zero-shot-object-detection-clip-4.png)\n<small>CLIP is able to encode different text and images into the same vector space.</small>\n\nCLIP's broad pretraining means it can perform effectively across many domains. We can adjust the task being performed (i.e. from classification to detection) with just a few lines of code. A big part of this flexibility if thanks to the multi-modal vector embeddings built by CLIP.\n\nThese vector embeddings allow us to switch from [text-to-image search](/learn/clip), [image classification](/learn/zero-shot-image-classification-clip), and object detection. We simply adjust how we preprocess data being fed into CLIP, or how we interpret the similarity scores between the CLIP embeddings. The model itself requires no modification.\n\nFor classification, we need to give CLIP a list of our class labels, and it will encode them into a vector space:\n\n![finding-class-images](./images/zero-shot-object-detection-clip-5.png)\n\n<small>By encoding both images and class labels into the same vector space, we can identify each image's most similar class label.</small>\n\nFrom there, we give CLIP the images we'd like to classify. CLIP will encode them in the same vector space, and we find which of the class label embeddings is nearest to our image embeddings.\n\n### Object Localization\n\nWe can apply similar logic to using CLIP in a zero-shot object localization setting. As before, we create a class label embedding like `\"a fluffy cat\"`. But, unlike before, we don't feed the entire image into CLIP.\n\nTo localize an object, we break the image into many small patches. We then pass a `window` over these patches, moving across the entire image and generating an image embedding for a unique window.\n\nWe can calculate the similarity between these patch image embeddings and our class label embeddings — returning a score for each patch.\n\nAfter calculating the similarity scores for every patch, we collate them into a map of relevance across the entire image. We use that \"map\" to identify the location of the object of interest.\n\n![patches](./images/zero-shot-object-detection-clip-6.png)\n<small>We split images into small patches, which we can use to create mini-images that are encoded and compared to each encoded label. Producing a set of scores for each part of the image.</small>\n\nFrom there, we can recreate the traditional approach of creating a \"bounding box\" around the object.\n\n<center><img src=\"./images/zero-shot-object-detection-clip-7.png\" alt=\"bounding box\" width=\"50%\" /></center>\n\n<small>We can use the scored patches of the image to find a bounding box that encapsulates the object of interest.</small>\n\nBoth of these visuals capture the same information but displays them in different ways.\n\n#### Occlusion Algorithm\n\nOcclusion is another method of localization where we slide a black patch across the image. The idea being that we dentify similarity by the \"absence\" of an object <sup>\\[4\\]\\[5\\]</sup>.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n    <source src=\"./images/zero-shot-object-detection-clip-8.mp4\" type=\"video/mp4\">\n</video>\n<small>The occlusion algorithm works by sliding a black square to hide parts of the image at each time step.</small>\n\nIf the black patch covers the object we are looking for, the similarity score will drop. We then take that position as the assumed location of our object.\n\n### Object Detection\n\nThere is a fine line between object localization and object detection. With object localization, we perform a \"classification\" of a single object followed by the localization of that object. With object detection, we perform localization for multiple classes and/or objects.\n\nWith our cat and butterfly image, we could search for two objects; `\"a fluffy cat\"` and `\"a butterfly\"`. We use object localization to identify each *individual* object, but by iteratively identifying multiple objects, this becomes *object detection*.\n\n<center><img src=\"./images/zero-shot-object-detection-clip-9.png\" alt=\"object detection with multiple classes\" style=\"width:50%;\" /></center>\n\n<small>Object detection differs from localization by allowing the detection of multiple classes and multiple objects belonging to each class.</small>\n\nWe stick with the bounding box visualizations for object detection, as the other method makes it harder to visualize multiple objects within the same image.\n\nWe have covered the idea behind object localization and detection in a zero-shot setting with CLIP. Now let's take a look at how to implement it.\n\n### Detection with CLIP\n\nBefore we move on to any classification, localization, or detection task, we need images to process. We will use a small demo dataset named `jamescalam/image-text-demo` hosted on Hugging Face *datasets*.\n\n{{< notebook file=\"clip-detection-dataset\" height=\"full\" >}}\n\nThe dataset contains the image of a butterfly landing on a cat's nose. We can view it in a Jupyter notebook with the following:\n\n```python\ndata[2]['image']\n```\n\n<center><img alt=\"image of a butterfly landing on the nose of a cat\" src=\"./images/zero-shot-object-detection-clip-10.png\" width=\"50%;\" /></center>\n\n<small>The image we will be using for image localization and detection.</small>\n\nWe have downloaded the image, but it is not in the format we need for localization. For that, we must break the image into smaller patches.\n\n#### Creating Patches\n\nTo create the patches, we must first convert our PIL image object into a PyTorch tensor. We can do this using `torchvision.transforms`.\n\n{{< notebook file=\"clip-detection-to-tensor\" height=\"full\" >}}\n\nOur tensor has `3` color channels (RGB), a height of `5184` pixels, and width of `3456` pixels.\n\nAssuming each patch has an equal height and width of 256 pixels, we must reshape this tensor into a tensor of shape `(1, 20, 13, 3, 256, 256)` where *20* and *13* of the number of patches in height and width of the image and *1* represents the batch dimension.\n\nWe first add the batch dimension and move the color channels' dimension behind the height and width dimensions.\n\n{{< notebook file=\"clip-detection-add-batch-reshape\" height=\"full\" >}}\n\nFollowing this, we broke up the image into horizontal patches first. All patches will be square with dimensionalities of *256x256*, so the horizontal patch height equals *256* pixels.\n\n{{< notebook file=\"clip-detection-horizontal-patches\" height=\"full\" >}}\n\n<center><img alt=\"horizontal patches applied to cat image\" src=\"./images/zero-shot-object-detection-clip-11.png\" width=\"40%;\" /></center>\n\n<small>Cat image split into *256* pixel high strips.</small>\n\nWe need one more unfold to create the vertical space between patches.\n\n{{< notebook file=\"clip-detection-vertical-patches\" height=\"full\" >}}\n\n<center><img alt=\"horizontal patches applied to cat image\" src=\"./images/zero-shot-object-detection-clip-12.png\" width=\"40%;\" /></center>\n\n<small>Cat image split into small *256x256* pixel patches.</small>\n\nEvery patch is tiny, and looking at a single patch gives us little-to-no information about the image's content. Rather than feeding single patches to CLIP, we merge multiple patches to create a big patch passed to CLIP.\n\n<center><img alt=\"horizontal patches applied to cat image\" src=\"./images/zero-shot-object-detection-clip-13.png\" width=\"40%;\" /></center>\n\n<small>The first *6x6* window viewed by CLIP.</small>\n\nWe call this grouping of patches a `window`. A larger `window` size captures more global views of the image, whereas a smaller `window` can produce a more precise map at the risk of missing larger objects. To slide across the image and create a `big_batch` at each step, we do the following:\n\n```python\nwindow = 6\nstride = 1\n\n# window slides from top to bottom\nfor Y in range(0, patches.shape[1]-window+1, stride):\n    # window slides from left to right\n    for X in range(0, patches.shape[2]-window+1, stride):\n        # initialize an empty big_patch array\n        big_patch = torch.zeros(patch*window, patch*window, 3)\n        # this gets the current batch of patches that will make big_batch\n        patch_batch = patches[0, Y:Y+window, X:X+window]\n        # loop through each patch in current batch\n        for y in range(patch_batch.shape[1]):\n            for x in range(patch_batch.shape[0]):\n                # add patch to big_patch\n                big_patch[\n                    y*patch:(y+1)*patch, x*patch:(x+1)*patch, :\n                ] = patch_batch[y, x].permute(1, 2, 0)\n        # display current big_patch\n        plt.imshow(big_patch)\n        plt.show()\n```\n\n<video autoplay loop muted playsinline class=\"responsive\">\n    <source src=\"./images/zero-shot-object-detection-clip-14.mp4\" type=\"video/mp4\">\n</video>\n<small>Sliding window as it progresses through the image. The small image to the bottom right is what CLIP sees.</small>\n\nWe will re-use this logic later when creating our patch image embeddings. Before we do that, we must initialize CLIP.\n\n#### CLIP and Localization\n\nThe Hugging Face *transformers* library contains an implementation of CLIP named [`openai/clip-vit-base-patch32`](https://huggingface.co/openai/clip-vit-base-patch32). We can download and initialize it like so:\n\n```python\nfrom transformers import CLIPProcessor, CLIPModel\nimport torch\n\n# define processor and model\nmodel_id = \"openai/clip-vit-base-patch32\"\n\nprocessor = CLIPProcessor.from_pretrained(model_id)\nmodel = CLIPModel.from_pretrained(model_id)\n\n# move model to device if possible\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmodel.to(device)\n```\n\nNote that we also move to model to a CUDA-enabled GPU *if possible* to reduce inference times.\n\nWith CLIP initialized, we can rerun the patch sliding logic, but this time we will calculate the similarity between each `big_patch` and the text label `\"a fluffy cat\"`.\n\n```python\nwindow = 6\nstride = 1\n\nscores = torch.zeros(patches.shape[1], patches.shape[2])\nruns = torch.ones(patches.shape[1], patches.shape[2])\n\nfor Y in range(0, patches.shape[1]-window+1, stride):\n    for X in range(0, patches.shape[2]-window+1, stride):\n        big_patch = torch.zeros(patch*window, patch*window, 3)\n        patch_batch = patches[0, Y:Y+window, X:X+window]\n        for y in range(window):\n            for x in range(window):\n                big_patch[\n                    y*patch:(y+1)*patch, x*patch:(x+1)*patch, :\n                ] = patch_batch[y, x].permute(1, 2, 0)\n        # we preprocess the image and class label with the CLIP processor\n        inputs = processor(\n            images=big_patch,  # big patch image sent to CLIP\n            return_tensors=\"pt\",  # tell CLIP to return pytorch tensor\n            text=\"a fluffy cat\",  # class label sent to CLIP\n            padding=True\n        ).to(device) # move to device if possible\n\n        # calculate and retrieve similarity score\n        score = model(**inputs).logits_per_image.item()\n        # sum up similarity scores from current and previous big patches\n        # that were calculated for patches within the current window\n        scores[Y:Y+window, X:X+window] += score\n        # calculate the number of runs on each patch within the current window\n        runs[Y:Y+window, X:X+window] += 1\n```\n\nHere we have also added `scores` and `runs` that we will use to calculate the *mean* score for each patch. We calculate the `scores` tensor as the sum of every `big_patch` score calculated while the patches were within the `window`.\n\nSome patches will be seen more often than others (for example, the top-left patch is seen once), so the scores will be much greater for patches viewed more frequently. That is why we use the `runs` tensor to keep track of the \"visit frequency\" for each patch. With both tensors populated, we calculate the mean score:\n\n```python\nscores /= runs\n```\n\nThe `scores` tensor typically contains a smooth gradient of values as a byproduct of the scoring function sliding over each window. This means the scores gradually fade to `0.0` the further they are from the object of interest.\n\nWe cannot accurately visualize the object location with the current scores. Ideally, we should push low scores to zero while maintaining a range of values for higher scores. We can do this by clipping our outputs and normalizing the remaining values.\n\n```python\n# clip the scores\nscores = np.clip(scores-scores.mean(), 0, np.inf)\n\n# normalize scores\nscores = (\n    scores - scores.min()) / (scores.max() - scores.min()\n)\n```\n\n![clipping](./images/zero-shot-object-detection-clip-15.png)\n<small>After clipping and normalization we return a more useful visual (right).</small>\n\nWith that, our patch scores are ready, and we can move on to visualizing the results.\n\n#### Visualize Localization\n\nEach patch in the $(20, 13)$ patches tensor is assigned a similarity score within the range of $0$ (not similar) to $1$ (perfect match).\n\nIf we can align the scores with the original image pixels, we can multiply each pixel by its corresponding similarity score. Those near $0$ will be dark, and near $1$ will maintain their original brightness.\n\nThe only problem is that these two tensors are *not* the same shape:\n\n```python\nscores.shape, patches.shape\n```\n\n```\n[Out]: (torch.Size([20, 13]), torch.Size([1, 20, 13, 3, 256, 256]))\n```\n\nWe need to reshape `patches` to align with scores. To do that, we use `squeeze` to remove the batch dimension at position `0` and then re-order the dimensions using `permute`.\n\n```python\n# transform the patches tensor\nadj_patches = patches.squeeze(0).permute(3, 4, 2, 0, 1)\nadj_patches.shape\n```\n\n```\n[Out]: torch.Size([256, 256, 3, 20, 13])\n```\n\nFrom there, we multiply the adjusted patches and `scores` to return the brightness-adjusted patches. These need to be permuted again to be visualized with `matplotlib`.\n\n```python\n# multiply patches by scores\nadj_patches = adj_patches * scores\n\n# rotate patches to visualize\nadj_patches = adj_patches.permute(3, 4, 2, 0, 1)\nadj_patches.shape\n\n```\n\n```\n[Out]: torch.Size([20, 13, 3, 256, 256])\n```\n\nNow we're ready to visualize:\n\n```python\nY = adj_patches.shape[0]\nX = adj_patches.shape[1]\n\nfig, ax = plt.subplots(Y, X, figsize=(X*.5, Y*.5))\nfor y in range(Y):\n    for x in range(X):\n        ax[y, x].imshow(adj_patches[y, x].permute(1, 2, 0))\n        ax[y, x].axis(\"off\")\n        ax[y, x].set_aspect('equal')\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.show()\n```\n\n<center><img alt=\"object localization applied for 'a fluffy cat'\" src=\"./images/zero-shot-object-detection-clip-16.png\" width=\"40%;\" /></center>\n\n<small>Object localization for *\"a fluffy cat\"*.</small>\n\nThat works well. We can repeat the same but with the prompt `\"a butterfly\"` to return:\n\n<center><img alt=\"object localization applied for 'a fluffy cat'\" src=\"./images/zero-shot-object-detection-clip-17.png\" width=\"40%;\" /></center>\n\n<small>Object localization for *\"a butterfly\"*.</small>\n\nCLIP shows another good result and demonstrates how easy it is to add new labels to classification and localization tasks with CLIP.\n\n#### Bounding Box\n\nBefore moving on to object detection, we need to rework the visualization to handle multiple objects.\n\nThe standard way to outline objects for localization and detection is to use a bounding box. We will do the same using the `scores` calculated previously for the `\"a butterfly\"` prompt.\n\nThe bounding box requires a defined edge, unlike our previous visual, which had a more continuous fade to black. To do this, we need to set a threshold for what is positive or negative, and we will use `0.5`.\n\n```python\n# scores higher than 0.5 are positive\ndetection = scores > 0.5\n```\n\nWe can now detect the non-zero positions with the `np.nonzero` function. The output values represent the $x, y$ coordinates of patches with `scores > 0.5`.\n\n{{< notebook file=\"clip-detection-non-zero\" height=\"full\" >}}\n\nThe first column represents the *x-coordinates* of non-zero positions, and the second column represents the respective *y-coordinates*.\n\n<center><img alt=\"true patch detection coordinates\" src=\"./images/zero-shot-object-detection-clip-18.png\" width=\"60%;\" /></center>\n\n<small>Detection coordinates created by `detection = scores > 0.5`.</small>\n\nOur bounding box will take each of the edges produced by these non-zero coordinates.\n\n<center><img alt=\"bounding box\" src=\"./images/zero-shot-object-detection-clip-22.png\" width=\"60%;\" /></center>\n\nWe need the minimum and maximum $x$ and $y$ coordinates to find the box corners.\n\n{{< notebook file=\"clip-detection-min-max-coords\" height=\"full\" >}}\n\n<center><img alt=\"bounding box using coordinates\" src=\"./images/zero-shot-object-detection-clip-19.png\" width=\"60%;\" /></center>\n\n<small>The *min* and *max* values of $x, y$ coordinates give us our bounding box corners within the patches array.</small>\n\nThese give us the bounding box coordinates based on patches rather than pixels. To get the pixel coordinates (for the visual), we multiply the coordinates by `patch`. After that, we calculate the box `height` and `width`.\n\n{{< notebook file=\"clip-detection-pixel-coords\" height=\"full\" >}}\n\n<center><img alt=\"bounding box using coordinates\" src=\"./images/zero-shot-object-detection-clip-20.png\" width=\"60%;\" /></center>\n\nWith the `x_min`, `y_min`, `width`, and `height` values we can use `matplotlib.patches` to create the bounding box. Before we do that, we convert the original PIL image into a `matplotlib`-friendly format.\n\n{{< notebook file=\"clip-detection-move-color-channel\" height=\"full\" >}}\n\nNow we visualize everything together:\n\n{{< notebook file=\"clip-detection-show-bounding-box\" height=\"full\" >}}\n\nThere we have our bounding box visual.\n\n### Object Detection\n\nWe finally have everything we need to perform **object detection** for multiple object classes within the same image. The logic is a loop over what we have already built, and we can package it into a neater function like so:\n\n```python\ndef detect(prompts, img, patch_size=256, window=6, stride=1, threshold=0.5):\n    # build image patches for detection\n    img_patches = get_patches(img, patch_size)\n    # convert image to format for displaying with matplotlib\n    image = np.moveaxis(img.data.numpy(), 0, -1)\n    # initialize plot to display image + bounding boxes\n    fig, ax = plt.subplots(figsize=(Y*0.5, X*0.5))\n    ax.imshow(image)\n    # process image through object detection steps\n    for i, prompt in enumerate(tqdm(prompts)):\n        scores = get_scores(img_patches, prompt, window, stride)\n        x, y, width, height = get_box(scores, patch_size, threshold)\n        # create the bounding box\n        rect = patches.Rectangle((x, y), width, height, linewidth=3, edgecolor=colors[i], facecolor='none')\n        # add the patch to the Axes\n        ax.add_patch(rect)\n    plt.show()\n```\n\n*(Find the [full code here](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/image-retrieval/clip-object-detection/zero-shot-object-detection-clip.ipynb))*\n\nNow we pass a list of class labels and the image to `detect`. The function will return our image with each detected object annotated with a bounding box.\n\n```python\ndetect([\"a cat\", \"a butterfly\"], img, window=4, stride=1)\n```\n\n<center><img alt=\"bounding box using coordinates\" src=\"./images/zero-shot-object-detection-clip-21.png\" width=\"60%;\" center /></center>\n\nThe current implementation is limited to displaying a single object from each class, but this can be solved with a small amount of additional logic.\n\n---\n\nThat's it for this walkthrough of *zero-shot* object localization and detection with OpenAI's CLIP. Zero-shot opens the doors to many organizations and domains that could not perform good object detection due to a lack of training data or compute resources — which is the case for the vast majority of companies.\n\nMulti-modality and CLIP are just part of a trend towards more broadly applicable ML with a much lower barrier to entry. Zero-to-few-shot learning unlocks those previously inaccessible projects and presents us with what will undoubtedly be a giant leap forward in ML capability and adoption across the globe.\n\n## Resources\n\n[**Colab Notebook**](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/image-retrieval/clip-object-detection/zero-shot-object-detection-clip.ipynb)\n\n[1] O. Russakovsky et al., [ImageNet Large Scale Visual Recognition Challenge](https://arxiv.org/abs/1409.0575) (2014)\n\n[2] A. Krizhevsky et al., [ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) (2012), NeurIPS\n\n[3] A. Radford, J. Kim, et al., [Learning Transferable Visual Models From Natural Language Supervision](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language.pdf) (2021)\n\n[4] F. Bianchi, [Domain-Specific Multi-Modal Machine Learning with CLIP](https://youtu.be/uqRSc-KSA1Y?t=1841) (2022), Pinecone Workshop\n\n[5] R. Pisoni, [Searching Across Images and Text: Intro to OpenAI's CLIP](https://youtu.be/W11lSifqJDs?t=1690) (2022), Pinecone Workshop\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbfd"
  },
  "filename": "bert-search-speed.md",
  "title": "post",
  "category": "\"Ludicrous BERT Search Speeds\"",
  "content": "---\nlayout: post\ntitle: \"Ludicrous BERT Search Speeds\"\nheadline: \"Ludicrous <span>BERT Search Speeds</span>\"\ncategories:\n  - Algorithms & Libraries\ntoc: >-\nweight: 3\nauthor:\n  name: Rajat Tripathi\n  position: Software Engineer\n  src: /images/company-rajat.jpeg\n  href: https://www.linkedin.com/in/rajat-tripathi-08/\ndescription: Semantic search applications require quality embeddings and fast queries, making BERT and Pinecone the perfect combination.\n# Open graph\nimages: ['/images/bert-search-speed-3.png']\n---\n\n*This is a comparison of Approximate Nearest Neighbor (ANN) speeds between Pinecone and several flavors of Elasticsearch. Thanks to [Dmitry Kan](https://dmitry-kan.medium.com/), whose article is referenced here, for reviewing drafts of this post and providing valuable suggestions.*\n\n---\n\nSemantic search applications have two core ingredients: Text documents represented as vector embeddings that capture meaning, and a search algorithm to retrieve semantically similar items.\n\nBERT is a popular word embedding model because its pretrained versions provide high-quality embeddings without any additional training. However, those embeddings are high-dimensional (768 in our experiment), making it unbearably slow to search with k-nearest-neighbor (KNN) algorithms.\n\nFortunately, there are ways to speed things up considerably. [One article](https://towardsdatascience.com/speeding-up-bert-search-in-elasticsearch-750f1f34f455) shows how to search through BERT (or any other) embeddings progressively faster in Elasticsearch, starting with native, then with the Elastiknn plugin, then with Approximate k-NN in Open Distro for Elasticsearch, and finally with a GSI Associative Processing Unit (APU).\n\nIn the end, the author accelerated a million-document search from 1,500ms with Elasticsearch to 93ms with the GSI APU. An improvement of 6.1x — not bad! Not to mention the reduced index sizes and indexing times.\n\nWe wanted to take this experiment even further. Using [Pinecone vector search](/) **we improved search speeds by 2.4x compared to Elasticsearch with GSI APU, and by 4.4x compared to Open Distro for Elasticsearch**. All while maintaining a rank@k recall of 0.991.\n\nHere’s how we did it.\n\n## ANN and Vector Indexes\n\nPinecone uses approximate-nearest-neighbor (ANN) search instead of k-nearest-neighbor (kNN). ANN is a method of performing nearest neighbor search where we trade off some accuracy for a massive boost in performance. Instead of going through the entire list of objects and finding the exact neighbors, it retrieves a “good guess” of an object’s neighbors.\n\nA [vector index](/learn/vector-database/) is a specialized data structure tailor-made for performing a nearest neighbor search (approximate or exact) on vectors. An index allows us to insert, fetch and query our vectors efficiently. Pinecone employs its search algorithm and index that scales up to billions of vectors with extremely high throughput.\n\n![How BERT works with Pinecone](/images/bert-search-speed-3.png)\n\n## The Setup\n\nThe environment is not too relevant since Pinecone is a distributed service and your query and index times depend on the network rather than the machine itself. Though a beefier notebook instance means less time to compute embeddings. I used a GCP Notebook instance (n1-standard-8 US-West) for this experiment.\n\nWe used the same dataset, model, and approach as the reference article, using Pinecone instead for performing the search. We used the [DBpedia dataset](http://downloads.dbpedia.org/wiki-archive/data-set-37.html) containing full abstracts of Wikipedia articles and [SBERT](https://www.sbert.net/) from SentenceTransformers for creating embeddings.\n\nYou can find code for pre-processing the data in the GitHub repository from the reference article. Since Pinecone is a managed service, we can create and work with an index using its python client directly from the notebook. The code is [available publicly](https://github.com/pinecone-io/examples/blob/master/bert_search_test/bertcomparison.ipynb).\n\nOnce we have the embeddings ready, searching with Pinecone is a three-step process: Create the index, upsert the embeddings, and query the index.\n\nHere are a few snippets from the code to see how we used Pinecone for this experiment.\n\n\n```python\n!pip install --quiet -U pinecone-client\n\nimport pinecone\npinecone.init(api_key=api_key)\n\nindex_name = 'bert-stats-test'\n\npinecone.create_index(index_name,metric='cosine', shards=shards)\n\ndef upload_items(items_to_upload: List, batch_size: int) -> float:\n    print(f\"\\nUpserting {len(items_to_upload)} vectors...\")\n    start = time.perf_counter()\n    upsert_cursor = index.upsert(items=items_to_upload,batch_size=batch_size)\n    end = time.perf_counter()\n    return (end - start) / 60.0\n```\n\n## The Test\n\nPinecone runs in the cloud and not on your local machine. We changed how we query from the original article to make the comparisons better. Since my instance was on GCP (and Pinecone on AWS), querying an index across the internet adds overhead like network, authentication, parsing, etc. To estimate the speeds better, we queried the index in two ways:\n* Send 10 single queries and average the speed.\n* Send 10 batched queries with batch size = 1000 and calculate the average.\n\nWith batched queries, we lower the overheads per query. One way to ensure lower latencies is deploying your application in the same region and cloud service provider as Pinecone. Currently, our production servers run on AWS in US-West, but we can deploy Pinecone in any region on AWS or GCP.\n\nBatching queries also has other practical applications apart from improving per query latency. Most notably, when you have a set of queries that you know you want to make beforehand, it’s better to send them in a batch.\n\nHere’s our query function:\n\n```python\ndef query(test_items: List, index):\n    print(f\"\\nQuerying...\")\n    times = []\n    #For single queries, we pick 10 queries\n    for test_item in test_items[:10]:\n        start = time.perf_counter()\n        #test_item is an array of [id,vector]\n        query_results = index.query(queries=[test_item[1]],disable_progress_bar=True) # querying vectors with top_k=10\n        end = time.perf_counter()\n        times.append((end-start))\n    #For batch queries, we pick 1000 vectors at perform 10 queries\n    print(f\"\\n Batch Querying...\")\n    batch_times = []\n    for i in range(0,10000, 1000):\n        start = time.perf_counter()\n        batch_items = test_items[i:i+1000]\n        vecs = [item[1] for item in batch_items]\n        query_results = index.query(queries=vecs,disable_progress_bar=True)\n        end = time.perf_counter()\n        batch_times.append((end-start))\n    return mean(times)*1000,mean(batch_times)*1000\n```\n\nUsing the setup and query method above, we saw the following results:\n\n![Summary of BERT indexing and search speeds with Pinecone.](/images/bert-search-speed-4.png)\n\nIndexing time in minutes:\n\n![Average BERT indexing speeds with Pinecone.](/images/bert-search-speed-2.png)\n\nSearch speed for single queries:\n\n![Average BERT search speeds with Pinecone.](/images/bert-search-speed-7.png)\n\nTotal search speed for 1,000 queries (batched):\n\n![Average BERT search speeds with Pinecone.](/images/bert-search-speed-6.png)\n\n### Estimating Per-Query Speed\n\nAs mentioned earlier, the results don’t accurately represent the Pinecone engine’s search speed due to network overheard dominating a large part of the final results. However, we can try to estimate the search speed per query without the overheads.\n\nWe can view the returned search speeds as:\n\n```\nnetwork_overhead + [num_queries] * [search_speed_per_query]\n```\n\nThe network_overhead may change a bit for every call but can be seen as a constant no matter the index size and batch size.\n\nFor 1 million documents and assuming the overhead is constant:\n\nSingle query:\n```\n  network_overhead + 1 * search_speed_per_query = 35.68ms\n```\nBatched query:\n```\n  network_overhead + 1000 * search_speed_per_query = 7020.4ms\n```\n\nSolving for search_speed_per_query:\n```\n  999 * search_speed_per_query = 7020.4 - 35.68\n  search_speed_per_query = 6.98 ms\n```\n\nUsing this logic let's look at the estimated search speed per query on Pinecone's engine\n\n![Average BERT search speeds per query with Pinecone.](/images/bert-search-speed-1.png)\n\n### Calculating Recall\n\nRecall is an important metric whenever we talk about ANN algorithms since they trade accuracy for speed. Since we do not have ground truth in terms of nearest neighbors for our queries, one way to calculate recall is to compare results between [approximate and exact searches](/docs/create-index/#parameters).\n\nWe calculated recall and approximation-loss as described in our [benchmark guide](/docs/examples/benchmark-vector-similarity-search/):\n\n> \"Rank-k recall is widespread due to its usage in a standard approximated nearest neighbor search benchmarking tool. It calculates the fraction of approximated (top-k) results with a score (i.e., 'metric' score) at least as high as the optimal, exact search, rank-k score. Usually, we robustify the threshold by adding a small 'epsilon' number to the rank-k score.\"\n\n> Approximation-loss computes the median top-k ordered score differences between the optimal and approximate-search scores for each test query. In other words, the differences between the optimal and approximate-search top-1 scores, top-2 scores, up to the top-k scores. The final measure is the average of these median values over a set of test queries.\n\nThese were the recall and approximation-loss results for 1 million vectors:\n\n```\n  Accuracy results over 10 test queries:\n  The average recall @rank-k is 0.991\n  The median approximation loss is 1.1920928955078125e-07\n```\n\nWe saw an average recall of 0.991, which means Pinecone maintains high recall while providing a big boost in search speeds.\n\n## Compared Results\n\nCompared to results in the original test, using Pinecone for SBERT embeddings is a way to further improve index sizes, indexing speed, and search speeds.\n\n**Pinecone vs. Elasticsearch (vanilla), 1M SBERT embeddings:**\n\n* 32x faster indexing\n* 5x smaller index size\n* 40x faster single-query (non-batched) searches\n\n**Pinecone vs. Open Distro for Elasticsearch from AWS, 200k SBERT embeddings:**\n\n* 54x faster indexing\n* 6x smaller index size\n* Over 100ms faster average search speed in batched searches\n* 4.4x faster single-query searches\n\nHowever, note that the [HNSW algorithm](/learn/hnsw/) inside Open Distro can be fine-tuned for higher throughput than seen in the reference article.\n\n**Pinecone vs. Elasticsearch with GSI APU plugin, 1M SBERT embeddings:**\n* 2.4x faster single-query searches\n\n---\n\n**1M SBERT Embeddings**\n\n<table class=\"table table-responsive table-dark\">\n  <tr>\n    <th></th>\n    <th>Indexing Speed (min)</th>\n    <th>Index Size (GB)</th>\n    <th>Search Speed (ms)</th>\n  </tr>\n  <tr>\n    <td>Pinecone (batch)</td>\n    <td>1.36</td>\n    <td>3</td>\n    <td>6.8</td>\n  </tr>\n  <tr>\n    <td>Pinecone (single)</td>\n    <td>1.36</td>\n    <td>3</td>\n    <td>38.3</td>\n  </tr>\n  <tr>\n    <td>Elasticsearch</td>\n    <td>44</td>\n    <td>15</td>\n    <td>1519.7</td>\n  </tr>\n  <tr>\n    <td>Elasticsearch + Elastiknn</td>\n    <td>51</td>\n    <td>15</td>\n    <td>663.8</td>\n  </tr>\n  <tr>\n    <td>Elasticsearch + GSI</td>\n    <td>44</td>\n    <td>15</td>\n    <td>92.6</td>\n  </tr>\n</table>\n\n**200k SBERT Embeddings, Approximate Nearest Neighbors**\n\n<table class=\"table table-responsive table-dark\">\n  <tr>\n    <th></th>\n    <th>Indexing Speed (min)</th>\n    <th>Index Size (GB)</th>\n    <th>Search Speed (ms)</th>\n  </tr>\n  <tr>\n    <td>Pinecone (batch)</td>\n    <td>0.21</td>\n    <td>0.6</td>\n    <td>1.1</td>\n  </tr>\n  <tr>\n    <td>Pinecone (single)</td>\n    <td>0.21</td>\n    <td>0.6</td>\n    <td>24.0</td>\n  </tr>\n  <tr>\n    <td>Open Distro</td>\n    <td>11.25</td>\n    <td>3.6</td>\n    <td>105.7</td>\n  </tr>\n</table>\n\nIf you would like to benchmark Pinecone for your use case and dataset — for both speed and accuracy — follow our [benchmarking guide for vector search services](/docs/examples/benchmark-vector-similarity-search/).\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbfe"
  },
  "filename": "k-nearest-neighbor.md",
  "title": "post",
  "category": "\"K-Nearest Neighbor (KNN) Explained\"",
  "content": "---\nlayout: post\ntitle: \"K-Nearest Neighbor (KNN) Explained\"\nheadline: \"K-Nearest Neighbor (KNN) Explained\"\ncategories:\n  - Algorithms & Libraries\ntoc: >-\nweight: 4\nauthor:\n  name: Diego Lopez Yse\n  position: Data Scientist\n  src: /images/diego-lopez-yse.jpeg\n  href: \"https://lopezyse.medium.com/\"\ndescription: K-Nearest Neighbor (KNN) Explained.\n# Open Graph\nimages: [\"https://www.pinecone.io/images/knn-proximity.png\"]\n---\n\nHave you ever heard of the Gestalt Principles? These are part of a theory of perception that examines how humans group similar objects in order to interpret the complex world around them.\n\nAccording to the Gestalt principle of proximity, elements that are closer together are perceived to be more related than elements that are farther apart, helping us understand and organize information faster and more efficiently.\n\n![KNN Proximity](/images/knn-proximity.jpg)\n<small>Our brains perceive these sets of closely placed elements as groups. Proximity is essential to our perception. Source: [UXMISFIT](https://uxmisfit.com/2019/04/23/ui-design-in-practice-gestalt-principles/)</small>\n\nLikewise, in Machine Learning, the concepts of proximity and similarity are tightly linked. Meaning that the closer two data points are, the more similar they are to one another than to other data points. The content recommendation systems you use every day for movies, texts, songs, and more, rely on this principle.\n\nOne Machine Learning algorithm that relies on the concepts of proximity and similarity is **K-Nearest Neighbor (KNN)**. KNN is a supervised learning algorithm capable of performing both classification and regression tasks.\n\n---\n\n**Note: As you'll see in this article, doing KNN-search or even ANN-search at scale can be slow and expensive. That's why we made [Pinecone](/) — an API for fast, fresh, filtered, and low-cost ANN search at any scale.**\n\n---\n\nAs opposed to unsupervised learning — where there is no output variable to guide the learning process and where data is explored by algorithms to find patterns — in supervised learning your existing data is already labeled and you know which behavior you want to predict in the new data you obtain.\n\nThe KNN algorithm predicts responses for new data (testing data) based upon its similarity with other known data (training) samples. It assumes that data with similar traits sit together and uses distance measures at its core.\n\nKNN belongs to the class of non-parametric models as it doesn't learn parameters from the training dataset to come up with a discriminative function to predict new unseen data. It operates by memorizing the training dataset.\n\nGiven its flexibility, KNN is used in industries such as:\n\n- Agriculture: to perform climate forecasting, estimating soil water parameters, or predicting crop yields.\n- Finance: to predict bankruptcies, understanding and managing financial risk.\n- Healthcare: to identify cancer risk factors, predict heart attacks, or analyze gene expression data.\n- Internet: using clickstream data from websites, to provide automatic recommendations to users on additional content.\n\n## Distance Measures\n\nHow near are two data points? It depends on how you measure it, and in Machine Learning, there isn’t just one way of measuring distances.\n\nA distance measure is an objective score that summarizes the relative difference between two objects in a problem domain and plays an important role in most algorithms. Some of the main measures are:\n\n- **Euclidean** is probably the most intuitive one and represents the shortest distance between two points. It’s calculated using the well-known Pythagorean theorem. Conceptually, it should be [used whenever we are comparing observations with continuous features](https://aigents.co/data-science-blog/publication/distance-metrics-for-machine-learning), like height, weight, or salaries. This distance measure is often the “default” distance used in algorithms like KNN.\n\n![Euclidean distance](/images/knn-euclidean-distance.png)\n<small>Euclidean distance between two points. Source: [VitalFlux](https://vitalflux.com/different-types-of-distance-measures-in-machine-learning/)</small>\n\n- **Manhattan** is used to estimate the distance to get from one data point to another if a grid-like path is taken. Unlike Euclidean distance, the Manhattan distance calculates the sum of the absolute values of the difference of the coordinates of two points. This way, instead of estimating a straight line between two points, we “walk” through available paths. The Manhattan distance is useful when our observations are distributed along a grid, like in chess or city blocks (when the [features of our observations are entire integers](https://aigents.co/data-science-blog/publication/distance-metrics-for-machine-learning) with no decimal parts).\n\n![Manhattan distance](/images/knn-manhattan-distance.png)\n<small>Manhattan distance between two points A and B. Source: [VitalFlux](https://vitalflux.com/different-types-of-distance-measures-in-machine-learning/)</small>\n\n- **Cosine** is employed to calculate similarity between two [vectors](https://www.pinecone.io/learn/vector-embeddings/). Through this measure, data objects in a dataset are treated as vectors, and similarity is calculated by the cosine of the angle between two vectors. Vectors which are most similar will have a value of 0 degrees between them (the value of cos = 0 is 1), while vectors that are most dissimilar will have a value of -1. The smaller the angle, the higher the similarity. This different perspective about distance can provide novel insights which might not be found using the previous distance metrics.\n\n![Cosign vectors](/images/knn-cosign-vectors.png)\n<small>Two cosine vectors that are aligned in the same orientation will have a similarity measurement of 1, whereas two vectors aligned perpendicularly will have a similarity of 0. If two vectors are diametrically opposed, meaning they are oriented in exactly opposite directions (i.e. back-to-back), then the similarity measurement is -1. Source: [DeepAI](https://deepai.org/machine-learning-glossary-and-terms/cosine-similarity)</small>\n\n- **Hamming** represents the number of points at which two corresponding pieces of data can be different. While comparing two vectors of equal length, Hamming distance is the number of bit positions in which they differ. This metric is generally used when comparing texts or binary vectors.\n\n![Hamming distance](/images/knn-hamming-distance.png)\n<small>How many bits are different between Data A and Data B? You can easily figure out the difference just by comparing bit by bit. In this case, you would figure out only one bit difference , ande: that’s the Hamming difference between both vectors. Source: [ShareTechnote](https://www.sharetechnote.com/html/Handbook_Communication_HammingDistance.html)</small>\n\nHere is a [complete review](https://www.liebertpub.com/doi/10.1089/big.2018.0175) of different distance measures applied to KNN.\n\n## How KNN works\n\nKNN performs classification or regression tasks for new data by calculating the distance between the new example and all the existing examples in the dataset. But how?\n\nHere’s the secret: The algorithm stores the entire dataset and classifies each new data point based on the existing data points that are similar to it. KNN makes predictions based on the training or “known” data only.\n\nAfter the user defines a distance function, like the ones we mentioned earlier, KNN calculates the distance between data points in order to find the closest data points from our training data for any new data point. The existing data points closest to the new data point using the defined distance will become the “k-neighbors”. For a classification task, KNN will use the most frequent of all values from the k-neighbors to predict the new data label. For a regression task, the algorithm will use the average of all values from the k-neighbors to predict the new data value.\n\n<video autoplay loop muted playsinline class=\"responsive\">\n  <source src=\"/images/knn.mp4\" type=\"video/mp4\">\n</video>\n\n<small class=\"video\">In a classification task with K=3, an unlabelled data point is labeled as pink due to the majority vote of the 3 most similar labeled examples. Source: [Towards Data Science](https://towardsdatascience.com/beginners-guide-to-k-nearest-neighbors-in-r-from-zero-to-hero-d92cd4074bdb)</small>\n\nAll KNN does is store the complete dataset, and without doing any calculation or modeling on top of it, measure the distance between the new data point and its closest data points. For this reason, and since there’s not really a learning process happening, KNN is called a “lazy” algorithm (as opposed to “eager” algorithms like Decision Trees that build generalized models before performing predictions on new data points).\n\n## How to find k?\n\nWith KNN, in order to make a classification/regression task, you need to define a number of neighbors, and that number is given by the parameter “k”. In other words, “k” determines the number of neighbors the algorithm looks at when assigning a value to any new observation.\n\nThis number can go from 1 (in which case the algorithm only looks at the closest neighbor for each prediction) to the total number of data points of the dataset (in which case the algorithm would predict the majority class of the complete dataset).\n\n![Finding K](/images/knn-k.png)\n<small>In the above example, if k=3 then the new data point will be classified as B. But if k=6, then it will be classified as A because the majority of points are from class A. Source: [ShareTechnote](https://www.sharetechnote.com/html/Handbook_Communication_HammingDistance.html)</small>\n\nSo how can you know the optimum value of “k”? We can decide based on the error calculation of a training and testing set. Separating the data into training and test sets allows for an objective model evaluation.\n\nOne popular approach is testing different numbers of “k” and measuring the resulting error, choosing the “k” value at which an increase will cause a very small decrease in the error sum, while a decrease will sharply increase the error sum. This point that defines the optimal number is known as the “elbow point”.\n\n![Error rates vs K value](/images/knn-error-rates.png)\n<small>The “k” value best suited for a dataset is selected by using the error rate (this is, the difference between the real and predicted values). After a certain point, the error rate becomes almost constant. Source: [AI in Plain English](https://ai.plainenglish.io/k-nearest-neighbors-knn-algorithm-for-machine-learning-ab6f17df4a7f)</small>\n\nAbove, we can see that after “k” > 23, the error rate stabilizes and becomes almost constant. For this reason, “k” = 23 seems to be the optimal value.\n\nAnother alternative for deciding the value of “k” is using grid search to find the best value. Grid search is a process that searches exhaustively through a specified numerical space for the algorithm to optimize a given parameter (which in this case is the error rate). Tools like Python’s [GridSearchCV](https://realpython.com/knn-python/) can automate the fit of KNN on the training set while validating the performance on the testing set in order to identify the optimal value of “k”.\n\n## Why KNN\n\nIn contrast to many other Machine Learning algorithms, KNN is simple to implement and intuitive. It’s flexible (has many distance metrics to choose from), evolves as new data is revealed, and has one single hyperparameter to tune (the value of “k”). It can detect linear or nonlinear distributed data, and since it is non-parametric, there are no assumptions to be met to implement it (i.e. [as opposed to linear regression models](https://www.fromthegenesis.com/pros-and-cons-of-k-nearest-neighbors/) that have plenty of assumptions to be met by the data before they can be employed).\n\nLike every algorithm, it has some downsides. The choice of “k” and the distance metric are critical to its performance and need to be carefully tuned. KNN is also very sensitive to outliers and imbalanced datasets. Also, since all the algorithm does is store the training data to perform its predictions, the memory needs grow linearly with the number of data points you provide for training.\n\nKNN is definitely an algorithm worth learning. Whether it’s for dealing with [missing data](https://www.analyticsvidhya.com/blog/2021/05/20-questions-to-test-your-skills-on-k-nearest-neighbour/) or [predicting image similarity](https://meesho.io/blog/predicting-visual-similarities-in-product-images-using-knn), this is a tool that you can’t miss!\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fbff"
  },
  "filename": "cosine-similarity.md",
  "title": "\"ebook-post\"",
  "category": "",
  "content": "---\nlayout: \"ebook-post\"\ncategories:\n  - Roughly Explained\ntitle: \"What is Cosine Similarity?\"\nheadline: \"What is Cosine Similarity?\"\ndescription: \"Cosine similarity is a measure of similarity between two vectors in a multi-dimensional space.\"\n---\n\nCosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. Two vectors with the same orientation have a cosine similarity of $1$, two vectors at $90°$ have a similarity of $0$, and two vectors diametrically opposed have a similarity of $-1$, independent of their magnitude.\n\nThe formula for cosine similarity is:\n\n$$\\text{Cosine similarity} = (\\text{Dot product of two vectors}) / (\\text{Product of Euclidean lengths of two vectors})$$\n\nCosine similarity is a commonly used measure in information retrieval and text mining, where it is often used to measure the similarity of documents. It is also used in collaborative filtering, where it is used to measure the similarity of users or items.\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc00"
  },
  "filename": "distance-between-vectors.md",
  "title": "\"ebook-post\"",
  "category": "",
  "content": "---\nlayout: \"ebook-post\"\ncategories:\n  - Roughly Explained\ntitle: \"What is the Distance Between Two Vectors?\"\nheadline: \"What is the Distance Between Two Vectors?\"\ndescription: \"The distance between two vectors is a measure of the difference between the two vectors.\"\n---\n\nThe distance between two vectors is a measure of how different they are from each other. It is a numerical value that can be calculated using a variety of methods, such as Euclidean distance, dot product, or cosine similarity. \n\nThis measure of distance is used in many machine learning and data science applications, such as clustering, classification, and recommendation systems. \n\nFor example, in a clustering algorithm, the distance between two vectors can be used to determine which cluster they should be assigned to. In a classification algorithm, the distance between two vectors can be used to determine which class they should be assigned to. In a recommendation system, the distance between two vectors can be used to determine which items should be recommended to a user.\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc01"
  },
  "filename": "max-a-posteriori-estimation.md",
  "title": "\"ebook-post\"",
  "category": "",
  "content": "---\nlayout: \"ebook-post\"\ncategories:\n  - Roughly Explained\ntitle: \"What is Maximum a Posteriori Estimation?\"\nheadline: \"What is Maximum a Posteriori Estimation?\"\ndescription: \"Maximum a posteriori (MAP) estimation is a method for estimating the parameters of a statistical model based on a set of observations.\"\n---\n\nMaximum a posteriori (MAP) estimation is a method of estimating the parameters of a statistical model. It is a type of Bayesian estimation, which uses Bayes' theorem to update the probability for a hypothesis as more evidence or information becomes available. \n\nMAP estimation can be used in machine learning and data science to estimate the parameters of a model, such as the weights of a neural network or the coefficients of a linear regression model. MAP estimation can also be used to estimate the probability of a given hypothesis, such as the probability of a given data point belonging to a certain class.\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc02"
  },
  "filename": "inner-product.md",
  "title": "\"ebook-post\"",
  "category": "",
  "content": "---\nlayout: \"ebook-post\"\ncategories:\n  - Roughly Explained\ntitle: \"What is an Inner Product?\"\nheadline: \"What is an Inner Product?\"\ndescription: \"An inner product is a mathematical operation that takes two vectors as input and produces a scalar as output.\"\n---\n\nAn inner product is a mathematical operation that takes two vectors of the same size and produces a scalar value. It is calculated by multiplying each element of one vector with the corresponding element of the other vector and then summing up the results.\n\nIt is used in many areas of mathematics, including linear algebra, calculus, and geometry. In machine learning and data science, inner products are used to measure the similarity between two vectors.\n\nFor example, in a neural network, the inner product of two vectors can be used to measure the similarity between two input vectors, or between an input vector and a weight vector. \n\nInner products can also be used to measure the similarity between two images, or between two text documents.\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc03"
  },
  "filename": "tanimoto-similarity.md",
  "title": "\"ebook-post\"",
  "category": "",
  "content": "---\nlayout: \"ebook-post\"\ncategories:\n  - Roughly Explained\ntitle: \"What is the Tanimoto Similarity?\"\nheadline: \"What is the Tanimoto Similarity?\"\ndescription: \"The Tanimoto similarity, also called Tanimoto similarity coefficient, is a measure of the similarity between two sets of data.\"\n---\n\nTanimoto similarity is a measure of similarity between two sets of data. It is a metric used to compare the similarity of two sets of data, and is often used in machine learning and data science. \n\nThe Tanimoto similarity is calculated by taking the intersection of two sets and dividing it by the sum of the sizes of the two sets. This gives a value between 0 and 1, where 0 indicates no similarity and 1 indicates perfect similarity.\n\nTanimoto similarity is often used in machine learning and data science to compare the similarity of two sets of data. For example, it can be used to compare the similarity of two sets of images, or two sets of text documents. It can also be used to compare the similarity of two sets of data points, such as two sets of customer data. In this case, the Tanimoto similarity can be used to identify customers who are similar in terms of their purchase history or other characteristics.\n\n### Tanimoto Similarity vs. Jaccard Coefficient\n\nIt is similar to the Jaccard coefficient, which is the ratio of the intersection of two sets to the union of two sets. \n\nThe Tanimoto similarity and Jaccard coefficient are both measures of similarity between two sets of data. The main difference between the two is that the Tanimoto similarity takes into account the size of the sets, while the Jaccard Index does not. The Tanimoto similarity is calculated by dividing the number of elements that are common to both sets by the total number of elements in both sets, while the Jaccard Index is calculated by dividing the number of elements that are common to both sets by the number of elements that are unique to either set."
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc04"
  },
  "filename": "sparse-vector.md",
  "title": "\"ebook-post\"",
  "category": "",
  "content": "---\nlayout: \"ebook-post\"\ncategories:\n  - Roughly Explained\ntitle: \"What is a Sparse Vector?\"\nheadline: \"What is a Sparse Vector?\"\ndescription: \"A sparse vector is a vector with many elements that are zero or close to zero, and only a few elements that are non-zero.\"\n---\n\nA sparse vector is a vector that contains mostly zeros, with only a few non-zero elements. It is a useful data structure for representing data that is mostly empty or has a lot of zeros. For example, if you have a vector of length 10,000 and only 10 elements are non-zero, then it is a sparse vector.\n\nIt is used in machine learning and data science when dealing with large datasets, as it can reduce the amount of memory needed to store the data. For example, if you have a dataset with millions of features where only a few features are important for each data point, you can represent it as a sparse vector, which will take up much less memory than a dense vector. \n\nSparse vectors can also be used in algorithms such as linear regression, where they can help reduce the computational complexity of the algorithm.\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc05"
  },
  "filename": "euclidean-distance.md",
  "title": "\"ebook-post\"",
  "category": "",
  "content": "---\nlayout: \"ebook-post\"\ncategories:\n  - Roughly Explained\ntitle: \"What is the Euclidean Distance?\"\nheadline: \"What is the Euclidean Distance?\"\ndescription: \"Euclidean distance is a measure of the straight-line distance between two points in Euclidean space.\"\n---\n\nThe Euclidean Distance is a measure of the distance between two points in a Euclidean space. It is calculated by taking the square root of the sum of the squared differences between the coordinates of the two points. For example, if two points have coordinates (x1, y1) and (x2, y2), then the Euclidean Distance between them is given by: \n\n$$d = sqrt((x2 - x1)^2 + (y2 - y1)^2)$$\n\nThe Euclidean Distance is a useful measure for many applications, such as clustering, classification, and regression. It is also used in machine learning algorithms such as k-means clustering and k-nearest neighbors.\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc06"
  },
  "filename": "jaccard-coefficient.md",
  "title": "\"ebook-post\"",
  "category": "",
  "content": "---\nlayout: \"ebook-post\"\ncategories:\n  - Roughly Explained\ntitle: \"What is the Jaccard Similarity?\"\nheadline: \"What is the Jaccard Similarity?\"\ndescription: \"The Jaccard similarity is a measure of similarity between two sets of data.\"\n---\n\nThe Jaccard similarity coefficient, or Jaccard Index, is a measure of similarity between two sets of data. It is calculated by taking the size of the intersection of the two sets and dividing it by the size of the union of the two sets. This gives us a value between 0 and 1, where 0 indicates no similarity and 1 indicates perfect similarity.\n\nThe Jaccard similarity coefficient can be used in machine learning and data science in a variety of ways. For example, it can be used to measure the similarity between two documents, or to measure the similarity between two sets of data points. It can also be used to measure the similarity between two clusters of data points, or to measure the similarity between two sets of features. In addition, it can be used to measure the similarity between two sets of words or phrases.\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc07"
  },
  "filename": "inductive-learning.md",
  "title": "\"ebook-post\"",
  "category": "",
  "content": "---\nlayout: \"ebook-post\"\ncategories:\n  - Roughly Explained\ntitle: \"What is Inductive Learning?\"\nheadline: \"What is Inductive Learning?\"\ndescription: \"Inductive learning is a type of learning that involves using observations or examples to induce general rules or principles.\"\n---\n\nInductive learning is a type of machine learning that uses data to make predictions or generalizations about a given problem. It is based on the idea that if a set of data points have certain characteristics, then future data points will also have those characteristics. \n\nThis type of learning is used in many areas of machine learning and data science, such as supervised learning, unsupervised learning, and reinforcement learning. For example, in supervised learning, inductive learning can be used to build a model that can predict the outcome of a given problem based on the data it has been trained on. \n\nIn unsupervised learning, inductive learning can be used to identify patterns in data and make predictions about future data points. In reinforcement learning, inductive learning can be used to identify the best action to take in a given situation.\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc08"
  },
  "filename": "_index.md",
  "title": "book",
  "category": "\"Roughly Explained\"",
  "content": "---\nlayout: book\ntitle: \"Roughly Explained\"\ndescription: Roughly Explained\nauthor: Pinecone\nintro: |\n  Simple explanations for words, phrases, and concepts related to Artificial Intelligence (AI), Machine Learning (ML), Information Retrieval (IR), and databases. \nemailSubmit: true\nsocialShare: true\nimage: /images/roughly-explained-ebook.png\nimages: ['/images/roughly-explained-ebook.png']\nintroChapter: \n    title: Introduction\n    text: |\n      AI, ML, IR, and cloud-native databases are fascinating topics with far-reaching applications and implications. But they can also be complicated and intimidating to beginners. These rough explanations of words, phrases, and concepts will help you brush up or get started in those fields.\n---"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc09"
  },
  "filename": "ai-search.md",
  "title": "use-case",
  "category": "AI Search",
  "content": "---\nlayout: use-case\ntitle: AI Search\nurl: /ai-search-use-case\naliases:\n  - /ai-search\n\nhero:\n  title: Search like you <span>mean it</span>\n  subtitle: | \n    **Fast AI search, at any scale, at a low cost with Pinecone**\n  description: |\n    AI search is what makes products from Google, Amazon, and Netflix delightful to their users. Your customers expect the same from your application.\n\n    The Pinecone vector database brings the power of AI search to engineering teams—regardless of size or AI expertise—by making it easy to build high-performance vector search applications. Search across billions of document data in milliseconds with usage-based pricing for high-volume production applications. \n  cta1:\n    href: https://app.pinecone.io/\n    text: Get Started Today\n  cta2:\n    href: /contact\n    text: Contact Us\n  img:\n    src: /images/ai-search.svg\n    alt: Pinecone Diagram\n    width: 550\n\n# What is AI search\ninfoSection:\n  title: What is <span>AI search</span>?\n  item1:\n    title: Lexical (keyword) search\n    text: Looks for patterns and exact word or string matches. Searching by keywords doesn’t always get you the right answer\n    img: /images/ai-search-lexical.svg\n  item2:\n    title: AI search\n    text: Uses the meaning of the search query and puts it into context; it returns relevant results even if they don’t exactly match the query. Searching by meaning or relevance gets you your answer and more! \n    img: /images/ai-search-ai.svg\n  item3:\n    title: Hybrid search\n    text: Combine the power of keyword and AI or semantic search for the best results. Get started today with our new [hybrid index](/learn/hybrid-search-intro/)\n    img: /images/ai-search-hybrid.svg\n\n# Ai search with Pinecone\npineconeSection:\n  eyebrow: Why Pinecone?\n  title: AI search with <span>Pinecone</span>\n  description: Launch, use, and scale your vector search service with our easy API, without worrying about infrastructure or algorithms. We'll keep it running smoothly and securely.\n  list:\n    - title: Easy\n      text: Get started on the free plan with an easy-to-use API or the Python client.\n      icon: '/images/easy-to-use-alt.svg'\n    - title: Performance at scale\n      text: Power search across billions of documents in milliseconds, combined with usage-based pricing for high-volume production applications.\n      icon: '/images/scalable-alt.svg'\n    - title: Pay for what you use\n      text: Start free, then pay only for what you use with usage-based pricing.\n      icon: '/images/usage-alt.svg'\n    - title: Fast\n      text: Ultra-low query latency, even with billions of items. Give users a great experience.\n      icon: '/images/fast-icon.svg'\n\n# Testimonial\ntestimonialSection:\n  text: Thanks to the Pinecone vector database we can run our high-performance applications across 10+ billion records without breaking a sweat.\n  author: Ohad Parush\n  position: Chief R&D Officer\n  logo: /images/gong-logo.png\n  logoWidth: 217\n\n# Use Cases\nuseCasesSection:\n  eyebrow: Use cases\n  title: What can you do with <span>AI search</span>?\n  list:\n    - title: Ecommerce (product) search\n      text: Transform the shopping experience by surfacing relevant products to your shopper’s search results.\n      icon: '/images/product-search.svg'\n    - title: Semantic search\n      text: Search for keywords across transcripts, resumes, and documents quickly and at scale. \n      icon: '/images/semantic-text-search2.svg'\n    - title: Audio or visual search \n      text: Easily search through millions of call transcriptions to identify the leading causes of customer calls or complaints.\n      icon: '/images/audio-search.svg'\n    - title: Question answering\n      text: Allow customers to  self-serve through applications matching questions with answers or adjacent relevant information.\n      icon: '/images/question-answering-2.svg'\n    - title: Code search\n      text: Engineers can search through code without knowing the exact syntax.\n      icon: '/images/code-search.svg'\n    - title: Multi modal search\n      text: Search by  keyword quickly across millions of images in a catalog.\n      icon: '/images/multi-modal-search2.svg'\n\n# Partners & integrations\npartnersSection:\n  title: Partners and integrations\n  logos:\n    - src: /images/openai-logo.png\n      alt: Open AI\n      width: 200\n      link: https://docs.pinecone.io/docs/openai\n\n    - src: /images/cohere.png\n      alt: Cohere\n      width: 170\n      link: https://docs.pinecone.io/docs/cohere\n\n    - src: /images/hugging-face.png\n      alt: Hugging Face\n      width: 65\n      link: https://docs.pinecone.io/docs/hugging-face-endpoints\n\n    - src: /images/deepset.png\n      alt: Deepset AI\n      width: 190\n      link: https://docs.pinecone.io/docs/haystack\n\n# Learn More\nlearnMoreSection:\n  title: Learn More\n  cards:\n    - title: \"E-book: Natural Language Processing (NLP) for Semantic Search\"\n      # text: Lorem ipsum dolor sit amet consectetuer elit gravidas.\n      image: /images/ebooks-icon.png\n      url: /learn/nlp/\n    - title: The Rise of Vector Data\n      # text: Lorem ipsum dolor sit amet consectetuer elit gravidas.\n      image: /images/blogs-icon.png\n      url: /learn/rise-vector-data/\n    - title: What is a vector database? \n      # text: Lorem ipsum dolor sit amet consectetuer elit gravidas rrerum adipliscit elit.\n      image: /images/blogs-icon.png\n      url: /learn/vector-database/\n    - title: \"Webinar: Semantic Search with NLP and a Vector Database\"\n      # text: Lorem ipsum dolor sit amet consectetuer elit gravidas.\n      image: /images/webinars-icon.png\n      url: https://www.youtube.com/watch?v=7RF03_WQJpQ\n\n# custom class for styling\ncustomClass: ai-search\n---\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc0a"
  },
  "filename": "_index.md",
  "title": "",
  "category": "false",
  "content": "---\n_build:\n  list: false\n  render: false\n---\n\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc0b"
  },
  "filename": "security.md",
  "title": "use-case",
  "category": "Security",
  "content": "---\nlayout: use-case\ntitle: Security\nurl: /security-use-case\n\nhero:\n  title: Vector database<br>built for <span>security</span>\n  subtitle: | \n    **Detect threats easily, in real-time with Pinecone**\n  description: |\n    Vector embeddings are a powerful tool that can improve the effectiveness of cybersecurity systems. By combining these embeddings with the power of Pinecone, you can add support for various use cases to your applications including fraud detection, anomaly detection, malware detection, and more. \n\n    Some of the largest enterprises trust Pinecone to store their data and power their critical applications. We work hard to earn and maintain that trust by treating security and reliability as a cornerstone of our company and product. We obsess over operations and security so you can focus on your application.\n  cta1:\n    href: https://app.pinecone.io/\n    text: Get Started Today\n  cta2:\n    href: /contact\n    text: Contact Us\n  img:\n    src: /images/security-use-case.svg\n    alt: Vector Database for Security\n    width: 650\n\n# Why Pinecone\npineconeSection:\n  eyebrow: Why Pinecone?\n  title: Trust Pinecone to manage <span>your data</span>\n  # description: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nunc vulputate libero et velit interdum, ac aliquet odio mattis lorem ipsum dolor sit.\n  list:\n    - title: Performance at scale\n      text: Power search across billions of documents in milliseconds, combined with usage-based pricing for high-volume production applications.\n      icon: '/images/scalable-alt.svg'\n    - title: Fast\n      text: Ultra-low query latency, even with billions of items. Give users a great experience.\n      icon: '/images/fast-icon.svg'\n    - title: Secure\n      text: SOC 2 Type II certified, GDPR-ready, and built to keep data secure.\n      icon: '/images/security.svg'\n    - title: Reliable\n      text: Industry leading response times and uptime SLAs.\n      icon: '/images/medal.svg'\n\n# Testimonial\ntestimonialSection:\n  title: Perception Point\n  logo: /images/perception-point-logo.svg\n  url: https://perception-point.io\n  description: \"Pinecone made it fast and easy to take vector search from research to production, without DevOps. Thanks to their performance and scalability we can provide accurate, reliable, and real-time threat detection to our customers.\"\n  authorName: Oded K.\n  authorTitle: R&D Group Lead\n  authorImage: /images/oded-kalev.jpeg\n  authorUrl: https://www.linkedin.com/in/oded-kalev-9a4667ba/\n\n# Use Cases\nuseCasesSection:\n  eyebrow: Use cases\n  title: What can you do with <span>Pinecone</span>?\n  list:\n\n    - title: Anomaly and threat detection\n      text: Capture and compare network information (i.e. souce, port number, protocol used) to detect anomalies or threats at scale.\n      icon: '/images/anomaly-detection-2.svg'\n\n    - title: Malware detection\n      text: Detect suspicious behavior as new software is downloaded to alert user of potential malware.\n      icon: '/images/malware-detection.svg'\n\n    - title: Spam / Phishing\n      text: Quickly detect and identify fraudulent email messages for your users. \n      icon: '/images/spam.svg'\n\n    - title: Fraud detection\n      text: Search through device fingerprints in real-time to detect fraudulent users and prevent access to a device.\n      icon: '/images/fraud-detection-2.svg'\n\n    - title: Observability\n      text: Detect similar security alerts to help your developer triage events faster. \n      icon: '/images/observability.svg'\n\n\n# Partners & integrations\npartnersSection:\n  title: Partners and integrations\n  logos:\n    - src: /images/openai-logo.png\n      alt: Open AI\n      width: 200\n      link: https://docs.pinecone.io/docs/openai\n\n    - src: /images/cohere.png\n      alt: Cohere\n      width: 170\n      link: https://docs.pinecone.io/docs/cohere\n\n    - src: /images/hugging-face.png\n      alt: Hugging Face\n      width: 65\n      link: https://docs.pinecone.io/docs/hugging-face-endpoints\n\n    - src: /images/deepset.png\n      alt: Deepset AI\n      width: 190\n      link: https://docs.pinecone.io/docs/haystack\n\n  learnMore: \"Learn more about our partners here: [pinecone.io/partners](/partners).\"\n\n# Learn More\nlearnMoreSection:\n  title: Learn More\n  cards:\n\n    - title: Expel case study\n      # text: Lorem ipsum dolor sit amet consectetuer elit gravidas.\n      image: /images/blogs-icon.png\n      url: /learn/expel-alert-similarity/\n\n    - title: Threat detection example app\n      # text: Lorem ipsum dolor sit amet consectetuer elit gravidas.\n      image: /images/documents-icon.png\n      url: https://docs.pinecone.io/docs/it-threat-detection\n\n    - title: Security webinar/panel recording \n      # text: Lorem ipsum dolor sit amet consectetuer elit gravidas rrerum adipliscit elit.\n      image: /images/videos-icon.png\n      url: https://www.youtube.com/watch?v=mWplVuntklI\n\n    # - title: \"Webinar: Semantic Search with NLP and a Vector Database\"\n    #   # text: Lorem ipsum dolor sit amet consectetuer elit gravidas.\n    #   image: /images/search-like-you-mean-it-thumbnail.jpg\n    #   url: https://www.youtube.com/watch?v=7RF03_WQJpQ\n\n# custom class for styling \ncustomClass: security-use-case\n---\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc0c"
  },
  "filename": "vector-search.md",
  "title": "index",
  "category": "\"Vector Database for Vector Search\"",
  "content": "---\nlayout: index\ntitle: \"Vector Database for Vector Search\"\nintro: \"The #1 vector database. Search through billions of items for similar matches to any object, in milliseconds. It’s the next generation of search, an API call away.\"\n\n# Hero\nhero:\n  title: Vector Search with Pinecone\n  description: The Pinecone <a href=\"/learn/vector-database/\">vector database</a> makes it easy to build high-performance vector search applications. Developer-friendly, fully managed, and easily scalable without infrastructure hassles.\n  cta1:\n    href: https://app.pinecone.io/\n    text: Sign Up for Free\n  cta2:\n    href: /contact/\n    text: or contact us\n  img:\n    src: /images2/pinecone-diagram.svg\n    alt: Pinecone Diagram\n    width: 447\n    height: 548\n\n# Use cases\nusecases:\n  eyebrow: Use Cases\n  title: What can you do with <span>vector search</span>?\n  description: |\n    Once you have [vector embeddings](/learn/vector-embeddings/), manage and search through them in Pinecone to power [semantic search](/ai-search/), recommenders, and other applications that rely on relevant information retrieval.\n  list:\n    - title: Search\n      icon: \"/images/semantic-text-search.svg\"\n      children:\n        - Semantic search\n        - Product search\n        - Multi-modal search\n        - Question-Answering\n\n    - title: Generation\n      icon: \"/images/deduplication2.svg\"\n      children:\n        - Chatbots\n        - Text generation\n        - Image generation\n\n    - title: Security\n      icon: \"/images/fraud-detection.svg\"\n      children:\n        - Anomaly Detection\n        - Fraud Detection\n        - Bot/threat detection\n        - Identity verification\n\n    - title: Personalization\n      icon: \"/images/personalization-alt.svg\"\n      children:\n        - Recommendations\n        - Feed ranking\n        - Ad targeting\n        - Candidate selection\n\n    - title: Analytics & ML\n      icon: \"/images/model-training.svg\"\n      children:\n        - Data labeling\n        - Model training\n        - Molecular search\n        - Generative AI\n\n    - title: Data Management\n      icon: \"/images/data-labeling.svg\"\n      children:\n        - Pattern matching\n        - Deduplication\n        - Grouping\n        - Tagging\n\n# Why Pinecone\nfeatures:\n  eyebrow: Why Pinecone\n  title: Fast, fresh, and filtered vector search.\n  list:\n    - title: Fast\n      text:\n      - Ultra-low query latency, even with billions of items. Give users a great experience.\n    - title: Fresh\n      text:\n      - Live index updates when you add, edit, or delete data. Your data is ready right away.\n    - title: Filtered\n      text:\n      - Combine vector search with metadata filters for more relevant and faster results.\n\n# Features\nfeatures2:\n  eyebrow: Managed Vector Search\n  title: |\n    Fully managed,</br><span>production-ready</span>\n  text: Launch, use, and scale your vector search service with our easy API, without worrying about infrastructure or algorithms. We'll keep it running smoothly and securely.\n  list:\n    - title: Easy to use\n      text: Get started on the free plan with an easy-to-use API or the Python client.\n      icon: '/images/easy-to-use.svg'\n    - title: Scalable\n      text: Scale from zero to billions of items, with no downtime and minimal latency impact.\n      icon: '/images/scalable.svg'\n    - title: Pay for what you use\n      text: Start free, then pay only for what you use with usage-based pricing.\n      icon: '/images/usage.svg'\n    - title: No ops overhead\n      text: No need to maintain infrastructure, monitor services, or troubleshoot algorithms.\n      icon: '/images/gear.svg'\n    - title: Reliable\n      text: Choose a cloud provider and region — we'll take care of uptime, consistency, and the rest.\n      icon: '/images/cloud.svg'\n    - title: Secure\n      text: Pinecone is SOC 2 Type II certified, GDPR-ready, and built to keep data secure.\n      icon: '/images/secure.svg'\n\n# Customers\ncustomers:\n  testimonial:\n    title: Perception Point\n    logo: /images/perception-point-logo.svg\n    url: https://perception-point.io\n    description: \"Pinecone made it fast and easy to take vector search from research to production, without DevOps. Thanks to their performance and scalability we can provide accurate, reliable, and real-time threat detection to our customers.\"\n    authorName: Oded K.\n    authorTitle: R&D Group Lead\n    authorImage: /images/oded-kalev.jpeg\n    authorUrl: https://www.linkedin.com/in/oded-kalev-9a4667ba/\n  logos:\n    - logo: /images/gong-logo.png\n      alt: Gong\n    - logo: /images/clubhouse-logo.png\n      alt: Clubhouse\n    - logo: /images/klue-logo.svg\n      alt: Klue\n    - logo: /images/expel-logo.png\n      alt: Expel\n    - logo: /images/course-hero.png\n      alt: Course Hero\n    - logo: /images/circleup-logo.svg\n      alt: CircleUp\n    - logo: /images/xandr-logo.png\n      alt: Xandr\n    - logo: /images/bamboohr-logo.svg\n      alt: BambooHR\n    - logo: /images/mem-logo.svg\n      alt: Mem\n    - logo: /images/cohere.png\n      alt: \"co:here\"\n---\n\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc0d"
  },
  "filename": "weaviate.md",
  "title": "true",
  "category": "true",
  "content": "---\nnoindex: true\nnoNav: true\nlayout: index-lp\ntitle: \"Weaviate Alternative for Vector Search\"\nintro: \"Search through billions of items for similar matches to any object, in milliseconds. It’s the next generation of search, an API call away.\"\n\n# Hero\nhero:\n  title: Production-ready alternative to Weaviate\n  description: Pinecone is a production-ready, fully managed vector database that makes it easy to build high-performance vector search applications. Users love the developer experience and not having to set up and manage infrastructure.\n  cta1:\n    href: https://app.pinecone.io/\n    text: Create account\n  cta2:\n    href: /contact/\n    text: or contact us\n  img:\n    src: /images2/pinecone-diagram.svg\n    alt: Pinecone Diagram\n    width: 447\n    height: 548\n\n# Markdown section\nmarkdownSection: |\n  ### Weaviate\n\n  Weaviate is an open-source vector search engine developed by SeMI Technologies. A managed service was recently announced, named Weaviate Cloud Service.\n\n  Users of Weaviate like that it provides modules for transforming raw data into vector embeddings. This is useful if you do not have any other means of deploying and managing embedding models and are not using a third-party embedding service like OpenAI, Cohere, or Hugging Face.\n\n  The “all-in-one” packaging makes Weaviate an option to consider for smaller projects that lack data engineering or ML engineering support.\n\n  A common complaint from users who tried Weaviate (both the self-hosted library and Weaviate Cloud Service) is the difficulty in getting it to run and scale reliably. For this reason, it may take considerable time and engineering effort to make Weaviate-based applications production-ready.\n\n\n  ### Pinecone\n\n  [Pinecone](https://www.pinecone.io) is a production-ready, fully managed vector database that makes it easy to build high-performance vector search applications. Users love the developer experience and not having to set up and manage infrastructure.\n\n  Pinecone does not host or run embeddings models. You need to run your embedding models and perform your transformations outside of Pinecone, and only then you can upload your embeddings into Pinecone.\n\n  Pinecone is cost-effective and provides great performance at scale. However, customers say developer experience is the biggest reason they chose Pinecone. The simple API, web console, and [documentation](https://www.pinecone.io/docs/) make it easy and quick to deploy and manage the vector database.\n\n# Use cases\nusecases:\n  eyebrow: Use Cases\n  title: What can you do with <span>vector search</span>?\n  description: |\n    Once you have <a href=\"/learn/vector-embeddings/\">vector embeddings</a>, manage and search through them in Pinecone to power <a href=\"/semantic-search/\">semantic search</a>, recommenders, and other applications that rely on relevant information retrieval.\n  list:\n    - title: Search\n      icon: \"/images/semantic-text-search.svg\"\n      children:\n        - Semantic search\n        - Product search\n        - Multi-modal search\n        - Question-Answering\n\n    - title: Generation\n      icon: \"/images/deduplication2.svg\"\n      children:\n        - Chatbots\n        - Text generation\n        - Image generation\n\n    - title: Security\n      icon: \"/images/fraud-detection.svg\"\n      children:\n        - Anomaly Detection\n        - Fraud Detection\n        - Bot/threat detection\n        - Identity verification\n\n    - title: Personalization\n      icon: \"/images/personalization-alt.svg\"\n      children:\n        - Recommendations\n        - Feed ranking\n        - Ad targeting\n        - Candidate selection\n\n    - title: Analytics & ML\n      icon: \"/images/model-training.svg\"\n      children:\n        - Data labeling\n        - Model training\n        - Molecular search\n        - Generative AI\n\n    - title: Data Management\n      icon: \"/images/data-labeling.svg\"\n      children:\n        - Pattern matching\n        - Deduplication\n        - Grouping\n        - Tagging\n\n# Why Pinecone\nfeatures:\n  eyebrow: Why Pinecone\n  title: Fast, fresh, and filtered vector search.\n  list:\n    - title: Fast\n      text:\n      - Ultra-low query latency, even with billions of items. Give users a great experience.\n    - title: Fresh\n      text:\n      - Live index updates when you add, edit, or delete data. Your data is ready right away.\n    - title: Filtered\n      text:\n      - Combine vector search with metadata filters for more relevant and faster results.\n\n# Features\nfeatures2:\n  eyebrow: Production-Ready Managed Service\n  title: |\n    Fully managed and production-ready <span>alternative to Weaviate</span>\n  text: Launch, use, and scale your vector search service with our easy API, without worrying about infrastructure or algorithms. We'll keep it running smoothly and securely.\n  list:\n    - title: Easy to use\n      text: Get started on the free plan with an easy-to-use API or the Python client.\n      icon: '/images/easy-to-use.svg'\n    - title: Scalable\n      text: Scale from zero to billions of items, with no downtime and minimal latency impact.\n      icon: '/images/scalable.svg'\n    - title: Pay for what you use\n      text: Start free, then pay only for what you use with usage-based pricing.\n      icon: '/images/usage.svg'\n    - title: No ops overhead\n      text: No need to maintain infrastructure, monitor services, or troubleshoot algorithms.\n      icon: '/images/gear.svg'\n    - title: Reliable\n      text: Choose a cloud provider and region — we'll take care of uptime, consistency, and the rest.\n      icon: '/images/cloud.svg'\n    - title: Secure\n      text: Pinecone is SOC 2 Type II certified, GDPR-ready, and built to keep data secure.\n      icon: '/images/secure.svg'\n\n# Customers\ncustomers:\n  testimonial:\n    title: Perception Point\n    logo: /images/perception-point-logo.svg\n    url: https://perception-point.io\n    description: \"Pinecone made it fast and easy to take vector search from research to production, without DevOps. Thanks to their performance and scalability we can provide accurate, reliable, and real-time threat detection to our customers.\"\n    authorName: Oded K.\n    authorTitle: R&D Group Lead\n    authorImage: /images/oded-kalev.jpeg\n    authorUrl: https://www.linkedin.com/in/oded-kalev-9a4667ba/\n  logos:\n    - logo: /images/gong-logo.png\n      alt: Gong\n    - logo: /images/clubhouse-logo.png\n      alt: Clubhouse\n    - logo: /images/klue-logo.svg\n      alt: Klue\n    - logo: /images/expel-logo.png\n      alt: Expel\n    - logo: /images/course-hero.png\n      alt: Course Hero\n    - logo: /images/circleup-logo.svg\n      alt: CircleUp\n    - logo: /images/xandr-logo.png\n      alt: Xandr\n    - logo: /images/bamboohr-logo.svg\n      alt: BambooHR\n    - logo: /images/mem-logo.svg\n      alt: Mem\n    - logo: /images/cohere.png\n      alt: \"co:here\"\n\n# Contact\nhero2:\n  title: Talk to the <span>vector search experts</span>\n  description: Send us your questions about choosing between Pinecone and Weaviate. We’ll schedule a time to learn and share more with you.\ncontact:\n    enabled: true\n    info: |\n        Want to try Pinecone?<br>\n        [Start here.](https://app.pinecone.io)\n\n        Press and general inquiries:<br>\n        info@pinecone.io\n\n        Support:<br>\n        support@pinecone.io<br>\n        [Read the Docs](/docs/)\n---"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc0e"
  },
  "filename": "vector-database.md",
  "title": "index",
  "category": "\"Vector Database for Vector Search\"",
  "content": "---\nlayout: index\ntitle: \"Vector Database for Vector Search\"\nintro: \"The #1 vector database. Search through billions of items for similar matches to any object, in milliseconds. It’s the next generation of search, an API call away.\"\n\n# Hero\nhero:\n  title: The Vector Database\n  description: The Pinecone <a href=\"/learn/vector-database/\">vector database</a> makes it easy to build high-performance vector search applications. Developer-friendly, fully managed, and easily scalable without infrastructure hassles.\n  cta1:\n    href: https://app.pinecone.io/\n    text: Sign Up for Free\n  cta2:\n    href: /contact/\n    text: or contact us\n  img:\n    src: /images2/pinecone-diagram.svg\n    alt: Pinecone Diagram\n    width: 447\n    height: 548\n\n# Use cases\nusecases:\n  eyebrow: Use Cases\n  title: What can you do with <span>vector databases</span>?\n  description: |\n    Once you have [vector embeddings](/learn/vector-embeddings/), manage and search through them in Pinecone to power [semantic search](/ai-search/), recommenders, and other applications that rely on relevant information retrieval.\n  list:\n    - title: Search\n      icon: \"/images/semantic-text-search.svg\"\n      children:\n        - Semantic search\n        - Product search\n        - Multi-modal search\n        - Question-Answering\n\n    - title: Generation\n      icon: \"/images/deduplication2.svg\"\n      children:\n        - Chatbots\n        - Text generation\n        - Image generation\n\n    - title: Security\n      icon: \"/images/fraud-detection.svg\"\n      children:\n        - Anomaly Detection\n        - Fraud Detection\n        - Bot/threat detection\n        - Identity verification\n\n    - title: Personalization\n      icon: \"/images/personalization-alt.svg\"\n      children:\n        - Recommendations\n        - Feed ranking\n        - Ad targeting\n        - Candidate selection\n\n    - title: Analytics & ML\n      icon: \"/images/model-training.svg\"\n      children:\n        - Data labeling\n        - Model training\n        - Molecular search\n        - Generative AI\n\n    - title: Data Management\n      icon: \"/images/data-labeling.svg\"\n      children:\n        - Pattern matching\n        - Deduplication\n        - Grouping\n        - Tagging\n\n# Why Pinecone\nfeatures:\n  eyebrow: Why Pinecone\n  title: Fast, fresh, and filtered vector search.\n  list:\n    - title: Fast\n      text:\n      - Ultra-low query latency, even with billions of items. Give users a great experience.\n    - title: Fresh\n      text:\n      - Live index updates when you add, edit, or delete data. Your data is ready right away.\n    - title: Filtered\n      text:\n      - Combine vector search with metadata filters for more relevant and faster results.\n\n# Features\nfeatures2:\n  eyebrow: Managed Vector Database\n  title: |\n    Fully managed,</br><span>production-ready</span>\n  text: Launch, use, and scale your vector search service with our easy API, without worrying about infrastructure or algorithms. We'll keep it running smoothly and securely.\n  list:\n    - title: Easy to use\n      text: Get started on the free plan with an easy-to-use API or the Python client.\n      icon: '/images/easy-to-use.svg'\n    - title: Scalable\n      text: Scale from zero to billions of items, with no downtime and minimal latency impact.\n      icon: '/images/scalable.svg'\n    - title: Pay for what you use\n      text: Start free, then pay only for what you use with usage-based pricing.\n      icon: '/images/usage.svg'\n    - title: No ops overhead\n      text: No need to maintain infrastructure, monitor services, or troubleshoot algorithms.\n      icon: '/images/gear.svg'\n    - title: Reliable\n      text: Choose a cloud provider and region — we'll take care of uptime, consistency, and the rest.\n      icon: '/images/cloud.svg'\n    - title: Secure\n      text: Pinecone is SOC 2 Type II certified, GDPR-ready, and built to keep data secure.\n      icon: '/images/secure.svg'\n\n# Customers\ncustomers:\n  testimonial:\n    title: Perception Point\n    logo: /images/perception-point-logo.svg\n    url: https://perception-point.io\n    description: \"Pinecone made it fast and easy to take vector search from research to production, without DevOps. Thanks to their performance and scalability we can provide accurate, reliable, and real-time threat detection to our customers.\"\n    authorName: Oded K.\n    authorTitle: R&D Group Lead\n    authorImage: /images/oded-kalev.jpeg\n    authorUrl: https://www.linkedin.com/in/oded-kalev-9a4667ba/\n  logos:\n    - logo: /images/gong-logo.png\n      alt: Gong\n    - logo: /images/clubhouse-logo.png\n      alt: Clubhouse\n    - logo: /images/klue-logo.svg\n      alt: Klue\n    - logo: /images/expel-logo.png\n      alt: Expel\n    - logo: /images/course-hero.png\n      alt: Course Hero\n    - logo: /images/circleup-logo.svg\n      alt: CircleUp\n    - logo: /images/xandr-logo.png\n      alt: Xandr\n    - logo: /images/bamboohr-logo.svg\n      alt: BambooHR\n    - logo: /images/mem-logo.svg\n      alt: Mem\n    - logo: /images/cohere.png\n      alt: \"co:here\"\n---\n\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc0f"
  },
  "filename": "opensearch.md",
  "title": "true",
  "category": "true",
  "content": "---\nnoindex: true\nnoNav: true\nlayout: index-lp\ntitle: \"OpenSearch Alternative for Vector Search\"\nintro: \"Search through billions of items for similar matches to any object, in milliseconds. It’s the next generation of search, an API call away.\"\n\n# Hero\nhero:\n  title: Scalable alternative to OpenSearch\n  description: Pinecone is a fully managed vector database that makes it easy and cost-efficient to build vector search applications. Users love the developer experience, high performance, and low cost.\n  cta1:\n    href: https://app.pinecone.io/\n    text: Create account\n  cta2:\n    href: /contact/\n    text: or contact us\n  img:\n    src: /images2/pinecone-diagram.svg\n    alt: Pinecone Diagram\n    width: 447\n    height: 548\n\n# Markdown section\nmarkdownSection: |\n  ### OpenSearch (managed by AWS)\n\n  OpenSearch is an open-source search solution with a managed service provided by AWS.\n\n  Users of OpenSearch like that it comes from a company they know, whose products they might already be using.\n\n  Users who tried OpenSearch for vector search have reported a steep tradeoff between cost and search latency, especially for datasets above 10M vector embeddings. Achieving low latency with an index of that size (or larger) could be prohibitively expensive for some. Additionally, setting up and optimizing OpenSearch might take considerable time for users who aren't already running it in production for other use cases.\n\n  ### Pinecone\n\n  [Pinecone](https://www.pinecone.io) is a fully managed vector database that makes it easy to build high-performance vector search applications. Users love the ability to start within minutes, scale up to over billions of vectors, and sit back while Pinecone handles all the operational complexity to keep latencies low and availability high. And with low, usage-based pricing, engineers don't have to compromise on performance.\n\n# Use cases\nusecases:\n  eyebrow: Use Cases\n  title: What can you do with <span>vector search</span>?\n  description: |\n    Once you have <a href=\"/learn/vector-embeddings/\">vector embeddings</a>, manage and search through them in Pinecone to power <a href=\"/semantic-search/\">semantic search</a>, recommenders, and other applications that rely on relevant information retrieval.\n  list:\n    - title: Search\n      icon: \"/images/semantic-text-search.svg\"\n      children:\n        - Semantic search\n        - Product search\n        - Multi-modal search\n        - Question-Answering\n\n    - title: Generation\n      icon: \"/images/deduplication2.svg\"\n      children:\n        - Chatbots\n        - Text generation\n        - Image generation\n\n    - title: Security\n      icon: \"/images/fraud-detection.svg\"\n      children:\n        - Anomaly Detection\n        - Fraud Detection\n        - Bot/threat detection\n        - Identity verification\n\n    - title: Personalization\n      icon: \"/images/personalization-alt.svg\"\n      children:\n        - Recommendations\n        - Feed ranking\n        - Ad targeting\n        - Candidate selection\n\n    - title: Analytics & ML\n      icon: \"/images/model-training.svg\"\n      children:\n        - Data labeling\n        - Model training\n        - Molecular search\n        - Generative AI\n\n    - title: Data Management\n      icon: \"/images/data-labeling.svg\"\n      children:\n        - Pattern matching\n        - Deduplication\n        - Grouping\n        - Tagging\n\n# Why Pinecone\nfeatures:\n  eyebrow: Why Pinecone\n  title: Fast, fresh, and filtered vector search.\n  list:\n    - title: Fast\n      text:\n      - Ultra-low query latency, even with billions of items. Give users a great experience.\n    - title: Fresh\n      text:\n      - Live index updates when you add, edit, or delete data. Your data is ready right away.\n    - title: Filtered\n      text:\n      - Combine vector search with metadata filters for more relevant and faster results.\n\n# Features\nfeatures2:\n  eyebrow: Scalable Managed Service\n  title: |\n    Fully managed and production-ready <span>alternative to OpenSearch from AWS</span>\n  text: Launch, use, and scale your vector search service with our easy API, without worrying about infrastructure or algorithms. We'll keep it running smoothly and securely.\n  list:\n    - title: Easy to use\n      text: Get started on the free plan with an easy-to-use API or the Python client.\n      icon: '/images/easy-to-use.svg'\n    - title: Scalable\n      text: Scale from zero to billions of items, with no downtime and minimal latency impact.\n      icon: '/images/scalable.svg'\n    - title: Pay for what you use\n      text: Start free, then pay only for what you use with usage-based pricing.\n      icon: '/images/usage.svg'\n    - title: No ops overhead\n      text: No need to maintain infrastructure, monitor services, or troubleshoot algorithms.\n      icon: '/images/gear.svg'\n    - title: Reliable\n      text: Choose a cloud provider and region — we'll take care of uptime, consistency, and the rest.\n      icon: '/images/cloud.svg'\n    - title: Secure\n      text: Pinecone is SOC 2 Type II certified, GDPR-ready, and built to keep data secure.\n      icon: '/images/secure.svg'\n\n# Customers\ncustomers:\n  testimonial:\n    title: Perception Point\n    logo: /images/perception-point-logo.svg\n    url: https://perception-point.io\n    description: \"Pinecone made it fast and easy to take vector search from research to production, without DevOps. Thanks to their performance and scalability we can provide accurate, reliable, and real-time threat detection to our customers.\"\n    authorName: Oded K.\n    authorTitle: R&D Group Lead\n    authorImage: /images/oded-kalev.jpeg\n    authorUrl: https://www.linkedin.com/in/oded-kalev-9a4667ba/\n  logos:\n    - logo: /images/gong-logo.png\n      alt: Gong\n    - logo: /images/clubhouse-logo.png\n      alt: Clubhouse\n    - logo: /images/klue-logo.svg\n      alt: Klue\n    - logo: /images/expel-logo.png\n      alt: Expel\n    - logo: /images/course-hero.png\n      alt: Course Hero\n    - logo: /images/circleup-logo.svg\n      alt: CircleUp\n    - logo: /images/xandr-logo.png\n      alt: Xandr\n    - logo: /images/bamboohr-logo.svg\n      alt: BambooHR\n    - logo: /images/mem-logo.svg\n      alt: Mem\n    - logo: /images/cohere.png\n      alt: \"co:here\"\n\n# Contact\nhero2:\n  title: Talk to the <span>vector search experts</span>\n  description: Send us your questions about choosing between Pinecone and OpenSearch on AWS. We’ll schedule a time to learn and share more with you.\ncontact:\n    enabled: true\n    info: |\n        Want to try Pinecone?<br>\n        [Start here.](https://app.pinecone.io)\n\n        Press and general inquiries:<br>\n        info@pinecone.io\n\n        Support:<br>\n        support@pinecone.io<br>\n        [Read the Docs](/docs/)\n---"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc10"
  },
  "filename": "milvus.md",
  "title": "true",
  "category": "true",
  "content": "---\nnoindex: true\nnoNav: true\nlayout: index-lp\ntitle: \"Milvus Alternative for Vector Search\"\nintro: \"Search through billions of items for similar matches to any object, in milliseconds. It’s the next generation of search, an API call away.\"\n\n# Hero\nhero:\n  title: Production-ready alternative to Milvus\n  description: Pinecone is a production-ready, fully managed vector database that makes it easy to build high-performance vector search applications. Users love the developer experience and not having to set up and manage infrastructure.\n  cta1:\n    href: https://app.pinecone.io/\n    text: Create account\n  cta2:\n    href: /contact/\n    text: or contact us\n  img:\n    src: /images2/pinecone-diagram.svg\n    alt: Pinecone Diagram\n    width: 447\n    height: 548\n\n# Markdown section\nmarkdownSection: |\n  ### Milvus\n\n  Milvus is an open-source vector search engine developed by Zilliz, based in China.\n\n  Users of Milvus like that it provides an open-source library for vector search. This is useful if you want to be part of an open-source community and potentially contribute back to the project.\n\n  A common complaint from users who tried Milvus is the difficulty in getting it to run, scale, and integrate with other systems reliably. Users who built production systems on Milvus 1.0 reported facing significant engineering challenges when Milvus 2.0 was released with breaking changes. For this reason, engineering teams should carefully evaluate the time and effort they are willing to spend on implementing and maintaining such a system.\n\n  ### Pinecone\n\n  [Pinecone](https://www.pinecone.io) is a production-ready, fully managed vector database that makes it easy to build high-performance vector search applications. Users love the developer experience and not having to set up and manage infrastructure.\n\n  Pinecone does not host or run embeddings models. You need to run your embedding models and perform your transformations outside of Pinecone, and only then you can upload your embeddings into Pinecone.\n\n  Pinecone is cost-effective and provides great performance at scale. However, customers say developer experience is the biggest reason they chose Pinecone. The simple API, web console, and [documentation](https://www.pinecone.io/docs/) make it easy and quick to deploy and manage the vector database.\n\n# Use cases\nusecases:\n  eyebrow: Use Cases\n  title: What can you do with <span>vector search</span>?\n  description: |\n    Once you have <a href=\"/learn/vector-embeddings/\">vector embeddings</a>, manage and search through them in Pinecone to power <a href=\"/semantic-search/\">semantic search</a>, recommenders, and other applications that rely on relevant information retrieval.\n  list:\n    - title: Search\n      icon: \"/images/semantic-text-search.svg\"\n      children:\n        - Semantic search\n        - Product search\n        - Multi-modal search\n        - Question-Answering\n\n    - title: Generation\n      icon: \"/images/deduplication2.svg\"\n      children:\n        - Chatbots\n        - Text generation\n        - Image generation\n\n    - title: Security\n      icon: \"/images/fraud-detection.svg\"\n      children:\n        - Anomaly Detection\n        - Fraud Detection\n        - Bot/threat detection\n        - Identity verification\n\n    - title: Personalization\n      icon: \"/images/personalization-alt.svg\"\n      children:\n        - Recommendations\n        - Feed ranking\n        - Ad targeting\n        - Candidate selection\n\n    - title: Analytics & ML\n      icon: \"/images/model-training.svg\"\n      children:\n        - Data labeling\n        - Model training\n        - Molecular search\n        - Generative AI\n\n    - title: Data Management\n      icon: \"/images/data-labeling.svg\"\n      children:\n        - Pattern matching\n        - Deduplication\n        - Grouping\n        - Tagging\n\n# Why Pinecone\nfeatures:\n  eyebrow: Why Pinecone\n  title: Fast, fresh, and filtered vector search.\n  list:\n    - title: Fast\n      text:\n      - Ultra-low query latency, even with billions of items. Give users a great experience.\n    - title: Fresh\n      text:\n      - Live index updates when you add, edit, or delete data. Your data is ready right away.\n    - title: Filtered\n      text:\n      - Combine vector search with metadata filters for more relevant and faster results.\n\n# Features\nfeatures2:\n  eyebrow: Production-Ready Managed Service\n  title: |\n    Fully managed and production-ready <span>alternative to Milvus</span>\n  text: Launch, use, and scale your vector search service with our easy API, without worrying about infrastructure or algorithms. We'll keep it running smoothly and securely.\n  list:\n    - title: Easy to use\n      text: Get started on the free plan with an easy-to-use API or the Python client.\n      icon: '/images/easy-to-use.svg'\n    - title: Scalable\n      text: Scale from zero to billions of items, with no downtime and minimal latency impact.\n      icon: '/images/scalable.svg'\n    - title: Pay for what you use\n      text: Start free, then pay only for what you use with usage-based pricing.\n      icon: '/images/usage.svg'\n    - title: No ops overhead\n      text: No need to maintain infrastructure, monitor services, or troubleshoot algorithms.\n      icon: '/images/gear.svg'\n    - title: Reliable\n      text: Choose a cloud provider and region — we'll take care of uptime, consistency, and the rest.\n      icon: '/images/cloud.svg'\n    - title: Secure\n      text: Pinecone is SOC 2 Type II certified, GDPR-ready, and built to keep data secure.\n      icon: '/images/secure.svg'\n\n# Customers\ncustomers:\n  testimonial:\n    title: Perception Point\n    logo: /images/perception-point-logo.svg\n    url: https://perception-point.io\n    description: \"Pinecone made it fast and easy to take vector search from research to production, without DevOps. Thanks to their performance and scalability we can provide accurate, reliable, and real-time threat detection to our customers.\"\n    authorName: Oded K.\n    authorTitle: R&D Group Lead\n    authorImage: /images/oded-kalev.jpeg\n    authorUrl: https://www.linkedin.com/in/oded-kalev-9a4667ba/\n  logos:\n    - logo: /images/gong-logo.png\n      alt: Gong\n    - logo: /images/clubhouse-logo.png\n      alt: Clubhouse\n    - logo: /images/klue-logo.svg\n      alt: Klue\n    - logo: /images/expel-logo.png\n      alt: Expel\n    - logo: /images/course-hero.png\n      alt: Course Hero\n    - logo: /images/circleup-logo.svg\n      alt: CircleUp\n    - logo: /images/xandr-logo.png\n      alt: Xandr\n    - logo: /images/bamboohr-logo.svg\n      alt: BambooHR\n    - logo: /images/mem-logo.svg\n      alt: Mem\n    - logo: /images/cohere.png\n      alt: \"co:here\"\n\n# Contact\nhero2:\n  title: Talk to the <span>vector search experts</span>\n  description: Send us your questions about choosing between Pinecone and Milvus. We’ll schedule a time to learn and share more with you.\ncontact:\n    enabled: true\n    info: |\n        Want to try Pinecone?<br>\n        [Start here.](https://app.pinecone.io)\n\n        Press and general inquiries:<br>\n        info@pinecone.io\n\n        Support:<br>\n        support@pinecone.io<br>\n        [Read the Docs](/docs/)\n---"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc11"
  },
  "filename": "elasticsearch.md",
  "title": "true",
  "category": "true",
  "content": "---\nnoindex: true\nnoNav: true\nlayout: index-lp\ntitle: \"Elasticsearch Alternative for Vector Search\"\nintro: \"Search through billions of items for similar matches to any object, in milliseconds. It’s the next generation of search, an API call away.\"\n\n# Hero\nhero:\n  title: Scalable alternative to Elasticsearch\n  description: Pinecone is a fully managed vector database that makes it easy and cost-efficient to build vector search applications. Users love the developer experience, high performance, and low cost.\n  cta1:\n    href: https://app.pinecone.io/\n    text: Create account\n  cta2:\n    href: /contact/\n    text: or contact us\n  img:\n    src: /images2/pinecone-diagram.svg\n    alt: Pinecone Diagram\n    width: 447\n    height: 548\n\n# Markdown section\nmarkdownSection: |\n  ### Elasticsearch (managed by Elastic)\n\n  Elasticsearch is an open-source search solution with a managed service provided by Elastic.\n\n  Users of Elasticsearch like that it comes from a company they know, whose products they might already be using.\n\n  Users who tried Elasticsearch for vector search have reported a steep tradeoff between cost and search latency, especially for datasets above 10M vector embeddings. Achieving low latency with an index of that size (or larger) could be prohibitively expensive for some.\n\n  ### Pinecone\n\n  [Pinecone](https://www.pinecone.io) is a fully managed vector database that makes it easy to build high-performance vector search applications. Users love the ability to start within minutes, scale up to over billions of vectors, and sit back while Pinecone handles all the operational complexity to keep latencies low and availability high. And with low, usage-based pricing, engineers don't have to compromise on performance.\n\n# Use cases\nusecases:\n  eyebrow: Use Cases\n  title: What can you do with <span>vector search</span>?\n  description: |\n    Once you have <a href=\"/learn/vector-embeddings/\">vector embeddings</a>, manage and search through them in Pinecone to power <a href=\"/semantic-search/\">semantic search</a>, recommenders, and other applications that rely on relevant information retrieval.\n  list:\n    - title: Search\n      icon: \"/images/semantic-text-search.svg\"\n      children:\n        - Semantic search\n        - Product search\n        - Multi-modal search\n        - Question-Answering\n\n    - title: Generation\n      icon: \"/images/deduplication2.svg\"\n      children:\n        - Chatbots\n        - Text generation\n        - Image generation\n\n    - title: Security\n      icon: \"/images/fraud-detection.svg\"\n      children:\n        - Anomaly Detection\n        - Fraud Detection\n        - Bot/threat detection\n        - Identity verification\n\n    - title: Personalization\n      icon: \"/images/personalization-alt.svg\"\n      children:\n        - Recommendations\n        - Feed ranking\n        - Ad targeting\n        - Candidate selection\n\n    - title: Analytics & ML\n      icon: \"/images/model-training.svg\"\n      children:\n        - Data labeling\n        - Model training\n        - Molecular search\n        - Generative AI\n\n    - title: Data Management\n      icon: \"/images/data-labeling.svg\"\n      children:\n        - Pattern matching\n        - Deduplication\n        - Grouping\n        - Tagging\n\n# Why Pinecone\nfeatures:\n  eyebrow: Why Pinecone\n  title: Fast, fresh, and filtered vector search.\n  list:\n    - title: Fast\n      text:\n      - Ultra-low query latency, even with billions of items. Give users a great experience.\n    - title: Fresh\n      text:\n      - Live index updates when you add, edit, or delete data. Your data is ready right away.\n    - title: Filtered\n      text:\n      - Combine vector search with metadata filters for more relevant and faster results.\n\n# Features\nfeatures2:\n  eyebrow: Scalable Managed Service\n  title: |\n    Fully managed and production-ready <span>alternative to Elasticsearch</span>\n  text: Launch, use, and scale your vector search service with our easy API, without worrying about infrastructure or algorithms. We'll keep it running smoothly and securely.\n  list:\n    - title: Easy to use\n      text: Get started on the free plan with an easy-to-use API or the Python client.\n      icon: '/images/easy-to-use.svg'\n    - title: Scalable\n      text: Scale from zero to billions of items, with no downtime and minimal latency impact.\n      icon: '/images/scalable.svg'\n    - title: Pay for what you use\n      text: Start free, then pay only for what you use with usage-based pricing.\n      icon: '/images/usage.svg'\n    - title: No ops overhead\n      text: No need to maintain infrastructure, monitor services, or troubleshoot algorithms.\n      icon: '/images/gear.svg'\n    - title: Reliable\n      text: Choose a cloud provider and region — we'll take care of uptime, consistency, and the rest.\n      icon: '/images/cloud.svg'\n    - title: Secure\n      text: Pinecone is SOC 2 Type II certified, GDPR-ready, and built to keep data secure.\n      icon: '/images/secure.svg'\n\n# Customers\ncustomers:\n  testimonial:\n    title: Perception Point\n    logo: /images/perception-point-logo.svg\n    url: https://perception-point.io\n    description: \"Pinecone made it fast and easy to take vector search from research to production, without DevOps. Thanks to their performance and scalability we can provide accurate, reliable, and real-time threat detection to our customers.\"\n    authorName: Oded K.\n    authorTitle: R&D Group Lead\n    authorImage: /images/oded-kalev.jpeg\n    authorUrl: https://www.linkedin.com/in/oded-kalev-9a4667ba/\n  logos:\n    - logo: /images/gong-logo.png\n      alt: Gong\n    - logo: /images/clubhouse-logo.png\n      alt: Clubhouse\n    - logo: /images/klue-logo.svg\n      alt: Klue\n    - logo: /images/expel-logo.png\n      alt: Expel\n    - logo: /images/course-hero.png\n      alt: Course Hero\n    - logo: /images/circleup-logo.svg\n      alt: CircleUp\n    - logo: /images/xandr-logo.png\n      alt: Xandr\n    - logo: /images/bamboohr-logo.svg\n      alt: BambooHR\n    - logo: /images/mem-logo.svg\n      alt: Mem\n    - logo: /images/cohere.png\n      alt: \"co:here\"\n\n# Contact\nhero2:\n  title: Talk to the <span>vector search experts</span>\n  description: Send us your questions about choosing between Pinecone and Elastic. We’ll schedule a time to learn and share more with you.\ncontact:\n    enabled: true\n    info: |\n        Want to try Pinecone?<br>\n        [Start here.](https://app.pinecone.io)\n\n        Press and general inquiries:<br>\n        info@pinecone.io\n\n        Support:<br>\n        support@pinecone.io<br>\n        [Read the Docs](/docs/)\n---"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc12"
  },
  "filename": "vertex.md",
  "title": "true",
  "category": "true",
  "content": "---\nnoindex: true\nnoNav: true\nlayout: index-lp\ntitle: \"Vertex AI Matching Engine Alternative for Vector Search\"\nintro: \"Search through billions of items for similar matches to any object, in milliseconds. It’s the next generation of search, an API call away.\"\n\n# Hero\nhero:\n  title: Dev-friendly alternative to Google Vertex AI Matching Engine\n  description: Pinecone makes it easy to build high-performance vector search applications. It’s a managed, cloud-native vector database with a simple API that you can start using and scaling within minutes. Developers love the simple API, full metadata filtering, extensive documentation, and low cost compared to Vertex AI Matching Engine from Google.\n  cta1:\n    href: https://app.pinecone.io/\n    text: Create account\n  cta2:\n    href: /contact/\n    text: or contact us\n  img:\n    src: /images2/pinecone-diagram.svg\n    alt: Pinecone Diagram\n    width: 447\n    height: 548\n\n# Table\ntableSection:\n  title: \"Compare: Matching Engine vs Pinecone\"\n  heading1: Google Vertex AI Matching Engine\n  heading2: Pinecone\n  list:\n    - category: Cost by index size\n      list:\n        - item: Data processing\n          \n          selfHosted: \"$3 per GiB\"\n          pinecone: Free\n        - item: 100k vectors\n          \n          selfHosted: $1.064\n          pinecone: $0.070\n        - item: 500k vectors\n          \n          selfHosted: $1.064\n          pinecone: $0.070\n        - item: 1M vectors\n          \n          selfHosted: $1.064\n          pinecone: $0.070\n        - item: 5M vectors\n          \n          selfHosted: $1.064\n          pinecone: $0.075\n        - item: 10M vectors\n          \n          selfHosted: $1.064\n          pinecone: $0.150\n    - category: Features\n      list:\n        - item: Vector search (ANN)\n          \n          selfHosted: true\n          pinecone: true\n        - item: Exact vector search (kNN)\n          \n          selfHosted: true\n          pinecone: true\n        - item: Real-time Index Updates\n          \n          selfHosted: false\n          pinecone: true\n        - item: Full metadata filtering\n          \n          selfHosted: false\n          pinecone: true\n        - item: Namespacing\n          \n          selfHosted: false\n          pinecone: true\n        - item: Query diversification\n          \n          selfHosted: true\n          pinecone: Coming soon\n        - item: Hybrid search (sparse + dense vectors)\n          \n          selfHosted: false\n          pinecone: Coming soon\n        - item: Run in GCP\n          \n          selfHosted: true\n          pinecone: true\n        - item: Run in AWS\n          \n          selfHosted: false\n          pinecone: true\n    - category: Support\n      list:\n        - item: Premium support\n          \n          selfHosted: Costs $29–$12.5k/month + 3–4% of monthly charges\n          pinecone: Included\n  ctaBtn:\n    url: https://app.pinecone.io\n    text: Get Started\n\n# Markdown section\nmarkdownSection: |\n  ### Vertex AI\n\n  [Vertex AI Matching Engine](https://cloud.google.com/vertex-ai/docs/matching-engine/overview) is a subset of Vertex AI services for semantic search. It includes (1) a set of model architectures for training embeddings, and (2) an approximate nearest neighbor (ANN) service. The two parts can be used together or separately. For instance, it is possible to use the Matching Engine’s model training capabilities to generate embeddings, and then use these embeddings elsewhere (eg, in Pinecone). [The Vertex AI Matching Engine ANN service](https://cloud.google.com/vertex-ai/docs/matching-engine/ann-service-overview) is based on ScaNN, Google’s ANN algorithm.\n\n  Besides the comparatively high cost and limited features, a common complaint from those who’ve tried it is the developer experience of using Vertex AI and its documentation. For instance, managing indexes is more complex than other similar services because it requires three steps: Creating the index (45+ minutes), creating index endpoints, and deploying the index (45+ minutes). \n\n  Quotes from users who’ve tried Matching Enginge:\n  “The [Matching Engine] API is painful to use.”\n  “[Matching Enginge] takes several hours to build a new index after every update.”\n  “For 2 indexes of 10M 768-dims, [Matching Engine] is $1,700/month. Pinecone is $216/month.”\n  “It's hard to set up an experiment”\n  “... Pinecone is much easier to use than Google Vertex Matching.”\n\n  ### Pinecone\n\n  [Pinecone](https://www.pinecone.io) makes it easy to build high-performance vector search applications. It’s a managed, cloud-native vector database with a simple API and no infrastructure hassles.\n\n  Pinecone does not host or run embeddings models. You need to run your embedding models and perform your transformations outside of Pinecone, and only then you can upload your embeddings into Pinecone.\n\n  Pinecone is cost-effective and provides great performance at scale. However, customers say **developer experience** is the biggest reason they chose Pinecone. The simple API, web console, and documentation make it easy and quick to deploy and manage the vector database.\n\n# Use cases\nusecases:\n  eyebrow: Use Cases\n  title: What can you do with <span>vector search</span>?\n  description: |\n    Once you have <a href=\"/learn/vector-embeddings/\">vector embeddings</a>, manage and search through them in Pinecone to power <a href=\"/semantic-search/\">semantic search</a>, recommenders, and other applications that rely on relevant information retrieval.\n  list:\n    - title: Search\n      icon: \"/images/semantic-text-search.svg\"\n      children:\n        - Semantic search\n        - Product search\n        - Multi-modal search\n        - Question-Answering\n\n    - title: Generation\n      icon: \"/images/deduplication2.svg\"\n      children:\n        - Chatbots\n        - Text generation\n        - Image generation\n\n    - title: Security\n      icon: \"/images/fraud-detection.svg\"\n      children:\n        - Anomaly Detection\n        - Fraud Detection\n        - Bot/threat detection\n        - Identity verification\n\n    - title: Personalization\n      icon: \"/images/personalization-alt.svg\"\n      children:\n        - Recommendations\n        - Feed ranking\n        - Ad targeting\n        - Candidate selection\n\n    - title: Analytics & ML\n      icon: \"/images/model-training.svg\"\n      children:\n        - Data labeling\n        - Model training\n        - Molecular search\n        - Generative AI\n\n    - title: Data Management\n      icon: \"/images/data-labeling.svg\"\n      children:\n        - Pattern matching\n        - Deduplication\n        - Grouping\n        - Tagging\n\n# Why Pinecone\nfeatures:\n  eyebrow: Why Pinecone\n  title: Fast, fresh, and filtered vector search.\n  list:\n    - title: Fast\n      text:\n      - Ultra-low query latency, even with billions of items. Give users a great experience.\n    - title: Fresh\n      text:\n      - Live index updates when you add, edit, or delete data. Your data is ready right away.\n    - title: Filtered\n      text:\n      - Combine vector search with metadata filters for more relevant and faster results.\n\n# Features\nfeatures2:\n  eyebrow: Managed Service\n  title: |\n    Fully managed,</br><span>production-ready</span>\n  text: Launch, use, and scale your vector search service with our easy API. No worrying about managing Google infrastructure or setting up API endpoints in Vertex.\n  list:\n    - title: Easy to use\n      text: Get started on the free plan with an easy-to-use API or the Python client.\n      icon: '/images/easy-to-use.svg'\n    - title: Scalable\n      text: Scale from zero to billions of items, with no downtime and minimal latency impact.\n      icon: '/images/scalable.svg'\n    - title: Pay for what you use\n      text: Start free, then pay only for what you use with usage-based pricing.\n      icon: '/images/usage.svg'\n    - title: No ops overhead\n      text: No need to maintain infrastructure, monitor services, or troubleshoot algorithms.\n      icon: '/images/gear.svg'\n    - title: Reliable\n      text: Choose a cloud provider and region — we'll take care of uptime, consistency, and the rest.\n      icon: '/images/cloud.svg'\n    - title: Secure\n      text: Pinecone is SOC 2 Type II certified, GDPR-ready, and built to keep data secure.\n      icon: '/images/secure.svg'\n\n# Customers\ncustomers:\n  testimonial:\n    title: Perception Point\n    logo: /images/perception-point-logo.svg\n    url: https://perception-point.io\n    description: \"Pinecone made it fast and easy to take vector search from research to production, without DevOps. Thanks to their performance and scalability we can provide accurate, reliable, and real-time threat detection to our customers.\"\n    authorName: Oded K.\n    authorTitle: R&D Group Lead\n    authorImage: /images/oded-kalev.jpeg\n    authorUrl: https://www.linkedin.com/in/oded-kalev-9a4667ba/\n  logos:\n    - logo: /images/gong-logo.png\n      alt: Gong\n    - logo: /images/clubhouse-logo.png\n      alt: Clubhouse\n    - logo: /images/klue-logo.svg\n      alt: Klue\n    - logo: /images/expel-logo.png\n      alt: Expel\n    - logo: /images/course-hero.png\n      alt: Course Hero\n    - logo: /images/circleup-logo.svg\n      alt: CircleUp\n    - logo: /images/xandr-logo.png\n      alt: Xandr\n    - logo: /images/bamboohr-logo.svg\n      alt: BambooHR\n    - logo: /images/mem-logo.svg\n      alt: Mem\n    - logo: /images/cohere.png\n      alt: \"co:here\"\n\n# Contact\nhero2:\n  title: Talk to our experts about <span>Pinecone vs. Vertex AI Matching Engine from Google</span>\n  description: Send us your questions about Pinecone and why developers choose us over Vertex AI Matching Engine from Google. We’ll schedule a time to learn and share more with you.\ncontact:\n    enabled: true\n    info: |\n        Want to try Pinecone?<br>\n        [Start here.](/start/)\n\n        Press and general inquiries:<br>\n        info@pinecone.io\n\n        Support:<br>\n        support@pinecone.io<br>\n        [Read the Docs](/docs/)\n---\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc13"
  },
  "filename": "faiss.md",
  "title": "true",
  "category": "true",
  "content": "---\nnoindex: true\nnoNav: true\nlayout: index-lp\ntitle: \"Faiss Alternative for Vector Search\"\nintro: \"Search through billions of items for similar matches to any object, in milliseconds. It’s the next generation of search, an API call away.\"\n\n# Hero\nhero:\n  title: Faiss-smackingly simple vector search\n  description: Pinecone makes it easy to build high-performance vector search applications. It’s a managed, cloud-native vector database with a simple API that you can start using and scaling within minutes. Unlike with Faiss, there's no need to build and maintain infrastructure, endlessly adjust parameters, or build custom add-ons like filtering.\n  cta1:\n    href: https://app.pinecone.io/\n    text: Create account\n  cta2:\n    href: /contact/\n    text: or contact us\n  img:\n    src: /images2/pinecone-diagram.svg\n    alt: Pinecone Diagram\n    width: 447\n    height: 548\n\n# Table\ntableSection:\n#   title: Why Managed <span>Faiss?</span>\n  heading1: Faiss\n  heading2: Pinecone\n  list:\n    - category: Vector Search\n      list:\n        - item: Approximate Nearest Neighbor Search\n          selfHosted: true\n          pinecone: true\n        - item: Exact Search\n          selfHosted: true\n          pinecone: true\n        - item: Real-time Index Updates\n          selfHosted: false\n          pinecone: true\n        - item: Metadata Filtering\n          selfHosted: false\n          pinecone: true\n        - item: Namespacing\n          selfHosted: false\n          pinecone: true\n        - item: Fetch Vectors\n          selfHosted: false\n          pinecone: true\n    - category: Deployment & Infrastructure\n      list:\n        - item: Distributed\n          selfHosted: true\n          pinecone: true\n        - item: Fully Managed\n          selfHosted: true\n          pinecone: true\n        - item: High Availability\n          selfHosted: false\n          pinecone: true\n        - item: Monitoring & Observability\n          selfHosted: false\n          pinecone: true\n        - item: Eventual Consistency\n          selfHosted: false\n          pinecone: true\n        - item: Data Persistence\n          selfHosted: false\n          pinecone: true\n        - item: Data Sharding & Replication\n          selfHosted: false\n          pinecone: true\n        - item: 24/7 Operational Support\n          selfHosted: false\n          pinecone: true\n    - category: Interfaces\n      list:\n        - item: Python Client\n          selfHosted: true\n          pinecone: true\n        - item: C++\n          selfHosted: true\n          pinecone: true\n        - item: REST API\n          selfHosted: false\n          pinecone: true\n        - item: Java Client\n          selfHosted: false\n          pinecone: true\n        - item: Go Client\n          selfHosted: false\n          pinecone: true\n        - item: Spark Connector\n          selfHosted: false\n          pinecone: true\n        - item: Web Console\n          selfHosted: false\n          pinecone: true\n  ctaBtn:\n    url: https://app.pinecone.io\n    text: Get Started\n\n# Use cases\nusecases:\n  eyebrow: Use Cases\n  title: What can you do with <span>vector search</span>?\n  description: |\n    Once you have <a href=\"/learn/vector-embeddings/\">vector embeddings</a>, manage and search through them in Pinecone to power <a href=\"/semantic-search/\">semantic search</a>, recommenders, and other applications that rely on relevant information retrieval.\n  list:\n    - title: Search\n      icon: \"/images/semantic-text-search.svg\"\n      children:\n        - Semantic search\n        - Product search\n        - Multi-modal search\n        - Question-Answering\n\n    - title: Generation\n      icon: \"/images/deduplication2.svg\"\n      children:\n        - Chatbots\n        - Text generation\n        - Image generation\n\n    - title: Security\n      icon: \"/images/fraud-detection.svg\"\n      children:\n        - Anomaly Detection\n        - Fraud Detection\n        - Bot/threat detection\n        - Identity verification\n\n    - title: Personalization\n      icon: \"/images/personalization-alt.svg\"\n      children:\n        - Recommendations\n        - Feed ranking\n        - Ad targeting\n        - Candidate selection\n\n    - title: Analytics & ML\n      icon: \"/images/model-training.svg\"\n      children:\n        - Data labeling\n        - Model training\n        - Molecular search\n        - Generative AI\n\n    - title: Data Management\n      icon: \"/images/data-labeling.svg\"\n      children:\n        - Pattern matching\n        - Deduplication\n        - Grouping\n        - Tagging\n\n# Why Pinecone\nfeatures:\n  eyebrow: Why Pinecone\n  title: Fast, fresh, and filtered vector search.\n  list:\n    - title: Fast\n      text:\n      - Ultra-low query latency, even with billions of items. Give users a great experience.\n    - title: Fresh\n      text:\n      - Live index updates when you add, edit, or delete data. Your data is ready right away.\n    - title: Filtered\n      text:\n      - Combine vector search with metadata filters for more relevant and faster results.\n\n# Features\nfeatures2:\n  eyebrow: Managed Service\n  title: |\n    Fully managed,</br><span>production-ready</span>\n  text: Launch, use, and scale your vector search service with our easy API, without worrying about infrastructure or algorithms. We'll keep it running smoothly and securely.\n  list:\n    - title: Easy to use\n      text: Get started on the free plan with an easy-to-use API or the Python client.\n      icon: '/images/easy-to-use.svg'\n    - title: Scalable\n      text: Scale from zero to billions of items, with no downtime and minimal latency impact.\n      icon: '/images/scalable.svg'\n    - title: Pay for what you use\n      text: Start free, then pay only for what you use with usage-based pricing.\n      icon: '/images/usage.svg'\n    - title: No ops overhead\n      text: No need to maintain infrastructure, monitor services, or troubleshoot algorithms.\n      icon: '/images/gear.svg'\n    - title: Reliable\n      text: Choose a cloud provider and region — we'll take care of uptime, consistency, and the rest.\n      icon: '/images/cloud.svg'\n    - title: Secure\n      text: Pinecone is SOC 2 Type II certified, GDPR-ready, and built to keep data secure.\n      icon: '/images/secure.svg'\n\n# Customers\ncustomers:\n  testimonial:\n    title: Perception Point\n    logo: /images/perception-point-logo.svg\n    url: https://perception-point.io\n    description: \"Pinecone made it fast and easy to take vector search from research to production, without DevOps. Thanks to their performance and scalability we can provide accurate, reliable, and real-time threat detection to our customers.\"\n    authorName: Oded K.\n    authorTitle: R&D Group Lead\n    authorImage: /images/oded-kalev.jpeg\n    authorUrl: https://www.linkedin.com/in/oded-kalev-9a4667ba/\n  logos:\n    - logo: /images/gong-logo.png\n      alt: Gong\n    - logo: /images/clubhouse-logo.png\n      alt: Clubhouse\n    - logo: /images/klue-logo.svg\n      alt: Klue\n    - logo: /images/expel-logo.png\n      alt: Expel\n    - logo: /images/course-hero.png\n      alt: Course Hero\n    - logo: /images/circleup-logo.svg\n      alt: CircleUp\n    - logo: /images/xandr-logo.png\n      alt: Xandr\n    - logo: /images/bamboohr-logo.svg\n      alt: BambooHR\n    - logo: /images/mem-logo.svg\n      alt: Mem\n    - logo: /images/cohere.png\n      alt: \"co:here\"\n\n# Contact\nhero2:\n  title: Talk to the <br><span>Vector Search</span> Experts\n  description: Send us your questions about Pinecone or details about your vector search needs. We’ll schedule a time to learn and share more with you.\ncontact:\n    enabled: true\n    info: |\n        Want to try Pinecone?<br>\n        [Start here.](/start/)\n\n        Press and general inquiries:<br>\n        info@pinecone.io\n\n        Support:<br>\n        support@pinecone.io<br>\n        [Read the Docs](/docs/)\n---\n\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc14"
  },
  "filename": "similarity-search.md",
  "title": "index",
  "category": "\"Vector Database for Vector Similarity Search\"",
  "content": "---\nlayout: index\ntitle: \"Vector Database for Vector Similarity Search\"\nintro: \"The #1 vector database. Search through billions of items for similar matches to any object, in milliseconds. It’s the next generation of search, an API call away.\"\n\n# Hero\nhero:\n  title: Similarity Search with Pinecone\n  description: The Pinecone <a href=\"/learn/vector-database/\">vector database</a> makes it easy to build high-performance similarity search applications. Developer-friendly, fully managed, and easily scalable without infrastructure hassles.\n  cta1:\n    href: https://app.pinecone.io/\n    text: Sign Up for Free\n  cta2:\n    href: /contact/\n    text: or contact us\n  img:\n    src: /images2/pinecone-diagram.svg\n    alt: Pinecone Diagram\n    width: 447\n    height: 548\n\n# Use cases\nusecases:\n  eyebrow: Use Cases\n  title: What can you do with <span>similarity search</span>?\n  description: |\n    Once you have [vector embeddings](/learn/vector-embeddings/), manage and search through them in Pinecone to power [semantic search](/ai-search/), recommenders, and other applications that rely on relevant information retrieval.\n  list:\n    - title: Search\n      icon: \"/images/semantic-text-search.svg\"\n      children:\n        - Semantic search\n        - Product search\n        - Multi-modal search\n        - Question-Answering\n\n    - title: Generation\n      icon: \"/images/deduplication2.svg\"\n      children:\n        - Chatbots\n        - Text generation\n        - Image generation\n\n    - title: Security\n      icon: \"/images/fraud-detection.svg\"\n      children:\n        - Anomaly Detection\n        - Fraud Detection\n        - Bot/threat detection\n        - Identity verification\n\n    - title: Personalization\n      icon: \"/images/personalization-alt.svg\"\n      children:\n        - Recommendations\n        - Feed ranking\n        - Ad targeting\n        - Candidate selection\n\n    - title: Analytics & ML\n      icon: \"/images/model-training.svg\"\n      children:\n        - Data labeling\n        - Model training\n        - Molecular search\n        - Generative AI\n\n    - title: Data Management\n      icon: \"/images/data-labeling.svg\"\n      children:\n        - Pattern matching\n        - Deduplication\n        - Grouping\n        - Tagging\n\n# Why Pinecone\nfeatures:\n  eyebrow: Why Pinecone\n  title: Fast, fresh, and filtered vector similarity search.\n  list:\n    - title: Fast\n      text:\n      - Ultra-low query latency, even with billions of items. Give users a great experience.\n    - title: Fresh\n      text:\n      - Live index updates when you add, edit, or delete data. Your data is ready right away.\n    - title: Filtered\n      text:\n      - Combine vector similarity search with metadata filters for more relevant and faster results.\n\n# Features\nfeatures2:\n  eyebrow: Managed Vector Similarity Search\n  title: |\n    Fully managed,</br><span>production-ready</span>\n  text: Launch, use, and scale your vector similarity search service with our easy API, without worrying about infrastructure or algorithms. We'll keep it running smoothly and securely.\n  list:\n    - title: Easy to use\n      text: Get started on the free plan with an easy-to-use API or the Python client.\n      icon: '/images/easy-to-use.svg'\n    - title: Scalable\n      text: Scale from zero to billions of items, with no downtime and minimal latency impact.\n      icon: '/images/scalable.svg'\n    - title: Pay for what you use\n      text: Start free, then pay only for what you use with usage-based pricing.\n      icon: '/images/usage.svg'\n    - title: No ops overhead\n      text: No need to maintain infrastructure, monitor services, or troubleshoot algorithms.\n      icon: '/images/gear.svg'\n    - title: Reliable\n      text: Choose a cloud provider and region — we'll take care of uptime, consistency, and the rest.\n      icon: '/images/cloud.svg'\n    - title: Secure\n      text: Pinecone is SOC 2 Type II certified, GDPR-ready, and built to keep data secure.\n      icon: '/images/secure.svg'\n\n# Customers\ncustomers:\n  testimonial:\n    title: Perception Point\n    logo: /images/perception-point-logo.svg\n    url: https://perception-point.io\n    description: \"Pinecone made it fast and easy to take vector similarity search from research to production, without DevOps. Thanks to their performance and scalability we can provide accurate, reliable, and real-time threat detection to our customers.\"\n    authorName: Oded K.\n    authorTitle: R&D Group Lead\n    authorImage: /images/oded-kalev.jpeg\n    authorUrl: https://www.linkedin.com/in/oded-kalev-9a4667ba/\n  logos:\n    - logo: /images/gong-logo.png\n      alt: Gong\n    - logo: /images/clubhouse-logo.png\n      alt: Clubhouse\n    - logo: /images/klue-logo.svg\n      alt: Klue\n    - logo: /images/expel-logo.png\n      alt: Expel\n    - logo: /images/course-hero.png\n      alt: Course Hero\n    - logo: /images/circleup-logo.svg\n      alt: CircleUp\n    - logo: /images/xandr-logo.png\n      alt: Xandr\n    - logo: /images/bamboohr-logo.svg\n      alt: BambooHR\n    - logo: /images/mem-logo.svg\n      alt: Mem\n    - logo: /images/cohere.png\n      alt: \"co:here\"\n---\n\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc15"
  },
  "filename": "_index.md",
  "title": "true",
  "category": "true",
  "content": "---\nnoindex: true\nnoNav: true\nlayout: index-lp\ntitle: \"Vector Database for Similarity Search\"\nintro: \"Search through billions of items for similar matches to any object, in milliseconds. It’s the next generation of search, an API call away.\"\n\n# Hero\nhero:\n  title: Search like you <span>mean it</span>\n  description: Pinecone makes it easy to build high-performance vector search applications. It’s a managed, cloud-native vector database with a simple API and no infrastructure hassles.\n  cta1:\n    href: https://app.pinecone.io/\n    text: Create account\n  cta2:\n    href: /contact/\n    text: or contact us\n  img:\n    src: /images2/pinecone-diagram.svg\n    alt: Pinecone Diagram\n    width: 447\n    height: 548\n\n# Table\ntableSection:\n#   title: Why Managed <span>Faiss?</span>\n  heading1: Self-Hosted Faiss\n  heading2: Pinecone Managed Faiss\n  list:\n    - category: Vector Similarity Search\n      list:\n        - item: Approximate Nearest Neighbor\n          selfHosted: true\n          pinecone: true\n        - item: Fast Exact Search\n          selfHosted: true\n          pinecone: true\n        - item: Live Index Updates\n          selfHosted: false\n          pinecone: true\n        - item: Namespacing\n          selfHosted: false\n          pinecone: true\n        - item: String Vector IDs\n          selfHosted: false\n          pinecone: true\n        - item: Vector Fresh Support\n          selfHosted: false\n          pinecone: true\n    - category: Deployment & Infrastructure\n      list:\n        - item: Environments\n          selfHosted: true\n          pinecone: true\n        - item: Distributed (Kubernetes)\n          selfHosted: true\n          pinecone: true\n        - item: High Availability\n          selfHosted: false\n          pinecone: true\n        - item: Observability\n          selfHosted: false\n          pinecone: true\n        - item: GPU Support\n          selfHosted: false\n          pinecone: true\n        - item: Eventual Consistency\n          selfHosted: false\n          pinecone: true\n        - item: Data Persistence\n          selfHosted: false\n          pinecone: true\n        - item: Data Sharding\n          selfHosted: false\n          pinecone: true\n        - item: Replication\n          selfHosted: false\n          pinecone: true\n        - item: 24/7 Operational Support\n          selfHosted: false\n          pinecone: true\n    - category: Interfaces\n      list:\n        - item: Python Client\n          selfHosted: true\n          pinecone: true\n        - item: C++\n          selfHosted: true\n          pinecone: true\n        - item: gRPC API\n          selfHosted: false\n          pinecone: true\n        - item: Java Client\n          selfHosted: false\n          pinecone: true\n        - item: REST API\n          selfHosted: false\n          pinecone: true\n        - item: Go Client\n          selfHosted: false\n          pinecone: true\n        - item: Web Console\n          selfHosted: false\n          pinecone: true\n  ctaBtn:\n    url: intro\n    text: Get Early Access\n\n# Use Cases\nusecases:\n  eyebrow: Use Cases\n  title: What can you do with <span>vector search</span>?\n  description: |\n    Once you have <a href=\"/learn/vector-embeddings/\">vector embeddings</a>, manage and search through them in Pinecone to power <a href=\"/semantic-search/\">semantic search</a>, recommenders, and other applications that rely on relevant information retrieval.\n  list:\n    - title: Search\n      icon: \"/images/semantic-text-search.svg\"\n      children:\n        - Semantic search\n        - Product search\n        - Multi-modal search\n        - Question-Answering\n\n    - title: Generation\n      icon: \"/images/deduplication2.svg\"\n      children:\n        - Chatbots\n        - Text generation\n        - Image generation\n\n    - title: Security\n      icon: \"/images/fraud-detection.svg\"\n      children:\n        - Anomaly Detection\n        - Fraud Detection\n        - Bot/threat detection\n        - Identity verification\n\n    - title: Personalization\n      icon: \"/images/personalization-alt.svg\"\n      children:\n        - Recommendations\n        - Feed ranking\n        - Ad targeting\n        - Candidate selection\n\n    - title: Analytics & ML\n      icon: \"/images/model-training.svg\"\n      children:\n        - Data labeling\n        - Model training\n        - Molecular search\n        - Generative AI\n\n    - title: Data Management\n      icon: \"/images/data-labeling.svg\"\n      children:\n        - Pattern matching\n        - Deduplication\n        - Grouping\n        - Tagging\n\n# Why Pinecone\nfeatures:\n  eyebrow: Why Pinecone\n  title: Fast, fresh, and filtered vector search.\n  list:\n    - title: Fast\n      text:\n      - Ultra-low query latency, even with billions of items. Give users a great experience.\n    - title: Fresh\n      text:\n      - Live index updates when you add, edit, or delete data. Your data is ready right away.\n    - title: Filtered\n      text:\n      - Combine vector search with metadata filters for more relevant and faster results.\n\n# Features\nfeatures2:\n  eyebrow: Managed Service\n  title: |\n    Fully managed,</br><span>production-ready</span>\n  text: Launch, use, and scale your vector search service with our easy API, without worrying about infrastructure or algorithms. We'll keep it running smoothly and securely.\n  list:\n    - title: Easy to use\n      text: Get started on the free plan with an easy-to-use API or the Python client.\n      icon: '/images/easy-to-use.svg'\n    - title: Scalable\n      text: Scale from zero to billions of items, with no downtime and minimal latency impact.\n      icon: '/images/scalable.svg'\n    - title: Pay for what you use\n      text: Start free, then pay only for what you use with usage-based pricing.\n      icon: '/images/usage.svg'\n    - title: No ops overhead\n      text: No need to maintain infrastructure, monitor services, or troubleshoot algorithms.\n      icon: '/images/gear.svg'\n    - title: Reliable\n      text: Choose a cloud provider and region — we'll take care of uptime, consistency, and the rest.\n      icon: '/images/cloud.svg'\n    - title: Secure\n      text: Pinecone is SOC 2 Type II certified, GDPR-ready, and built to keep data secure.\n      icon: '/images/secure.svg'\n\n# Customers\ncustomers:\n  testimonial:\n    title: Perception Point\n    logo: /images/perception-point-logo.svg\n    url: https://perception-point.io\n    description: \"Pinecone made it fast and easy to take vector search from research to production, without DevOps. Thanks to their performance and scalability we can provide accurate, reliable, and real-time threat detection to our customers.\"\n    authorName: Oded K.\n    authorTitle: R&D Group Lead\n    authorImage: /images/oded-kalev.jpeg\n    authorUrl: https://www.linkedin.com/in/oded-kalev-9a4667ba/\n  logos:\n    - logo: /images/gong-logo.png\n      alt: Gong\n    - logo: /images/clubhouse-logo.png\n      alt: Clubhouse\n    - logo: /images/klue-logo.svg\n      alt: Klue\n    - logo: /images/expel-logo.png\n      alt: Expel\n    - logo: /images/course-hero.png\n      alt: Course Hero\n    - logo: /images/circleup-logo.svg\n      alt: CircleUp\n    - logo: /images/xandr-logo.png\n      alt: Xandr\n    - logo: /images/bamboohr-logo.svg\n      alt: BambooHR\n    - logo: /images/mem-logo.svg\n      alt: Mem\n    - logo: /images/cohere.png\n      alt: \"co:here\"\n      \n# Contact\nhero2:\n  title: Talk to the <br><span>Vector Search</span> Experts\n  description: Send us your questions about Pinecone or details about your vector search needs. We’ll schedule a time to learn and share more with you.\ncontact:\n    enabled: true\n    info: |\n        Want to try Pinecone?<br>\n        [Start here.](/start/)\n\n        Press and general inquiries:<br>\n        info@pinecone.io\n\n        Support:<br>\n        support@pinecone.io<br>\n        [Read the Docs](/docs/)\n---\n\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc16"
  },
  "filename": "openai-ml-qa.md",
  "title": "application",
  "category": "OpenAI ML QA",
  "content": "---\nlayout: application\ntitle: OpenAI ML QA\ndescription: A Q&A tool that takes discussions and docs from some of the best Python ML libraries and collates their content into a natural language search and Q&A tool.\nimages: [\"/images/apps-image2.jpg\"]\n---\n\n<iframe\n    src=\"https://pinecone-openai-ml-qa.hf.space\"\n    frameborder=\"0\"\n    width=\"100%\"\n    height=\"700\"\n></iframe>\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc17"
  },
  "filename": "celebrity-match.md",
  "title": "application",
  "category": "Find Your Celebrity Match",
  "content": "---\nlayout: application\ntitle: Find Your Celebrity Match\ndescription: An image-based vector search application which can be used to discover celebrities with similar facial features.\nimages: [\"/images/apps-image1.jpg\"]\n---\n\n<iframe\n\tsrc=\"https://pinecone-find-your-celebrity-match.hf.space\"\n\tallow=\"camera\"\n\tframeborder=\"0\"\n\twidth=\"100%\"\n\theight=\"700\"\n></iframe>\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc18"
  },
  "filename": "diffusion-image-search.md",
  "title": "application",
  "category": "Diffusion Image Search",
  "content": "---\nlayout: application\ntitle: Diffusion Image Search\ndescription: Search through stable diffusion images using natural language prompts.\nimages: [\"/images/apps-diffusion-image-search.png\"]\n---\n\n<iframe\n\tsrc=\"https://pinecone-diffusion-image-search.hf.space\"\n\tframeborder=\"0\"\n\twidth=\"100%\"\n\theight=\"700\"\n></iframe>\n"
},{
  "_id": {
    "$oid": "63e10fe78f495dc68a42fc19"
  },
  "filename": "_index.md",
  "title": "application-list",
  "category": "Application Showcase",
  "content": "---\nlayout: application-list\ntitle: Application Showcase\ndescription: A showcase of applications built using Pinecone.\n\nhero:\n  title: Application <span>Showcase</span>\n  description: |\n    A showcase of applications built using Pinecone.<br><br>\n    Want to see your project here? Tell us: info@pinecone.io\n\napplications:\n  sectionTitle: Demos by <span>Pinecone</span>\n  list:\n    - title: Find Your Celebrity Match\n      description: An image-based vector search application which can be used to discover celebrities with similar facial features.\n      image: /images/apps-image1.jpg\n      url: /showcase/celebrity-match/\n\n    - title: OpenAI ML Q&A\n      description: A Q&A tool that uses OpenAI LLMs to create a natural language search and Q&A tool with discussions and docs from some of the best Python ML libraries.\n      image: /images/apps-image2.jpg\n      url: /showcase/openai-ml-qa/\n\n    - title: Diffusion Image Search\n      description: Search through stable diffusion images using natural language prompts.\n      image: /images/apps-diffusion-image-search.png\n      url: /showcase/diffusion-image-search/\n\ncommunityApps:\n  sectionTitle: Built by the <span>Community</span>\n  list:\n    - title: Atlas\n      description: Search for anything on YouTube and go direct to the relevant clips.\n      image: /images/apps-atlas-search.png\n      url: https://atlas.atila.ca\n\n    - title: All-In on AI\n      description: A semantic search index that lets you search across every episode of the All-In podcast.\n      image: /images/apps-allin.jpg\n      url: https://all-in-on-ai.vercel.app/\n\n    - title: Bible Semantic Search\n      description: An app for performing semantic search on the King James Bible.\n      image: /images/apps-semantic-search.jpg\n      url: https://chrislee973-bible-semantic-search-app-0qc5om.streamlit.app/\n\n    - title: Mood Surf\n      description: A discovery engine to explore topics that intrigue you.\n      image: /images/apps-moodsurf.jpg\n      url: https://mood.surf/\n\n    #- title: Huberman AI\n    #  description: Ask a question to Huberman AI and get answers based on the Huberman Lab episodes.\n    #  image: /images/apps-huberman.jpg\n    #  url: https://huberman.rile.yt/\n---\n"
}]