[{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e209"
  },
  "title": "Examples",
  "category": "630fc5235d91a70054705fb7",
  "content": "applications with Pinecone. You can view their source code to jumpstart your own application.  Our [Learn section](https://pinecone.io/learn) explains the basics of vector search and vector databases.  # Semantic search  <div class=\"example-cards\">    <!-- Semantic text search -->   <a href=\"/docs/semantic-text-search\">     <span>Semantic text search</span>     <p>How to create a simple semantic text search using Pinecone's similarity search service.</p>   </a>    <!-- Basic hybrid search -->   <a href=\"/docs/basic-hybrid-search\">     <span>Basic hybrid search</span>     <p>How to pair semantic search with a basic keyword filter.</p>   </a>    <!-- Generative QA with OpenAI -->   <a href=\"/docs/gen-qa-openai\">     <span>Generative question answering (OpenAI)</span>     <p>How to build retrieval enhanced generative QA systems with OpenAI.</p>   </a>    <!-- Extractive question answering -->   <a href=\"/docs/extractive-question-answering\">     <span>Extractive question answering</span>     <p>How to build an extractive QA application with similarity search.</p>   </a>    <!-- Abstractive question answering -->   <a href=\"/docs/abstractive-question-answering\">     <span>Abstractive question answering</span>     <p>How to build an abstractive (generative) QA application with similarity search.</p>   </a>    <!-- Tabular question answering -->   <a href=\"/docs/table-qa\">     <span>Tabular question answering</span>     <p>How to build a question-answering application for extracting answers from tables with similarity search.</p>   </a>    <!-- NER search -->   <a href=\"/docs/ner-search\">     <span>NER search</span>     <p>How to automatically extract entities from text and use them to improve search results.</p>   </a>    <!-- Video transcription search -->   <a href=\"/docs/video-search\">     <span>Video transcription search</span>     <p>How to create an app that searches video transcription data.</p>   </a>    <!-- GIF description search -->   <a href=\"/docs/gif-search\">     <span>GIF description search</span>     <p>How to create a GIF search app.</p>   </a> </div>  # Image, audio, and video search  <div class=\"example-cards\">   <!-- Image similarity search -->   <a href=\"/docs/image-similarity-search\">     <span>Image similarity search</span>     <p>How to build advanced image search applications.</p>   </a>    <!-- Facial similarity search -->   <a href=\"/docs/facial-similarity-search\">     <span>Facial similarity search</span>     <p>Find your celebrity doppelganger with facial similarity search.</p>   </a>    <!-- Audio similarity search -->   <a href=\"/docs/audio-search\">     <span>Audio similarity search</span>     <p>How to build advanced audio search applications.</p>   </a> </div>  # Recommendation systems  <div class=\"example-cards\">    <!-- Personalized content recommendations -->   <a href=\"/docs/personalized-content-recommendations\">     <span>Personalized content recommendations</span>     <p>How to use Pinecone to create a simple personalized article or content recommender. </p>   </a>    <!-- Movie recommender -->   <a href=\"/docs/movie-recommender\">     <span>Movie recommender</span>     <p>How to create a movie recommendation system with the MovieLens dataset.</p>   </a> </div>  # Other examples  <div class=\"example-cards\">    <!-- Document deduplication -->   <a href=\"/docs/document-deduplication\">     <span>Document deduplication</span>     <p>How to create a simple application for identifying duplicate documents.</p>   </a>    <!-- IT threat detection -->   <a href=\"/docs/it-threat-detection\">     <span>IT threat detection</span>     <p>How to build an application for detecting rare events in IT threat detection.</p>   </a>    <!-- Extreme classification -->   <a href=\"/docs/extreme-classification\">     <span>Extreme classification</span>     <p>How to label new texts automatically when there is an enormous number of potential labels.</p>   </a>    <!-- Time series similarity search -->   <a href=\"/docs/time-series\">     <span>Time series similarity search</span>     <p>How to perform time-series \"pattern\" matching using a similarity search service.</p>   </a> </div> ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e20b"
  },
  "title": "Manage indexes",
  "category": "630fc5235d91a70054705fb6",
  "content": "In this section, we explain how you can get a list of your indexes, create an index, delete an index, and describe an index.  To learn about the concepts related to indexes, see [Indexes](indexes).  > ⚠️  Warning > > Indexes on the Starter (free) plan are deleted after 7 days of inactivity. To > prevent this, send any API request or log into the console. This will count > as activity.  <html><iframe width=\"450\" height=\"253\" src=\"https://www.youtube.com/embed/DCQrrnFbLt8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></html>  ## Getting information on your indexes  List all your Pinecone indexes:  ```python pinecone.list_indexes() ``` ```shell curl curl -i https://controller.YOUR_ENVIRONMENT.pinecone.io/databases \\   -H 'Api-Key: YOUR_API_KEY' ```  Get the configuration and current status of an index named \"pinecone-index\":  ```python pinecone.describe_index(\"pinecone-index\") ``` ```shell curl curl -i -X GET https://controller.YOUR_ENVIRONMENT.pinecone.io/databases/example-index \\   -H 'Api-Key: YOUR_API_KEY' ```   ## Creating an index  The simplest way to create an index is as follows. This gives you an index with a single pod that will perform approximate nearest neighbor (ANN) search using cosine similarity:  ```python pinecone.create_index(\"example-index\", dimension=128) ``` ```shell curl curl -i -X POST https://controller.YOUR_ENVIRONMENT.pinecone.io/databases \\   -H 'Api-Key: YOUR_API_KEY' \\   -H 'Content-Type: application/json' \\   -d '{     \"name\": \"example-index\",     \"dimension\": 128   }' ```  A more complex index can be created as follows. This creates an index that measures similarity by Euclidean distance and runs on 4 s1 (storage-optimized) pods of size `x1`:  [//]: # \"TODO: Add these code samples to Docs Code Samples Test.ipynb\"   ```python pinecone.create_index(\"example-index\", dimension=128, metric=\"euclidean\", pods=4, pod_type=\"s1.x1\") ``` ```shell curl curl -i -X POST https://controller.YOUR_ENVIRONMENT.pinecone.io/databases \\   -H 'Api-Key: YOUR_API_KEY' \\   -H 'Content-Type: application/json' \\   -d '{     \"name\": \"example-index\",     \"dimension\": 128,     \"metric\": \"euclidean\",     \"pods\": 4,     \"pod_type\": \"p1.x1\"   }' ```  ### Create an index from a collection  To create an index from a [collection](collections), use the [`create_index`](/reference/create_index/) operation and provide a [`source_collection`](/reference/create_index/#!path=source_collection&t=request) parameter containing the name of the collection from which you wish to create an index. The new index is queryable and writable.  Creating an index from a collection generally takes about 10 minutes. Creating a p2 index from a collection can take several hours when the number of vectors is on the order of 1M.  **Example**  The following example creates an index named `example-index` with 128 dimensions from a collection named `example-collection`.  ```python pinecone.create_index(\"example-index\", dimension=128, source_collection=\"example-collection\") ``` ```shell curl curl -i -X POST https://controller.us-west1-gcp.pinecone.io/databases \\   -H 'Api-Key: YOUR_API_KEY' \\   -H 'Content-Type: application/json' \\   -d '{     \"name\": \"example-index\",     \"source_collection\":\"example-collection\"}   }' ```  For more information about each pod type and size, see [Indexes](indexes).  For the full list of parameters available to customize an index, see the [create_index API reference](/reference/create_index/).  ### Create an index from a public collection  To create an index from a [public collection](collections#public-collections-contain-real-world-data), follow these steps:  1. Open the [Pinecone console](https://app.pinecone.io). 1. Click the name of the project in which you want to create the index. 1. In the left menu, click **Public Collections**. 1. Find the public collection from which you want to create an index. Next to that public collection, click **CREATE INDEX**. 1. When index creation is complete, the console redirects you to view the new index.  To learn more about using specific public collections, see the example documentation for the [OpenAPI Trec](/integrations/openai), [Cohere Trec](/integrations/cohere), and [SQuAD](extractive-question-answering/) collections.  ## Changing pod sizes  The default pod size is `x1`. After index creation, you can increase the pod size for an index.  Increasing the pod size of your index does not result in downtime. Reads and writes continue uninterrupted during the scaling process. Currently, you cannot reduce the pod size of your indexes. Your number of replicas and your total number of pods remain the same, but each pod changes size. Resizing completes in about 10 minutes.  To learn more about pod sizes, see [Indexes](indexes/#pods-pod-types-and-pod-sizes).  ### Increasing the pod size for an index  To change the pod size of an existing index, use the [configure_index](/reference/configure_index/) operation and append the new size to the `pod_type` parameter, separated by a period (.).  **Example**  The following example assumes that `my_index` has size `x1` and changes the size to `x2`.  ```python pinecone.configure_index(\"my_index\", pod_type=\"s1.x2\") ``` ```shell curl curl -i -X PATCH https://controller.us-west1-gcp.pinecone.io/databases/example-index \\   -H 'Api-Key: YOUR_API_KEY' \\   -H 'Content-Type: application/json' \\   -d '{                 \"pod_type\": \"s1.x2\"     }   }' ```  ### Checking the status of a pod size change  To check the status of a pod size change, use the [describe_index](/reference/describe_index/) operation. The `status` field in the results contains the key-value pair `\"state\":\"ScalingUp\"` or `\"state\":\"ScalingDown\"` during the resizing process and the key-value pair `\"state\":\"Ready\"` after the process is complete.  The index fullness metric provided by [`describe_index_stats`](/reference/describe_index_stats) may be inaccurate until the resizing process is complete.  **Example**  The following example uses `describe_index` to get the index status of the index `example-index`. The `status` field contains the key-value pair `\"state\":\"ScalingUp\"`, indicating that the resizing process is still ongoing.  ```python pinecone.describe_index(\"example-index\") ``` ```shell curl curl -i -X GET https://controller.us-west1-gcp.pinecone.io/databases/example-index \\   -H 'Api-Key: YOUR_API_KEY' ```  Results:  ```json { \"database\": { \"name\": \"example-index\", \"dimensions\": \"768\", \"metric\": \"cosine\", \"pods\": 6, \"replicas\": 2, \"shards\": 3, \"pod_type\": \"p1.x2\", \"index_config\": {}, \"status\": {     \"ready\": true,     \"state\": \"ScalingUp\" \t} } } ```  ## Replicas  You can increase the number of replicas for your index to increase throughput (QPS). All indexes start with replicas=1.  **Example**  The following example uses the [`configure_index`](/reference/configure_index/) operation to set the number of replicas for the index `example-index` to 4.  ```python pinecone.configure_index(\"example-index\", replicas=4) ``` ```shell curl curl -i -X PATCH https://controller.us-west1-gcp.pinecone.io/databases/example-index \\   -H 'Api-Key: YOUR_API_KEY' \\   -H 'Content-Type: application/json' \\   -d '{     \"replicas\": 4   }' ```  See the [configure_index API reference](/referenceconfigure_index/) for more details.  ## Selective metadata indexing  By default, Pinecone indexes all [metadata](metadata-filtering). When you index metadata fields, you can filter vector search queries using those fields. When you store metadata fields without indexing them, you [keep memory utilization low](metadata-filtering/#supported-metadata-types), especially when you have many unique metadata values, and therefore can fit more vectors per pod.  When you create a new index, you can specify which metadata fields to index using the `metadata_config` parameter.  ```python metadata_config = {     \"indexed\": [\"metadata-field-name\"] }  pinecone.create_index(\"example-index\", dimension=128,                       metadata_config=metadata_config) ``` ```shell curl curl -i -X POST https://controller.YOUR_ENVIRONMENT.pinecone.io/databases \\   -H 'Api-Key: YOUR_API_KEY' \\   -H 'Content-Type: application/json' \\   -d '{     \"name\": \"example-index\",     \"dimension\": 128,     \"metadata_config\": {       \"indexed\": [\"metadata-field-name\"]     }   }' ```  The value for the `metadata_config` parameter is a JSON object containing the names of the metadata fields to index.  ```JSON {     \"indexed\": [         \"metadata-field-1\",         \"metadata-field-2\",         \"metadata-field-n\"     ] } ```  When you provide a `metadata_config` object, Pinecone only indexes the metadata fields present in that object: any metadata fields absent from the `metadata_config` object are not indexed.  When a metadata field is indexed, you can [filter your queries](metadata-filtering) using that metadata field; if a metadata field is not indexed, metadata filtering ignores that field.  ### Examples  The following example creates an index that only indexes the `genre` metadata field. Queries against this index that filter for the `genre` metadata field may return results; queries that filter for other metadata fields behave as though those fields do not exist.   ```python metadata_config = {     \"indexed\": [\"genre\"] }  pinecone.create_index(\"example-index\", dimension=128,                       metadata_config=metadata_config) ``` ```shell curl curl -i -X POST https://controller.us-west1-gcp.pinecone.io/databases \\   -H 'Api-Key: YOUR_API_KEY' \\   -H 'Content-Type: application/json' \\   -d '{     \"name\": \"example-index\",     \"dimension\": 128,     \"metadata_config\": {       \"indexed\": [\"genre\"]     }   }' ```  ## Deleting an index  This operation will delete all of the data and the computing resources associated with the index.  > ℹ️  Note > > When you create an index, it runs as a service until you delete it. Users are > billed for running indexes, so we recommend you delete any indexes you're not > using. This will minimize your costs.  Delete a Pinecone index named \"pinecone-index\":  ```python pinecone.delete_index(\"example-index\") ```  ```shell curl curl -i -X DELETE https://controller.YOUR_ENVIRONMENT.pinecone.io/databases/example-index \\   -H 'Api-Key: YOUR_API_KEY' ``` ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e20d"
  },
  "title": "Insert data",
  "category": "630fc5235d91a70054705fb6",
  "content": "After creating a Pinecone index, you can start inserting vector embeddings and metadata into the index.  <html><iframe width=\"450\" height=\"253\" src=\"https://www.youtube.com/embed/HjeW6ed2dmI\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></html>  ## Inserting the vectors  1. Connect to the index:  ```python index = pinecone.Index(\"pinecone-index\") ``` ```shell curl # Not applicable ```  2. Insert the data as a list of `(id, vector)` tuples. Use the `Upsert` operation to write vectors into a namespace:  ```python # Insert sample data (5 8-dimensional vectors) index.upsert([     (\"A\", [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]),     (\"B\", [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]),     (\"C\", [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]),     (\"D\", [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]),     (\"E\", [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]) ]) ``` ```shell curl curl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/upsert \\   -H 'Api-Key: YOUR_API_KEY' \\   -H 'Content-Type: application/json' \\   -d '{     \"vectors\": [       {         \"id\": \"A\",         \"values\": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]       },       {         \"id\": \"B\",         \"values\": [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]       },       {         \"id\": \"C\",         \"values\": [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]       },       {         \"id\": \"D\",         \"values\": [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]       },       {         \"id\": \"E\",         \"values\": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]       }     ]   }' ```  Immediately after the upsert response is received, vectors may not be visible to queries yet. In most situations, you can check if the vectors have been received by checking for the vector counts returned by `describe_index_stats()` to be updated. This technique may not work if the index has multiple replicas. The database is eventually consistent.  ## Batching upserts  For clients upserting larger amounts of data, you should insert data into an index in batches of 100 vectors or fewer over multiple upsert requests.  **Example**  ```python import random import itertools  def chunks(iterable, batch_size=100):     \"\"\"A helper function to break an iterable into chunks of size batch_size.\"\"\"     it = iter(iterable)     chunk = tuple(itertools.islice(it, batch_size))     while chunk:         yield chunk         chunk = tuple(itertools.islice(it, batch_size))  vector_dim = 128 vector_count = 10000  # Example generator that generates many (id, vector) pairs example_data_generator = map(lambda i: (f'id-{i}', [random.random() for _ in range(vector_dim)]), range(vector_count))  # Upsert data with 100 vectors per upsert request for ids_vectors_chunk in chunks(example_data_generator, batch_size=100):     index.upsert(vectors=ids_vectors_chunk)  # Assuming `index` defined elsewhere ```  ## Sending upserts in parallel  By default, all vector operations block until the response has been received. But using our client they can be made asynchronous. For the [Batching Upserts](#batching-upserts) example this can be done as follows:  ```python # Upsert data with 100 vectors per upsert request asynchronously # - Create pinecone.Index with pool_threads=30 (limits to 30 simultaneous requests) # - Pass async_req=True to index.upsert() with pinecone.Index('example-index', pool_threads=30) as index:     # Send requests in parallel     async_results = [         index.upsert(vectors=ids_vectors_chunk, async_req=True)         for ids_vectors_chunk in chunks(example_data_generator, batch_size=100)     ]     # Wait for and retrieve responses (this raises in case of error)     [async_result.get() for async_result in async_results] ``` ```shell # Not applicable ```  Pinecone is thread-safe, so you can launch multiple read requests and multiple write requests in parallel. Launching multiple requests can help with improving your throughput. However, reads and writes can’t be performed in parallel, therefore writing in large batches might affect query latency and vice versa.  If you experience slow uploads, see [Performance tuning](performance-tuning) for advice.  ## Partitioning an index into namespaces  You can organize the vectors added to an index into partitions, or \"namespaces,\" to limit queries and other vector operations to only one such namespace at a time. For more information, see: [Namespaces](namespaces).  ## Inserting vectors with metadata  You can insert vectors that contain metadata as key-value pairs.  You can then use the metadata to filter for those criteria when sending the query. Pinecone will search for similar vector embeddings only among those items that match the filter. For more information, see: [Metadata Filtering](metadata-filtering).  [//]: # (This sample code is a copy from metadata-filtering) ```python index.upsert([     (\"A\", [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], {\"genre\": \"comedy\", \"year\": 2020}),     (\"B\", [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2], {\"genre\": \"documentary\", \"year\": 2019}),     (\"C\", [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3], {\"genre\": \"comedy\", \"year\": 2019}),     (\"D\", [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4], {\"genre\": \"drama\"}),     (\"E\", [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], {\"genre\": \"drama\"}) ]) ``` ```shell curl curl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/upsert \\   -H 'Api-Key: YOUR_API_KEY' \\   -H 'Content-Type: application/json' \\   -d '{     \"vectors\": [       {         \"id\": \"A\",         \"values\": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],         \"metadata\": {\"genre\": \"comedy\", \"year\": 2020}       },       {         \"id\": \"B\",         \"values\": [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],         \"metadata\": {\"genre\": \"documentary\", \"year\": 2019}       },       {         \"id\": \"C\",         \"values\": [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],         \"metadata\": {\"genre\": \"comedy\", \"year\": 2019}       },       {         \"id\": \"D\",         \"values\": [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4],         \"metadata\": {\"genre\": \"drama\"}       },       {         \"id\": \"E\",         \"values\": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],         \"metadata\": {\"genre\": \"drama\"}       }     ]   }' ``` ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e20f"
  },
  "title": "Troubleshooting",
  "category": "630fc5235d91a70054705fb6",
  "content": "This section describes common issues and how to solve them. Need help? [Ask your question in our support forum.](https://community.pinecone.io/c/support/) Standard, Enterprise, and Dedicated customers can also [contact support](https://support.pinecone.io) for help.  ## Unable to pip install  Version 3 of Python uses pip3. Use the following commands at the command line (the terminal):  ``` pip3 install -U pinecone-client ```  ## Index is missing after inactivity  Indexes on the Starter (free) plan are deleted after 14 days of inactivity. To prevent this, you can send any API request to Pinecone and the counter will reset.  ## Slow uploads or high latencies  To minimize latency when accessing Pinecone:  - Switch to a cloud environment. For example: EC2, GCE, [Google Colab](https://colab.research.google.com), [GCP AI Platform Notebook](https://cloud.google.com/ai-platform-notebooks), or [SageMaker Notebook](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html). If you experience slow uploads or high query latencies, it might be because you are accessing Pinecone from your home network. - Consider deploying your application in the same environment as your Pinecone service. For users on the Starter (free) plan, the environment is GCP US-West (Oregon). - See [performance tuning](performance-tuning) for more tips.  ## High query latencies with batching  If you're batching queries, try reducing the number of queries per call to 1 query vector. You can make these calls in parallel and expect roughly the same performance as with batching.  ## Upsert throttling when using the gRPC client  It's possible to get write-throttled sooner when upserting using the gRPC index. If you see this often, then we recommend using a backoff algorithm while upserting.  ## Pods are full  There is a limit to how much vector data a single pod can hold. Create an index with more pods to hold more data. Check [the usage estimator](https://www.pinecone.io/pricing/#cost) to determine the *minimum* number of pods, or [contact us](https://www.pinecone.io/contact/) for help estimating and configuring larger indexes.  If your metadata has high cardinality, such as having a unique value for every vector in a large index, the index will take up more memory than estimated. This could result in the pods being full sooner than you expected. Consider only indexing metadata to be used for filtering, and storing the rest in a separate key-value store.  See the [Manage Indexes documentation](manage-indexes) for information on how to specify the number of pods for your index.   ## Security concerns  We work hard to earn and maintain trust by treating security and reliability as a cornerstone of our company and product. Pinecone is SOC 2 Type II compliant and GDPR-ready. See the [Trust & Security page](https://www.pinecone.io/security/) for more information. [Contact us](https://www.pinecone.io/contact/) to report any security concerns. ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e211"
  },
  "title": "Manage data",
  "category": "630fc5235d91a70054705fb6",
  "content": "In addition to [inserting](insert-data) and [querying](query-data) data, there are other ways you can interact with vector data in a Pinecone index. This section walks through the various vector operations available.  <html><iframe width=\"450\" height=\"253\" src=\"https://www.youtube.com/embed/cqzWyNWU8oo\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></html>  ## Connect to an index  If you're using a Pinecone client library to access an index, you'll need to open a session with the index:  ```python # Connect to the index index = pinecone.Index(\"pinecone-index\") ``` ```shell curl # Not applicable ```  ## Specify an index endpoint  Pinecone indexes each have their own DNS endpoint. For cURL and other direct API calls to a Pinecone index, you'll need to know the dedicated endpoint for your index.  Index endpoints take the following form:  `https://{index-name}-{project-name}.svc.YOUR_ENVIRONMENT.pinecone.io`  + `{index-name}` is the name you gave your index when you created it. + `{project-name}` is the Pinecone project name that your API key is associated   with. This can be retrieved using the `whoami` operation below. + `YOUR_ENVIRONMENT` is the [cloud region for your Pinecone project](projects#project-environment)..  ### Call `whoami` to retrieve your project name.  The following command retrieves your Pinecone project name.  ```python pinecone.whoami() ``` ```shell curl curl -i https://controller.YOUR_ENVIRONMENT.pinecone.io/actions/whoami -H 'Api-Key: YOUR_API_KEY' ```  ## Describe index statistics  Get statistics about an index, such as vector count per namespace:  ```python index.describe_index_stats() ``` ```shell curl curl -i -X GET https://YOUR_INDEX-PROJECT_NAME.svc.YOUR_ENVIRONMENT.pinecone.io/describe_index_stats \\   -H 'Api-Key: YOUR_API_KEY' ```  ## Fetching vectors  The `Fetch` operation looks up and returns vectors, by id, from an index. The returned vectors include the vector data and/or metadata. Typical fetch latency is under 5ms.  Fetch items by their ids:  ```python index.fetch([\"id-1\", \"id-2\"])  # Returns: # {'namespace': '', #  'vectors': {'id-1': {'id': 'id-1', #                       'values': [0.568879, 0.632687092, 0.856837332, ...]}, #              'id-2': {'id': 'id-2', #                       'values': [0.00891787093, 0.581895, 0.315718859, ...]}}} ``` ```shell curl curl -i -X GET \"https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/fetch?ids=id-1&ids=id-2\" \\   -H 'Api-Key: YOUR_API_KEY' # Output: # { #   \"vectors\": { #     \"id-1\": { #       \"id\": \"id-1\", #       \"values\": [0.568879, 0.632687092, 0.856837332, ...] #     }, #     \"id-2\": { #       \"id\": \"id-2\", #       \"values\": [0.00891787093, 0.581895, 0.315718859, ...] #     } #   }, #   \"namespace\": \"\" # } ```  ## Updating vectors  There are two methods for updating vectors and metadata, using *full* or *partial* updates.  ### Full update  Full updates modify the entire item, that is vectors and metadata. Updating an item by id is done the same way as [inserting items](insert-data). (Write operations in Pinecone are [idempotent](https://en.wikipedia.org/wiki/Idempotence).)  The `Upsert` operation writes vectors into an index.  > ℹ️  Note > > If a new value is upserted for an existing vector id, it will overwrite the > previous value.  1. Update the value of the item `(\"id-3\", [3.3, 3.3])`:  ```python index.upsert([(\"id-3\", [3.3, 3.3])]) ``` ```shell curl curl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/upsert \\   -H 'Api-Key: YOUR_API_KEY' \\   -H 'Content-Type: application/json' \\   -d '{     \"vectors\": [       {         \"id\": \"id-0\",         \"values\": [3.3, 3.3]       }     ]   }' ``` 2.  Fetch the item again. We should get `(\"id-3\", [3.3, 3.3])`:  ```python index.fetch([\"id-3\"]) ``` ```shell curl curl -i -X GET https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/fetch?ids=id-3 \\   -H 'Api-Key: YOUR_API_KEY' ```  ### Partial update  The `Update` operation performs partial updates that allow changes to *part* of an item. Given an id, we can update the vector value with the `values` argument or update metadata with the `set_metadata` argument.  > ⚠️  Warning > > The `Update` operation does not validate the existence of ids within an > index. If a non-existent id is given then no changes are made and a `200 OK` > will be returned.  To update the value of item `(\"id-3\", [3., 3.], {\"type\": \"doc\", \"genre\": \"drama\"})`:  ```python index.update(id=\"id-3\", values=[4., 2.]) ``` ```shell curl curl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/update \\   -H 'Api-Key: YOUR_API_KEY' \\   -H 'Content-Type: application/json' \\   -d '{        \"id\": \"id-3\",        \"values\": [           3.3,           3.3        ]      }' ```  The updated item would now be `(\"id-3\", [4., 2.], {\"type\": \"doc\", \"genre\": \"drama\"})`.  When updating metadata only specified fields will be modified. If a specified field does not exist, it is added.  > ℹ️  Note > > Metadata updates apply *only* to fields passed to the `set_metadata` > argument. Any other fields will remain unchanged.  To update the metadata of item `(\"id-3\", [4., 2.], {\"type\": \"doc\", \"genre\": \"drama\"})`:  ```python index.update(id=\"id-3\", set_metadata={\"type\": \"web\", \"new\": \"true\"}) ``` ```shell curl curl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/update \\   -H 'Api-Key: YOUR_API_KEY' \\   -H 'Content-Type: application/json' \\   -d '{        \"id\": \"id-3\",        \"setMetadata\": {           \"type\": \"web\",           \"new\": \"true\"        }      }' ```  The updated item would now be `(\"id-3\", [4., 2.], {\"type\": \"web\", \"genre\": \"drama\", \"new\": \"true\"})`.  Both vector and metadata can be updated at once by including both `values` and `set_metadata` arguments. To update the `\"id-3\"` item we write:  ```python index.update(id=\"id-3\", values=[1., 2.], set_metadata={\"type\": \"webdoc\"}) ``` ```shell curl curl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/update \\   -H 'Api-Key: YOUR_API_KEY' \\   -H 'Content-Type: application/json' \\   -d '{         \"id\": \"id-3\",         \"values\": [1., 2.],         \"set_metadata\": {\"type\": \"webdoc\"}       }   }' ```  The updated item would now be `(\"id-3\", [1., 2.], {\"type\": \"webdoc\", \"genre\": \"drama\", \"new\": \"true\"})`.  ## Deleting vectors  The `Delete` operation deletes vectors, by ID, from an index.  Alternatively, it can also delete all vectors from an index or [namespace](namespaces).  When deleting large numbers of vectors, limit the scope of delete operations to hundreds of vectors per operation.  Instead of deleting all vectors in an index, [delete the index](https://www.pinecone.io/docs/manage-indexes/#deleting-an-index) and [recreate it](https://www.pinecone.io/docs/manage-indexes/#creating-an-index).  ### Delete vectors by ID  To delete vectors by their IDs, specify an `ids` parameter to `delete`. The `ids` parameter is an array of strings containing vector IDs.  **Example**  ```python index.delete(ids=[\"id-1\", \"id-2\"], namespace='example-namespace') ``` ```shell curl curl -i -X DELETE \"https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io.pinecone.io/vectors/delete?ids=id-1&ids=id-2&namespace=example-namespace\" \\   -H 'Api-Key: YOUR_API_KEY' ```  ### Delete vectors by namespace  To delete all vectors from a namespace, specify `delete_all=True` and provide a `namespace` parameter.  > ℹ️  Note > > If you delete all vectors from a single namespace, it will also delete the > namespace.  **Example**:  ```python index.delete(delete_all=True, namespace='example-namespace') ``` ```shell curl curl -i -X DELETE \"https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/delete?delete_all=true&namespace=example-namespace\" \\   -H 'Api-Key: YOUR_API_KEY' ```  ### Delete vectors by metadata  To delete vectors by metadata, [pass a metadata filter expression to the delete operation](https://www.pinecone.io/docs/metadata-filtering/#deleting-vectors-by-metadata-filter).   ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e213"
  },
  "title": "Performance tuning",
  "category": "630fc5235d91a70054705fb6",
  "content": "This section provides some tips for getting the best performance out of Pinecone.  ## Basic performance checklist  - **Switch to a cloud environment.** For example: EC2, GCE, [Google Colab](https://colab.research.google.com), [GCP AI Platform Notebook](https://cloud.google.com/ai-platform-notebooks), or [SageMaker Notebook](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html). If you experience slow uploads or high query latencies, it might be because you are accessing Pinecone from your home network. - **Deploy your application and your Pinecone service in the same region.** For users on the Free plan, Pinecone runs in GCP US-West (Oregon). [Contact us](https://www.pinecone.io/contact/) if you need a dedicated deployment. - **Reuse connections.** We recommend you reuse the same `pinecone.Index()` instance when you are upserting and querying the same index. - **Operate within known [limits](limits).**  ## How to increase throughput  To increase throughput (QPS), increase the number of replicas for your index.  **Example**  The following example increases the number of replicas for `example-index` to 4.  ```python pinecone.configure_index(\"example-index\", replicas=4) ``` ```shell curl curl -i -X PATCH https://controller.us-west1-gcp.pinecone.io/databases/example-index \\   -H 'Api-Key: YOUR_API_KEY' \\   -H 'Content-Type: application/json' \\   -d '{     \"replicas\": 4   }' ```  See the [configure_index API reference](/reference/configure_index/) for more details.  <embed src=\"/snippets/_test.md\" />  ## Using the gRPC client to get higher upsert speeds  Pinecone has a gRPC flavor of the standard client ([installation](installation#installing-the-grpc-flavor-of-the-standard-client)) that can provide higher upsert speeds for multi-pod indexes.  To connect to an index via the gRPC client:  ```python  index = pinecone.GRPCIndex(\"index-name\")  ```  The syntax for upsert, query, fetch, and delete with the gRPC client remain the same as the standard client.  We recommend you use parallel upserts to get the best performance.  ```python  index = pinecone.GRPCIndex('example-index') def chunker(seq, batch_size):   return (seq[pos:pos + batch_size] for pos in range(0, len(seq), batch_size)) async_results = [         index.upsert(vectors=chunk, async_req=True)         for chunk in chunker(data, batch_size=100)     ] # Wait for and retrieve responses (in case of error) [async_result.result() for async_result in async_results]  ```  We recommend you use the gRPC client for multi-pod indexes only. The performance of the standard and gRPC clients are similar in a single-pod index.  It's possible to get write throttled faster when upserting using the gRPC index. If you see this often, we recommend you use a backoff algorithm while upserting.  Pinecone is thread-safe, so you can launch multiple read requests and multiple write requests in parallel. Launching multiple requests can help with improving your throughput. However, reads and writes can’t be performed in parallel, therefore writing in large batches might affect query latency and vice versa. ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e215"
  },
  "title": "Moving to production",
  "category": "630fc5235d91a70054705fb6",
  "content": "## Introduction  The goal of this document is to prepare users to begin using their Pinecone indexes in production by anticipating production issues and identifying best practices for production indexes. Because these issues are highly workload-specific, the recommendations here are general.  ## Overview  Once you have become familiar with Pinecone and experimented with creating indexes and queries that reflect your intended workload, you may be planning to use your indexes to serve production queries. Before you do, there are several steps you can take that can prepare your project for production workloads, anticipate production issues, and enable reliability and growth.   Consider the following areas before moving your indexes to production:  ## Prepare your project structure  One of the first steps towards a production-ready Pinecone index is configuring your project correctly. Consider [creating a separate project](https://www.pinecone.io/docs/manage-projects/#creating-a-new-project) for your development and production indexes, to allow for testing changes to your index before deploying them to production. Ensure that you have properly [configured user access](https://www.pinecone.io/docs/manage-projects/#user-roles) to your production environment so that only those users who need to access the production index can do so. Consider how best to manage the API key associated with your production project.  ## Test your query results  Before you move your index to production, make sure that your index is returning accurate results in the context of your application. Consider [identifying the appropriate metrics](https://www.pinecone.io/learn/offline-evaluation/) for evaluating your results.   ## Estimate the appropriate number and size of pods and replicas  Depending on your data and the types of workloads you intend to run, your project may require a different [number and size of pods](https://www.pinecone.io/docs/indexes/#pods-pod-types-and-pod-sizes) and [replicas](https://www.pinecone.io/docs/manage-indexes/#replicas). Factors to consider include the number of vectors, the dimensions per vector, the amount and cardinality of metadata, and the acceptable queries per second (QPS). Use the [index fullness metric](/reference/describe_index_stats_post) to identify how much of your current resources your indexes are using. You can [use collections to create indexes](https://www.pinecone.io/docs/manage-indexes/#create-an-index-from-a-collection) with different pod types and sizes to experiment.  ## Load test your indexes  Before moving your project to production, consider determining whether your index configuration can serve the load of queries you anticipate from your application. You can write load tests in Python from scratch or using a load testing framework like [Locust](https://locust.io/).  ## Back up your indexes  In order to enable long-term retention, compliance archiving, and deployment of new indexes, consider backing up your production indexes by [creating collections](https://www.pinecone.io/docs/back-up-indexes/).   ## Tune for performance  Before serving production workloads, identify ways to [improve latency](https://www.pinecone.io/docs/performance-tuning/) by making changes to your deployment, project configuration, or client.   ## Configure monitoring   Prepare to observe production performance and availability by [configuring monitoring](https://www.pinecone.io/docs/monitoring/) with Prometheus or OpenMetrics on your production indexes.  ## Plan for scaling  Before going to production, consider planning ahead for how you might scale your indexes when the need arises. Identify metrics that may indicate the need to scale, such as [index fullness](/reference/describe_index_stats_post) and [average request latency](https://www.pinecone.io/docs/monitoring/#average-request-latency). Plan for increasing the number of pods, changing to a more performant [pod type](https://www.pinecone.io/docs/indexes/#pods-pod-types-and-pod-sizes), [vertically scaling](https://www.pinecone.io/learn/testing-p2-collections-scaling/#vertical-scaling-on-p1-and-s1) the [size of your pods](https://www.pinecone.io/docs/indexes/#pods-pod-types-and-pod-sizes), increasing the number of [replicas](https://www.pinecone.io/docs/manage-indexes/#replicas), or increasing storage capacity with a [storage-optimized pod type](https://www.pinecone.io/docs/indexes/#pods-pod-types-and-pod-sizes).  ## Know how to get support  If you need help, visit [support.pinecone.io](https://support.pinecone.io), or talk to the [Pinecone community](https://www.pinecone.io/community/). Ensure that your [plan tier](https://www.pinecone.io/pricing/) matches the support and availability SLAs you need. This may require you to upgrade to Enterprise. ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e217"
  },
  "title": "Using namespaces",
  "category": "630fc5235d91a70054705fb8",
  "content": "Pinecone allows you to partition the vectors in an index into **namespaces**. Queries and other operations are then limited to one namespace, so different requests can search different subsets of your index.  For example, you might want to define a namespace for indexing articles by **content**, and another for indexing articles by **title**. For a complete example, see: [Semantic Text Search (Example)](semantic-text-search/).  Every index is made up of one or more namespaces. Every vector exists in exactly one namespace.  Namespaces are uniquely identified by a namespace name, which almost all operations accept as a parameter to limit their work to the specified namespace. When you don't specify a namespace name for an operation, Pinecone uses the default namespace name of `\"\"` (the empty string).  ## Creating a namespace  A destination namespace can be specified when vectors are upserted. If the namespace doesn't exist, it is created implicitly.  The example below will create a \"my-first-namespace\" namespace if it doesn’t already exist:  ```python # Upsert vectors while creating a new namespace index.upsert(vectors=[('id-1', [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])],              namespace='my-first-namespace') ``` ```shell curl curl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/upsert \\   -H 'Api-Key: YOUR_API_KEY' \\   -H 'Content-Type: application/json' \\   -d '{     \"vectors\": [       {         \"id\": \"id-1\",         \"values\": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]       }     ],     \"namespace\": \"my-first-namespace\"   }' ```  Then you can submit queries and other operations specifying that namespace as a parameter. For example, to query the vectors in namespace \"my-first-namespace\":  ```python index.query(vector=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],             top_k=1,             namespace='my-first-namespace') ``` ```shell curl curl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/query \\   -H 'Api-Key: YOUR_API_KEY' \\   -H 'Content-Type: application/json' \\   -d '{     \"topK\": 1,     \"vector\": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],     \"namespace\": \"my-first-namespace\"   }' ```  ## Creating more than one namespace  You can create more than one namespace. For example, insert data into separate namespaces:  ```python import numpy as np  # Create three sets of 8-dimensional vectors vectors_a = np.random.rand(15, 8).tolist() vectors_b = np.random.rand(20, 8).tolist() vectors_c = np.random.rand(30, 8).tolist()  # Create ids ids_a = map(str, np.arange(15).tolist()) ids_b = map(str, np.arange(20).tolist()) ids_c = map(str, np.arange(30).tolist())  # Insert into separate namespaces index.upsert(vectors=zip(ids_a,vectors_a),namespace='namespace_a') index.upsert(vectors=zip(ids_b,vectors_b),namespace='namespace_b')  # if no namespaces are specified, the index uses the default namespace index.upsert(vectors=zip(ids_c,vectors_c))  # At this point, index.describe_index_stats() returns: # {'dimension': 8, #  'namespaces': {'': {'vector_count': 30}, #                 'namespace_a': {'vector_count': 15}, #                 'namespace_b': {'vector_count': 20}}} ``` ```shell curl # No example ```  ## Operations across all namespaces  All vector operations apply to a single namespace, with one exception:  The `DescribeIndexStatistics` operation returns per-namespace statistics about the contents of **all** namespaces in an index. [More details](/reference/describe_index_stats_post) ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e219"
  },
  "title": "Query data",
  "category": "630fc5235d91a70054705fb6",
  "content": "After your data is [indexed](insert-data), you can start sending queries to Pinecone.  The `Query` operation searches the index using a query vector. It retrieves the IDs of the most similar vectors in the index, along with their similarity scores. It can optionally include the result vectors' values and metadata too. You specify the number of vectors to retrieve each time you send a query. They are always ordered by similarity from most similar to least similar.  ## Sending a query  When you send a query, you provide a `vector` and retrieve the `top-k` most similar vectors for each query. For example, this example sends a query vector and retrieves three matching vectors:  ```python index.query(   vector=[0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],   top_k=3,   include_values=True )  # Returns: # {'matches': [{'id': 'C', #               'score': -1.76717265e-07, #               'values': [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]}, #                   {'id': 'B', #                    'score': 0.080000028, #                    'values': [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]}, #                   {'id': 'D', #                    'score': 0.0800001323, #                    'values': [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]}], #               'namespace': ''} ``` ```shell curl curl -i -X POST https://hello-pinecone-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/query \\   -H 'Api-Key: YOUR_API_KEY' \\   -H 'Content-Type: application/json' \\   -d '{     \"vector\":[0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],     \"topK\": 3,     \"includeValues\": true   }'  # Output: # { #  \"matches\":[ #      { #       \"id\": \"C\", #       \"score\": -1.76717265e-07, #       \"values\": [0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3] #      }, #      { #       \"id\": \"B\", #       \"score\": 0.080000028, #       \"values\": [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2] #      }, #      { #       \"id\": \"D\", #       \"score\": 0.0800001323, #       \"values\": [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4] #      } #  ], #  \"namespace\": \"\" # } ```  Depending on your data and your query, you may not get top_k results. This happens when top_k is larger than the number of possible matching vectors for your query.  ## Querying by namespace  You can organize the vectors added to an index into partitions, or \"namespaces,\" to limit queries and other vector operations to only one such namespace at a time. For more information, see: [Namespaces](namespaces).  ## Using metadata filters in queries  You can add metadata to document embeddings within Pinecone, and then filter for those criteria when sending the query. Pinecone will search for similar vector embeddings only among those items that match the filter. For more information, see: [Metadata Filtering](metadata-filtering).  [//]: # (This sample code is a copy from metadata-filtering) ```python index.query(     vector=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],     filter={         \"genre\": {\"$eq\": \"documentary\"},         \"year\": 2019     },     top_k=1,     include_metadata=True ) ``` ```shell curl curl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/query \\   -H 'Api-Key: YOUR_API_KEY' \\   -H 'Content-Type: application/json' \\   -d '{     \"vector\": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],     \"filter\": {\"genre\": {\"$in\": [\"comedy\", \"documentary\", \"drama\"]}},     \"topK\": 1,     \"includeMetadata\": true   }' ```   ## Limitations  Avoid returning vector data and metadata when top_k>1000. This means queries with top_k over 1000 should not contain: `include_metadata=True`  or `include_data=True`. For more limitations, see: [Limits](limits). ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e21b"
  },
  "title": "Back up indexes",
  "category": "630fc5235d91a70054705fb6",
  "content": "## Overview  This document describes how to make backup copies of your indexes using [collections](collections).  To learn how to create an index from a collection, see [Manage indexes](manage-indexes/#creating-an-index).   > ⚠️  Warning > > This document uses [collections](collections). This is a **public preview** > feature. Test thoroughly before using this feature with production workloads.  ## Create a backup using a collection  To create a backup of your index, use the [`create_collection`](/reference/create_collection) operation. A collection is a static copy of your index that only consumes storage.  **Example**  The following example creates a collection named `example-collection` from an index named `example-index`.  ```python pinecone.create_collection(\"example-collection\", \"example-index\") ``` ```shell curl curl -i -X POST https://controller.us-west1-gcp.pinecone.io/collections \\   -H 'Api-Key: YOUR_API_KEY' \\   -H 'Content-Type: application/json' \\   -d '{         \"name\": \"example-collection\",         \"source\": \"example-index\"   }' ```  ## Check the status of a collection  To retrieve the status of the process creating a collection and the size of the collection, use the [`describe_collection`](/reference/describe_collection) operation. Specify the name of the collection to check. You can only call `describe_collection` on a collection in the current project.  The `describe_collection` operation returns an object containing key-value pairs representing the name of the collection, the size in bytes, and the creation status of the collection.  **Example**  The following example gets the creation status and size of a collection named `example-collection`.  ```python pinecone.describe_collection(\"example-collection\") ``` ```shell curl curl -i -X GET https://controller.us-west1-gcp.pinecone.io/collections/example-collection \\   -H 'Api-Key: YOUR_API_KEY' ```  Results: ```shell CollectionDescription(name='test-collection', size=3818809, status='Ready') ```  ## List your collections  To get a list of the collections in the current project, use the [`list_collections`](/reference/list_collections) operation.  **Example**  The following example gets a list of all collections in the current project.  ```python pinecone.list_collections() ``` ```shell curl curl -i -X GET https://controller.us-west1-gcp.pinecone.io/collections \\   -H 'Api-Key: YOUR_API_KEY' ```  Results ```shell example-collection ```  ## Delete a collection  To delete a collection, use the [`delete_collection`](/reference/delete_collection) operation. Specify the name of the collection to delete.  Deleting the collection takes several minutes. During this time, the `describe_collection` operation returns the status `\"deleting\"`.  **Example**  The following example deletes the collection `example-collection`.  ```python pinecone.delete_collection(\"example-collection\") ``` ```shell curl curl -i -X DELETE https://controller.us-west1-gcp.pinecone.io/collections/example-collection \\   -H 'Api-Key: YOUR_API_KEY' ```  ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e21d"
  },
  "title": "Manage billing",
  "category": "630fc5235d91a70054705fb6",
  "content": "## Overview  This category contains guides for tasks related to Pinecone billing.  ## Tasks  + [Setting up GCP Marketplace billing](setting-up-gcp-marketplace-billing) + [Changing your billing plan](changing-your-billing-plan) ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e21f"
  },
  "title": "Metadata filtering",
  "category": "630fc5235d91a70054705fb6",
  "content": "You can limit your vector search based on metadata. Pinecone lets you attach metadata key-value pairs to vectors in an index, and specify filter expressions when you query the index.  Searches with metadata filters retrieve exactly the number of nearest-neighbor results that match the filters. For most cases, the search latency will be even lower than unfiltered searches.  For more background information on metadata filtering, see: [The Missing WHERE Clause in Vector Search](https://www.pinecone.io/learn/vector-search-filtering/).  <html><iframe width=\"450\" height=\"253\" src=\"https://www.youtube.com/embed/tn_Y19oB5bs\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></html>  ## Supported metadata types  You can associate a metadata payload with each vector in an index, as key-value pairs in a JSON object where keys are strings and values are one of:  * String * Number (integer or floating point, gets converted to a 64 bit floating point) * Booleans (true, false) * List of String  > ℹ️  Note > > **High cardinality consumes more memory:** Pinecone indexes metadata to allow > for filtering. If the metadata contains many unique values — such as a unique > identifier for each vector — the index will consume significantly more > memory. Consider using [selective metadata > indexing](manage-indexes/#selective-metadata-indexing) to avoid indexing > high-cardinality metadata that is not needed for filtering.  > ⚠️  Warning > > Null metadata values are not supported. Instead of setting a key to hold a > null value, we recommend you remove that key from the metadata payload.  For example, the following would be valid metadata payloads: ```json {     \"genre\": \"action\",     \"year\": 2020,     \"length_hrs\": 1.5 }  {     \"color\": \"blue\",     \"fit\": \"straight\",     \"price\": 29.99,     \"is_jeans\": true } ```  ## Metadata query language  > ℹ️  Note > > Pinecone's filtering query language is based on [MongoDB's query and > projection > operators](https://docs.mongodb.com/manual/reference/operator/query/). We > currently support a subset of those selectors.  The metadata filters can be combined with AND and OR:  - `$eq` - Equal to *(number, string, boolean)* - `$ne` - Not equal to *(number, string, boolean)* - `$gt` - Greater than *(number)* - `$gte` - Greater than or equal to *(number)* - `$lt` - Less than *(number)* - `$lte` - Less than or equal to *(number)* - `$in` - In array *(string or number)* - `$nin` - Not in array *(string or number)*  ### Using arrays of strings as metadata values or as metadata filters  A vector with metadata payload... ```json {\"genre\":[\"comedy\",\"documentary\"]} ```  ...means the `\"genre\"` takes on both values.  For example, queries with the following filters will match the vector: ```json {\"genre\":\"comedy\"}  {\"genre\": {\"$in\":[\"documentary\",\"action\"]}}  {\"$and\": [{\"genre\": \"comedy\"}, {\"genre\":\"documentary\"}]} ```  Queries with the following filter will **not** match the vector: ```json {\"$and\": [{\"genre\": \"comedy\"}, {\"genre\":\"drama\"}]} ```  And queries with the following filters will **not** match the vector because they are invalid. They will result in a query compilation error: ``` # INVALID QUERY: {\"genre\": [\"comedy\", \"documentary\"]} ``` ``` # INVALID QUERY: {\"genre\": {\"$eq\": [\"comedy\", \"documentary\"]}} ```  ## Inserting metadata into an index Metadata can be included in upsert requests as you insert your vectors.  For example, here's how to insert vectors with metadata representing movies into an index: ```python import pinecone  pinecone.init(api_key=\"your-api-key\", environment=\"us-west1-gcp\") index = pinecone.Index(\"example-index\")  index.upsert([     (\"A\", [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], {\"genre\": \"comedy\", \"year\": 2020}),     (\"B\", [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2], {\"genre\": \"documentary\", \"year\": 2019}),     (\"C\", [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3], {\"genre\": \"comedy\", \"year\": 2019}),     (\"D\", [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4], {\"genre\": \"drama\"}),     (\"E\", [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], {\"genre\": \"drama\"}) ]) ``` ```shell curl curl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/upsert \\   -H 'Api-Key: YOUR_API_KEY' \\   -H 'Content-Type: application/json' \\   -d '{     \"vectors\": [       {         \"id\": \"A\",         \"values\": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],         \"metadata\": {\"genre\": \"comedy\", \"year\": 2020}       },       {         \"id\": \"B\",         \"values\": [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],         \"metadata\": {\"genre\": \"documentary\", \"year\": 2019}       },       {         \"id\": \"C\",         \"values\": [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],         \"metadata\": {\"genre\": \"comedy\", \"year\": 2019}       },       {         \"id\": \"D\",         \"values\": [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4],         \"metadata\": {\"genre\": \"drama\"}       },       {         \"id\": \"E\",         \"values\": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],         \"metadata\": {\"genre\": \"drama\"}       }     ]   }' ```  ## Querying an index with metadata filters  Metadata filter expressions can be included with queries to limit the search to only vectors matching the filter expression.  For example, we can search the previous movies index for documentaries from the year 2019. This also uses the `include_metadata` flag so that vector metadata is included in the response.  > ⚠️  Warning > > For performance reasons, do not return vector data and metadata when > `top_k>1000`. Queries with `top_k` over 1000 should not contain > `include_metadata=True` or `include_data=True`.  ```python index.query(     vector=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],     filter={         \"genre\": {\"$eq\": \"documentary\"},         \"year\": 2019     },     top_k=1,     include_metadata=True )  # Returns: # {'matches': [{'id': 'B', #               'metadata': {'genre': 'documentary', 'year': 2019.0}, #               'score': 0.0800000429, #               'values': []}], #  'namespace': ''} ``` ```shell curl curl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/query \\   -H 'Api-Key: YOUR_API_KEY' \\   -H 'Content-Type: application/json' \\   -d '{     \"vector\": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],     \"filter\": {\"genre\": {\"$in\": [\"comedy\", \"documentary\", \"drama\"]}},     \"topK\": 1,     \"includeMetadata\": true   }'  # Output: # { #       \"matches\": [ #         { #           \"id\": \"B\", #           \"score\": 0.0800000429, #           \"values\": [], #           \"metadata\": { #             \"genre\": \"documentary\", #             \"year\": 2019 #           } #         } #       ], #       \"namespace\": \"\" #     } ```  ### More example filter expressions  A comedy, documentary, or drama: ```json {     \"genre\": {\"$in\": [\"comedy\", \"documentary\", \"drama\"]} } ```  A drama from 2020: ```json {     \"genre\": {\"$eq\": \"drama\"},     \"year\": {\"$gte\": 2020} } ```  A drama from 2020 (equivalent to the previous example): ```json {     \"$and\": [         {\"genre\": {\"$eq\": \"drama\"}},         {\"year\": {\"$gte\": 2020}}     ] } ```  A drama or a movie from 2020: ```json {     \"$or\": [         {\"genre\": {\"$eq\": \"drama\"}},         {\"year\": {\"$gte\": 2020}}     ] } ```  ## Deleting vectors by metadata filter To specify vectors to be deleted by metadata values, pass a metadata filter expression to the delete operation. This deletes all vectors matching the metadata filter expression.  **Example**  This example deletes all vectors with genre \"documentary\" and year 2019 from an index. ```python index.delete(     filter={         \"genre\": {\"$eq\": \"documentary\"},         \"year\": 2019     } ) ``` ```shell curl curl -i -X POST https://YOUR_INDEX-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/delete \\   -H 'Api-Key: YOUR_API_KEY' \\   -H 'Content-Type: application/json' \\   -d '{     \"filter\": {\"genre\": {\"$in\": [\"comedy\", \"documentary\", \"drama\"]}}   }' ``` ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e221"
  },
  "title": "Monitoring",
  "category": "630fc5235d91a70054705fb6",
  "content": "This document describes how to configure monitoring for your Pinecone index using Prometheus or compatible tools.  ## Overview  You can ingest performance metrics from Pinecone indexes into your own Prometheus instances, or into Prometheus- and OpenMetrics-compatible monitoring tools. The Prometheus metric endpoint is for users who want to monitor and store system health metrics using their own Prometheus metrics logger.  > ⚠️  Warning > > This feature is in public preview and is only available to Enterprise or > Enterprise Dedicated users.  ## Connect  Metrics are available at a URL like the following:  `https://metrics.YOUR_ENVIRONMENT.pinecone.io/metrics`  Your API key must be passed via the Authorization header as a bearer token like the following:  `Authorization: Bearer \\<api-key\\>`  Only the metrics for the project associated with the API key are available at this URL.  For Prometheus, configure prometheus.yml as follows:  ```yaml scrape_configs:   - job_name: pinecone-job-1     authorization:       credentials: <api-key-here>     scheme: https     static_configs:       - targets: ['metrics.YOUR_ENVIRONMENT.pinecone.io'] ```  [See Prometheus docs](https://prometheus.io/docs/prometheus/latest/configuration/configuration/) for more configuration details.  ## Available Metrics  The metrics available are as follows:  <div>  <table class=\"table table-responsive\">   <thead>    <tr>     <th>      Name     </th>     <th>      Type     </th>     <th>      Description     </th>     <th>      Labels     </th>    </tr>   </thead>   <tbody>    <tr>     <td>      pinecone_vector_count     </td>     <td>      gauge     </td>     <td>      pinecone_vector_count gives the number of items per pod in the index.<br/>      <i>      Labels:<br/>      - pid: Process identifier<br/>      - index_name: Name of the index<br/>      - project_name: Pinecone project name<br/>      </i>     </td>    </tr>    <tr>     <td>      pinecone_request_count_total     </td>     <td>      counter     </td>     <td>      pinecone_request_count_total gives the number of data plane calls made by clients.<br/>      <i>      Labels:<br/>      - pid: Process identifier<br/>      - index_name: Name of the index<br/>      - project_name: Pinecone project name<br/>      - request_type: One of upsert, delete, fetch, query, describe_index_stats<br/>      </i>     </td>    </tr>    <tr>     <td>      pinecone_request_error_count_total     </td>     <td>      counter     </td>     <td>      pinecone_request_error_count_total gives the number of data plane calls made by clients that resulted in errors.<br/>      <i>      Labels:<br/>      - pid: Process identifier<br/>      - index_name: Name of the index<br/>      - project_name: Pinecone project name<br/>      - request_type: One of upsert, delete, fetch, query, describe_index_stats<br/>      </i>     </td>    </tr>    <tr>     <td>      pinecone_request_latency_seconds     </td>     <td>      histogram     </td>     <td>      pinecone_request_latency_seconds gives the distribution of server-side processing latency for pinecone data plane calls.<br/>      <i>      Labels:<br/>      - pid: Process identifier<br/>      - index_name: Name of the index<br/>      - project_name: Pinecone project name<br/>      - request_type: One of upsert, delete, fetch, query, describe_index_stats<br/>      </i>     </td>    </tr>    <tr>     <td>      pinecone_index_fullness     </td>     <td>      gauge     </td>     <td>      pinecone_index_fullness gives the fullness of the index on a scale of 0 to 1.<br/>      <i>      Labels:<br/>      - pid: Process identifier<br/>      - index_name: Name of the index<br/>      - project_name: Pinecone project name<br/>      </i>     </td>    </tr>   </tbody>  </table> </div>  ## Example queries  The following Prometheus queries gather information about your Pinecone index.  ### Average request latency  The following query returns the average latency in seconds for all requests against the Pinecone index `example-index`.  ``` avg by (request_type) (pinecone_request_latency_seconds{index_name=\"example-index\"}) ```  The following query returns the vector count for the Pinecone index `example-index`.  ``` sum ((avg by (app) (pinecone_vector_count{index_name=\"example-index\"}))) ```  The following query returns the total number of requests against the Pinecone index `example-index` over one minute.  ``` sum by (request_type)(increase(pinecone_request_count_total{index_name=\"example-index\"}[60s])) ```  The following query returns the total number of upsert requests against the Pinecone index `example-index` over one minute.  ``` sum by (request_type)(increase(pinecone_request_count_total{index_name=\"example-index\", request_type=\"upsert\"}[60s])) ```  The following query returns the total errors returned by the Pinecone index `example-index` over one minute.  ``` sum by (request_type) (increase(pinecone_request_error_count{       index_name=\"example-index\"}[60s])) ```  The following query returns the index fullness metric for the Pinecone index `example-index`.  ``` round(max (pinecone_index_fullness{index_name=\"example-index\"} * 100)) ``` ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e223"
  },
  "title": "Manage projects",
  "category": "630fc5235d91a70054705fb6",
  "content": "## Overview  This category contains guides for tasks related to Pinecone projects.  ## Tasks  + [Create a project](create-project) + [Add users to a project](add-users-to-projects-and-organizations) + [Change the pod limit for a project](change-project-pod-limit) + [Rename a project](rename-project) ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e225"
  },
  "title": "Create a project",
  "category": "630fc5235d91a70054705fb6",
  "content": "## Overview  > ℹ️ Info > > Starter (free) users can only have 1 owned project. To create a new project, Starter users must upgrade to the Standard or Enterprise plan or delete their default project.  Follow these steps to create a new project:  1. Access the [Pinecone Console](https;//app.pinecone.io).  1. Click **Organizations** in the left menu.  1. In the **Organizations** view, click the **PROJECTS** tab.  1. Click the **+CREATE PROJECT** button.  1. Enter the **Project Name**.  1. Select a [cloud provider and region](projects#project-environment).  1. Enter the [project pod limit](projects/#project-pod-limit).  1. Click **CREATE PROJECT**.  ## Next steps  * [Add users to your project](add-users-to-projects-and-organizations). * [Create an index](manage-indexes#creating-an-index). ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e227"
  },
  "title": "Change project pod limit",
  "category": "630fc5235d91a70054705fb6",
  "content": "## Overview  If you are a [project owner](projects#project-roles), follow these steps to change the maximum total number of [pods](indexes#pods-pod-types-and-pod-sizes) in your project.  ## Change project pod limit in console  1. Access the [Pinecone Console](https://app.pinecone.io). 1. Click **Settings** in the left menu. 1. In the **Settings** view, click the **PROJECTS** tab. 1. Next to the project you want to update, click <img src=\"https://raw.githubusercontent.com/pinecone-io/img/main/edit-icon.png\" alt=\"pencil icon\" height=\"20\"/>. 1. Under **Pod Limit**, enter the new number of pods. 1. Click **SAVE CHANGES**. ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e229"
  },
  "title": "Add users to projects and organizations",
  "category": "630fc5235d91a70054705fb6",
  "content": "## Overview   If you are a [project](/manage-projects) or [organization](/organizations) owner, follow these steps to add users to organizations and projects.   ## Add users to projects and organizations  1. Access the [Pinecone Console](https://app.pinecone.io).  1. Click **Settings** in the left menu.  1. In the **Settings** view, click the **USERS** tab.  1. Click **+INVITE USER**.  1. (Organization owner only) Select an [organization role](organizations#organization-roles).  1. Select one or more projects.  1. Select a [project role](projects#project-roles).  1. Enter the user's email address.  1. Click **+INVITE USER**.  When you invite another user to join your organization or project, Pinecone sends them an email containing a link that enables them to gain access to the organization or project. If they already have a Pinecone account, they still receive an email, but they can also immediately view the project. ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e22b"
  },
  "title": "Rename a project",
  "category": "630fc5235d91a70054705fb6",
  "content": "## Overview  If you are a [project owner](projects#project-roles), follow these steps to change the name ofyour project.  1. Access the [Pinecone Console](https://app.pinecone.io).  1. Click **Settings** in the left menu.  1. In the **Settings** view, click the **PROJECTS** tab.  1. Next to the project you want to update, click ![edit icon](https://raw.githubusercontent.com/pinecone-io/img/main/edit-icon.png).  1. Under **Project Name**, enter the new project name.  1. Click **SAVE CHANGES**. ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e22d"
  },
  "title": "Changing your billing plan",
  "category": "630fc5235d91a70054705fb6",
  "content": "This document describes how to change the billing plan for your Pinecone organization through the Pinecone console..  > ℹ️  Note > > Accounts created by [signing up through GCP Marketplace](setting-up-gcp-marketplace-billing) must change billing plans through the Pinecone console using this workflow.   To change your billing plan, you must be the [organization owner](organizations#organization-owners) for your organization.  To change your billing plan through the Pinecone console, follow these steps:  1. Log in to the [Pinecone console](https://app.pinecone.io). 1. In the left menu, click **Organizations**. 1. Click the **Billing** tab. 1. Under the plan you want, click **Upgrade**.   ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e22f"
  },
  "title": "Setting up billing through GCP Marketplace",
  "category": "630fc5235d91a70054705fb6",
  "content": "This document describes how to configure pay-as-you-go billing for your Pinecone organization through Google Cloud Platform (GCP) Marketplace. To commit to annual spending, [contact Pinecone](https://www.pinecone.io/contact).  > ℹ️  Note > > This workflow creates a new Pinecone [organization](organizations). If you already have an organization, signing up through GCP Marketplace creates an additional organization.   To configure Pinecone billing through the GCP Marketplace, follow these steps:  1. Log in to the [GCP Marketplace](https://console.cloud.google.com/marketplace). Your project must be enabled for purchase by your billing administrator. 1. Search for the [Pinecone listing](https://console.cloud.google.com/marketplace/product/pinecone-public/pinecone). 1. Click **Subscribe**. 1. Read and agree to the terms and conditions. 1. Click **Subscribe**. 1. On the **Your order request has been sent to Pinecone** dialog, click **Sign up with Pinecone**. 1. On the **You're leaving Google** dialog, click **OK**. This takes you to the Pinecone sign-in page. 1. Sign in to Pinecone.io.  When you sign in, Pinecone creates a new organization linked to your GCP billing.  If you already have a Pinecone organization, you can select the new \"GCP Linked\" organization in the top-left drop-down menu in the console. After creating the new organization, contact [Pinecone support](https://support.pinecone.io) for help migrating to the new organization.  ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e231"
  },
  "title": "Elasticsearch",
  "category": "630fc5235d91a70054705fb4",
  "content": "[Elasticsearch](https://www.elastic.co/) is a powerful open-source search engine and analytics platform that is widely used as a document store for keyword-based text search.  Pinecone is a [vector database](https://www.pinecone.io/learn/vector-database/) widely used for production applications — such as semantic search, recommenders, and threat detection — that require fast and fresh vector search at the scale of tens or hundreds of millions (or even billions) of embeddings. Although Pinecone offers [hybrid search](https://docs.pinecone.io/docs/hybrid-search) for keyword-aware semantic search, Pinecone is not a document store and does not replace Elasticsearch for keyword-only retrieval.  If you already use Elasticsearch and want to add Pinecone’s low-latency and large-scale vector search to your applications, this guide will show you how. You will see how to:  - Add an embedding model to Elasticsearch - Transform text data into vector embeddings within Elasticsearch - Load those vector embeddings into Pinecone, with corresponding IDs and metadata.  ## Uploading the embedding model  We first need to upload the embedding model to our Elastic instance. To do so, we’ll use the `[eland](https://github.com/elastic/eland)` Elastic client. We’ll have to clone the \"eland\" repository and build the docker image before running it:  ```bash git clone git@github.com:elastic/eland.git cd eland docker build -t elastic/eland . ```  In this example, we’ll use the `[sentence-transformers/msmarco-MiniLM-L-12-v3](https://huggingface.co/sentence-transformers/msmarco-MiniLM-L-12-v3)` model from [Hugging Face](https://huggingface.co/) — although you could use any model you’d like. To upload the model to your Elasticsearch deployment, run the following command:  ```bash docker run -it --rm elastic/eland \\    eland_import_hub_model \\    --url https://<user>:<password>@<host>:<port>/ \\    --hub-model-id sentence-transformers/msmarco-MiniLM-L-12-v3 \\    --task-type text_embedding \\    --start ```  Note that you’ll have to replace the placeholders with your Elasticsearch instance user, password, host, and port. If you set up your own Elasticsearch instance, you would have already set the username and password when initially setting up the instance. If you’re using the hosted Elastic Stack, you can find the username and password in the \"Security\" section of the Elastic Stack console.  We can quickly test the uploaded model by running the following command in the Elasticsearch developer console:  ``` POST /_ml/trained_models/sentence-transformers__msmarco-minilm-l-12-v3/deployment/_infer {  \"docs\": {    \"text_field\": \"Hello World!\"  } } ```  We should get the following result:  ```json {  \"predicted_value\": [    -0.06176435202360153,    -0.008180409669876099,    0.3309500813484192,    0.38672536611557007,    ...  ] } ```  This is the vector embedding for our query. We’re now ready to upload our dataset and apply the model to produce the vector embeddings.  ## Uploading the dataset  Next, upload a dataset of documents to Elasticsearch. In this example, we’ll use a subset of the MSMacro dataset. You can [download the file](https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-passagetest2019-top1000.tsv.gz) or run the following command:  ```bash curl -O https://msmarco.blob.core.windows.net/msmarcoranking/msmarco-passagetest2019-top1000.tsv.gz gunzip msmarco-passagetest2019-top1000.tsv ```  In this example, we’ll be using the hosted Elastic Stack, which makes it easier to use various integrations. We’ll use the \"Upload\" integration to load the data into an Elasticsearch index.  ![add-data](https://raw.githubusercontent.com/pinecone-io/img/main/elastic_1.png)  We’ll drag the unzipped TSV file. The Upload integration will sample the data for us and show the following:  ![data-preview](https://raw.githubusercontent.com/pinecone-io/img/main/elastic_2.png)  We’ll click the \"Import\" button and continue to name the index:  ![import](https://raw.githubusercontent.com/pinecone-io/img/main/elastic_3.png)  Once the import is complete, you’ll see the following:  ![import-complete](https://raw.githubusercontent.com/pinecone-io/img/main/elastic_4.png)  Clicking \"View index in Discover\" will reveal the index view where we can look at the uploaded data:  ![discover-index](https://raw.githubusercontent.com/pinecone-io/img/main/elastic_5.png)  ## Creating the embeddings  We’ve now created an index for our data. Next, we’ll create a pipeline to produce a vector embedding for each document. We’ll head to the Elasticsearch developer console and issue the following command to create the pipeline:  ``` PUT _ingest/pipeline/produce-embeddings {  \"description\": \"Vector embedding pipeline\",  \"processors\": [    {      \"inference\": {        \"model_id\": \"sentence-transformers__msmarco-minilm-l-12-v3\",        \"target_field\": \"text_embedding\",        \"field_map\": {          \"text\": \"text_field\"        }      }    }  ],  \"on_failure\": [    {      \"set\": {        \"description\": \"Index document to 'failed-<index>'\",        \"field\": \"_index\",        \"value\": \"failed-{{{_index}}}\"      }    },    {      \"set\": {        \"description\": \"Set error message\",        \"field\": \"ingest.failure\",        \"value\": \"{{_ingest.on_failure_message}}\"      }    }  ] } ```  The \"processor\" definition tells Elasticsearch which model to use and which field to read from. The \"on_failure\" definition defines the failure behavior that Elasticsearch will apply &mdash; specifically, which error message to write and which file to write them into.  Once the embedding pipeline is created, we’ll re-index our \"msmacro-raw\" index, applying the embedding pipeline to produce the new embeddings. In the developer console, execute the following command:  ``` POST _reindex?wait_for_completion=false {  \"source\": {    \"index\": \"msmacro-raw\"  },  \"dest\": {    \"index\": \"msmacro-with-embeddings\",    \"pipeline\": \"text-embeddings\"  } } ```  This will kick off the embedding pipeline. We’ll get a task id which we can track with the following command:  ``` GET _tasks/<task_id> ```  Looking at the index, we can see that the embeddings have been created in an object called \"text_embeddings\" under the field \"predicted_value\".  To make the loading process a bit easier, we’re going to pluck the \"predicted_value\" field and add it as its own column:  ``` POST _reindex?wait_for_completion=false {  \"source\": {    \"index\": \"msmacro-with-embeddings\"  },  \"dest\": {    \"index\": \"msmacro-with-embeddings-flat\"  },  \"script\": {    \"source\": \"ctx._source.predicted_value = ctx._source.text_embedding.predicted_value\"  } } ```  Next, we’ll load the embeddings into Pinecone. Since the index size is considerable, we’ll use Apache Spark to parallelize the process.  ## Moving the Elasticsearch index to Pinecone  In this example, we’ll be using Databricks to handle the process of loading Elasticsearch index to Pinecone. We’ll add the Elasticsearch Spark from Maven by navigating to the “Libraries” tab in the cluster settings view, and clicking “Install new”:  ![cluster-setup](https://raw.githubusercontent.com/pinecone-io/img/main/elastic_6.png)  Use the following Maven coordinates:  `org.elasticsearch:elasticsearch-spark-30_2.12:8.5.2`  We’ll add the Pinecone Databricks connectors from S3:  `s3://pinecone-jars/spark-pinecone-uberjar.jar`  Restart the cluster if needed. Next, we’ll create a new notebook, attach it to the cluster and import the required dependencies:  ```scala import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.apache.spark.SparkContext import org.apache.spark.SparkContext._ import org.elasticsearch.spark._ ```  We’ll initialize the Spark context:  ```scala val spark = SparkSession.builder.appName(\"elasticSpark\").master(\"local[*]\").getOrCreate() ```  Next, we’ll read the index from Elasticsearch:  ```scala val df = (spark.read      .format( \"org.elasticsearch.spark.sql\" )      .option( \"es.nodes\",   \"<ELASTIC_URL>\" )      .option( \"es.net.http.auth.user\", \"<ELASTIC_USER>\" )      .option( \"es.net.http.auth.pass\", \"<ELASTIC_PASSWORD>\" )      .option( \"es.port\",    443     )      .option( \"es.nodes.wan.only\", \"true\" )      .option(\"es.net.ssl\", \"true\")      .option(\"es.read.field.as.array.include\",\"predicted_value:1\")      .load( \"msmacro-with-embeddings\")  ) ```  Note that to ensure the index is read correctly into the dataframe, we must specify that the “predicted_value” field is an array with a depth of 1, as shown below:  ```scala   .option(\"es.read.field.as.array.include\",\"predicted_value:1\") ```  Next, we’ll use the Pinecone Spark connector to load this dataframe into a Pinecone index. We’ll start by creating an index in the [Pinecone console](https://app.pinecone.io/). Log in to the console and click “Create Index”. Then, name your index, and configure it to use 384 dimensions.  ![create-index](https://raw.githubusercontent.com/pinecone-io/img/main/elastic_7.png)  When you’re done configuring the index, click “Create Index”.  We have to do some prep work to get the dataframe ready for indexing. In order to index the original document with the embeddings we’ve created, we’ll create the following UDF which will encode the original document as a Base64 string. This will ensure the metadata object will remain a valid JSON object regardless of the content of the document.  ```scala import org.apache.spark.sql.SparkSession import org.apache.spark.sql.functions.udf import java.util.Base64  val text_to_metadata = udf((text: String) => \"{ \\\"document\\\" : \\\"\" +  Base64.getEncoder.encodeToString(text.getBytes(\"UTF-8\")) + \"\\\" }\") ```  We’ll apply the UDF and get rid of some unnecessary columns:  ```scala val clean_df = df.drop(\"text_embedding\").withColumnRenamed(\"predicted_value\", \"vector\").withColumn(\"metadata\", text_to_metadata(col(\"text_field\"))).withColumn(\"namespace\", lit(\"\")).drop(\"text_field\") ```  Next, we’ll use the Pinecone Spark connector:  ```scala val pineconeOptions = Map(   \"pinecone.apiKey\" -> \"<PINECONE_API_KEY>\",   \"pinecone.environment\" -> \"us-west1-gcp\",   \"pinecone.projectName\" -> \"<PROJECT_IDENTIFIER>\",   \"pinecone.indexName\" -> \"elastic-index\" )  clean_df.write   .options(pineconeOptions)   .format(\"io.pinecone.spark.pinecone.Pinecone\")   .mode(SaveMode.Append)   .save() ```  Our vectors have been added to our Pinecone index!  To query the index, we’ll need to generate a vector embedding for our query first, using the `sentence-transformers/msmarco-MiniLM-L-12-v3` model. Then, we’ll use the Pinecone client to issue the query. We'll do this in a Python notebook.  We’ll start by installing the required dependencies:  ``` !pip install -qU pinecone-client sentence-transformers pandas ```  Next, we’ll set up the client:  ```python import pinecone  # connect to pinecone environment pinecone.init(    api_key=\"<PINECONE API KEY>\",    environment=\"us-west1-gcp\" ) ```  We’ll set up the index:  ```python index_name = \"elastic-index\" index = pinecone.Index(index_name) ```  We’ll create a helper function that will decode the encoded documents we get:  ```python def decode_entries(entries):    return list(map(lambda entry: {        \"id\": entry[\"id\"],        \"score\": entry[\"score\"],        \"document\": base64.b64decode(entry[\"metadata\"][\"document\"]).decode(\"UTF-8\"),    }, entries)) ```  Next, we’ll create a function that will encode our query, query the index and convert the display the data using Pandas:  ```python def queryIndex(query, num_results):  vector = model.encode(query).tolist()  result = index.query(vector, top_k=num_results, include_metadata=True)  return pd.DataFrame(decode_entries(result.matches)) ```  Finally, we’ll test our index:  ```python display(queryIndex(\"star trek\", 10)) ```  Should yield the results: ![results](https://raw.githubusercontent.com/pinecone-io/img/main/elastic_8.png)  ## Summary  In conclusion, by following the steps outlined in this post, you can easily upload an embedding model to Elasticsearch, ingest raw textual data, create the embeddings, and load them into Pinecone. With this approach, you can take advantage of the benefits of integrating Elasticsearch and Pinecone. As mentioned, while Elasticsearch is optimized for indexing documents, Pinecone provides vector storage and search capabilities that can handle hundreds of millions and even billions of vectors. ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e233"
  },
  "title": "OpenAI",
  "category": "630fc5235d91a70054705fb4",
  "content": "<div class=\"source\">   <!-- Source Link -->   <a href=\"https://github.com/pinecone-io/examples/blob/master/integrations/openai/\" class=\"source-link\"><img src=\"../images/source.svg\" /> View Source</a>   <!-- Colab Link -->   <a href=\"https://colab.research.google.com/github/pinecone-io/examples/blob/master/integrations/openai/semantic_search_openai.ipynb\" class=\"source-link\"><img src=\"../images/colab.svg\" /> Open in Colab</a> </div>  In this guide you will learn how to use the [OpenAI Embedding API](https://beta.openai.com/docs/guides/embeddings) to generate language embeddings, and then index those embeddings in the [Pinecone vector database](https://www.pinecone.io) for fast and scalable vector search.  This is a powerful and common combination for building semantic search, question-answering, threat-detection, and other applications that rely on NLP and search over a large corpus of text data.  The basic workflow looks like this: * Embed and index   * Use the OpenAI Embedding API to generate vector embeddings of your documents (or any text data).   * Upload those vector embeddings into Pinecone, which can store and index millions/billions of these vector embeddings, and search through them at ultra-low latencies. * Search   * Pass your query text or document through the OpenAI Embedding API again.   * Take the resulting vector embedding and send it as a [query](/docs/query-data) to Pinecone.   * Get back semantically similar documents, even if they don't share any keywords with the query.  ![Basic workflow of OpenAI and Pinecone](https://files.readme.io/6a3ea5a-pinecone-openai-overview.png)  Let's get started...  ## Environment Setup  We start by installing the OpenAI and Pinecone clients, we will also need HuggingFace *Datasets* for downloading the TREC dataset that we will use in this guide.  ```bash pip install -U openai pinecone-client datasets ```  ## Creating Embeddings  To create embeddings we must first initialize our connection to OpenAI Embeddings, we sign up for an API key at [OpenAI](https://beta.openai.com/signup).  ```python import openai  openai.organization = \"<<YOUR_ORG_KEY>>\" # get this from top-right dropdown on OpenAI under organization > settings openai.api_key = \"<<YOUR_API_KEY>>\" # get API key from top-right dropdown on OpenAI website  openai.Engine.list()  # check we have authenticated ```  The `openai.Engine.list()` function should return a list of models that we can use. We will use OpenAI's Babbage model.  ```python MODEL = \"text-similarity-babbage-001\"  res = openai.Embedding.create(     input=[         \"Sample document text goes here\",         \"there will be several phrases in each batch\"     ], engine=MODEL ) ```  In `res` we should find a JSON-like object containing two 2048-dimensional embeddings, these are the vector representations of the two inputs provided above. To access the embeddings directly we can write:  ```python # extract embeddings to a list embeds = [record['embedding'] for record in res['data']] ```  We will use this logic when creating our embeddings for the **T**ext **RE**trieval **C**onference (TREC) question classification dataset later.  ## Initializing a Pinecone Index  Next, we initialize an index to store the vector embeddings. For this we need a Pinecone API key, [sign up for one here](https://app.pinecone.io).  ```python import pinecone  # initialize connection to pinecone (get API key at app.pinecone.io) pinecone.init(     api_key=\"<<YOUR_API_KEY>>\",     environment=\"us-west1-gcp\" ) # check if 'openai' index already exists (only create index if not) if 'openai' not in pinecone.list_indexes():     pinecone.create_index('openai', dimension=len(embeds[0])) # connect to index index = pinecone.Index('openai') ```  ## Populating the Index  With both OpenAI and Pinecone connections initialized, we can move onto populating the index. For this, we need the TREC dataset.  ```python from datasets import load_dataset  # load the first 1K rows of the TREC dataset trec = load_dataset('trec', split='train[:1000]') ```  Then we create a vector embedding for each question using OpenAI (as demonstrated earlier), and `upsert` the ID, vector embedding, and original text for each phrase to Pinecone.  > ⚠️  Warning > > High-cardinality metadata values (like the unique text values we use here) > can reduce the number of vectors that fit on a single pod. See > [Limits](/limits/) for more.  ```python from tqdm.auto import tqdm  # this is our progress bar  batch_size = 32  # process everything in batches of 32 for i in tqdm(range(0, len(trec['text']), batch_size)):     # set end position of batch     i_end = min(i+batch_size, len(trec['text']))     # get batch of lines and IDs     lines_batch = trec['text'][i: i+batch_size]     ids_batch = [str(n) for n in range(i, i_end)]     # create embeddings     res = openai.Embedding.create(input=lines_batch, engine=MODEL)     embeds = [record['embedding'] for record in res['data']]     # prep metadata and upsert batch     meta = [{'text': line} for line in lines_batch]     to_upsert = zip(ids_batch, embeds, meta)     # upsert to Pinecone     index.upsert(vectors=list(to_upsert)) ```  ## Querying  With our data indexed, we're now ready to move onto performing searches. This follows a similar process to indexing. We start with a text `query`, that we would like to use to find similar sentences. As before we encode this with OpenAI's text similarity Babbage model to create a *query vector* `xq`. We then use `xq` to query the Pinecone index.  ```python query = \"What caused the 1929 Great Depression?\"  xq = openai.Embedding.create(input=query, engine=MODEL)['data'][0]['embedding'] ```  Now we query.  ```python res = index.query([xq], top_k=5, include_metadata=True) ```  The response from Pinecone includes our original text in the `metadata` field, let's print out the `top_k` most similar questions and their respective similarity scores.  ```python for match in res['matches']:     print(f\"{match['score']:.2f}: {match['metadata']['text']}\") ```  ``` [Out]: 0.95: Why did the world enter a global depression in 1929 ? 0.87: When was `` the Great Depression '' ? 0.86: What crop failure caused the Irish Famine ? 0.82: What caused the Lynmouth floods ? 0.79: What caused Harry Houdini 's death ? ```  Looks good, let's make it harder and replace *\"depression\"* with the incorrect term *\"recession\"*.  ```python query = \"What was the cause of the major recession in the early 20th century?\"  # create the query embedding xq = openai.Embedding.create(input=query, engine=MODEL)['data'][0]['embedding']  # query, returning the top 5 most similar results res = index.query([xq], top_k=5, include_metadata=True)  for match in res['matches']:     print(f\"{match['score']:.2f}: {match['metadata']['text']}\") ```  ``` [Out]: 0.92: Why did the world enter a global depression in 1929 ? 0.85: What crop failure caused the Irish Famine ? 0.83: When was `` the Great Depression '' ? 0.82: What are some of the significant historical events of the 1990s ? 0.82: What is considered the costliest disaster the insurance industry has ever faced ? ```  Let's perform one final search using the definition of depression rather than the word or related words.  ```python query = \"Why was there a long-term economic downturn in the early 20th century?\"  # create the query embedding xq = openai.Embedding.create(input=query, engine=MODEL)['data'][0]['embedding']  # query, returning the top 5 most similar results res = index.query([xq], top_k=5, include_metadata=True)  for match in res['matches']:     print(f\"{match['score']:.2f}: {match['metadata']['text']}\") ```  ``` [Out]: 0.93: Why did the world enter a global depression in 1929 ? 0.83: What crop failure caused the Irish Famine ? 0.82: When was `` the Great Depression '' ? 0.82: How did serfdom develop in and then leave Russia ? 0.80: Why were people recruited for the Vietnam War ? ```  It's clear from this example that the semantic search pipeline is clearly able to identify the meaning between each of our queries. Using these embeddings with Pinecone allows us to return the most semantically similar questions from the already indexed TREC dataset. ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e235"
  },
  "title": "Databricks",
  "category": "630fc5235d91a70054705fb4",
  "content": "**Using Databricks and Pinecone to create and index vector embeddings at scale**  Databricks, built on top of [Apache Spark](https://spark.apache.org/), is a powerful platform for data processing and analytics, known for its ability to efficiently handle large datasets. In this guide, we will show you how to use Spark (with [Databricks](https://www.databricks.com/)) to create [vector embeddings](https://www.pinecone.io/learn/vector-embeddings/) and load them into [Pinecone](https://pinecone.io/).  First, let’s discuss why using Databricks and Pinecone is necessary in this context. When you process less than a million records, using a single machine might be sufficient. But when you work with hundreds of millions of records, you have to start thinking about how the operation scales. We need to consider two things:  1. How efficiently can we generate the embeddings at scale? 2. How efficiently would we be able to ingest and update these embeddings, at scale?  Databricks is a great tool for creating embeddings at scale: it allows us to parallelize the process over multiple machines and leverage GPUs to accelerate the process.  Pinecone lets us efficiently ingest, update and query hundreds of millions or even billions of embeddings. As a managed service, Pinecone can guarantee a very high degree of reliability and performance when it comes to datasets of this size.  Pinecone provides a specialized connector for Databricks that is optimized to ingest data from Databricks and into Pinecone. That allows the ingestion process to be completed much faster than it would have if we were to use Pinecone’s REST or gRPC APIs on a large-scale dataset.  Together, Pinecone and Databricks make a great combination for managing the entire lifecycle of vector embeddings at scale.   ## Why Databricks?  Databricks is a Unified Analytics Platform on top of Apache Spark. The primary advantage of using Spark is its ability to distribute the workload across a cluster of machines, allowing it to process large amounts of data quickly and efficiently. By adding more machines or increasing the number of cores on each machine, it is easy to horizontally scale the cluster as needed to handle larger workloads.  At the core of Spark is the map-reduce pattern, where data is divided into partitions and a series of transformations is applied to each partition in parallel. The results from each partition are then automatically collected and aggregated into the final result. This approach makes Spark both fast and fault-tolerant, as it can retry failed tasks without requiring the entire workload to be reprocessed.  In addition to its parallel processing capabilities, Spark allows developers to write code in popular languages like Python and Scala, which are then optimized for parallel execution under the covers. This makes it easier for developers to focus on the data processing itself, rather than worrying about the details of distributed computing.  Vector embedding is a computationally intensive task, where parallelization can save many hours of precious computation time and resources. Leveraging GPUs with Spark can produce even better results — enjoying the benefits of the fast computation of a GPU combined with parallelization will ensure optimal performance.  Databricks makes it easier to work with Apache Spark: it provides easy set-up and tear-down of clusters, dependency management, compute allocation, storage solution integrations, and more.   ## Why Pinecone?  Pinecone is a vector database that makes it easy to build high-performance vector search applications. It offers a number of key benefits for dealing with vector embeddings at scale, including ultra-low query latency at any scale, live index updates when you add, edit, or delete data, and the ability to combine vector search with metadata filtering or [keyword search](https://docs.pinecone.io/docs/hybrid-search) for more relevant results. As mentioned before, Pinecone can easily handle very large scales of hundreds of millions and even billions of vector embeddings. Additionally, Pinecone is fully managed, so it's easy to use and scale.  With Pinecone, you can easily index and search through vector embeddings. It is ideal for a variety of use cases such as semantic text search, question-answering, visual search, recommendation systems, and more.   In this example, we'll create embeddings based on the [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) model from [Hugging Face](https://huggingface.co/). We'll then use a dataset with a large volume of documents to produce the embeddings and upsert them into Pinecone. Note that the actual model and dataset we'll use are immaterial for this example. This method should work on any embeddings you may want to create, with whatever dataset you may choose.  In order to create embeddings at scale, we need to do four things:  1. Set up a Spark cluster 2. Load the dataset into partitions 3. Apply an embedding model on each entry to produce the embedding 4. Save the results  Let's get started!  ## Setting up a Spark Cluster  Using Databricks makes it easy to speed up the creation of our embedding even more by using GPUs instead of CPUs in our cluster. To do this, navigate to the \"Compute\" section in your Databricks console, and select the following options:  ![cluster-setup](https://raw.githubusercontent.com/pinecone-io/img/main/databricks_1.png)  Next, we'll add the Pinecone Spark connector to our cluster. Navigate to the \"Libraries\" tab and click \"Instal\" new”.  ![install](https://raw.githubusercontent.com/pinecone-io/img/main/databricks_2.png)  Select \"DBF\"/S3” and paste the following S3 URI:  `s3://pinecone-jars/spark-pinecone-uberjar.jar`  ![s3-install](https://raw.githubusercontent.com/pinecone-io/img/main/databricks_3.png)  To complete the installation, click \"Install\". To use the new cluster, create a new notebook and attach it to the newly created cluster.  ## Environment Setup  We'll start by installing some dependencies:  ``` %pip install datasets transformers pinecone-client torch ```  Next, we'll set up the connection to Pinecone. You'll have to retrieve the following information from [your Pinecone console](https://app.pinecone.io):  1. API Key: navigate to your project and click the \"API Keys\" button on the sidebar 2. Environment: check the browser url to fetch the environment. `https://app.pinecone.io/organizations/[org-id]/projects/[environment]:[project_name]/indexes`  Your index name will be the same index name used when we initialized the index (in this case, news).  ```python import pinecone  api_key = # <YOUR_PINECONE_API_KEY> environment = 'us-west1-gcp' pinecone.init(api_key=api_key, environment=environment) ```  Next, we'll create a new index in Pinecone, where our vector embeddings will be saved:  ```python index_name = 'news'  if index_name in pinecone.list_indexes():    pinecone.delete_index(index_name) pinecone.create_index(name=index_name, dimension=384) index = pinecone.Index(index_name=index_name) ```  ## Load the dataset into partitions  In this example, we'll use a collection of news articles as our example dataset. We'll use Hugging Face's datasets library and load the data into our environment:  ```python from datasets import list_datasets, load_dataset  dataset_name = \"allenai/multinews_sparse_max\" dataset = load_dataset(dataset_name, split=\"train\") ```  Next, we'll convert the dataset from the Hugging Face format and repartition it:  ```python dataset.to_parquet('/dbfs/tmp/dataset_parquet.pq') num_workers = 10 dataset_df = spark.read.parquet('/tmp/dataset_parquet.pq').repartition(num_workers) ```  Once the repartition is complete, we get back a DataFrame, which is a distributed collection of the data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. As mentioned above, each partition in the dataframe has an equal amount of the original data.  The dataset doesn't have identifiers associated with each document, so let's add them:  ```python from pyspark.sql.types import StringType from pyspark.sql.functions import monotonically_increasing_id  dataset_df = dataset_df.withColumn('id', monotonically_increasing_id().cast(StringType())) ```  As its name suggests, withColumn adds a column to the dataframe, containing a simple increasing identifier that we cast to a string. Great! Now we have identifiers for each document. Let's move on to creating the embeddings for each document.  ## Create a function for transforming text into embeddings  In this example, we will create a UDF (User Defined Function) to create the embeddings, using the AutoTokenizer and AutoModel classes from the Hugging Face transformers library. The UDF will be applied to each partition in a dataframe. When applied to a partition, a UDF is executed on each row in the partition. The UDF will tokenize the document using AutoTokenzier and then pass the result to the model (in this case we're using [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)). Finally, we'll produce the embeddings themselves by extracting the last hidden layer from the result.  Once the UDF is created, it can be applied to a dataframe to transform the data in the specified column. The Python UDF will be sent to the Spark workers, where it will be used to transform the data. After the transformation is complete, the results will be sent back to the driver program and stored in a new column.  ```python from transformers import AutoTokenizer, AutoModel  def create_embeddings(partitionData):    tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")    model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")     for row in partitionData:        document = str(row.document)        inputs = tokenizer(document, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)        result = model(**inputs)        embeddings = result.last_hidden_state[:, 0, :].cpu().detach().numpy()        lst = embeddings.flatten().tolist()        yield [row.id, lst, '', '{}'] ```  ## Applying the UDF to the data  A dataframe in Spark is a higher-level abstraction built on top of a more fundamental building block called an RDD - or Resilient Distributed Dataset. We're going to use the `mapPartitions` function that gives us finer control over the execution of our UDF, by explicitly applying it to each partition of the RDD.  ```python embeddings = dataset_df.rdd.mapPartitions(create_embeddings) ```  Next, we’ll convert the resulting RDD back into a dataframe with the schema required by Pinecone:  ```python from pyspark.sql.types import StructType,StructField, ArrayType, FloatType  schema = StructType([     StructField(\"id\",StringType(),True),     StructField(\"vector\",ArrayType(FloatType()),True),     StructField(\"namespace\",StringType(),True),     StructField(\"metadata\", StringType(), True),   ])  embeddings_df = spark.createDataFrame(data=embeddings,schema=schema) ```  ## Upserting the embeddings  Lastly, we'll use the Pinecone Spark connector to save the embeddings to our index.  ```python  (     df.write     .option(\"pinecone.apiKey\", api_key)     .option(\"pinecone.environment\", environment)     .option(\"pinecone.projectName\", pinecone.whoami().projectname)     .option(\"pinecone.indexName\", index_name)     .format(\"io.pinecone.spark.pinecone.Pinecone\")     .mode(\"append\")     .save() )  ```  The process of writing the embeddings to Pinecone should take approximately 15 seconds. When it completes, you’ll see the following:  ``` spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@41638051  pineconeOptions: scala.collection.immutable.Map[String,String] = Map(pinecone.apiKey -><YOUR API KEY>, pinecone.environment -> us-west1-gcp, pinecone.projectName -><YOUR PROJECT NAME>, pinecone.indexName -> \"news\") ```  This means the process was completed successfully and the embeddings have been stored in Pinecone.   ## Summary  Creating vector embeddings for large datasets can be challenging, but Databricks a great tool to accomplish the task. Databricks makes it easy to set up a GPU cluster and handle the required dependencies, allowing for efficient creation of embeddings at scale.  Databricks and Pinecone are the perfect combination for working with very large vector datasets. Pinecone provides a way to efficiently store and retrieve the vectors created by Databricks, making it easy and performant to work with a huge number of vectors. Overall, the combination of Databricks and Pinecone provides a powerful and effective solution for creating embeddings for very large datasets. By parallelizing the embedding generation and the data ingestion processes, we can create a fast and resilient pipeline that will be able to index and update large volumes of vectors. ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e237"
  },
  "title": "Cohere",
  "category": "630fc5235d91a70054705fb4",
  "content": "<div class=\"source\">   <!-- Source Link -->   <a href=\"https://github.com/pinecone-io/examples/blob/master/integrations/cohere/\" class=\"source-link\"><img src=\"../images/source.svg\" /> View Source</a>   <!-- Colab Link -->   <a href=\"https://colab.research.google.com/github/pinecone-io/examples/blob/master/integrations/cohere/semantic_search_trec.ipynb\" class=\"source-link\"><img src=\"../images/colab.svg\" /> Open in Colab</a> </div>  In this guide you will learn how to use the [Cohere Embed API endpoint](https://docs.cohere.ai/reference/embed) to generate language embeddings, and then index those embeddings in the [Pinecone vector database](https://www.pinecone.io) for fast and scalable vector search.  This is a powerful and common combination for building semantic search, question-answering, threat-detection, and other applications that rely on NLP and search over a large corpus of text data.  The basic workflow looks like this: * Embed and index   * Use the Cohere Embed API endpoint to generate vector embeddings of your documents (or any text data).   * Upload those vector embeddings into Pinecone, which can store and index millions/billions of these vector embeddings, and search through them at ultra-low latencies. * Search   * Pass your query text or document through the Cohere Embed API endpoint again.   * Take the resulting vector embedding and send it as a [query](/docs/query-data) to Pinecone.   * Get back semantically similar documents, even if they don't share any keywords with the query.  ![Basic workflow of Cohere with Pinecone](https://files.readme.io/fd0ba7b-pinecone-cohere-overview.png)  Let's get started...  https://files.readme.io/fd0ba7b-pinecone-cohere-overview.png  ## Environment Setup  We start by installing the Cohere and Pinecone clients, we will also need HuggingFace *Datasets* for downloading the TREC dataset that we will use in this guide.  ```bash pip install -U cohere pinecone-client datasets ```  ## Creating Embeddings  To create embeddings we must first initialize our connection to Cohere, we sign up for an API key at [Cohere](https://os.cohere.ai/).  ```python import cohere  co = cohere.Client(\"<<YOUR_API_KEY>>\") ```  We will load the **T**ext **RE**trieval **C**onference (TREC) question classification dataset which contains 5.5K labeled questions. We will take the first 1K samples for this walkthrough, but this can be scaled to millions or even billions of samples.  ```python from datasets import load_dataset  # load the first 1K rows of the TREC dataset trec = load_dataset('trec', split='train[:1000]') ```  Each sample in `trec` contains two label features and the *text* feature, which we will be using. We can pass the questions from the *text* feature to Cohere to create embeddings.  ```python embeds = co.embed(     texts=trec['text'],     model='small',     truncate='LEFT' ).embeddings ```  We can check the dimensionality of the returned vectors, for this we will convert it from a list of lists to a Numpy array. We will need to save the embedding dimensionality from this to be used when initializing our Pinecone index later.  ```python import numpy as np  shape = np.array(embeds).shape print(shape) ```  ``` [Out]: (1000, 1024) ```  Here we can see the `1024` embedding dimensionality produced by Cohere's small model, and the `1000` samples we built embeddings for.  ## Storing the Embeddings  Now that we have our embeddings we can move on to indexing them in the Pinecone vector database. For this we need a Pinecone API key, [sign up for one here](https://app.pinecone.io).  We first initialize our connection to Pinecone, and then create a new index for storing the embeddings (we will call it `\"cohere-pinecone-trec\"`). When creating the index we specify that we would like to use the cosine similarity metric to align with Cohere's embeddings, and also pass the embedding dimensionality of `1024`.  ```python import pinecone  pinecone.init(\"<<YOUR_API_KEY>>\", environment='us-west1-gcp')  index_name = 'cohere-pinecone-trec'  # if the index does not exist, we create it if index_name not in pinecone.list_indexes():     pinecone.create_index(         index_name,         dimension=shape[1],         metric='cosine'     )  # connect to index index = pinecone.Index(index_name) ```  Now we can begin populating the index with our embeddings. Pinecone expects us to provide a list of tuples in the format *(id, vector, metadata)*, where the *metadata* field is an optional extra field where we can store anything we want in a dictionary format. For this example, we will store the original text of the embeddings.  > ⚠️  Warning > > High-cardinality metadata values (like the unique text values we use here) > can reduce the number of vectors that fit on a single pod. See > [Limits](/limits/) for more.  While uploading our data, we will batch everything to avoid pushing too much data in one go.  ```python batch_size = 128  ids = [str(i) for i in range(shape[0])] # create list of metadata dictionaries meta = [{'text': text} for text in trec['text']]  # create list of (id, vector, metadata) tuples to be upserted to_upsert = list(zip(ids, embeds, meta))  for i in range(0, shape[0], batch_size):     i_end = min(i+batch_size, shape[0])     index.upsert(vectors=to_upsert[i:i_end])  # let's view the index statistics print(index.describe_index_stats()) ```  ``` [Out]: {'dimension': 1024,  'index_fullness': 0.0,  'namespaces': {'': {'vector_count': 1000}}} ```  We can see from `index.describe_index_stats` that we have a *1024-dimensionality* index populated with *1000* embeddings. The `indexFullness` metric tells us how full our index is, at the moment it is empty. Using the default value of one *p1* pod we can fit around 750K embeddings before the `indexFullness` reaches capacity. The [Usage Estimator](https://www.pinecone.io/pricing/) can be used to identify the number of pods required for a given number of *n*-dimensional embeddings.  ## Semantic Search  Now that we have our indexed vectors we can perform a few search queries. When searching we will first embed our query using Cohere, and then search using the returned vector in Pinecone.  ```python query = \"What caused the 1929 Great Depression?\"  # create the query embedding xq = co.embed(     texts=[query],     model='small',     truncate='LEFT' ).embeddings  print(np.array(xq).shape)  # query, returning the top 5 most similar results res = index.query(xq, top_k=5, include_metadata=True) ```  The response from Pinecone includes our original text in the `metadata` field, let's print out the `top_k` most similar questions and their respective similarity scores.  ```python for match in res['matches']:     print(f\"{match['score']:.2f}: {match['metadata']['text']}\") ```  ``` [Out]: 0.83: Why did the world enter a global depression in 1929 ? 0.75: When was `` the Great Depression '' ? 0.50: What crop failure caused the Irish Famine ? 0.34: What war did the Wanna-Go-Home Riots occur after ? 0.34: What were popular songs and types of songs in the 1920s ? ```  Looks good, let's make it harder and replace *\"depression\"* with the incorrect term *\"recession\"*.  ```python query = \"What was the cause of the major recession in the early 20th century?\"  # create the query embedding xq = co.embed(     texts=[query],     model='small',     truncate='LEFT' ).embeddings  # query, returning the top 5 most similar results res = index.query(xq, top_k=5, include_metadata=True)  for match in res['matches']:     print(f\"{match['score']:.2f}: {match['metadata']['text']}\") ```  ``` [Out]: 0.66: Why did the world enter a global depression in 1929 ? 0.61: When was `` the Great Depression '' ? 0.43: What are some of the significant historical events of the 1990s ? 0.43: What crop failure caused the Irish Famine ? 0.37: What were popular songs and types of songs in the 1920s ? ```  Let's perform one final search using the definition of depression rather than the word or related words.  ```python query = \"Why was there a long-term economic downturn in the early 20th century?\"  # create the query embedding xq = co.embed(     texts=[query],     model='small',     truncate='LEFT' ).embeddings  # query, returning the top 10 most similar results res = index.query(xq, top_k=10, include_metadata=True)  for match in res['matches']:     print(f\"{match['score']:.2f}: {match['metadata']['text']}\") ```  ``` [Out]: 0.71: Why did the world enter a global depression in 1929 ? 0.62: When was `` the Great Depression '' ? 0.40: What crop failure caused the Irish Famine ? 0.38: What are some of the significant historical events of the 1990s ? 0.38: When did the Dow first reach ? ```  It's clear from this example that the semantic search pipeline is clearly able to identify the meaning between each of our queries. Using these embeddings with Pinecone allows us to return the most semantically similar questions from the already indexed TREC dataset. ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e239"
  },
  "title": "Hugging Face Inference Endpoints",
  "category": "630fc5235d91a70054705fb4",
  "content": "Hugging Face Inference Endpoints allows access to straightforward model inference. Coupled with Pinecone we can generate and index high-quality vector embeddings with ease.  Let's get started by initializing an Inference Endpoint for generating vector embeddings.  ## Endpoints  We start by heading over to the [Hugging Face Inference Endpoints homepage](https://ui.endpoints.huggingface.co/endpoints) and signing up for an account if needed. After, we should find ourselves on this page:  ![endpoints 0](https://github.com/pinecone-io/examples/blob/master/integrations/hugging-face/endpoints/assets/hf-endpoints-0.png?raw=true)  We click on **Create new endpoint**, choose a model repository (eg name of the model), endpoint name (this can be anything), and select a cloud environment. Before moving on it is *very important* that we set the **Task** to **Sentence Embeddings** (found within the *Advanced configuration* settings).  ![endpoints 1](https://github.com/pinecone-io/examples/blob/master/integrations/hugging-face/endpoints/assets/hf-endpoints-1.png?raw=true)  ![endpoints 2](https://github.com/pinecone-io/examples/blob/master/integrations/hugging-face/endpoints/assets/hf-endpoints-2.png?raw=true)  Other important options include the *Instance Type*, by default this uses CPU which is cheaper but also slower. For faster processing we need a GPU instance. And finally, we set our privacy setting near the end of the page.  After setting our options we can click **Create Endpoint** at the bottom of the page. This action should take use to the next page where we will see the current status of our endpoint.  ![endpoints 3](https://github.com/pinecone-io/examples/blob/master/integrations/hugging-face/endpoints/assets/hf-endpoints-3.png?raw=true)  Once the status has moved from **Building** to **Running** (this can take some time), we're ready to begin creating embeddings with it.  ## Creating Embeddings  Each endpoint is given an **Endpoint URL**, it can be found on the endpoint **Overview** page. We need to assign this endpoint URL to the `endpoint_url` variable.  ![endpoints 4](https://github.com/pinecone-io/examples/blob/master/integrations/hugging-face/endpoints/assets/hf-endpoints-4.png?raw=true)   ```python endpoint = \"<<ENDPOINT_URL>>\" ```  We will also need the organization API token, we find this via the organization settings on Hugging Face (`https://huggingface.co/organizations/<ORG_NAME>/settings/profile`). This is assigned to the `api_org` variable.  ![endpoints 5](https://github.com/pinecone-io/examples/blob/master/integrations/hugging-face/endpoints/assets/hf-endpoints-5.png?raw=true)   ```python api_org = \"<<API_ORG_TOKEN>>\" ```  Now we're ready to create embeddings via Inference Endpoints. Let's start with a toy example.   ```python import requests  # add the api org token to the headers headers = {     'Authorization': f'Bearer {api_org}' } # we add sentences to embed like so json_data = {\"inputs\": [\"a happy dog\", \"a sad dog\"]} # make the request res = requests.post(     endpoint,     headers=headers,     json=json_data ) ```  We should see a `200` response.   ```python res ```         <Response [200]>    Inside the response we should find two embeddings...   ```python len(res.json()['embeddings']) ```         2    We can also see the dimensionality of our embeddings like so:   ```python dim = len(res.json()['embeddings'][0]) dim ```         768    We will need more than two items to search through, so let's download a larger dataset. For this we will use Hugging Face datasets.   ```python from datasets import load_dataset  snli = load_dataset(\"snli\", split='train') snli ```      Downloading: 100%|██████████| 1.93k/1.93k [00:00<00:00, 992kB/s]     Downloading: 100%|██████████| 1.26M/1.26M [00:00<00:00, 31.2MB/s]     Downloading: 100%|██████████| 65.9M/65.9M [00:01<00:00, 57.9MB/s]     Downloading: 100%|██████████| 1.26M/1.26M [00:00<00:00, 43.6MB/s]      Dataset({         features: ['premise', 'hypothesis', 'label'],         num_rows: 550152     })    SNLI contains 550K sentence pairs, many of these include duplicate items so we will take just one set of these (the *hypothesis*) and deduplicate them.   ```python passages = list(set(snli['hypothesis'])) len(passages) ```         480042    We will drop to 50K sentences so that the example is quick to run, if you have time, feel free to keep the full 480K.   ```python passages = passages[:50_000] ```  ## Vector DB  With our endpoint and dataset ready, all that we're missing is a vector database. For this, we need to initialize our connection to Pinecone, this requires a [free API key](https://app.pinecone.io/).   ```python import pinecone  # connect to pinecone environment pinecone.init(     api_key=\"<<YOUR_API_KEY>>\",     environment=\"us-west1-gcp\" ) ```  Now we create a new index called `'hf-endpoints'`, the name isn't important *but* the `dimension` must align to our endpoint model output dimensionality (we found this in `dim` above) and the model metric (typically `cosine` is okay, but not for all models).   ```python index_name = 'hf-endpoints'  # check if the hf-endpoints index exists if index_name not in pinecone.list_indexes():     # create the index if it does not exist     pinecone.create_index(         index_name,         dimension=dim,         metric=\"cosine\"     )  # connect to hf-endpoints index we created index = pinecone.Index(index_name) ```  ## Create and Index Embeddings  Now we have all of our components ready; endpoints, dataset, and Pinecone. Let's go ahead and create our dataset embeddings and index them within Pinecone.   ```python from tqdm.auto import tqdm  # we will use batches of 64 batch_size = 64  for i in tqdm(range(0, len(passages), batch_size)):     # find end of batch     i_end = min(i+batch_size, len(passages))     # extract batch     batch = passages[i:i_end]     # generate embeddings for batch via endpoints     res = requests.post(         endpoint,         headers=headers,         json={\"inputs\": batch}     )     emb = res.json()['embeddings']     # get metadata (just the original text)     meta = [{'text': text} for text in batch]     # create IDs     ids = [str(x) for x in range(i, i_end)]     # add all to upsert list     to_upsert = list(zip(ids, emb, meta))     # upsert/insert these records to pinecone     _ = index.upsert(vectors=to_upsert)  # check that we have all vectors in index index.describe_index_stats() ```      100%|██████████| 782/782 [11:02<00:00,  1.18it/s]       {'dimension': 768,      'index_fullness': 0.1,      'namespaces': {'': {'vector_count': 50000}},      'total_vector_count': 50000}    With everything indexed we can begin querying. We will take a few examples from the *premise* column of the dataset.   ```python query = snli['premise'][0] print(f\"Query: {query}\") # encode with HF endpoints res = requests.post(endpoint, headers=headers, json={\"inputs\": query}) xq = res.json()['embeddings'] # query and return top 5 xc = index.query(xq, top_k=5, include_metadata=True) # iterate through results and print text print(\"Answers:\") for match in xc['matches']:     print(match['metadata']['text']) ```      Query: A person on a horse jumps over a broken down airplane.     Answers:     The horse jumps over a toy airplane.     a lady rides a horse over a plane shaped obstacle     A person getting onto a horse.     person rides horse     A woman riding a horse jumps over a bar.   These look good, let's try a couple more examples.   ```python query = snli['premise'][100] print(f\"Query: {query}\") # encode with HF endpoints res = requests.post(endpoint, headers=headers, json={\"inputs\": query}) xq = res.json()['embeddings'] # query and return top 5 xc = index.query(xq, top_k=5, include_metadata=True) # iterate through results and print text print(\"Answers:\") for match in xc['matches']:     print(match['metadata']['text']) ```      Query: A woman is walking across the street eating a banana, while a man is following with his briefcase.     Answers:     A woman eats a banana and walks across a street, and there is a man trailing behind her.     A woman eats a banana split.     A woman is carrying two small watermelons and a purse while walking down the street.     The woman walked across the street.     A woman walking on the street with a monkey on her back.   And one more...   ```python query = snli['premise'][200] print(f\"Query: {query}\") # encode with HF endpoints res = requests.post(endpoint, headers=headers, json={\"inputs\": query}) xq = res.json()['embeddings'] # query and return top 5 xc = index.query(xq, top_k=5, include_metadata=True) # iterate through results and print text print(\"Answers:\") for match in xc['matches']:     print(match['metadata']['text']) ```      Query: People on bicycles waiting at an intersection.     Answers:     A pair of people on bikes are waiting at a stoplight.     Bike riders wait to cross the street.     people on bicycles     Group of bike riders stopped in the street.     There are bicycles outside.   All of these results look excellent. If you are not planning on running your endpoint and vector DB beyond this tutorial, you can shut down both.  **Once the index is deleted, you cannot use it again.**  Shut down the endpoint by navigating to the Inference Endpoints **Overview** page and selecting **Delete endpoint**. Delete the Pinecone index with:   ```python pinecone.delete_index(index_name) ```  --- ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e23b"
  },
  "title": "Haystack",
  "category": "630fc5235d91a70054705fb4",
  "content": "In this guide we will see how to integrate Pinecone and the popular [Haystack library](https://github.com/deepset-ai/haystack) for *Question-Answering*.  ## Installing Haystack  We start by installing the latest version of Haystack with all dependencies required for the `PineconeDocumentStore`.  ```python !pip install -U farm-haystack>=1.3.0 pinecone-client datasets ```  ## Initializing the PineconeDocumentStore  We initialize a `PineconeDocumentStore` by providing an API key and environment name. [Create an account](https://app.pinecone.io) to get your free API key.  ```python from haystack.document_stores import PineconeDocumentStore  document_store = PineconeDocumentStore(     api_key='<<YOUR_API_KEY>>',     index='haystack-extractive-qa',     similarity=\"cosine\",     embedding_dim=384 ) ```      INFO - haystack.document_stores.pinecone -  Index statistics: name: haystack-extractive-qa, embedding dimensions: 384, record count: 0  ## Data Preparation  Before adding data to the document store, we must download and convert data into the Document format that Haystack uses.  We will use the SQuAD dataset available from Hugging Face Datasets.  ```python from datasets import load_dataset  # load the squad dataset data = load_dataset(\"squad\", split=\"train\") ```  Next, we remove duplicates and unecessary columns.  ```python # convert to a pandas dataframe df = data.to_pandas() # select only title and context column df = df[[\"title\", \"context\"]] # drop rows containing duplicate context passages df = df.drop_duplicates(subset=\"context\") df.head() ```  <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     } </style> <table border=\"1\" class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>title</th>       <th>context</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>University_of_Notre_Dame</td>       <td>Architecturally, the school has a Catholic cha...</td>     </tr>     <tr>       <th>5</th>       <td>University_of_Notre_Dame</td>       <td>As at most other universities, Notre Dame's st...</td>     </tr>     <tr>       <th>10</th>       <td>University_of_Notre_Dame</td>       <td>The university is the major seat of the Congre...</td>     </tr>     <tr>       <th>15</th>       <td>University_of_Notre_Dame</td>       <td>The College of Engineering was established in ...</td>     </tr>     <tr>       <th>20</th>       <td>University_of_Notre_Dame</td>       <td>All of Notre Dame's undergraduate students are...</td>     </tr>   </tbody> </table> </div>  Then convert these records into the Document format.  ```python from haystack import Document  docs = [] for d in df.iterrows():     d = d[1]     # create haystack document object with text content and doc metadata     doc = Document(         content=d[\"context\"],         meta={             \"title\": d[\"title\"],             'context': d['context']         }     )     docs.append(doc) ```  This `Document` format contains two fields; *'content'* for the text content or paragraphs, and *'meta'* where we can place any additional information that can later be used to apply metadata filtering in our search.  Now we upsert the documents to Pinecone.  ```python # upsert the data document to pinecone index document_store.write_documents(docs) ```  ## Initialize Retriever  The next step is to create embeddings from these documents. We will use Haystacks `EmbeddingRetriever` with a SentenceTransformer model (`multi-qa-MiniLM-L6-cos-v1`) which has been designed for question-answering.   ```python from haystack.retriever.dense import EmbeddingRetriever  retriever = EmbeddingRetriever(     document_store=document_store,     embedding_model=\"multi-qa-MiniLM-L6-cos-v1\",     model_format=\"sentence_transformers\" ) ```  Then we run the `PineconeDocumentStore.update_embeddings` method with the `retriever` provided as an argument. GPU acceleration can greatly reduce the time required for this step.  ```python document_store.update_embeddings(     retriever,     batch_size=16 ) ```  ## Inspect Documents and Embeddings  We can get documents by their ID with the `PineconeDocumentStore.get_documents_by_id` method.  ```python d = document_store.get_documents_by_id(ids=['49091c797d2236e73fab510b1e9c7f6b'], return_embedding=True)[0] ```  From here we return can view document content with `d.content` and the document embedding with `d.embedding`.  ## Initializing an Extractive QA Pipeline  An `ExtractiveQAPipeline` contains three key components by default:  * a document store (`PineconeDocumentStore`) * a retriever model * a reader model  We use the `deepset/electra-base-squad2` model from the HuggingFace model hub as our reader model.   ```python from haystack.nodes import FARMReader  reader = FARMReader(     model_name_or_path='deepset/electra-base-squad2',      use_gpu=True ) ```  We are now ready to initialize the `ExtractiveQAPipeline`.   ```python from haystack.pipelines import ExtractiveQAPipeline  pipe = ExtractiveQAPipeline(reader, retriever) ```  ## Asking Questions  Using our QA pipeline we can begin querying with `pipe.run`.   ```python from haystack.utils import print_answers  query = \"What was Albert Einstein famous for?\" # get the answer answer = pipe.run(     query=query,     params={         \"Retriever\": {\"top_k\": 1},     } ) # print the answer(s) print_answers(answer) ```       Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.53 Batches/s]           Query: What was Albert Einstein famous for?     Answers:     [   <Answer {         'answer': 'his theories of special relativity and general relativity', 'type': 'extractive', 'score': 0.993550717830658,         'context': 'Albert Einstein is known for his theories of special relativity and general relativity. He also made important contributions to statistical mechanics,',         'offsets_in_document': [{'start': 29, 'end': 86}],         'offsets_in_context': [{'start': 29, 'end': 86}],          'document_id': '23357c05e3e46bacea556705de1ea6a5',         'meta': {             'context': 'Albert Einstein is known for his theories of special relativity and general relativity. He also made important contributions to statistical mechanics, especially his mathematical treatment of Brownian motion, his resolution of the paradox of specific heats, and his connection of fluctuations and dissipation. Despite his reservations about its interpretation, Einstein also made contributions to quantum mechanics and, indirectly, quantum field theory, primarily through his theoretical studies of the photon.', 'title': 'Modern_history'         }     }>]   ```python query = \"How much oil is Egypt producing in a day?\" # get the answer answer = pipe.run(     query=query,     params={         \"Retriever\": {\"top_k\": 1},     } ) # print the answer(s) print_answers(answer) ```      Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.81 Batches/s]           Query: How much oil is Egypt producing in a day?     Answers:     [   <Answer {         'answer': '691,000 bbl/d', 'type': 'extractive', 'score': 0.9999906420707703,         'context': 'Egypt was producing 691,000 bbl/d of oil and 2,141.05 Tcf of natural gas (in 2013), which makes Egypt as the largest oil producer not member of the Or',         'offsets_in_document': [{'start': 20, 'end': 33}],         'offsets_in_context': [{'start': 20, 'end': 33}],         'document_id': '57ed9720050a17237e323da5e3969a9b',         'meta': {             'context': 'Egypt was producing 691,000 bbl/d of oil and 2,141.05 Tcf of natural gas (in 2013), which makes Egypt as the largest oil producer not member of the Organization of the Petroleum Exporting Countries (OPEC) and the second-largest dry natural gas producer in Africa. In 2013, Egypt was the largest consumer of oil and natural gas in Africa, as more than 20% of total oil consumption and more than 40% of total dry natural gas consumption in Africa. Also, Egypt possesses the largest oil refinery capacity in Africa 726,000 bbl/d (in 2012). Egypt is currently planning to build its first nuclear power plant in El Dabaa city, northern Egypt.', 'title': 'Egypt'         }     }>]           ```python query = \"What are the first names of the youtube founders?\" # get the answer answer = pipe.run(     query=query,     params={         \"Retriever\": {\"top_k\": 1},     } ) # print the answer(s) print_answers(answer) ```       Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.83 Batches/s]           Query: What are the first names of the youtube founders?     Answers:     [   <Answer {         'answer': 'Hurley and Chen', 'type': 'extractive', 'score': 0.9998972713947296,         'context': 'According to a story that has often been repeated in the media, Hurley and Chen developed the idea for YouTube during the early months of 2005, after ',         'offsets_in_document': [{'start': 64, 'end': 79}],         'offsets_in_context': [{'start': 64, 'end': 79}],         'document_id': 'bd1cbd61ab617d840c5f295e21e80092',         'meta': {             'context': 'According to a story that has often been repeated in the media, Hurley and Chen developed the idea for YouTube during the early months of 2005, after they had experienced difficulty sharing videos that had been shot at a dinner party at Chen\\'s apartment in San Francisco. Karim did not attend the party and denied that it had occurred, but Chen commented that the idea that YouTube was founded after a dinner party \"was probably very strengthened by marketing ideas around creating a story that was very digestible\".', 'title': 'YouTube'         }     }>]    We can return multiple answers by setting the `top_k` parameter.   ```python query = \"Who was the first person to step foot on the moon?\" # get the answer answer = pipe.run(     query=query,     params={         \"Retriever\": {\"top_k\": 3},     } ) # print the answer(s) print_answers(answer) ```         Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.71 Batches/s]     Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.78 Batches/s]     Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.88 Batches/s]           Query: Who was the first person to step foot on the moon?     Answers:     [   <Answer {         'answer': 'Armstrong', 'type': 'extractive', 'score': 0.9998227059841156,          'context': 'The trip to the Moon took just over three days. After achieving orbit, Armstrong and Aldrin transferred into the Lunar Module, named Eagle, and after ',          'offsets_in_document': [{'start': 71, 'end': 80}],          'offsets_in_context': [{'start': 71, 'end': 80}],          'document_id': 'f74e1bf667e68d72e45437a7895df921',          'meta': {             'context': 'The trip to the Moon took just over three days. After achieving orbit, Armstrong and Aldrin transferred into the Lunar Module, named Eagle, and after a landing gear inspection by Collins remaining in the Command/Service Module Columbia, began their descent. After overcoming several computer overload alarms caused by an antenna switch left in the wrong position, and a slight downrange error, Armstrong took over manual flight control at about 180 meters (590 ft), and guided the Lunar Module to a safe landing spot at 20:18:04 UTC, July 20, 1969 (3:17:04 pm CDT). The first humans on the Moon would wait another six hours before they ventured out of their craft. At 02:56 UTC, July 21 (9:56 pm CDT July 20), Armstrong became the first human to set foot on the Moon.', 'title': 'Space_Race'             }         }>, <Answer {         'answer': 'Frank Borman', 'type': 'extractive', 'score': 0.7770257890224457,          'context': 'On December 21, 1968, Frank Borman, James Lovell, and William Anders became the first humans to ride the Saturn V rocket into space on Apollo 8. They ',          'offsets_in_document': [{'start': 22, 'end': 34}],          'offsets_in_context': [{'start': 22, 'end': 34}],          'document_id': '2bc046ba90d94fe201ccde9d20552200',          'meta': {             'context': \"On December 21, 1968, Frank Borman, James Lovell, and William Anders became the first humans to ride the Saturn V rocket into space on Apollo 8. They also became the first to leave low-Earth orbit and go to another celestial body, and entered lunar orbit on December 24. They made ten orbits in twenty hours, and transmitted one of the most watched TV broadcasts in history, with their Christmas Eve program from lunar orbit, that concluded with a reading from the biblical Book of Genesis. Two and a half hours after the broadcast, they fired their engine to perform the first trans-Earth injection to leave lunar orbit and return to the Earth. Apollo 8 safely landed in the Pacific ocean on December 27, in NASA's first dawn splashdown and recovery.\", 'title': 'Space_Race'             }         }>, <Answer {         'answer': 'Aldrin', 'type': 'extractive', 'score': 0.6680101901292801,          'context': ' were, \"That\\'s one small step for [a] man, one giant leap for mankind.\" Aldrin joined him on the surface almost 20 minutes later. Altogether, they spe',          'offsets_in_document': [{'start': 240, 'end': 246}],          'offsets_in_context': [{'start': 72, 'end': 78}],          'document_id': 'ae1c366b1eaf5fc9d32a8d81f76bd795',          'meta': {             'context': 'The first step was witnessed by at least one-fifth of the population of Earth, or about 723 million people. His first words when he stepped off the LM\\'s landing footpad were, \"That\\'s one small step for [a] man, one giant leap for mankind.\" Aldrin joined him on the surface almost 20 minutes later. Altogether, they spent just under two and one-quarter hours outside their craft. The next day, they performed the first launch from another celestial body, and rendezvoused back with Columbia.', 'title': 'Space_Race'             }         }>     ]     --- ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e23d"
  },
  "title": "Overview",
  "category": "630fc5235d91a70054705fb5",
  "content": "Pinecone makes it easy to build high-performance **vector search** applications. It’s a managed, cloud-native vector database with a **simple API** and no infrastructure hassles.  Key benefits of Pinecone:  * Fast: Ultra-low query latency at any scale, even with billions of items. * Fresh: Live index updates when you add, edit, or delete data. * Filtered: Combine vector search with metadata filters for more relevant, faster results. * Fully managed: Easy to start, use, and scale, while we keep things running smoothly and securely.   ## Key concepts  #### Vector search  Unlike traditional search methods that revolve around keywords, it is done by indexing and searching through ML-generated representations of data — vector embeddings — to find items most similar to the query.  #### Vector embeddings  [Vector embeddings](https://www.pinecone.io/learn/vector-embeddings/), or “vectors,” are sets of floating-point numbers that represent objects. They are generated by [embedding models](https://www.pinecone.io/learn/sentence-embeddings/) trained to capture the semantic similarity of objects in a given set.  You need to have vector embeddings to use Pinecone.  #### Vector database  A [vector database](https://www.pinecone.io/learn/vector-database/) indexes and stores vector embeddings for efficient management and fast retrieval. Unlike a standalone [vector index](https://www.pinecone.io/learn/vector-indexes/), a vector database like Pinecone provides additional capabilities such as index management, data management, metadata storage and filtering, and horizontal scaling.  ## Example use cases  Want to see more and start with working example notebooks? See: [Example Applications](/docs/examples)  * Semantic text search: Convert text data into vector embeddings using an [NLP](https://www.pinecone.io/learn/nlp/) transformer (eg, [a sentence embedding model](https://www.pinecone.io/learn/sentence-embeddings/)), then index and search through those vectors using Pinecone. * Question-answering: Index a set of questions as vectors and retrieve the most similar question’s answer for any new question. * Image similarity search: Transform image data into vector embeddings and build an index with Pinecone. Then convert query images into vectors and retrieve similar images. * Product recommendations: Generate product recommendations for ecommerce based on vectors representing users.   ## Overview of the workflow  ![workflow](https://raw.githubusercontent.com/pinecone-io/img/main/workflow.png)  Follow these guides to set up your index::  1. [Create an index](https://docs.pinecone.io/docs/manage-indexes) 2. [Connect to an index](https://docs.pinecone.io/docs/manage-data#connect) 3. [Insert the data](https://docs.pinecone.io/docs/insert-data) (and vectors) into the index  Once you have an index with data, follow these guides to start using your index:  - [Query the data](https://docs.pinecone.io/docs/query-data)   - [Filter the data](https://docs.pinecone.io/docs/metadata-filtering) - [Fetch data](https://docs.pinecone.io/docs/manage-data#fetching-an-item) - [Insert more data](https://docs.pinecone.io/docs/insert-data) or update existing vectors - [Manage the index](https://docs.pinecone.io/docs/manage-indexes) - [Manage data](https://docs.pinecone.io/docs/manage-data)  ## Pricing and deployment options  [Visit the pricing page](https://www.pinecone.io/pricing/) for pricing and deployment options.  Get started with Pinecone ---------------  [Go to the quickstart guide](https://docs.pinecone.io/docs/quickstart) to get a production-ready vector search service up and running in minutes. ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e23f"
  },
  "title": "Quickstart",
  "category": "630fc5235d91a70054705fb5",
  "content": " This guide explains how to set up a Pinecone client and get a production-ready similarity search service up and running in minutes.  ## 1. Install Pinecone (optional)  This step is optional. Do this step only if you want to use the Python client.  Use the following shell command to install Pinecone:  ```python pip install pinecone-client ```  For more information on how to install Pinecone clients, including troubleshooting, see [Installation](installation).  ## 2. Get and verify your Pinecone API key  To use Pinecone, you must have an API key. To find your API key, open the [Pinecone console](https://app.pinecone.io), click your project name, and click **API Keys**. This view also displays the environment for your project. Note both your API key and your environment.  To verify that your Pinecone API key works, use the following commands:  ```python import pinecone  pinecone.init(api_key=\"YOUR_API_KEY\", environment=\"YOUR_ENVIRONMENT\") ``` ```shell curl curl -i https://controller.YOUR_ENVIRONMENT.pinecone.io/actions/whoami -H 'Api-Key: YOUR_API_KEY' ``` .   If you don't receive an error message, then your API key is valid.  ## 3. Hello, Pinecone!  You can complete the remaining steps in three ways:  - Use the [\"Hello, Pinecone!\" colab notebook](https://colab.research.google.com/github/pinecone-io/examples/blob/master/quick_tour/hello_pinecone.ipynb) to write and execute Python in your browser. - Copy the commands below into your local installation of Python. - Use the cURL API commands below.  1\\. Initialize Pinecone  ```python import pinecone pinecone.init(api_key=\"YOUR_API_KEY\", environment=\"YOUR_ENVIRONMENT\") ``` ```shell curl # Not applicable ```  2\\. Create an index.  The commands below create an index named \"quickstart\" that performs approximate nearest-neighbor search using the [Euclidean distance metric](https://www.pinecone.io/docs/manage-indexes/#distance-metrics) for 8-dimensional vectors.  Index creation takes roughly a minute.  ```python pinecone.create_index(\"quickstart\", dimension=8, metric=\"euclidean\", pod_type=\"p1\") ``` ```shell curl curl -i -X POST \\   -H 'Content-Type: application/json' \\   -H 'Api-Key: YOUR_API_KEY_HERE' \\   https://controller.YOUR_ENVIRONMENT.pinecone.io/databases \\   -d '{     \"name\": \"quickstart\",     \"dimension\": 8,     \"metric\": \"euclidean\"   }' ```   > ⚠️  Warning > Indexes on the Starter (free) plan are deleted after 14 days of inactivity. To prevent this, send any API request or log into the console. This will count as activity.  3\\. Retrieve a list of your indexes.   Once your index is created, its name appears in the index list.   Use the following commands to return a list of your indexes.  ```python pinecone.list_indexes() # Returns: # ['quickstart'] ``` ```shell curl curl -i https://controller.YOUR_ENVIRONMENT.pinecone.io/databases \\   -H \"Api-Key: YOUR_API_KEY\" # Output: # [\"quickstart\"] ```  4\\. Connect to the index (Client only).  Before you can query your index using a client, you must connect to the index.  Use the following commands to connect to your index.  ```python index = pinecone.Index(\"quickstart\") ``` ```shell curl # Not applicable ```  5\\. Insert the data.  To ingest vectors into your index, use the [`upsert`](https://www.pinecone.io/docs/api/operation/upsert/) operation.   The upsert operation inserts a new vector in the index or updates the vector if a vector with the same ID is already present.  The following commands upsert 5 8-dimensional vectors into your index.  ```python # Upsert sample data (5 8-dimensional vectors) index.upsert([     (\"A\", [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]),     (\"B\", [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]),     (\"C\", [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]),     (\"D\", [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]),     (\"E\", [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]) ]) ``` ```shell curl curl -i -X POST https://quickstart-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/upsert \\   -H 'Api-Key: YOUR_API_KEY' \\   -H 'Content-Type: application/json' \\   -d '{     \"vectors\": [       {         \"id\": \"A\",         \"values\": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]       },       {         \"id\": \"B\",         \"values\": [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]       },       {         \"id\": \"C\",         \"values\": [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]       },       {         \"id\": \"D\",         \"values\": [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]       },       {         \"id\": \"E\",         \"values\": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]       }     ]   }' ```  The cURL command above uses the [endpoint](manage-data/#specify-an-index-endpoint) for your Pinecone index.   > ℹ️  Note > When upserting larger amounts of data, [upsert data in batches](insert-data/#batching-upserts) of 100 vectors or fewer over multiple upsert requests.   6\\. Get statistics about your index.  The following commands return statistics about the contents of your index.  ```python index.describe_index_stats() # Returns: # {'dimension': 8, 'index_fullness': 0.0, 'namespaces': {'': {'vector_count': 5}}} ``` ```shell curl curl -i https://quickstart-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/describe_index_stats \\   -H 'Api-Key: YOUR_API_KEY'  # Output: # { #   \"namespaces\": { #     \"\": { #       \"vectorCount\": 5 #     } #   }, #   \"dimension\": 8 # } ```  7\\. Query the index and get similar vectors.  The following example queries the index for the three (3) vectors that are most similar to an example 8-dimensional vector using the Euclidean distance metric specified in step 2 (\"Create an index.\") above.  ```python index.query(   vector=[0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],   top_k=3,   include_values=True ) # Returns: # {'matches': [{'id': 'C', #               'score': 0.0, #               'values': [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]}, #              {'id': 'D', #               'score': 0.0799999237, #               'values': [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]}, #              {'id': 'B', #               'score': 0.0800000429, #               'values': [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]}], #  'namespace': ''} ``` ```shell curl curl -i -X POST https://quickstart-YOUR_PROJECT.svc.YOUR_ENVIRONMENT.pinecone.io/query \\   -H 'Api-Key: YOUR_API_KEY' \\   -H 'Content-Type: application/json' \\   -d '{     \"vector\": [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],     \"topK\": 3,     \"includeValues\": true   }'  # Output: # { #       \"matches\":[ #         { #           \"id\": \"C\", #           \"score\": -1.76717265e-07, #           \"values\": [0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3] #         }, #         { #           \"id\": \"B\", #           \"score\": 0.080000028, #           \"values\": [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2] #         }, #         { #           \"id\": \"D\", #           \"score\": 0.0800001323, #           \"values\": [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4] #         } #       ], #       \"namespace\": \"\" #     } ```  8\\. Delete the index.  Once you no longer need the index, use the [`delete_index`](https://www.pinecone.io/docs/api/operation/delete_index/) operation to delete it.   The following commands delete the index.  ```python pinecone.delete_index(\"quickstart\") ``` ```shell curl curl -i -X DELETE https://controller.YOUR_ENVIRONMENT.pinecone.io/databases/quickstart \\   -H 'Api-Key: YOUR_API_KEY' ```  > ⚠️ Warning > After you delete an index, you cannot use it again.  ## Next steps  Now that you’re successfully making indexes with your API key, you can [start inserting data](insert-data) or view [more examples](examples). ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e241"
  },
  "title": "Choosing index type and size",
  "category": "630fc5235d91a70054705fb5",
  "content": "## Introduction  When planning your Pinecone deployment, it is important to understand the approximate storage requirements of your vectors to choose the appropriate pod type and number. This page will give guidance on sizing to help you plan accordingly.  As with all guidelines, these considerations are general and may not apply to your specific use case. We caution you to always test your deployment and ensure that the index configuration you are using is appropriate to your requirements.  [Collections](/docs/collections/) make it easy to create new versions of your index with different pod types and sizes, and we encourage you to take advantage of that feature to test different configurations. This guide is merely an overview of sizing considerations and should not be taken as a definitive guide.   Users on the Standard, Enterprise, and Enterprise Dedicated plans can [contact support](https://support.pinecone.io) for further help with sizing and testing.  # Overview  There are five main considerations when deciding how to configure your Pinecone index:  + Number of vectors + Dimensionality of your vectors + Size of metadata on each vector + QPS throughput + Cardinality of indexed metadata  Each of these considerations comes with requirements for index size, pod type, and replication strategy.   ## Number of vectors  The most important consideration in sizing is the [number of vectors](/docs/insert-data/) you plan on working with. As a rule of thumb, a single p1 pod can store approximately 1M vectors, while a s1 pod can store 5M vectors. However, this can be affected by other factors, such as dimensionality and metadata, which are explained below.   ## Dimensionality of vectors  The rules of thumb above for how many vectors can be stored in a given pod assumes a typical configuration of 768 [dimensions per vector](/docs/manage-indexes/#creating-an-index). As your individual use case will dictate the dimensionality of your vectors, the amount of space required to store them may necessarily be larger or smaller.   Each dimension on a single vector consumes 4 bytes of memory and storage per dimension, so if you expect to have 1M vectors with 768 dimensions each, that’s about 3GB of storage without factoring in metadata or other overhead. Using that reference, we can estimate the typical pod size and number needed for a given index. Table 1 below gives some examples of this.  **Table 1: Estimated number of pods per 1M vectors by dimensionality**  | Pod type | Dimensions | Estimated max vectors per shard |  | -------- | ---------- | ------------------------------- |   | p1       | 512        | 1,250,000                       |  |          | 768        | 1,000,000                       |  |          | 1024       | 675,000                         |  | p2       | 512        | 1,2500,000                      |  |          | 768        | 1,100,000                       |  |          | 1024       | 1,000,000                       |  | s1\t   | 512        | 8,000,000                       |  |          | 768        | 5,000,000                       |  |          | 1024       | 4,000,000                       |   Pinecone does not support fractional pod deployments, so always round up to the next nearest whole number when choosing your pods.   # Queries per second (QPS)  QPS speeds are governed by a combination of the [pod type](https://www.pinecone.io/docs/indexes/#pods-pod-types-and-pod-sizes) of the index, the number of [replicas](/docs/manage-indexes/#replicas), and the `top_k` value of queries. The pod type is the primary factor driving QPS, as the different pod types are optimized for different approaches.  The [p1 pods](/docs/indexes/#p1-pods) are performance-optimized pods which provide very low query latencies, but hold fewer vectors per pod than [s1 pods](/docs/indexes/#s1-pods). They are ideal for applications with low latency requirements (<100ms). The s1 pods are optimized for storage and provide large storage capacity and lower overall costs with slightly higher query latencies than p1 pods. They are ideal for very large indexes with moderate or relaxed latency requirements.   The [p2 pod type](/docs/indexes/#p2-pods) provides greater query throughput with lower latency. They support 200 QPS per replica and return queries in less than 10ms. This means that query throughput and latency are better than s1 and p1, especially for low dimension vectors (<512D).  As a rule, a single p1 pod with 1M vectors of 768 dimensions each and no replicas can handle about 20 QPS. It’s possible to get greater or lesser speeds, depending on the size of your metadata, number of vectors, the dimensionality of your vectors, and the `top_K` value for your search. See Table 2 below for more examples.  **Table 2: QPS by pod type and `top_k` value**\\*  | Pod type | `top_k` 10 | `top_k` 250 | `top_k` 1000 | | -------- | ---------- | ----------- | ------------ | | p1       | 30         | 25          | 20           | | p2       | 150        | 50          | 20           | | s1       | 10         | 10          | 10           |  \\*The QPS values in Table 2 represent baseline QPS with 1M vectors and 768 dimensions.  Adding replicas is the simplest way to [increase your QPS](/docs/performance-tuning/#how-to-increase-throughput). Each replica increases the throughput potential by roughly the same QPS, so aiming for 150 QPS using p1 pods means using the primary pod and 5 replicas. Using threading or multiprocessing in your application is also important, as issuing single queries sequentially still subjects you to delays from any underlying latency. The [Pinecone gRPC client](/docs/performance-tuning/#using-the-grpc-client-to-get-higher-upsert-speeds) can also be used to increase throughput of upserts.  ## Metadata cardinality and size  The last consideration when planning your indexes is the cardinality and size of your [metadata](/docs/insert-data/#inserting-vectors-with-metadata). While the increases are small when talking about a few million vectors, they can have a real impact as you grow to hundreds of millions or billions of vectors.   Indexes with very high cardinality, like those storing a unique user ID on each vector, can have significant memory requirements, resulting in fewer vectors fitting per pod. Also, if the size of the metadata per vector is larger, the index requires more storage. Limiting which metadata fields are indexed using [selective metadata indexing](/docs/manage-indexes/#selective-metadata-indexing) can help lower memory usage.  ## Pod sizes  You can also start with one of the larger [pod sizes](/docs/indexes/#pod-size-and-performance), like p1.x2. Each step up in pod size doubles the space available for your vectors. We recommend starting with x1 pods and scaling as you grow. This way, you don’t start with too large a pod size and have nowhere else to go up, meaning you have to migrate to a new index before you’re ready.  ## Example applications  The following examples will showcase how to use the sizing guidelines above to choose the appropriate type, size, and number of pods for your index.   ### Example 1: Semantic search of news articles  In our first example, we’ll use the [demo app for semantic search](semantic-text-search) from our documentation. In this case, we’re only working with 204,135 vectors. The vectors use 300 dimensions each, well under the general measure of 768 dimensions. Using the rule of thumb above of up to 1M vectors per p1 pod, we can run this app comfortably with a single p1.x1 pod.   ### Example 2: Facial recognition  For this example, suppose you’re building an application to identify customers using facial recognition for a secure banking app. Facial recognition can work with as few as 128 dimensions, but in this case, because the app will be used for access to finances, we want to make sure we’re certain that the person using it is the right one. We plan for 100M customers and use 2048 dimensions per vector.  We know from our rules of thumb above that 1M vectors with 768 dimensions fit nicely in a p1.x1 pod. We can just divide those numbers into the new targets to get the ratios we’ll need for our pod estimate:      100M / 1M = 100 base p1 pods     2048 / 768 = 2.667 vector ratio     2.667 * 100 = 267 rounding up  So we need 267 p1.x1 pods. We can reduce that by switching to s1 pods instead, sacrificing latency by increasing storage availability. They hold five times the storage of p1.x1, so the math is simple:      267 / 5 = 54 rounding up  So we estimate that we need 54 s1.x1 pods to store very high dimensional data for the face of each of the bank’s customers.  ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e243"
  },
  "title": "Installation",
  "category": "630fc5235d91a70054705fb5",
  "content": "## Installing Pinecone client libraries  Use the following shell command to install the [Pinecone Python client](https://github.com/pinecone-io/pinecone-python-client) for use with Python versions 3.6+:  ```python pip3 install -U pinecone-client ```  ## Installing Pinecone in a Jupyter notebook  Alternatively, you can install Pinecone in a Jupyter notebook:  ```python !pip3 install -U pinecone-client ```  We strongly recommend installing Pinecone in a virtual environment. For more information on using Python virtual environments, see:  * [PyPA Python Packaging User Guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/) * [Python Virtual Environments: A Primer](https://realpython.com/python-virtual-environments-a-primer/)   ## Installing the gRPC flavor of the standard client  Pinecone also comes with a gRPC flavor of the standard client. To install it, use the following command:  ```python pip3 install -U \"pinecone-client[grpc]\" ```  The gRPC client comes with more dependencies compared to the standard client. To get faster upsert speeds, we recommend you use the gRPC client with multi-pod indexes. See the [performance tuning](performance-tuning) section for more details. ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e245"
  },
  "title": "Extractive Question Answering",
  "category": "630fc5235d91a70054705fb7",
  "content": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/search/question-answering/extractive-question-answering.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/search/question-answering/extractive-question-answering.ipynb) [![Open Github](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/github-shield.svg)](https://github.com/pinecone-io/examples/tree/master/search/question-answering/extractive-question-answering.ipynb)  This notebook demonstrates how Pinecone helps you build an extractive question-answering application. To build an extractive question-answering system, we need three main components:  - A vector index to store and run semantic search - A retriever model for embedding context passages - A reader model to extract answers  We will use the SQuAD dataset, which consists of **questions** and **context** paragraphs containing question **answers**. We generate embeddings for the context passages using the retriever, index them in the vector database, and query with semantic search to retrieve the top k most relevant contexts containing potential answers to our question. We then use the reader model to extract the answers from the returned contexts.  # Install Dependencies  ```python !pip install -qU datasets pinecone-client sentence-transformers torch ```   # Load Dataset  Now let's load the SQuAD dataset from the HuggingFace Model Hub. We load the dataset into a pandas dataframe and filter the title and context columns, and we drop any duplicate context passages.  ```python from datasets import load_dataset  # load the squad dataset into a pandas dataframe df = load_dataset(\"squad\", split=\"train\").to_pandas() # select only title and context column df = df[[\"title\", \"context\"]] # drop rows containing duplicate context passages df = df.drop_duplicates(subset=\"context\") df ``` <div id=\"df-5935beba-26a2-40e4-bfca-a3dda53e101c\">   <div class=\"colab-df-container\">     <table border=\"1\" class=\"dataframe\">       <thead>         <tr style=\"text-align: right;\">           <th></th>           <th>title</th>           <th>context</th>         </tr>       </thead>       <tbody>         <tr>           <th>0</th>           <td>University_of_Notre_Dame</td>           <td>Architecturally, the school has a Catholic cha...</td>         </tr>         <tr>           <th>5</th>           <td>University_of_Notre_Dame</td>           <td>As at most other universities, Notre Dame's st...</td>         </tr>         <tr>           <th>10</th>           <td>University_of_Notre_Dame</td>           <td>The university is the major seat of the Congre...</td>         </tr>         <tr>           <th>15</th>           <td>University_of_Notre_Dame</td>           <td>The College of Engineering was established in ...</td>         </tr>         <tr>           <th>20</th>           <td>University_of_Notre_Dame</td>           <td>All of Notre Dame's undergraduate students are...</td>         </tr>         <tr>           <th>...</th>           <td>...</td>           <td>...</td>         </tr>         <tr>           <th>87574</th>           <td>Kathmandu</td>           <td>Institute of Medicine, the central college of ...</td>         </tr>         <tr>           <th>87579</th>           <td>Kathmandu</td>           <td>Football and Cricket are the most popular spor...</td>         </tr>         <tr>           <th>87584</th>           <td>Kathmandu</td>           <td>The total length of roads in Nepal is recorded...</td>         </tr>         <tr>           <th>87589</th>           <td>Kathmandu</td>           <td>The main international airport serving Kathman...</td>         </tr>         <tr>           <th>87594</th>           <td>Kathmandu</td>           <td>Kathmandu Metropolitan City (KMC), in order to...</td>         </tr>       </tbody>     </table>     <p>18891 rows × 2 columns</p>   </div> </div>  # Initialize Pinecone Index  The Pinecone index stores vector representations of our context passages which we can retrieve using another vector (query vector). We first need to initialize our connection to Pinecone to create our vector index. For this, we need a free [API key](https://app.pinecone.io/) and an environment value. You can find your environment value in the [Pinecone console](https://app.pinecone.io) under **API Keys**.,  We initialize the connection like so:  ```python import pinecone  # connect to pinecone environment pinecone.init(     api_key=\"YOUR_API_KEY\",     environment=\"YOUR_ENVIRONMENT\" ) ```  Now we create a new index called \"question-answering\" — we can name the index anything we want. We specify the metric type as \"cosine\" and dimension as 384 because the retriever we use to generate context embeddings is optimized for cosine similarity and outputs 384-dimension vectors.  ```python index_name = \"extractive-question-answering\"  # check if the extractive-question-answering index exists if index_name not in pinecone.list_indexes():     # create the index if it does not exist     pinecone.create_index(         index_name,         dimension=384,         metric=\"cosine\"     )  # connect to extractive-question-answering index we created index = pinecone.Index(index_name) ```  # Initialize Retriever  Next, we need to initialize our retriever. The retriever will mainly do two things:  - Generate embeddings for all context passages (context vectors/embeddings) - Generate embeddings for our questions (query vector/embedding)  The retriever will generate embeddings in a way that the questions and context passages containing answers to our questions are nearby in the vector space. We can use cosine similarity to calculate the similarity between the query and context embeddings to find the context passages that contain potential answers to our question.  We will use a SentenceTransformer model named ``multi-qa-MiniLM-L6-cos-v1`` designed for semantic search and trained on 215M (question, answer) pairs from diverse sources as our retriever.  ```python import torch from sentence_transformers import SentenceTransformer  # set device to GPU if available device = 'cuda' if torch.cuda.is_available() else 'cpu' # load the retriever model from huggingface model hub retriever = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1', device=device) retriever ```     SentenceTransformer(       (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel        (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})       (2): Normalize()     )  # Generate Embeddings and Upsert  Next, we need to generate embeddings for the context passages. We will do this in batches to help us more quickly generate embeddings and upload them to the Pinecone index. When passing the documents to Pinecone, we need an id (a unique value), context embedding, and metadata for each document representing context passages in the dataset. The metadata is a dictionary containing data relevant to our embeddings, such as the article title, context passage, etc.  ```python from tqdm.auto import tqdm  # we will use batches of 64 batch_size = 64  for i in tqdm(range(0, len(df), batch_size)):     # find end of batch     i_end = min(i+batch_size, len(df))     # extract batch     batch = df.iloc[i:i_end]     # generate embeddings for batch     emb = retriever.encode(batch[\"context\"].tolist()).tolist()     # get metadata     meta = batch.to_dict(orient=\"records\")     # create unique IDs     ids = [f\"{idx}\" for idx in range(i, i_end)]     # add all to upsert list     to_upsert = list(zip(ids, emb, meta))     # upsert/insert these records to pinecone     _ = index.upsert(vectors=to_upsert)  # check that we have all vectors in index index.describe_index_stats() ```     100%|██████████| 296/296 [02:57<00:00, 1.99it/s]      {'dimension': 384,      'index_fullness': 0.0,      'namespaces': {'': {'vector_count': 18891}},      'total_vector_count': 18891}  # Initialize Reader  We use the `deepset/electra-base-squad2` model from the HuggingFace model hub as our reader model. We load this model into a \"question-answering\" pipeline from HuggingFace transformers and feed it our questions and context passages individually. The model gives a prediction for each context we pass through the pipeline.  ```python from transformers import pipeline  model_name = \"deepset/electra-base-squad2\" # load the reader model into a question-answering pipeline reader = pipeline(tokenizer=model_name, model=model_name, task=\"question-answering\", device=device) ```  Now all the components we need are ready. Let's write some helper functions to execute our queries. The `get_context` function retrieves the context embeddings containing answers to our question from the Pinecone index, and the `extract_answer` function extracts the answers from these context passages.  ```python # gets context passages from the pinecone index def get_context(question, top_k):     # generate embeddings for the question     xq = retriever.encode([question]).tolist()     # search pinecone index for context passage with the answer     xc = index.query(xq, top_k=top_k, include_metadata=True)     # extract the context passage from pinecone search result     c = [x[\"metadata\"][\"context\"] for x in xc[\"matches\"]]     return c  question = \"How much oil is Egypt producing in a day?\" context = get_context(question, top_k = 1) context ```     ['Egypt was producing 691,000 bbl/d of oil and 2,141.05 Tcf of natural gas (in 2013), which makes Egypt as the largest oil producer not member of the Organization of the Petroleum Exporting Countries (OPEC) and the second-largest dry natural gas producer in Africa. In 2013, Egypt was the largest consumer of oil and natural gas in Africa, as more than 20% of total oil consumption and more than 40% of total dry natural gas consumption in Africa. Also, Egypt possesses the largest oil refinery capacity in Africa 726,000 bbl/d (in 2012). Egypt is currently planning to build its first nuclear power plant in El Dabaa city, northern Egypt.']   As we can see, the retiever is working and returns the context passage that contains the answer to our question. Now let's use the reader to extract the exact answer from the context passage.  ```python from pprint import pprint  # extracts answer from the context passage def extract_answer(question, context):     results = []     for c in context:         # feed the reader the question and contexts to extract answers         answer = reader(question=question, context=c)         # add the context to answer dict for printing both together         answer[\"context\"] = c         results.append(answer)     # sort the result based on the score from reader model     sorted_result = pprint(sorted(results, key=lambda x: x[\"score\"], reverse=True))     return sorted_result  extract_answer(question, context) ```     [{'answer': '691,000 bbl/d',       'context': 'Egypt was producing 691,000 bbl/d of oil and 2,141.05 Tcf of '                  'natural gas (in 2013), which makes Egypt as the largest oil '                  'producer not member of the Organization of the Petroleum '                  'Exporting Countries (OPEC) and the second-largest dry natural '                  'gas producer in Africa. In 2013, Egypt was the largest consumer '                  'of oil and natural gas in Africa, as more than 20% of total oil '                  'consumption and more than 40% of total dry natural gas '                  'consumption in Africa. Also, Egypt possesses the largest oil '                  'refinery capacity in Africa 726,000 bbl/d (in 2012). Egypt is '                  'currently planning to build its first nuclear power plant in El '                  'Dabaa city, northern Egypt.',       'end': 33,       'score': 0.9999852180480957,       'start': 20}]      The reader model predicted with 99% accuracy the correct answer *691,000 bbl/d* as seen from the context passage. Let's run few more queries.  ```python question = \"What are the first names of the men that invented youtube?\" context = get_context(question, top_k=1) extract_answer(question, context) ```     [{'answer': 'Hurley and Chen',       'context': 'According to a story that has often been repeated in the media, '                  'Hurley and Chen developed the idea for YouTube during the early '                  'months of 2005, after they had experienced difficulty sharing '                  \"videos that had been shot at a dinner party at Chen's apartment \"                  'in San Francisco. Karim did not attend the party and denied that '                  'it had occurred, but Chen commented that the idea that YouTube '                  'was founded after a dinner party \"was probably very strengthened '                  'by marketing ideas around creating a story that was very '                  'digestible\".',       'end': 79,       'score': 0.9999276399612427,       'start': 64}]      ```python question = \"What is Albert Eistein famous for?\" context = get_context(question, top_k=1) extract_answer(question, context) ```      [{'answer': 'his theories of special relativity and general relativity',       'context': 'Albert Einstein is known for his theories of special relativity '                  'and general relativity. He also made important contributions to '                  'statistical mechanics, especially his mathematical treatment of '                  'Brownian motion, his resolution of the paradox of specific '                  'heats, and his connection of fluctuations and dissipation. '                  'Despite his reservations about its interpretation, Einstein also '                  'made contributions to quantum mechanics and, indirectly, quantum '                  'field theory, primarily through his theoretical studies of the '                  'photon.',       'end': 86,       'score': 0.9500371217727661,       'start': 29}]      Let's run another question. This time for top 3 context passages from the retriever.  ```python question = \"Who was the first person to step foot on the moon?\" context = get_context(question, top_k=3) extract_answer(question, context) ```      [{'answer': 'Armstrong',       'context': 'The trip to the Moon took just over three days. After achieving '                  'orbit, Armstrong and Aldrin transferred into the Lunar Module, '                  'named Eagle, and after a landing gear inspection by Collins '                  'remaining in the Command/Service Module Columbia, began their '                  'descent. After overcoming several computer overload alarms '                  'caused by an antenna switch left in the wrong position, and a '                  'slight downrange error, Armstrong took over manual flight '                  'control at about 180 meters (590 ft), and guided the Lunar '                  'Module to a safe landing spot at 20:18:04 UTC, July 20, 1969 '                  '(3:17:04 pm CDT). The first humans on the Moon would wait '                  'another six hours before they ventured out of their craft. At '                  '02:56 UTC, July 21 (9:56 pm CDT July 20), Armstrong became the '                  'first human to set foot on the Moon.',       'end': 80,       'score': 0.9998037815093994,       'start': 71},      {'answer': 'Aldrin',       'context': 'The first step was witnessed by at least one-fifth of the '                  'population of Earth, or about 723 million people. His first '                  \"words when he stepped off the LM's landing footpad were, \"                  '\"That\\'s one small step for [a] man, one giant leap for '                  'mankind.\" Aldrin joined him on the surface almost 20 minutes '                  'later. Altogether, they spent just under two and one-quarter '                  'hours outside their craft. The next day, they performed the '                  'first launch from another celestial body, and rendezvoused back '                  'with Columbia.',       'end': 246,       'score': 0.6958656907081604,       'start': 240},      {'answer': 'Frank Borman',       'context': 'On December 21, 1968, Frank Borman, James Lovell, and William '                  'Anders became the first humans to ride the Saturn V rocket into '                  'space on Apollo 8. They also became the first to leave low-Earth '                  'orbit and go to another celestial body, and entered lunar orbit '                  'on December 24. They made ten orbits in twenty hours, and '                  'transmitted one of the most watched TV broadcasts in history, '                  'with their Christmas Eve program from lunar orbit, that '                  'concluded with a reading from the biblical Book of Genesis. Two '                  'and a half hours after the broadcast, they fired their engine to '                  'perform the first trans-Earth injection to leave lunar orbit and '                  'return to the Earth. Apollo 8 safely landed in the Pacific ocean '                  \"on December 27, in NASA's first dawn splashdown and recovery.\",       'end': 34,       'score': 0.49247056245803833,       'start': 22}]      We return the correct answer first, followed by relevant answers on similar parallel topics. We have recieved great results.  # Example Application   To try out an application like this one, see this [example application](https://huggingface.co/spaces/pinecone/extractive-question-answering). ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e247"
  },
  "title": "Metadata Filtered Search",
  "category": "630fc5235d91a70054705fb7",
  "content": " [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/search/metadata-filtered-search/metadata-filtered-search.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/search/metadata-filtered-search/metadata-filtered-search.ipynb) [![Open github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/pinecone-io/examples/blob/master/search/metadata-filtered-search/metadata-filtered-search.ipynb)  Pinecone offers a production-ready vector database for high performance and reliable *semantic search* at scale. But did you know Pinecone's semantic search can be paired with the more traditional keyword search?  [Semantic search](https://www.pinecone.io/learn/semantic-search/) is a compelling technology allowing us to search using abstract concepts and *meaning* rather than relying on specific words. However, sometimes a simple keyword search can be just as valuable — especially if we know the exact wording of what we're searching for.  <video autoplay loop muted playsinline style=\"width:100%;\">   <source src=\"https://www.pinecone.io/images/basic-hybrid-search.mp4\" type=\"video/mp4\"> </video> <small>Hybrid search allows us to use Pinecone’s <a href=\"https://www.pinecone.io/learn/vector-search-filtering/\">single-stage filtering</a> to restrict the search scope using specific keywords, then continue with a semantic search.</small>  Pinecone allows you to pair semantic search with a basic keyword filter. If you know that the document you're looking for contains a specific word or set of words, you simply tell Pinecone to restrict the search to *only include* documents with those keywords.  We even support functionality for keyword search using sets of words with *AND, OR, NOT* logic.  In this article, we will explore these features through a start-to-finish example of basic keyword search in Pinecone.  <p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/iCkftKsnQgg\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>  ## Data Preparation and Upsert  The first thing we need to do is create some data. We will keep things simple with *10* sentences.  ```python all_sentences = [     \"purple is the best city in the forest\",     \"No way chimps go bananas for snacks!\",     \"it is not often you find soggy bananas on the street\",     \"green should have smelled more tranquil but somehow it just tasted rotten\",     \"joyce enjoyed eating pancakes with ketchup\",     \"throwing bananas on to the street is not art\",     \"as the asteroid hurtled toward earth becky was upset her dentist appointment had been canceled\",     \"I'm getting way too old. I don't even buy green bananas anymore.\",     \"to get your way you must not bombard the road with yellow fruit\",     \"Time flies like an arrow; fruit flies like a banana\" ] ```  On the *semantic* side of our search, we will introduce a new *query sentence* and search for the most semantically similar. To do this, we will need to create some sentence embeddings using our sentences. We will use a pretrained model from `sentence-transformers` for this.  ```python from sentence_transformers import SentenceTransformer  model = SentenceTransformer('flax-sentence-embeddings/all_datasets_v3_mpnet-base')  all_embeddings = model.encode(all_sentences) all_embeddings.shape ```  ``` (10, 768) ```  We now have *10* sentence embeddings, each with a dimensionality of *768*. If we just wanted semantic search, we could move onto *upserting* the data — but there is **one more step for keyword search**.  Keyword search requires *keywords*, so we make a list of words (or *'tokens'*) for each sentence. To do this we can use a *word-level* tokenizer from Hugging Face’s `transformers`.  ```python from transformers import AutoTokenizer  # transfo-xl tokenizer uses word-level encodings tokenizer = AutoTokenizer.from_pretrained('transfo-xl-wt103')  all_tokens = [tokenizer.tokenize(sentence.lower()) for sentence in all_sentences] all_tokens[0] ```  ``` ['purple', 'is', 'the', 'best', 'city', 'in', 'the', 'forest'] ```  We have all the data we need for our semantic and keyword search, so we can move on to initializing a connection to our Pinecone instance. All we need here is an [API key](https://app.pinecone.io), and then we can create a new index called `keyword-search` (you can name it anything you like). You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**.  ```python import pinecone  # connect to pinecone environment pinecone.init(     api_key=\"YOUR_API_KEY\",     environment=\"YOUR_ENVIRONMENT\" )  pinecone.list_indexes()  # check if keyword-search index already exists  pinecone.create_index(name='keyword-search', dimension=all_embeddings.shape[1]) index = pinecone.Index('keyword-search') ```  All we do now is `upsert` our data — which we reformat into a list of tuples where each tuple is structured as `(id, values, metadata)`.  ```python upserts = [(v['id'], v['values'], v['metadata']) for v in data] # then we upsert index.upsert(vectors=upserts) ```  ``` {'upsertedCount': 10.0} ```  It’s also possible to upsert data to Pinecone using cURL. For this we reformat our data and save it as a JSON file.  ```python import json  # reformat the data upserts = {'vectors': []} for i, (embedding, tokens) in enumerate(zip(all_embeddings, all_tokens)):     vector = {'id':f'{i}',               'values': embedding.tolist(),               'metadata':{'tokens':tokens}}     upserts['vectors'].append(vector)  # save to JSON with open('./upsert.json', 'w') as f:     json.dump(upserts, f, indent=4) ```  Here we’ve build a JSON file containing a list of *10* records within the `vectors` key. Each record contains ID, embeddings, and metadata in the format:  ```json {     \"id\": \"i\",     \"values\": [0.001, 0.001, ...],     \"metadata\": {         \"tokens\": [\"purple\", \"is\", ...]     } } ```  To upsert with curl, we first need the index URL — which can be found in your [Pinecone dashboard](https://app.pinecone.io), it should look something like:  ``` https://keyword-search-1234.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/upsert ```  With that, we upsert:  ```bash !curl -X POST \\     https://keyword-search-1234.svc.YOUR_ENVIRONMENT.pinecone.io/vectors/upsert \\     -H ‘Content-Type: application/json’ \\     -H ‘Api-Key: <YOUR-API-KEY>’ \\     -d @./upsert.json ```  Now that we've upserted the data to our index, we can move on to semantic and keyword search.  ## Semantic and Keyword Search  We'll start with a semantic search *without* keywords. As we did with our indexed sentences, we need to `encode` a *query sentence*.  ```python query_sentence = \"there is an art to getting your way and throwing bananas on to the street is not it\" xq = model.encode(query_sentence).tolist() ```  We then find the most semantically similar sentences to this query vector `xq` with `query` — we will return all *ten* sentences by setting `top_k=10`.  ```python result = index.query(xq, top_k=10, includeMetadata=True) result ```  ``` {'matches': [{'id': '5',               'metadata': {'tokens': ['throwing',                                       'bananas',                                       'on',                                       'to',                                       'the',                                       'street',                                       'is',                                       'not',                                       'art']},               'score': 0.732851923,               'values': []},              {'id': '8',               'metadata': {'tokens': ['to',                                       'get',                                       'your',                                       'way',                                       'you',                                       'must',                                       'not',                                       'bombard',                                       'the',                                       'road',                                       'with',                                       'yellow',                                       'fruit']},               'score': 0.574427,               'values': []}],  'namespace': ''} ```  ```python [x['id'] for x in result['matches']] ```  ``` ['5', '8', '2', '1', '9', '7', '0', '3', '4', '6'] ```  The response shows both the most similar sentence IDs and their respective metadata field, which contains the list of *tokens* we created earlier.  We perform a keyword search by filtering records with the *tokens* metadata field. If we wanted to only return records that contain the token `'bananas'` we can like so:  ```python result = index.query(xq, top_k=10, filter={'tokens': 'bananas'}) ids = [x['id'] for x in result['matches']] ids ```  ``` ['5', '2', '1', '7'] ```  Immediately we can see that we return far fewer sentences. This is because there are only *four* records that contain the word *'bananas'*. We can use those `ids` to see which sentences we've returned.  ```python for i in ids:     print(all_sentences[i]) ```  ``` throwing bananas on to the street is not art it is not often you find soggy bananas on the street No way chimps go bananas for snacks! I'm getting way too old. I don't even buy green bananas anymore. ```  Looks great! We can extend the keyword search filter to include multiple words — specifying whether we'd like to return results that contain *all* words using `$and`, or *any* word using `$or`/`$in`.  If we wanted to return records that contain either *'bananas'* **or** *'way'* with [metadata filtering](https://www.pinecone.io/docs/metadata-filtering/):   ```{'$or': [{'tokens': 'bananas'}, {'tokens': 'way'}]}```  This filter will return *any* records that satisfy one or more of these conditions — where the *tokens* list contains *’bananas’* **or** the *tokens* list contains *’way’*.  ```python result = index.query(xq, top_k=10, filter={'$or': [                          {'tokens': 'bananas'},                          {'tokens': 'way'}                      ]})  ids = [int(x['id']) for x in result['matches']] for i in ids:     print(all_sentences[i]) ```  Alternatively, we can write these multi-keyword `$or` queries using the `$in` condition. This modifier tells Pinecone to filter for records where the *tokens* list contains *any* word from the list we define.  ```python result = index.query(xq, top_k=10, filter={     'tokens': {'$in': ['bananas', 'way']} })  ids = [int(x['id']) for x in result['matches']] for i in ids:     print(all_sentences[i]) ```  ``` throwing bananas on to the street is not art to get your way you must not bombard the road with yellow fruit it is not often you find soggy bananas on the street No way chimps go bananas for snacks! I'm getting way too old. I don't even buy green bananas anymore. ```  Both `$or` and `$in` produce the same logic above. What if we wanted records that contain *both* *'bananas'* **and** *'way'*? All we do is swap `$or` for `$and`.  ```python result = index.query(xq, top_k=10, filter={'$and': [                          {'tokens': 'bananas'},                          {'tokens': 'way'}                      ]})  ids = [int(x['id']) for x in result['matches']] for i in ids:     print(all_sentences[i]) ```  ``` No way chimps go bananas for snacks! I'm getting way too old. I don't even buy green bananas anymore. ```  If we have *a lot* of keywords, including *every single one* with the `$and` condition manually would not be fun, so we write something like this instead:  ```python keywords = ['bananas', 'way', 'green'] filter_dict = [{'tokens': word} for word in keywords] filter_dict ```  ``` [{'tokens': 'bananas'}, {'tokens': 'way'}, {'tokens': 'green'}] ```  ```python result = index.query(xq, top_k=10, filter={'$and': filter_dict})  ids = [int(x['id']) for x in result['matches']] for i in ids:     print(all_sentences[i]) ```  ``` I'm getting way too old. I don't even buy green bananas anymore. ```  And now we're restricting our semantic search to records that contain *any* word from *'bananas'*, *'way'*, or *'green'*.  If we like we can add negation to our logic too. For example we may want all sentences that *do not* contain *’bananas’* but *do* contain *’way’*. To do this we add **not equals** `$ne` to the *’bananas’* condition.  ```python result = index.query(xq, top_k=10, filter={'$and': [                          {'tokens': {'$ne': 'bananas'}},                          {'tokens': 'way'}                      ]})  ids = [int(x['id']) for x in result['matches']] for i in ids:     print(all_sentences[i]) ```  ``` to get your way you must not bombard the road with yellow fruit ```  Or if we want to *not* return sentences that contain any of several words, we use the **not in** `$nin` modifier.  ```python result = index.query(xq, top_k=10, filter={'tokens':     {'$nin': ['bananas', 'way']} })  ids = [int(x['id']) for x in result['matches']] for i in ids:     print(all_sentences[i]) ```  ``` Time flies like an arrow; fruit flies like a banana purple is the best city in the forest green should have smelled more tranquil but somehow it just tasted rotten joyce enjoyed eating pancakes with ketchup as the asteroid hurtled toward earth becky was upset her dentist appointment had been canceled ```  That's it for this introduction to keyword search in Pinecone. We've set up and upserted our sentence embeddings for semantic search and a token list for keyword search. Then we explored how we restrict our search to records containing a specific keyword, or even *set of keywords* using the two `$and` / `$or` modifiers. ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e249"
  },
  "title": "Sentiment Analysis",
  "category": "630fc5235d91a70054705fb7",
  "content": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/analytics-and-ml/data-mining/sentiment-analysis/sentiment-analysis.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/analytics-and-ml/data-mining/sentiment-analysis/sentiment-analysis.ipynb) [![Open Github](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/github-shield.svg)](https://github.com/pinecone-io/examples/tree/master/analytics-and-ml/data-mining/sentiment-analysis/sentiment-analysis.ipynb)  Sentiment analysis, often known as opinion mining, is a technique used in natural language processing (NLP) to determine the emotional undertone of a text. This is a common method used by organizations to identify and group opinions about their product, service, and ideas. In this notebook, we will apply this technique to the hotel industry and understand customer perception and potential areas that need improvement. To do this, we will:  1.\tGenerate Sentiment labels and scores based on customer reviews. 2.\tStore them in a Pinecone index as metadata (alongside respective text vectors). 3.\tQuery Pinecone index on selected areas and understand customer opinions.  Let's get started.   # Install Dependencies   ```python !pip install sentence_transformers pinecone-client datasets seaborn matplotlib ```  # Load and Prepare Dataset  We use a dataset containing ~90k hotel reviews provided by customers. This dataset can be loaded from the HuggingFace dataset hub as follows:   ```python from datasets import load_dataset  # load the dataset and convert to pandas dataframe df = load_dataset(     \"ashraq/hotel-reviews\",     split=\"train\" ).to_pandas() ```   ```python # keep only the first 800 characters from the reviews df[\"review\"] = df[\"review\"].str[:800] # glimpse the dataset df.head() ```   <div id=\"df-e185102e-db37-41cb-8524-1691d0ac13a3\">   <div class=\"colab-df-container\">     <div>       <style scoped>           .dataframe tbody tr th:only-of-type {               vertical-align: middle;           }           .dataframe tbody tr th {               vertical-align: top;           }           .dataframe thead th {               text-align: right;           }       </style>       <table border=\"1\" class=\"dataframe\">         <thead>           <tr style=\"text-align: right;\">             <th></th>             <th>review_date</th>             <th>hotel_name</th>             <th>review</th>           </tr>         </thead>         <tbody>           <tr>             <th>0</th>             <td>8/3/2017</td>             <td>Park Plaza County Hall London</td>             <td>Extra bed was the worst breakfast queue was r...</td>           </tr>           <tr>             <th>1</th>             <td>8/3/2017</td>             <td>Park Plaza County Hall London</td>             <td>Just the location and view</td>           </tr>           <tr>             <th>2</th>             <td>8/3/2017</td>             <td>Park Plaza County Hall London</td>             <td>Around the corner from the London eye and use...</td>           </tr>           <tr>             <th>3</th>             <td>8/2/2017</td>             <td>Park Plaza County Hall London</td>             <td>I wish you had wheat free snacks</td>           </tr>           <tr>             <th>4</th>             <td>8/2/2017</td>             <td>Park Plaza County Hall London</td>             <td>You re always my hotel of choice You re staff...</td>           </tr>         </tbody>       </table>     </div>     <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e185102e-db37-41cb-8524-1691d0ac13a3')\" title=\"Convert this dataframe to an interactive table.\" style=\"display:none;\">       <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg>     </button>     <style>       .colab-df-container {         display:flex;         flex-wrap:wrap;         gap: 12px;       }       .colab-df-convert {         background-color: #E8F0FE;         border: none;         border-radius: 50%;         cursor: pointer;         display: none;         fill: #1967D2;         height: 32px;         padding: 0 0 0 0;         width: 32px;       }       .colab-df-convert:hover {         background-color: #E2EBFA;         box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);         fill: #174EA6;       }       [theme=dark] .colab-df-convert {         background-color: #3B4455;         fill: #D2E3FC;       }       [theme=dark] .colab-df-convert:hover {         background-color: #434B5C;         box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);         filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));         fill: #FFFFFF;       }     </style>     <script>       const buttonEl = document.querySelector('#df-e185102e-db37-41cb-8524-1691d0ac13a3 button.colab-df-convert');       buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none';       async function convertToInteractive(key) {         const element = document.querySelector('#df-e185102e-db37-41cb-8524-1691d0ac13a3');         const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {});         if (!dataTable) return;         const docLinkHtml = 'Like what you see? Visit the ' +           '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'           + ' to learn more about interactive tables.';         element.innerHTML = '';         dataTable['output_type'] = 'display_data';         await google.colab.output.renderOutput(dataTable, element);         const docLink = document.createElement('div');         docLink.innerHTML = docLinkHtml;         element.appendChild(docLink);       }     </script>   </div> </div>     # Initialize Sentiment Analysis Model  We will use a RoBERTa model fine-tuned for sentiment analysis to analyze the hotel reviews. The model can be loaded from the HuggingFace model hub as follows:   ```python import torch  # set device to GPU if available device = torch.cuda.current_device() if torch.cuda.is_available() else None ```   ```python from transformers import (     pipeline,     AutoTokenizer,     AutoModelForSequenceClassification     )  model_id = \"cardiffnlp/twitter-roberta-base-sentiment\"  # load the model from huggingface model = AutoModelForSequenceClassification.from_pretrained(     model_id,     num_labels=3 )  # load the tokenizer from huggingface tokenizer = AutoTokenizer.from_pretrained(model_id)  # load the tokenizer and model into a sentiment analysis pipeline nlp = pipeline(     \"sentiment-analysis\",     model=model,     tokenizer=tokenizer,     device=device     ) ```   The sentiment analysis model returns `LABEL_0` for negative, `LABEL_1` for neutral and `LABEL_2` for positive labels. We can add them to a dictionary to easily access them when showing the results.   ```python labels = {     \"LABEL_0\": \"negative\",     \"LABEL_1\": \"neutral\",     \"LABEL_2\": \"positive\" } ```   ```python # view review number 241 test_review = df[\"review\"][241] test_review ```         ' Room was small for a superior room and poorly lit especially as it was an inside room and overlooked the inside wall of the hotel No view therefore needed better lighting within Restaurant tables were not well laid and had to go searching for cutlery at breakfast '     ```python # get the sentiment label and score for review number 241 nlp(test_review) ```         [{'label': 'LABEL_0', 'score': 0.7736575603485107}]    Our pipeline is working as expected and accurately predicts the correct label for the review.  # Initialize Retriever  A retriever model is used to embed passages and queries, and it creates embeddings such that queries and passages with similar meanings are close in the vector space. We will use a sentence-transformer model as our retriever. The model can be loaded as follows:   ```python from sentence_transformers import SentenceTransformer  # load the model from huggingface retriever = SentenceTransformer(     'sentence-transformers/all-MiniLM-L6-v2',     device=device ) retriever ```  # Initialize Pinecone Index  Now we need to initialize our Pinecone index. The Pinecone index stores vector representations of our passages which we can retrieve using another vector (the query vector). We first need to initialize our connection to Pinecone. For this, we need a free [API key](https://app.pinecone.io/). You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**. We initialize the connection like so:   ```python import pinecone  # connect to pinecone environment pinecone.init(     api_key=\"<<YOUR_API_KEY>>\",     environment=\"YOUR_ENVIRONMENT\" ) ```  Now we can create our vector index. We will name it `sentiment-mining` (feel free to choose any name you prefer). We specify the metric type as `cosine` and dimension as `384` as these are the vector space and dimensionality of the vectors generated by the retriever model.   ```python index_name = \"sentiment-mining\"  # check if the sentiment-mining index exists if index_name not in pinecone.list_indexes():     # create the index if it does not exist     pinecone.create_index(         index_name,         dimension=384,         metric=\"cosine\"     )  # connect to sentiment-mining index we created index = pinecone.Index(index_name) ```  # Generate Embeddings and Upsert  We generate embeddings for all the reviews in the dataset. Alongside the embeddings, we also include the sentiment label and score in the Pinecone index as metadata. Later we will use this data to understand customer opinions.  Let's first write a helper function to generate sentiment labels and scores for a batch of reviews.   ```python def get_sentiment(reviews):     # pass the reviews through sentiment analysis pipeline     sentiments = nlp(reviews)     # extract only the label and score from the result     l = [labels[x[\"label\"]] for x in sentiments]     s = [x[\"score\"] for x in sentiments]     return l, s ```   ```python # get sentiment labels for few reviews get_sentiment(df[\"review\"][:3].tolist()) ```         (['negative', 'neutral', 'positive'],      [0.906525194644928, 0.7716173529624939, 0.8975034952163696])    We need to convert the review dates to timestamps to filter query results for a given period. This is helpful if you want to understand customer sentiment over a specific period. Let's write another helper function to convert dates to timestamps.   ```python import dateutil.parser  # convert date to timestamp def get_timestamp(dates):     timestamps = [dateutil.parser.parse(d).timestamp() for d in dates]     return timestamps ```   ```python get_timestamp([df[\"review_date\"][0]])[0] ```         1501718400.0    Now we create the embeddings. We do this in batches of `64` to avoid overwhelming machine resources or API request limits.   ```python from tqdm.auto import tqdm  # we will use batches of 64 batch_size = 64  for i in tqdm(range(0, len(df), batch_size)):     # find end of batch     i_end = min(i+batch_size, len(df))     # extract batch     batch = df.iloc[i:i_end]     # generate embeddings for batch     emb = retriever.encode(batch[\"review\"].tolist()).tolist()     # convert review_date to timestamp to enable period filters     timestamp = get_timestamp(batch[\"review_date\"].tolist())     batch[\"timestamp\"] = timestamp     # get sentiment label and score for reviews in the batch     label, score = get_sentiment(batch[\"review\"].tolist())     batch[\"label\"] = label     batch[\"score\"] = score     # get metadata     meta = batch.to_dict(orient=\"records\")     # create unique IDs     ids = [f\"{idx}\" for idx in range(i, i_end)]     # add all to upsert list     to_upsert = list(zip(ids, emb, meta))     # upsert/insert these records to pinecone     _ = index.upsert(vectors=to_upsert)   # check that we have all vectors in index index.describe_index_stats() ```  We have successfully indexed all customer reviews and relevant metadata. We can move on to opinion mining.  # Opinion Mining  Now that we have all the customer reviews indexed, we will search for a few areas that customers usually consider when staying at a hotel and analyze the general opinion of the customers. Pinecone vector database makes it very flexible to do this as we can easily search for any topic and get customer reviews relevant to the search query along with sentiment labels as metadata.  We will start with a general question about the room sizes of hotels in London and return the top 500 reviews to analyze the overall customer sentiment.   ```python query = \"are the customers satisfied with the room sizes in London hotels?\" # generate dense vector embeddings for the query xq = retriever.encode(query).tolist() # query pinecone result = index.query(xq, top_k=500, include_metadata=True) ```  Let's take a look at a few reviews from the search results.   ```python result[\"matches\"][:2] ```         [{'id': '57268',       'metadata': {'hotel_name': 'Millennium Gloucester Hotel London',                    'label': 'neutral',                    'review': ' The size of the room compared to other london hotels '                              'And the location and friednly staff ',                    'review_date': datetime.date(2015, 10, 9),                    'score': 0.7984868884086609,                    'timestamp': 1444348800.0},       'score': 0.819122493,       'sparseValues': {},       'values': []}, {'id': '36931',       'metadata': {'hotel_name': 'DoubleTree by Hilton London Docklands Riverside',                    'label': 'positive',                    'review': ' Rooms great but on the small size but typical for a '                              'London hotel',                    'review_date': datetime.date(2015, 11, 8),                    'score': 0.8672299981117249,                    'timestamp': 1446940800.0},       'score': 0.816708684,       'sparseValues': {},       'values': []}]     ```python result[\"matches\"][-2:] ```         [{'id': '60964',       'metadata': {'hotel_name': 'St James Court A Taj Hotel London',                    'label': 'positive',                    'review': ' The location is perfect and I got a late checkout as '                              'I requested The layout of one of the rooms made it '                              'feel like smaller in comparison with the others but '                              'they were very helpful and changed it the next '                              'morning Perfect hotel from a frequent traveller to '                              'london this hotel will be my first choice from now on',                    'review_date': datetime.date(2016, 3, 6),                    'score': 0.9831811785697937,                    'timestamp': 1457222400.0},       'score': 0.643939376,       'sparseValues': {},       'values': []}, {'id': '46122',       'metadata': {'hotel_name': 'Park Grand London Kensington',                    'label': 'positive',                    'review': ' Bedroom was very comfortable and a great size for '                              'London as many rooms are rather small Had a super '                              'king bed which was fabulous Breakfast was very good '                              'and the staff both on reception and serving breakfast '                              'were on the whole very pleasant',                    'review_date': datetime.date(2015, 11, 10),                    'score': 0.985270082950592,                    'timestamp': 1447113600.0},       'score': 0.643873811,       'sparseValues': {},       'values': []}]    We have reviews relevant to room sizes from top to bottom of the search results. Now let's see the overall perception of the customers on London hotel room sizes. First, we need to extract the sentiment labels from the query results and count them. We will write a function to do this.   ```python def count_sentiment(result):     # store count of sentiment labels     sentiments = {         \"negative\": 0,         \"neutral\": 0,         \"positive\": 0,     }     # iterate through search results     for r in result[\"matches\"]:         # extract the sentiment label and increase its count         sentiments[r[\"metadata\"][\"label\"]] += 1     return sentiments ```   ```python sentiment = count_sentiment(result) sentiment ```         {'negative': 54, 'neutral': 161, 'positive': 285}    Let's plot the result for a better view.   ```python import seaborn as sns  # plot a barchart using seaborn sns.barplot(x=list(sentiment.keys()), y = list(sentiment.values())) ```         <matplotlib.axes._subplots.AxesSubplot at 0x7f4a962d9890>          ![png](https://raw.githubusercontent.com/pinecone-io/img/main/sentiment-mining-example-46_1.png)        The customers are generally satisfied with the room sizes, although many are still neutral and negative.  We can be even more specific when searching for reviews with the help of Pinecone's metadata filtering. For instance, we can specify a period in our query to ensure that search results only contain customer reviews for that period. This is helpful if you want to understand the customer's opinion during a specific period.  Let's do this for the same query as before. We will filter reviews from 25th December to 31st December 2015. Previously we added the `review_date` field as a timestamp to the metadata during indexing. We can convert the start and end date of the period to timestamp using the `get_timestamp` function and use a `$gte` (greater than or equal to) and a `$lte` (less than or equal to) filter to get reviews from only the selected period.   ```python # generate timestamps for start and end time of the period start_time = get_timestamp([\"2015-12-25\"])[0] end_time = get_timestamp([\"2015-12-31\"])[0] ```   ```python query = \"are the customers satisified with the room sizes of hotels in London?\" # generate query embeddings xq = retriever.encode(query).tolist() # query pinecone with query embeddings and the period filter result = index.query(     xq,     top_k=500,     include_metadata=True,     filter={\"timestamp\": {\"$gte\": start_time, \"$lte\":end_time}}) # get an overall count of customer sentiment sentiment = count_sentiment(result) # plot a barchart using seaborn sns.barplot(x=list(sentiment.keys()), y = list(sentiment.values())) ```         <matplotlib.axes._subplots.AxesSubplot at 0x7f4a95c2ee90>          ![png](https://raw.githubusercontent.com/pinecone-io/img/main/sentiment-mining-example-50_1.png)        We have a slightly different result now. Almost the same number of customers had either a neutral or negative view of the room size during the selected period.   ```python hotels =[     \"Strand Palace Hotel\",     \"Britannia International Hotel Canary Wharf\",     \"Grand Royale London Hyde Park\",     \"Intercontinental London The O2\", ] ```  We will look into five main areas:   1. Room Size   2. Cleanliness   3. Staff   4. Food   5. AC  We have a query for each of these areas below.   ```python queries = {     \"Room Size\": \"are customers happy with the room sizes?\",     \"Cleanliness\": \"are customers satisfied with the cleanliness of the rooms?\",     \"Staff\": \"did the customers like how they were treated by the staff?\",     \"Food\": \"did the customers enjoy the food?\",     \"AC\": \"customer opinion on the AC\" } ```  We need to iterate through all the hotels and run these queries for each hotel. This would give us customer reviews relevant to the selected hotel areas. After that, we count the sentiment labels and plot results for each hotel.   ```python import matplotlib.pyplot as plt import pandas as pd  hotel_sentiments = []  # iterate through the hotels for hotel in hotels:     result = []     # iterate through the keys and values in the queries dict     for area, query in queries.items():         # generate query embeddings         xq = retriever.encode(query).tolist()         # query pinecone with query embeddings and the hotel filter         xc = index.query(xq, top_k=500, include_metadata=True, filter={\"hotel_name\": hotel})         # get an overall count of customer sentiment         sentiment = count_sentiment(xc)         # sort the sentiment to show area and each value side by side         for k, v in sentiment.items():             data = {                 \"area\": area,                 \"label\": k,                 \"value\": v              }             # add the data to result list             result.append(data)     # convert the           hotel_sentiments.append({\"hotel\": hotel, \"df\": pd.DataFrame(result)})  ```  Let's see what our final data look like.   ```python hotel_sentiments[0][\"df\"] ```      <div id=\"df-401ac15f-793e-4bce-8a1a-10e2b983d865\">   <div class=\"colab-df-container\">     <div>       <style scoped>           .dataframe tbody tr th:only-of-type {               vertical-align: middle;           }           .dataframe tbody tr th {               vertical-align: top;           }           .dataframe thead th {               text-align: right;           }       </style>       <table border=\"1\" class=\"dataframe\">         <thead>           <tr style=\"text-align: right;\">             <th></th>             <th>area</th>             <th>label</th>             <th>value</th>           </tr>         </thead>         <tbody>           <tr>             <th>0</th>             <td>Room Size</td>             <td>negative</td>             <td>127</td>           </tr>           <tr>             <th>1</th>             <td>Room Size</td>             <td>neutral</td>             <td>187</td>           </tr>           <tr>             <th>2</th>             <td>Room Size</td>             <td>positive</td>             <td>186</td>           </tr>           <tr>             <th>3</th>             <td>Cleanliness</td>             <td>negative</td>             <td>90</td>           </tr>           <tr>             <th>4</th>             <td>Cleanliness</td>             <td>neutral</td>             <td>67</td>           </tr>           <tr>             <th>5</th>             <td>Cleanliness</td>             <td>positive</td>             <td>343</td>           </tr>           <tr>             <th>6</th>             <td>Staff</td>             <td>negative</td>             <td>68</td>           </tr>           <tr>             <th>7</th>             <td>Staff</td>             <td>neutral</td>             <td>36</td>           </tr>           <tr>             <th>8</th>             <td>Staff</td>             <td>positive</td>             <td>396</td>           </tr>           <tr>             <th>9</th>             <td>Food</td>             <td>negative</td>             <td>95</td>           </tr>           <tr>             <th>10</th>             <td>Food</td>             <td>neutral</td>             <td>54</td>           </tr>           <tr>             <th>11</th>             <td>Food</td>             <td>positive</td>             <td>351</td>           </tr>           <tr>             <th>12</th>             <td>AC</td>             <td>negative</td>             <td>175</td>           </tr>           <tr>             <th>13</th>             <td>AC</td>             <td>neutral</td>             <td>100</td>           </tr>           <tr>             <th>14</th>             <td>AC</td>             <td>positive</td>             <td>225</td>           </tr>         </tbody>       </table>     </div>     <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-401ac15f-793e-4bce-8a1a-10e2b983d865')\" title=\"Convert this dataframe to an interactive table.\" style=\"display:none;\">     <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\" width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg>     </button>     <style>       .colab-df-container {         display:flex;         flex-wrap:wrap;         gap: 12px;       }       .colab-df-convert {         background-color: #E8F0FE;         border: none;         border-radius: 50%;         cursor: pointer;         display: none;         fill: #1967D2;         height: 32px;         padding: 0 0 0 0;         width: 32px;       }       .colab-df-convert:hover {         background-color: #E2EBFA;         box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);         fill: #174EA6;       }       [theme=dark] .colab-df-convert {         background-color: #3B4455;         fill: #D2E3FC;       }       [theme=dark] .colab-df-convert:hover {         background-color: #434B5C;         box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);         filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));         fill: #FFFFFF;       }     </style>     <script>       const buttonEl = document.querySelector('#df-401ac15f-793e-4bce-8a1a-10e2b983d865 button.colab-df-convert');       buttonEl.style.display = google.colab.kernel.accessAllowed ? 'block' : 'none';       async function convertToInteractive(key) {         const element = document.querySelector('#df-401ac15f-793e-4bce-8a1a-10e2b983d865');         const dataTable = await google.colab.kernel.invokeFunction('convertToInteractive', [key], {});         if (!dataTable) return;         const docLinkHtml = 'Like what you see? Visit the ' +           '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'           + ' to learn more about interactive tables.';         element.innerHTML = '';         dataTable['output_type'] = 'display_data';         await google.colab.output.renderOutput(dataTable, element);         const docLink = document.createElement('div');         docLink.innerHTML = docLinkHtml;         element.appendChild(docLink);       }     </script>   </div> </div>     We may now plot the final data to make inference.   ```python # create the figure and axes to plot barchart for all hotels fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(25, 4.5)) plt.subplots_adjust(hspace=0.25)  counter = 0 # iterate through each hotel in the list and plot a barchart for d, ax in zip(hotel_sentiments, axs.ravel()):     # plot barchart for each hotel     sns.barplot(x=\"label\", y=\"value\", hue=\"area\", data=d[\"df\"], ax=ax)     # display the hotel names     ax.set_title(d[\"hotel\"])     # remove x labels     ax.set_xlabel(\"\")     # remove legend from all charts except for the first one     counter += 1     if counter != 1: ax.get_legend().remove() # display the full figure plt.show() ```        ![png](https://raw.githubusercontent.com/pinecone-io/img/main/sentiment-mining-example-60_0.png)        The following observations can be made for the hotels based on the sentiment analysis: 1.  **Strand Palace Hotel:** most customers were pleased with the staff, food, and cleanliness of the rooms, while a considerable number of them were not very satisfied with the room sizes and the AC. 2.  **Britannia International Hotel Canary Wharf:** customers were quite happy with the room size, but the majority were not satisfied with the AC. 3.  **Grand Royale London Hyde Park**: the majority of the customers were not satisfied with the room size, while a good number of them were pretty satisfied with the food, staff, AC, and cleanliness of the rooms. 4.  **Intercontinental London The O2**: the majority of the customers were really happy with the selected five areas, making this hotel the best among the selected hotels.   Although we have experimented with a few selected areas, you can get creative with your queries and get the sentiment around your area of interest immediately. This approach can even be applied to live data as the Pinecone index refreshes in real-time and performs vector searches across billions of documents with millisecond latency. ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e24b"
  },
  "content": "# Audio Similarity Search  This notebook shows how to use Pinecone as the vector DB within an audio search application. Audio search can be used to find songs and metadata within a catalog, finding similar sounds in an audio library, or detecting who's speaking in an audio file.  We will index a set of audio recordings as vector embeddings. These vector embeddings are rich, mathematical representations of the audio recordings, making it possible to determine how similar the recordings are to one another. We will then take some new (unseen) audio recording, search through the index to find the most similar matches, and play the returned audio in this notebook.  # Install Dependencies   ```python !pip install -qU pinecone-client panns-inference datasets librosa ```  # Load Dataset  In this demo, we will use audio from the *ESC-50 dataset* — a labeled collection of 2000 environmental audio recordings, which are 5-second-long each. The dataset can be loaded from the HuggingFace model hub as follows:   ```python from datasets import load_dataset  # load the dataset from huggingface model hub data = load_dataset(\"ashraq/esc50\", split=\"train\") data ```      Dataset({         features: ['filename', 'fold', 'target', 'category', 'esc10', 'src_file', 'take', 'audio'],         num_rows: 2000     })    The audios in the dataset are sampled at 44100Hz and loaded into NumPy arrays. Let's take a look.   ```python # select the audio feature and display top three audios = data[\"audio\"] audios[:3] ```       [{'path': None,       'array': array([0., 0., 0., ..., 0., 0., 0.]),       'sampling_rate': 44100},      {'path': None,       'array': array([-0.01184082, -0.10336304, -0.14141846, ...,  0.06985474,               0.04049683,  0.00274658]),       'sampling_rate': 44100},      {'path': None,       'array': array([-0.00695801, -0.01251221, -0.01126099, ...,  0.215271  ,              -0.00875854, -0.28903198]),       'sampling_rate': 44100}]    We only need the Numpy arrays as these contain all of the audio data. We will later input these Numpy arrays directly into our embedding model to generate audio embeddings.   ```python import numpy as np  # select only the audio data from the dataset and store in a numpy array audios = np.array([a[\"array\"] for a in data[\"audio\"]]) ```  # Load Audio Embedding Model  We will use an audio tagging model trained from *PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition* paper to generate our audio embeddings. We use the *panns_inference* Python package, which provides an easy interface to load and use the model.   ```python from panns_inference import AudioTagging  # load the default model into the gpu. model = AudioTagging(checkpoint_path=None, device='cuda') # change device to cpu if a gpu is not available ```      Checkpoint path: /root/panns_data/Cnn14_mAP=0.431.pth     GPU number: 1   # Initialize Pinecone Index  The Pinecone index stores the audio embeddings, which we can later retrieve using another vector embedding (a *query* audio vector). We first need to initialize our connection to Pinecone and create our vector index. For this, we need a [free API key](https://app.pinecone.io/) and then we initialize the connection like so:   ```python import pinecone  # connect to pinecone environment pinecone.init(     api_key=\"YOUR_API_KEY\",     environment=\"YOUR_ENV\"  # find next to API key ) ```  Now we create our index. We need to give it a name (you can choose anything, we use `\"audio-search-demo\"` here). The dimension is set to `2048` as the model we use to generate audio embeddings output 2048-dimension vectors. Finally, we use `cosine` as our similarity metric as the model is trained to embed audio into a cosine metric space.   ```python index_name = \"audio-search-demo\"  # check if the audio-search index exists if index_name not in pinecone.list_indexes():     # create the index if it does not exist     pinecone.create_index(         index_name,         dimension=2048,         metric=\"cosine\"     )  # connect to audio-search index we created index = pinecone.Index(index_name) ```  # Generate Embeddings and Upsert  Now we generate the embeddings using the audio embedding model. We must do this in batches as processing all items at once will exhaust machine memory limits and API request limits.   ```python from tqdm.auto import tqdm  # we will use batches of 64 batch_size = 64  for i in tqdm(range(0, len(audios), batch_size)):     # find end of batch     i_end = min(i+batch_size, len(audios))     # extract batch     batch = audios[i:i_end]     # generate embeddings for all the audios in the batch     _, emb = model.inference(batch)     # create unique IDs     ids = [f\"{idx}\" for idx in range(i, i_end)]     # add all to upsert list     to_upsert = list(zip(ids, emb.tolist()))     # upsert/insert these records to pinecone     _ = index.upsert(vectors=to_upsert)  # check that we have all vectors in index index.describe_index_stats() ```       {'dimension': 2048,      'index_fullness': 0.0,      'namespaces': {'': {'vector_count': 2000}},      'total_vector_count': 2000}    We now have *2000* audio records indexed in Pinecone, we're ready to begin querying.  # Querying  Let's first listen to an audio from our dataset. We will generate embeddings for the audio and use it to find similar audios from the Pinecone index.   ```python from IPython.display import Audio, display  # we set an audio number to select from the dataset audio_num = 400 # get the audio data of the audio number query_audio = data[audio_num][\"audio\"][\"array\"] # get the category of the audio number category = data[audio_num][\"category\"] # print the category and play the audio print(\"Query Audio:\", category) Audio(query_audio, rate=44100) ```      Query Audio: car_horn   We have got the sound of a car horn. Let's generate an embedding for this sound.   ```python # reshape query audio query_audio = query_audio[None, :] # get the embeddings for the audio from the model _, xq = model.inference(query_audio) xq.shape ```         (1, 2048)    We have now converted the audio into a 2048-dimension vector the same way we did for all the other audio we indexed. Let's use this to query our Pinecone index.   ```python # query pinecone index with the query audio embeddings results = index.query(xq.tolist(), top_k=3) results ```         {'matches': [{'id': '400', 'score': 1.0, 'values': []},                  {'id': '1667', 'score': 0.842124522, 'values': []},                  {'id': '1666', 'score': 0.831768811, 'values': []}],      'namespace': ''}    Notice that the top result is the audio number 400 from our dataset, which is our query audio (the most similar item should always be the query itself). Let's listen to the top three results.    ```python # play the top 3 similar audios for r in results[\"matches\"]:     # select the audio data from the databse using the id as an index     a = data[int(r[\"id\"])][\"audio\"][\"array\"]     display(Audio(a, rate=44100)) ```   We have great results, everything aligns with what seems to be a busy city street with car horns.  Let's write a helper function to run the queries using audio from our dataset easily. We do not need to embed these audio samples again as we have already, they are just stored in Pinecone. So, we specify the `id` of the query audio to search with and tell Pinecone to search with that.   ```python def find_similar_audios(id):     print(\"Query Audio:\")     # select the audio data from the databse using the id as an index     query_audio = data[id][\"audio\"][\"array\"]     # play the query audio     display(Audio(query_audio, rate=44100))     # query pinecone index with the query audio id     result = index.query(id=str(id), top_k=5)     print(\"Result:\")     # play the top 5 similar audios     for r in result[\"matches\"]:         a = data[int(r[\"id\"])][\"audio\"][\"array\"]         display(Audio(a, rate=44100)) ```   ```python find_similar_audios(1642) ```   Here we return a set of revving motors (they seem to either be vehicles or lawnmowers).   ```python find_similar_audios(452) ```   And now a more relaxing set of birds chirping in nature.  Let's use another audio sample from elsewhere (eg not this dataset) and see how the search performs with this.   ```python !wget https://storage.googleapis.com/audioset/miaow_16k.wav ```      --2022-09-25 20:47:00--  https://storage.googleapis.com/audioset/miaow_16k.wav     Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.20.128, 108.177.98.128, 74.125.197.128, ...     Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.20.128|:443... connected.     HTTP request sent, awaiting response... 200 OK     Length: 215546 (210K) [audio/x-wav]     Saving to: ‘miaow_16k.wav.1’          miaow_16k.wav.1     100%[===================>] 210.49K  --.-KB/s    in 0.004s            2022-09-25 20:47:00 (54.1 MB/s) - ‘miaow_16k.wav.1’ saved [215546/215546]        We can load the audio into a Numpy array as follows:   ```python import librosa  a, _ = librosa.load(\"miaow_16k.wav\", sr=44100) Audio(a, rate=44100) ```   Now we generate the embeddings for this audio and query the Pinecone index.   ```python # reshape query audio query_audio = a[None, :] # get the embeddings for the audio from the model _, xq = model.inference(query_audio)  # query pinecone index with the query audio embeddings results = index.query(xq.tolist(), top_k=3)  # play the top 3 similar audios for r in results[\"matches\"]:     a = data[int(r[\"id\"])][\"audio\"][\"array\"]     display(Audio(a, rate=44100)) ```   Our audio search application has identified a set of similar cat sounds, which is excellent.  # Delete the Index  Delete the index once you are sure that you do not want to use it anymore. Once the index is deleted, you cannot use it again.   ```python pinecone.delete_index(index_name) ``` ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e24d"
  },
  "title": "Threat Detection",
  "category": "630fc5235d91a70054705fb7",
  "content": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/security/it-threat-detection/it-threat-detection.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/security/it-threat-detection/it-threat-detection.ipynb) [![Open Github](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/github-shield.svg)](https://github.com/pinecone-io/examples/tree/master/security/it-threat-detection/it-threat-detection.ipynb)  This notebook shows how to use Pinecone's similarity search as a service to build an application for detecting rare events. Such application is common in cyber-security and fraud detection domains wherein only a tiny fraction of the events are malicious.   Here we will build a network intrusion detector. Network intrusion detection systems monitor incoming and outgoing network traffic flow, raising alarms whenever a threat is detected. Here we use a deep-learning model and similarity search in detecting and classifying network intrusion traffic.  We will start by indexing a set of labeled traffic events in the form of vector embeddings. Each event is either benign or malicious. The vector embeddings are rich, mathematical representations of the network traffic events. It is making it possible to determine how similar the network events are to one another using similarity-search algorithms built into Pinecone. Here we will transform network traffic events into vectors using a deep learning model from recent academic work.   We will then take some new (unseen) network events and search through the index to find the most similar matches, along with their labels. In such a way, we will propagate the matched labels to classify the unseen events as benign or malicious. Mind that the intrusion detection task is a challenging classification task because malicious events are sporadic. The similarity search service helps us sift the most relevant historical labeled events. That way, we identify these rare events while keeping a low rate of false alarms.    ## Setting up Pinecone  We will first install and initialize Pinecone. You can get your [API Key here](https://www.pinecone.io/start). You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**.   ```python !pip install -qU pinecone-client ```   ```python import pinecone import os # Load Pinecone API key api_key = os.getenv('PINECONE_API_KEY') or 'YOUR_API_KEY' # Set Pinecone environment. Find next to API key in console env = os.getenv('PINECONE_ENVIRONMENT') or 'YOUR_ENVIRONMENT' pinecone.init(api_key=api_key, environment=env) #List all present indexes associated with your key, should be empty on the first run pinecone.list_indexes() ```         []    ## Installing other dependencies   ```python !pip install -qU pip python-dateutil tensorflow==2.5 keras==2.4.0 scikit-learn matplotlib==3.1.0 seaborn ```   ```python from collections import Counter import matplotlib.pyplot as plt import pandas as pd import seaborn as sns from tensorflow import keras from tensorflow.keras.models import Model import tensorflow.keras.backend as K from sklearn.metrics import accuracy_score, precision_score, recall_score from sklearn.metrics import confusion_matrix ```  We will use some of the code from a recent [academic work](https://github.com/rambasnet/DeepLearning-IDS). Let's clone the repository that we will use to prepare data.   ```python !git clone -q https://github.com/rambasnet/DeepLearning-IDS.git  ```  ## Define a New Pinecone Index   ```python # Pick a name for the new service index_name = 'it-threats' ```   ```python # Make sure service with the same name does not exist if index_name in pinecone.list_indexes():     pinecone.delete_index(index_name) ```  **Create an index**   ```python pinecone.create_index(name=index_name, dimension=128, metric='euclidean') ```  **Connect to the index**  We create an index object, a class instance of pinecone.Index , which will be used to interact with the created index.   ```python index = pinecone.Index(index_name=index_name) ```  ## Upload Here we transform network events into vector embeddings, then upload them into Pinecone's vector index.   ### Prepare Data  The datasets we use consist of benign (normal) network traffic and malicious traffic generated from several different network attacks. We will focus on web attacks only.   The web attack category consists of three common attacks:  - Cross-site scripting (BruteForce-XSS),  - SQL-Injection (SQL-Injection),  - Brute force administrative and user passwords (BruteForce-Web)  The original data was recorded over two days.  **Download data for 22-02-2018 and 23-02-2018**     Files should be downloaded to the current directory. We will be using one date for training and generating vectors, and another one for testing.   ```python !wget \"https://cse-cic-ids2018.s3.ca-central-1.amazonaws.com/Processed%20Traffic%20Data%20for%20ML%20Algorithms/Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv\" -q --show-progress !wget \"https://cse-cic-ids2018.s3.ca-central-1.amazonaws.com/Processed%20Traffic%20Data%20for%20ML%20Algorithms/Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\" -q --show-progress ```      Thursday-22-02-2018 100%[===================>] 364.91M  3.07MB/s    in 2m 6s        Friday-23-02-2018_T 100%[===================>] 365.10M  3.07MB/s    in 1m 53s     Let's look at the data events first.   ```python data = pd.read_csv('Friday-23-02-2018_TrafficForML_CICFlowMeter.csv') data.Label.value_counts() ```         Benign              1048009     Brute Force -Web        362     Brute Force -XSS        151     SQL Injection            53     Name: Label, dtype: int64    **Clean the data** using a python script from the cloned repository.   ```python !python DeepLearning-IDS/data_cleanup.py \"Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\" \"result23022018\" ```      cleaning Friday-23-02-2018_TrafficForML_CICFlowMeter.csv     total rows read = 1048576     all done writing 1042868 rows; dropped 5708 rows   Load the file that you got from the previous step.   ```python data_23_cleaned = pd.read_csv('result23022018.csv') data_23_cleaned.head() ```     <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     } </style> <table class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>Dst Port</th>       <th>Protocol</th>       <th>Timestamp</th>       <th>Flow Duration</th>       <th>Tot Fwd Pkts</th>       <th>Tot Bwd Pkts</th>       <th>TotLen Fwd Pkts</th>       <th>TotLen Bwd Pkts</th>       <th>Fwd Pkt Len Max</th>       <th>Fwd Pkt Len Min</th>       <th>...</th>       <th>Fwd Seg Size Min</th>       <th>Active Mean</th>       <th>Active Std</th>       <th>Active Max</th>       <th>Active Min</th>       <th>Idle Mean</th>       <th>Idle Std</th>       <th>Idle Max</th>       <th>Idle Min</th>       <th>Label</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>22</td>       <td>6</td>       <td>1.519374e+09</td>       <td>1532698</td>       <td>11</td>       <td>11</td>       <td>1179</td>       <td>1969</td>       <td>648</td>       <td>0</td>       <td>...</td>       <td>32</td>       <td>0.0</td>       <td>0.0</td>       <td>0</td>       <td>0</td>       <td>0.0</td>       <td>0.000000e+00</td>       <td>0</td>       <td>0</td>       <td>Benign</td>     </tr>     <tr>       <th>1</th>       <td>500</td>       <td>17</td>       <td>1.519374e+09</td>       <td>117573855</td>       <td>3</td>       <td>0</td>       <td>1500</td>       <td>0</td>       <td>500</td>       <td>500</td>       <td>...</td>       <td>8</td>       <td>0.0</td>       <td>0.0</td>       <td>0</td>       <td>0</td>       <td>58786927.5</td>       <td>2.375324e+07</td>       <td>75583006</td>       <td>41990849</td>       <td>Benign</td>     </tr>     <tr>       <th>2</th>       <td>500</td>       <td>17</td>       <td>1.519374e+09</td>       <td>117573848</td>       <td>3</td>       <td>0</td>       <td>1500</td>       <td>0</td>       <td>500</td>       <td>500</td>       <td>...</td>       <td>8</td>       <td>0.0</td>       <td>0.0</td>       <td>0</td>       <td>0</td>       <td>58786924.0</td>       <td>2.375325e+07</td>       <td>75583007</td>       <td>41990841</td>       <td>Benign</td>     </tr>     <tr>       <th>3</th>       <td>22</td>       <td>6</td>       <td>1.519374e+09</td>       <td>1745392</td>       <td>11</td>       <td>11</td>       <td>1179</td>       <td>1969</td>       <td>648</td>       <td>0</td>       <td>...</td>       <td>32</td>       <td>0.0</td>       <td>0.0</td>       <td>0</td>       <td>0</td>       <td>0.0</td>       <td>0.000000e+00</td>       <td>0</td>       <td>0</td>       <td>Benign</td>     </tr>     <tr>       <th>4</th>       <td>500</td>       <td>17</td>       <td>1.519374e+09</td>       <td>89483474</td>       <td>6</td>       <td>0</td>       <td>3000</td>       <td>0</td>       <td>500</td>       <td>500</td>       <td>...</td>       <td>8</td>       <td>4000364.0</td>       <td>0.0</td>       <td>4000364</td>       <td>4000364</td>       <td>21370777.5</td>       <td>1.528092e+07</td>       <td>41989576</td>       <td>7200485</td>       <td>Benign</td>     </tr>   </tbody> </table> <p>5 rows × 80 columns</p> </div>     ```python data_23_cleaned.Label.value_counts() ```         Benign              1042301     Brute Force -Web        362     Brute Force -XSS        151     SQL Injection            53     Name: Label, dtype: int64    ### Load the Model  Here we load the pretrained model. The model is trained using the data from the same date.  We have modified [the original model](https://github.com/rambasnet/DeepLearning-IDS/blob/master/keras_tensorflow_models/02-23-2018.csv_adam_10_10_multiclass_baseline_model_1561316601.model) slightly and changed the number of classes from four (Benign, BruteForce-Web, BruteForce-XSS, SQL-Injection) to two (Benign and Attack). In the step below we will download and unzip our modified model.   ```python !wget -q -O it_threat_model.model.zip \"https://drive.google.com/uc?export=download&id=1VYMHOk_XMAc-QFJ_8CAPvWFfHnLpS2J_\"  !unzip -q it_threat_model.model.zip  ```   ```python model = keras.models.load_model('it_threat_model.model') model.summary() ```      WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.     Model: \"sequential\"     _________________________________________________________________     Layer (type)                 Output Shape              Param #        =================================================================     dense (Dense)                (None, 128)               10240          _________________________________________________________________     dense_1 (Dense)              (None, 64)                8256           _________________________________________________________________     dense_2 (Dense)              (None, 1)                 65             =================================================================     Total params: 18,561     Trainable params: 18,561     Non-trainable params: 0     _________________________________________________________________    ```python # Select the first layer layer_name = 'dense'  intermediate_layer_model = Model(inputs=model.input,                                  outputs=model.get_layer(layer_name).output) ```  ### Upload Data   Let's define the item's ids in a way that will reflect the event's label.  Then, we index the events in Pinecone's vector index.   ```python from tqdm import tqdm items_to_upload = []  model_res = intermediate_layer_model.predict(K.constant(data_23_cleaned.iloc[:,:-1]))  for i, res in tqdm(zip(data_23_cleaned.iterrows(), model_res), total=len(model_res)):     benign_or_attack = i[1]['Label'][:3]     items_to_upload.append((benign_or_attack + '_' + str(i[0]), res.tolist())) ```      100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1042867/1042867 [01:43<00:00, 10067.22it/s]    ```python import itertools  def chunks(iterable, batch_size=100):     it = iter(iterable)     chunk = tuple(itertools.islice(it, batch_size))     while chunk:         yield chunk         chunk = tuple(itertools.islice(it, batch_size)) ```  You can lower the NUMBER_OF_ITEMS and, by doing so, limit the number of uploaded items.    ```python NUMBER_OF_ITEMS = len(items_to_upload)  for batch in chunks(items_to_upload[:NUMBER_OF_ITEMS], 50):     index.upsert(vectors=batch) ```   ```python items_to_upload.clear() ```  Let's verify all items were inserted.    ```python index.describe_index_stats() ```         {'dimension': 128, 'namespaces': {'': {'vector_count': 1042867}}}    ## Query  First, we will randomly select a Benign/Attack event and query the vector index using the event embedding. Then, we will use data from different day, that contains same set of attacks to query on a bigger sample.   ### Evaluate the Rare Event Classification Model  We will use network intrusion dataset for 22-02-2018 for querying and testing the Pinecone.  First, let's clean the data.   ```python !python DeepLearning-IDS/data_cleanup.py \"Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv\" \"result22022018\" ```      cleaning Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv     total rows read = 1048576     all done writing 1042966 rows; dropped 5610 rows    ```python data_22_cleaned = pd.read_csv('result22022018.csv') data_22_cleaned.head() ```     <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     } </style> <table class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>Dst Port</th>       <th>Protocol</th>       <th>Timestamp</th>       <th>Flow Duration</th>       <th>Tot Fwd Pkts</th>       <th>Tot Bwd Pkts</th>       <th>TotLen Fwd Pkts</th>       <th>TotLen Bwd Pkts</th>       <th>Fwd Pkt Len Max</th>       <th>Fwd Pkt Len Min</th>       <th>...</th>       <th>Fwd Seg Size Min</th>       <th>Active Mean</th>       <th>Active Std</th>       <th>Active Max</th>       <th>Active Min</th>       <th>Idle Mean</th>       <th>Idle Std</th>       <th>Idle Max</th>       <th>Idle Min</th>       <th>Label</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>22</td>       <td>6</td>       <td>1.519288e+09</td>       <td>20553406</td>       <td>10</td>       <td>7</td>       <td>1063</td>       <td>1297</td>       <td>744</td>       <td>0</td>       <td>...</td>       <td>20</td>       <td>1027304.0</td>       <td>0.0</td>       <td>1027304</td>       <td>1027304</td>       <td>1.952608e+07</td>       <td>0.000000e+00</td>       <td>19526080</td>       <td>19526080</td>       <td>Benign</td>     </tr>     <tr>       <th>1</th>       <td>34989</td>       <td>6</td>       <td>1.519288e+09</td>       <td>790</td>       <td>2</td>       <td>0</td>       <td>848</td>       <td>0</td>       <td>848</td>       <td>0</td>       <td>...</td>       <td>20</td>       <td>0.0</td>       <td>0.0</td>       <td>0</td>       <td>0</td>       <td>0.000000e+00</td>       <td>0.000000e+00</td>       <td>0</td>       <td>0</td>       <td>Benign</td>     </tr>     <tr>       <th>2</th>       <td>500</td>       <td>17</td>       <td>1.519288e+09</td>       <td>99745913</td>       <td>5</td>       <td>0</td>       <td>2500</td>       <td>0</td>       <td>500</td>       <td>500</td>       <td>...</td>       <td>8</td>       <td>4000203.0</td>       <td>0.0</td>       <td>4000203</td>       <td>4000203</td>       <td>3.191524e+07</td>       <td>3.792787e+07</td>       <td>75584115</td>       <td>7200679</td>       <td>Benign</td>     </tr>     <tr>       <th>3</th>       <td>500</td>       <td>17</td>       <td>1.519288e+09</td>       <td>99745913</td>       <td>5</td>       <td>0</td>       <td>2500</td>       <td>0</td>       <td>500</td>       <td>500</td>       <td>...</td>       <td>8</td>       <td>4000189.0</td>       <td>0.0</td>       <td>4000189</td>       <td>4000189</td>       <td>3.191524e+07</td>       <td>3.792788e+07</td>       <td>75584130</td>       <td>7200693</td>       <td>Benign</td>     </tr>     <tr>       <th>4</th>       <td>500</td>       <td>17</td>       <td>1.519288e+09</td>       <td>89481361</td>       <td>6</td>       <td>0</td>       <td>3000</td>       <td>0</td>       <td>500</td>       <td>500</td>       <td>...</td>       <td>8</td>       <td>4000554.0</td>       <td>0.0</td>       <td>4000554</td>       <td>4000554</td>       <td>2.137020e+07</td>       <td>1.528109e+07</td>       <td>41990741</td>       <td>7200848</td>       <td>Benign</td>     </tr>   </tbody> </table> <p>5 rows × 80 columns</p> </div>     ```python data_22_cleaned.Label.value_counts() ```         Benign              1042603     Brute Force -Web        249     Brute Force -XSS         79     SQL Injection            34     Name: Label, dtype: int64    Let's define a sample that will include all different types of web attacks for this specific date.   ```python data_sample = data_22_cleaned[-2000:] data_sample.Label.value_counts() ```         Benign              1638     Brute Force -Web     249     Brute Force -XSS      79     SQL Injection         34     Name: Label, dtype: int64    Now, we will query the test dataset and save predicted and expected results to create a confusion matrix.   ```python y_true = [] y_pred = []  BATCH_SIZE = 100  for i in tqdm(range(0, len(data_sample), BATCH_SIZE)):     test_data = data_sample.iloc[i:i+BATCH_SIZE, :]          # Create vector embedding using the model     test_vector = intermediate_layer_model.predict(K.constant(test_data.iloc[:, :-1]))     # Query using the vector embedding     query_results = []      for xq in test_vector.tolist():         query_res = index.query(xq, top_k=50)         query_results.append(query_res)          ids = [res.id for result in query_results for res in result.matches]          for label, res in zip(test_data.Label.values, query_results):         # Add to the true list         if label == 'Benign':             y_true.append(0)         else:             y_true.append(1)                  counter = Counter(match.id.split('_')[0] for match in res.matches)          # Add to the predicted list         if counter['Bru'] or counter['SQL']:             y_pred.append(1)         else:             y_pred.append(0)  ```      100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [10:48<00:00, 32.44s/it]    ```python # Create confusion matrix conf_matrix = confusion_matrix(y_true, y_pred)  # Show confusion matrix ax = plt.subplot() sns.heatmap(conf_matrix, annot=True, ax = ax, cmap='Blues', fmt='g', cbar=False)  # Add labels, title and ticks ax.set_xlabel('Predicted') ax.set_ylabel('Acctual') ax.set_title('Confusion Matrix') ax.xaxis.set_ticklabels(['Benign', 'Attack']) ax.yaxis.set_ticklabels(['Benign', 'Attack']) ```         [Text(0, 0.5, 'Benign'), Text(0, 1.5, 'Attack')]           ![Confusion matrix](https://raw.githubusercontent.com/pinecone-io/img/main/it-threat-detection-1.png)        Now we can calculate overall accuracy and per class accuracy.   ```python # Calculate accuracy acc = accuracy_score(y_true, y_pred, normalize=True, sample_weight=None) precision = precision_score(y_true, y_pred) recall = recall_score(y_true, y_pred)  print(f\"Accuracy: {acc:.3f}\") print(f\"Precision: {precision:.3f}\") print(f\"Recall: {recall:.3f}\") ```      Accuracy: 0.923     Precision: 0.995     Recall: 0.577        ```python # Calculate per class accuracy cmd = confusion_matrix(y_true, y_pred, normalize=\"true\").diagonal() per_class_accuracy_df = pd.DataFrame([(index, round(value,4)) for index, value in zip(['Benign', 'Attack'], cmd)], columns = ['type', 'accuracy']) per_class_accuracy_df = per_class_accuracy_df.round(2) display(per_class_accuracy_df) ```   <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     } </style> <table class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>type</th>       <th>accuracy</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>Benign</td>       <td>1.00</td>     </tr>     <tr>       <th>1</th>       <td>Attack</td>       <td>0.58</td>     </tr>   </tbody> </table> </div>   We got great results using Pinecone! Let's see what happens if we skip the similarity search step and predict values from the model directly. In other words, let's use the model that created the embeddings as a classifier. It would be interesting to compare its and the similarity search approach accuracy.    ```python from keras.utils.np_utils import normalize import numpy as np  data_sample = normalize(data_22_cleaned.iloc[:, :-1])[-2000:] y_pred_model = model.predict(normalize(data_sample)).flatten() y_pred_model = np.round(y_pred_model) ```   ```python # Create confusion matrix conf_matrix = confusion_matrix(y_true, y_pred_model)  # Show confusion matrix ax = plt.subplot() sns.heatmap(conf_matrix, annot=True, ax = ax, cmap='Blues', fmt='g', cbar=False)  # Add labels, title and ticks ax.set_xlabel('Predicted') ax.set_ylabel('Acctual') ax.set_title('Confusion Matrix') ax.xaxis.set_ticklabels(['Benign', 'Attack']) ax.yaxis.set_ticklabels(['Benign', 'Attack']) ```         [Text(0, 0.5, 'Benign'), Text(0, 1.5, 'Attack')]          ![Confusion matix](https://raw.githubusercontent.com/pinecone-io/img/main/it-threat-detection-2.png)         ```python # Calculate accuracy acc = accuracy_score(y_true, y_pred_model, normalize=True, sample_weight=None) precision = precision_score(y_true, y_pred_model) recall = recall_score(y_true, y_pred_model)  print(f\"Accuracy: {acc:.3f}\") print(f\"Precision: {precision:.3f}\") print(f\"Recall: {recall:.3f}\") ```      Accuracy: 0.871     Precision: 1.000     Recall: 0.287        ```python # Calculate per class accuracy cmd = confusion_matrix(y_true, y_pred_model, normalize=\"true\").diagonal() per_class_accuracy_df = pd.DataFrame([(index, round(value,4)) for index, value in zip(['Benign', 'Attack'], cmd)], columns = ['type', 'accuracy']) per_class_accuracy_df = per_class_accuracy_df.round(2) display(per_class_accuracy_df) ```   <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }   .dataframe tbody tr th {         vertical-align: top;     }   .dataframe thead th {         text-align: right;     } </style> <table class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>type</th>       <th>accuracy</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>Benign</td>       <td>1.00</td>     </tr>     <tr>       <th>1</th>       <td>Attack</td>       <td>0.29</td>     </tr>   </tbody> </table> </div>   As we can see, the direct application of our model produced much worse results. Pinecone's similarity search over the same model's embeddings improved our threat detection (i.e., \"Attack\") accuracy by over 50%!  ### Result summary  Using standard vector embeddings with Pinecone's similarity search service, we detected 85% of the attacks while keeping a low 3% false-positive rate. We also showed that our similarity search approach outperforms the direct classification approach that utilizes the classifier's embedding model. Similarity search-based detection gained 50% higher accuracy compared to the direct detector.  [Original published results](https://github.com/rambasnet/DeepLearning-IDS/blob/master/graphics/confusion_matrices/) for 02-22-2018 show that the model was able to correctly detect 208520 benign cases out of 208520 benign cases, and 24 (18+1+5) attacks out of 70 attacks in the test set making this model **34.3% accurate in predicting attacks**. For testing purposes, 20% of the data for 02-22-2018 was used.   ![02-22-2018--6-15%281%29.png](https://raw.githubusercontent.com/rambasnet/DeepLearning-IDS/master/graphics/confusion_matrices/02-22-2018--6-15(1).png)  As you can see, the model's performance for creating embeddings for Pinecone was much higher.   The model we have created follows the academic paper ([model for the same date](https://github.com/rambasnet/DeepLearning-IDS/blob/master/keras_tensorflow_models/) (02-23-2018)) and is slightly modified, but still a straightforward, sequential, shallow model. We have changed the number of classes from four (Benign, BruteForce-Web, BruteForce-XSS, SQL-Injection) to two (Benign and Attack), only interested in whether we are detecting an attack or not. We have also changed validation metrics to precision and recall. These changes improved our results. Yet, there is still room for further improvements, for example, by adding more data covering multiple days and different types of attacks.  ## Delete the Index  Delete the index once you are sure that you do not want to use it anymore. Once it is deleted, you cannot reuse it.   ```python pinecone.delete_index(index_name) ``` ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e24f"
  },
  "title": "GIF Search",
  "category": "630fc5235d91a70054705fb7",
  "content": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/search/semantic-search/gif-search/gif-search.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/search/semantic-search/gif-search/gif-search.ipynb) [![Open Github](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/github-shield.svg)](https://github.com/pinecone-io/examples/tree/master/search/semantic-search/gif-search/gif-search.ipynb)  We will use the [Tumblr GIF Description Dataset](http://raingo.github.io/TGIF-Release/), which contains over 100k animated GIFs and 120K sentences describing its visual content. Using this data with a *vector database* and *retriever* we are able to create an NLP-powered GIF search tool.  There are a few packages that must be installed for this notebook to run:   ```python pip install -U pandas pinecone-client sentence-transformers tqdm ```  We must also set the following notebook parameters to display the GIF images we will be working with.   ```python from IPython.display import HTML from IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = \"all\" ```  ## Download and Extract Dataset  First let's download and extract the dataset. The dataset is available [here](https://github.com/raingo/TGIF-Release) on GitHub. We can use the link below to download the dataset directly. We can also access the link from a browser to directly download the files.   ```python # Use wget to download the master.zip file which contains the dataset !wget https://github.com/raingo/TGIF-Release/archive/master.zip ```   ```python # Use unzip to extract the master.zip file !unzip master.zip ```  ## Explore the Dataset  Now let's explore the downloaded files. The data we want is in *tgif-v1.0.tsv* file in the *data* folder. We can use *pandas* library to open the file. We need to set delimiter as `\\t` as the file contains tab separated values.   ```python import pandas as pd ```   ```python # Load dataset to a pandas dataframe df = pd.read_csv(     \"./TGIF-Release-master/data/tgif-v1.0.tsv\",     delimiter=\"\\t\",     names=['url', 'description'] ) df.head() ```     <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     } </style> <table border=\"1\" class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>url</th>       <th>description</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>https://38.media.tumblr.com/9f6c25cc350f12aa74...</td>       <td>a man is glaring, and someone with sunglasses ...</td>     </tr>     <tr>       <th>1</th>       <td>https://38.media.tumblr.com/9ead028ef62004ef6a...</td>       <td>a cat tries to catch a mouse on a tablet</td>     </tr>     <tr>       <th>2</th>       <td>https://38.media.tumblr.com/9f43dc410be85b1159...</td>       <td>a man dressed in red is dancing.</td>     </tr>     <tr>       <th>3</th>       <td>https://38.media.tumblr.com/9f659499c8754e40cf...</td>       <td>an animal comes close to another in the jungle</td>     </tr>     <tr>       <th>4</th>       <td>https://38.media.tumblr.com/9ed1c99afa7d714118...</td>       <td>a man in a hat adjusts his tie and makes a wei...</td>     </tr>   </tbody> </table> </div>    *Note the dataset does not contain the actual GIF files. But it has URLs we can use to download/access the GIF files. This is great as we do not need to store/download all the GIF files. We can directly load the required GIF files using the URL when displaying the search results.*  There are some duplicate descriptions in the dataset.   ```python len(df) ```         125782     ```python # Number of *unique* GIFs in the dataset len(df[\"url\"].unique()) ```         102068     ```python dupes = df['url'].value_counts().sort_values(ascending=False) dupes.head() ```         https://38.media.tumblr.com/ddbfe51aff57fd8446f49546bc027bd7/tumblr_nowv0v6oWj1uwbrato1_500.gif    4     https://33.media.tumblr.com/46c873a60bb8bd97bdc253b826d1d7a1/tumblr_nh7vnlXEvL1u6fg3no1_500.gif    4     https://38.media.tumblr.com/b544f3c87cbf26462dc267740bb1c842/tumblr_n98uooxl0K1thiyb6o1_250.gif    4     https://33.media.tumblr.com/88235b43b48e9823eeb3e7890f3d46ef/tumblr_nkg5leY4e21sof15vo1_500.gif    4     https://31.media.tumblr.com/69bca8520e1f03b4148dde2ac78469ec/tumblr_npvi0kW4OD1urqm0mo1_400.gif    4     Name: url, dtype: int64    Let's take a look at one of these duplicated URLs and it's descriptions.   ```python dupe_url = \"https://33.media.tumblr.com/88235b43b48e9823eeb3e7890f3d46ef/tumblr_nkg5leY4e21sof15vo1_500.gif\" dupe_df = df[df['url'] == dupe_url]  # let's take a look at this GIF and it's duplicated descriptions for _, gif in dupe_df.iterrows():     HTML(f\"<img src={gif['url']} style='width:120px; height:90px'>\")     print(gif[\"description\"]) ```     <img src=https://33.media.tumblr.com/88235b43b48e9823eeb3e7890f3d46ef/tumblr_nkg5leY4e21sof15vo1_500.gif style='width:120px; height:90px'>        two girls are singing music pop in a concert      <img src=https://33.media.tumblr.com/88235b43b48e9823eeb3e7890f3d46ef/tumblr_nkg5leY4e21sof15vo1_500.gif style='width:120px; height:90px'>        a woman sings sang girl on a stage singing      <img src=https://33.media.tumblr.com/88235b43b48e9823eeb3e7890f3d46ef/tumblr_nkg5leY4e21sof15vo1_500.gif style='width:120px; height:90px'>        two girls on a stage sing into microphones.      <img src=https://33.media.tumblr.com/88235b43b48e9823eeb3e7890f3d46ef/tumblr_nkg5leY4e21sof15vo1_500.gif style='width:120px; height:90px'>        two girls dressed in black are singing.   There is no reason for us to remove these duplicates, as shown here, every description is accurate. You can spot check a few of the other URLs but they all seem to be the same where we have several *accurate* descriptions for a single GIF.  That leaves us with 125,781 descriptions for 102,067 GIFs. We will use these descriptions to create *context* vectors that will be indexed in a vector database to create our GIF search tool. Let's take a look at a few more examples of GIFs and their descriptions.   ```python for _, gif in df[:5].iterrows():   HTML(f\"<img src={gif['url']} style='width:120px; height:90px'>\")   print(gif[\"description\"]) ```     <img src=https://38.media.tumblr.com/9f6c25cc350f12aa74a7dc386a5c4985/tumblr_mevmyaKtDf1rgvhr8o1_500.gif style='width:120px; height:90px'>        a man is glaring, and someone with sunglasses appears.      <img src=https://38.media.tumblr.com/9ead028ef62004ef6ac2b92e52edd210/tumblr_nok4eeONTv1s2yegdo1_400.gif style='width:120px; height:90px'>        a cat tries to catch a mouse on a tablet      <img src=https://38.media.tumblr.com/9f43dc410be85b1159d1f42663d811d7/tumblr_mllh01J96X1s9npefo1_250.gif style='width:120px; height:90px'>        a man dressed in red is dancing.      <img src=https://38.media.tumblr.com/9f659499c8754e40cf3f7ac21d08dae6/tumblr_nqlr0rn8ox1r2r0koo1_400.gif style='width:120px; height:90px'>        an animal comes close to another in the jungle      <img src=https://38.media.tumblr.com/9ed1c99afa7d71411884101cb054f35f/tumblr_mvtuwlhSkE1qbnleeo1_500.gif style='width:120px; height:90px'>        a man in a hat adjusts his tie and makes a weird face.   We can see that the description of the GIF accurately describes what is happening in the GIF, we can use these descriptions to search through our GIFs.  Using this data, we can build the GIF search tool with just *two* components:  * a **retriever** to embed GIF descriptions * a **vector database** to store GIF description embeddings and retrieve relevant GIFs  ## Initialize Pinecone Index  The vector database stores vector representations of our GIF descriptions which we can retrieve using another vector (query vector). We will use the Pinecone vector database, a fully managed vector database that can store and search through billions of records in milliseconds. You could use any other vector database such as FAISS to build this tool. But you may need to manage the database yourself.  To initialize the database, we sign up for a [free Pinecone API key](https://app.pinecone.io/) and `pip install pinecone-client`. You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**. Once ready, we initialize our index with:   ```python import pinecone  # Connect to pinecone environment pinecone.init(     api_key=\"<<YOUR_API_KEY>>\",     environment=\"YOUR_ENVIRONMENT\" )  index_name = 'gif-search'  # check if the gif-search exists if index_name not in pinecone.list_indexes():     # create the index if it does not exist     pinecone.create_index(         index_name,         dimension=384,         metric=\"cosine\"     )  # Connect to gif-search index we created index = pinecone.Index(index_name) ```  Here we specify the name of the index where we will store our GIF descriptions and their URLs, the similarity metric, and the embedding dimension of the vectors. The similarity metric and embedding dimension can change depending on the embedding model used. However, most retrievers use \"cosine\" and 768.  ## Initialize Retriever  Next, we need to initialize our retriever. The retriever will mainly do two things:  1.\tGenerate embeddings for all the GIF descriptions (context vectors/embeddings) 2.\tGenerate embeddings for the query (query vector/embedding)  The retriever will generate the embeddings in a way that the queries and GIF descriptions with similar meanings are in a similar vector space. Then we can use cosine similarity to calculate this similarity between the query and context embeddings and find the most relevant GIF to our query.  We will use a `SentenceTransformer` model trained based on Microsoft's MPNet as our retriever. This model performs well out-of-the-box when searching based on generic semantic similarity.    ```python from sentence_transformers import SentenceTransformer ```   ```python # Initialize retriever with SentenceTransformer model  retriever = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2') retriever ```         SentenceTransformer(       (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel        (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})       (2): Normalize()     )    ## Generate Embeddings and Upsert  Now our retriever and the pinecone index are initialized. Next, we need to generate embeddings for the GIF descriptions. We will do this in batches to help us more quickly generate embeddings. This means our retriever will generate embeddings for 64 GIF descriptions at once instead of generating them individually (much faster) and send a single API call for each batch of 64 (also much faster).  When passing the documents to pinecone, we need an id (a unique value), embedding (embeddings for the GIF descriptions we have generated earlier), and metadata for each document representing GIFs in the dataset. The metadata is a dictionary containing data relevant to our embeddings. For the GIF search tool, we only need the URL and description.   ```python from tqdm.auto import tqdm  # we will use batches of 64 batch_size = 64  for i in tqdm(range(0, len(df), batch_size)):     # find end of batch     i_end = min(i+batch_size, len(df))     # extract batch     batch = df.iloc[i:i_end]     # generate embeddings for batch     emb = retriever.encode(batch['description'].tolist()).tolist()     # get metadata     meta = batch.to_dict(orient='records')     # create IDs     ids = [f\"{idx}\" for idx in range(i, i_end)]     # add all to upsert list     to_upsert = list(zip(ids, emb, meta))     # upsert/insert these records to pinecone     _ = index.upsert(vectors=to_upsert)       # check that we have all vectors in index index.describe_index_stats() ```         0%|          | 0/1966 [00:00<?, ?it/s]          {'dimension': 384,      'index_fullness': 0.05,      'namespaces': {'': {'vector_count': 125782}}}    We can see all our documents are now in the pinecone index. Let's run some queries to test our GIF search tool.  ## Querying  We have two functions, `search_gif`, to handle our search query, and `display_gif`, to display the search results.  The `search_gif` function generates vector embedding for the search query using the retriever model and then runs the query on the pinecone index. `index.query` will compute the cosine similarity between the query embedding and the GIF description embeddings as we set the metric type as \"cosine\" when we initialize the pinecone index. The function will return the URL of the top 10 most relevant GIFs to our search query.   ```python def search_gif(query):     # Generate embeddings for the query     xq = retriever.encode(query).tolist()     # Compute cosine similarity between query and embeddings vectors and return top 10 URls     xc = index.query(xq, top_k=10,                     include_metadata=True)     result = []     for context in xc['matches']:         url = context['metadata']['url']         result.append(url)     return result ```  The `display_gif` can display multiple GIFs using its URLs in the jupyter notebook in a grid style. We use this function to display the top 10 GIFs returned by the `search_gif` function.   ```python def display_gif(urls):     figures = []     for url in urls:         figures.append(f'''             <figure style=\"margin: 5px !important;\">               <img src=\"{url}\" style=\"width: 120px; height: 90px\" >             </figure>         ''')     return HTML(data=f'''         <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">         {''.join(figures)}         </div>     ''') ```  Let's begin testing some queries.   ```python gifs = search_gif(\"a dog being confused\") display_gif(gifs) ```      <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">  <figure style=\"margin: 5px !important;\">   <img src=\"https://38.media.tumblr.com/af53df8d946bbca23be97691db0ecd5e/tumblr_nq3l305zdF1s71nvbo1_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://33.media.tumblr.com/a574ab035e7edc7708db423ee67f3ac4/tumblr_nq1zodZJNx1uoke7ao1_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://38.media.tumblr.com/94703ea885174ffc97c44d57487d7ee9/tumblr_na6oo2PKSC1silsr6o1_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://38.media.tumblr.com/fa6a31e326066bb27776066150c8c810/tumblr_np38ipgJPd1tkkgpso1_250.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://38.media.tumblr.com/241d89939a5714c2db4566d9108245fe/tumblr_n9xv6aqQ5A1qmgppeo1_250.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://31.media.tumblr.com/a00ae69f826dbe89a5bdabad567ac88d/tumblr_n8x5e6ZcFW1sjpl9lo1_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://31.media.tumblr.com/28a9aac3c21941e1c61dd9ab4390c3f5/tumblr_nhdr3clKDa1sntw1mo1_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://31.media.tumblr.com/5cbd531e1d8cc7fefffdb8a68ec62b1d/tumblr_naysx8YTzn1tzl1owo1_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://38.media.tumblr.com/bc300fcbae8e4eb65c3901a246f46e4c/tumblr_niu5dzNP7G1u62tooo1_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://38.media.tumblr.com/1c3edb33951b52020b9271185942b2b2/tumblr_nflm4phy0P1u4txqeo1_250.gif\" style=\"width: 120px; height: 90px\" > </figure>  </div>      ```python gifs = search_gif(\"animals being cute\") display_gif(gifs) ```      <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">  <figure style=\"margin: 5px !important;\">   <img src=\"https://33.media.tumblr.com/73841eb3b37ad5277b324359a83bb19e/tumblr_ngnz25VQpD1twctp1o1_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://38.media.tumblr.com/7b8ebff7051b8a7d0502294465559861/tumblr_na8n60gCmT1tiamw8o1_500.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://38.media.tumblr.com/49223a5564c8d7dfafe115063ba88c8a/tumblr_nnrps82EvG1sxvevjo1_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://31.media.tumblr.com/aa9c98f92f06cc3484ae395194db6d7f/tumblr_naeyc6yQWy1tahfdeo1_250.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://38.media.tumblr.com/e7a1d7ed5f2289db13e1812a91c0eedf/tumblr_nf8r2nWajt1s236zjo1_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://38.media.tumblr.com/7d8f9cac33b4fc76908a37bf28ab6fca/tumblr_noswtqDMKC1tyncywo1_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://38.media.tumblr.com/136d1d103edf3a82c2332bf8ef28d6d3/tumblr_nhm8rleyTk1u333yco2_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://33.media.tumblr.com/0f97de4f3cc8dca408ca4ab036460412/tumblr_njmp6tj53K1thqmhto1_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://31.media.tumblr.com/be2b34de9ff751da15cbde3144d25007/tumblr_nh4oj86LJO1slj978o1_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://38.media.tumblr.com/5f45e9a56121b070ddceca58b37e9ace/tumblr_njaggwVmdn1un7vpco1_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  </div>    ```python gifs = search_gif(\"an animal dancing\") display_gif(gifs) ```      <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">  <figure style=\"margin: 5px !important;\">   <img src=\"https://33.media.tumblr.com/7ada83ae354be1d83ea4407fea789ab8/tumblr_na0e6razjV1s71nvbo1_250.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://33.media.tumblr.com/3be31f4531ed041ff9b80465b56d810e/tumblr_nr0dycLuRO1useffdo1_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://33.media.tumblr.com/f0edc38b8dacce783bebcdf41db55a93/tumblr_npapb2c4Wz1uolkubo1_250.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://33.media.tumblr.com/2bf0f300d9ecfbcedf2dd3ba2b40b5e5/tumblr_ne5p2oTuCj1tdmffyo1_250.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://38.media.tumblr.com/3e1f37fea789bb1508d40e8c30f791ae/tumblr_na3xfcUdnK1tiamx1o1_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://38.media.tumblr.com/0b04187cb51a8889b0f41e5fbe390df2/tumblr_nbcltiDvcq1s7ri4yo1_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://38.media.tumblr.com/e52a1d77dc0a679840a715c02035e5da/tumblr_nfbeo6Qqr91tl8fnfo1_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://38.media.tumblr.com/a67f2f007b9881080aa3fe3584847bc5/tumblr_nc1wzyMaJP1tzj4j8o1_250.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://38.media.tumblr.com/61e9abf3681eeacea18dae288f084d62/tumblr_nbw9gwXM0e1tk2ngvo1_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://31.media.tumblr.com/78355496a2ed41f0aa9fe855f9460bc3/tumblr_nais3s7sWa1s3att3o1_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  </div>     Let's describe the third GIF with the ginger dog dancing on his hind legs.   ```python gifs = search_gif(\"a fluffy dog being cute and dancing like a person\") display_gif(gifs) ```      <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">  <figure style=\"margin: 5px !important;\">   <img src=\"https://38.media.tumblr.com/a67f2f007b9881080aa3fe3584847bc5/tumblr_nc1wzyMaJP1tzj4j8o1_250.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://33.media.tumblr.com/2bf0f300d9ecfbcedf2dd3ba2b40b5e5/tumblr_ne5p2oTuCj1tdmffyo1_250.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://33.media.tumblr.com/ec768e8a6f881fbc0f329932c8591a88/tumblr_mpqwb14Fsq1rjcfxro1_250.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://33.media.tumblr.com/7ada83ae354be1d83ea4407fea789ab8/tumblr_na0e6razjV1s71nvbo1_250.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://38.media.tumblr.com/f8b6d3d79b59462019c2daf2ba8b4148/tumblr_np762bxBYV1t7jda2o1_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://38.media.tumblr.com/a5ae79c2d62c592d7565684a72af8f2c/tumblr_nageslBNqC1tstoffo1_500.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://38.media.tumblr.com/3e1f37fea789bb1508d40e8c30f791ae/tumblr_na3xfcUdnK1tiamx1o1_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://33.media.tumblr.com/aec9cbbdf826f98307e6d5f3d544a4c2/tumblr_mmlrbhGDAO1qaqutao1_500.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://38.media.tumblr.com/0b04187cb51a8889b0f41e5fbe390df2/tumblr_nbcltiDvcq1s7ri4yo1_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://33.media.tumblr.com/14f9b213a7355096c14b0af3a7768f5d/tumblr_npexfuFU2K1ti77bgo1_400.gif\" style=\"width: 120px; height: 90px\" > </figure>  </div>     These look like pretty good, interesting results.  ## Example application  To try out an application like this one, see this [example application](https://huggingface.co/spaces/pinecone/gif-search).   --- ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e251"
  },
  "title": "Document Deduplication",
  "category": "630fc5235d91a70054705fb7",
  "content": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/search/semantic-search/deduplication/deduplication_scholarly_articles.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/search/semantic-search/deduplication/deduplication_scholarly_articles.ipynb) [![Open github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/pinecone-io/examples/blob/master/search/semantic-search/deduplication/deduplication_scholarly_articles.ipynb)   This notebook demonstrates how to use Pinecone's similarity search to create a simple application to identify duplicate documents.   The goal is to create a data deduplication application for eliminating near-duplicate copies of academic texts. In this example, we will perform the deduplication of a given text in two steps. First, we will sift a small set of candidate texts using a similarity-search service. Then, we will apply a near-duplication detector over these candidates.   The similarity search will use a vector representation of the texts. With this, semantic similarity is translated to proximity in a vector space. For detecting near-duplicates, we will employ a classification model that examines the raw text.   ## Install Dependencies   ```python !pip install -qU pinecone-client !pip install -qU datasketch mmh3 ipywidgets !pip install -qU gensim==4.0.1 !pip install -qU sentence-transformers --no-cache-dir !pip install -qU datasets ```  ## Download and Process Dataset  This tutorial will use the [Deduplication Dataset 2020](https://core.ac.uk/documentation/dataset/), which consists of 100,000 scholarly documents. We will use Hugging Face Datasets to download the dataset found at [*pinecone/core-2020-05-10-deduplication*](https://huggingface.co/datasets/pinecone/core-2020-05-10-deduplication).   ```python from datasets import load_dataset  core = load_dataset(\"pinecone/core-2020-05-10-deduplication\", split=\"train\") core ```                Dataset({         features: ['core_id', 'doi', 'original_abstract', 'original_title', 'processed_title', 'processed_abstract', 'cat', 'labelled_duplicates'],         num_rows: 100000     })    We convert the dataset into Pandas dataframe format like so:   ```python df = core.to_pandas() df.head() ```      <div id=\"df-ac4b6c06-6b41-4df5-8555-e7ca549dcea5\">   <div class=\"colab-df-container\">     <style scoped>         .dataframe tbody tr th:only-of-type {             vertical-align: middle;         }         .dataframe tbody tr th {             vertical-align: top;         }         .dataframe thead th {             text-align: right;         }     </style>     <table class=\"dataframe\">       <thead>         <tr style=\"text-align: right;\">           <th></th>           <th>core_id</th>           <th>doi</th>           <th>original_abstract</th>           <th>original_title</th>           <th>processed_title</th>           <th>processed_abstract</th>           <th>cat</th>           <th>labelled_duplicates</th>         </tr>       </thead>       <tbody>         <tr>           <th>0</th>           <td>11251086</td>           <td>10.1016/j.ajhg.2007.12.013</td>           <td>Unobstructed vision requires a particular refr...</td>           <td>Mutation of solute carrier SLC16A12 associates...</td>           <td>mutation of solute carrier slc16a12 associates...</td>           <td>unobstructed vision refractive lens differenti...</td>           <td>exact_dup</td>           <td>[82332306]</td>         </tr>         <tr>           <th>1</th>           <td>11309751</td>           <td>10.1103/PhysRevLett.101.193002</td>           <td>Two-color multiphoton ionization of atomic hel...</td>           <td>Polarization control in two-color above-thresh...</td>           <td>polarization control in two-color above-thresh...</td>           <td>multiphoton ionization helium combining extrem...</td>           <td>exact_dup</td>           <td>[147599753]</td>         </tr>         <tr>           <th>2</th>           <td>11311385</td>           <td>10.1016/j.ab.2011.02.013</td>           <td>Lectin’s are proteins capable of recognising a...</td>           <td>Optimisation of the enzyme-linked lectin assay...</td>           <td>optimisation of the enzyme-linked lectin assay...</td>           <td>lectin’s capable recognising oligosaccharide t...</td>           <td>exact_dup</td>           <td>[147603441]</td>         </tr>         <tr>           <th>3</th>           <td>11992240</td>           <td>10.1016/j.jpcs.2007.07.063</td>           <td>In this work, we present a detailed transmissi...</td>           <td>Vertical composition fluctuations in (Ga,In)(N...</td>           <td>vertical composition fluctuations in (ga,in)(n...</td>           <td>microscopy interfacial uniformity wells grown ...</td>           <td>exact_dup</td>           <td>[148653623]</td>         </tr>         <tr>           <th>4</th>           <td>11994990</td>           <td>10.1016/S0169-5983(03)00013-3</td>           <td>Three-dimensional (3D) oscillatory boundary la...</td>           <td>Three-dimensional streaming flows driven by os...</td>           <td>three-dimensional streaming flows driven by os...</td>           <td>oscillatory attached deformable walls boundari...</td>           <td>exact_dup</td>           <td>[148656283]</td>         </tr>       </tbody>     </table>   </div> </div>     We will use the following columns from the dataset for our task. 1. **core_id** - Unique indentifier for each article  2. **processed_abstract** - This is obtained by applying preprocssing steps like [this](https://spacy.io/usage/processing-pipelines) to the original abstract of the article from the column **original abstract**.  3. **processed_title** - Same as the abstract but for the title of the article.  4. **cat** - Every article falls into one of the three possible categories: 'exact_dup', 'near_dup', 'non_dup'  5. **labelled_duplicates** - A list of core_ids of articles that are duplicates of current article  Let's calculate the frequency of duplicates per article. Observe that half of the articles have no duplicates, and only a small fraction of the articles have more than ten duplicates.   ```python lens = df.labelled_duplicates.apply(len) lens.value_counts() ```         0     50000     1     36166     2      7620     3      3108     4      1370     5       756     6       441     7       216     8       108     10       66     9        60     11       48     13       28     12       13     Name: labelled_duplicates, dtype: int64    Reformat some of the columns to prevent later issues.   ```python # Make sure no processed abstracts are excessively long for upsert to Pinecone df[\"processed_abstract\"] = df[\"processed_abstract\"].str[:8000] ```  We will make use of the text data to create vectors for every article. We combine the **processed_abstract** and **processed_title** of the article to create a new **combined_text** column.    ```python # Define a new column for calculating embeddings df[\"combined_text\"] = df[\"processed_title\"] + \" \" + df[\"processed_abstract\"] ```  ## Initialize Pinecone Index   ```python import pinecone  # Connect to pinecone environment pinecone.init(     api_key=\"YOUR_API_KEY\",     environment=\"YOUR_ENVIRONMENT\" )  # Pick a name for the new index index_name = \"deduplication\"  # Check if the deduplication index exists if index_name not in pinecone.list_indexes():     # Create the index if it does not exist     pinecone.create_index(         index_name,         dimension=300,         metadata_config={\"indexed\": [\"processed_abstract\"]}     )  # Connect to deduplication index we created index = pinecone.Index(index_name) ``` [Get a free Pinecone API key](https://www.pinecone.io/start/) if you don’t have one already. You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**.   ## Initialize Embedding Model  We will use the [Average Word Embedding GloVe](https://nlp.stanford.edu/projects/glove/) model to transform text into vector embeddings. We then upload the embeddings into the Pinecone vector index.   ```python import torch from sentence_transformers import SentenceTransformer  # set device to GPU if available device = 'cuda' if torch.cuda.is_available() else 'cpu' model = SentenceTransformer(\"average_word_embeddings_glove.6B.300d\", device=device) model ```         SentenceTransformer(       (0): WordEmbeddings(         (emb_layer): Embedding(400001, 300)       )       (1): Pooling({'word_embedding_dimension': 300, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})     )    ## Generate Embeddings and Upsert   ```python from tqdm.auto import tqdm  # We will use batches of 256 batch_size = 256 for i in tqdm(range(0, len(df), batch_size)):     # Find end of batch     i_end = min(i+batch_size, len(df))     # Extract batch     batch = df.iloc[i:i_end]     # Generate embeddings for batch     emb = model.encode(batch[\"combined_text\"].to_list()).tolist()     # extract both indexed and not indexed metadata     meta = batch[[\"processed_abstract\"]].to_dict(orient=\"records\")     # create IDs     ids = batch.core_id.astype(str)     # add all to upsert list     to_upsert = list(zip(ids, emb, meta))     # upsert/insert these records to pinecone     _ = index.upsert(vectors=to_upsert)      # check that we have all vectors in index index.describe_index_stats() ```          100%|██████████| 391/391 [03:25<00:00, 2.47it/s]           {'dimension': 300,      'index_fullness': 0.1,      'namespaces': {'': {'vector_count': 100000}}}    ## Searching for Candidates  Now that we have created vectors for the articles and inserted them in the index, we will create a test set for querying. For each article in the test set we will query the index to get the most similar articles, they are the candidates on which we will performs the next classification step.  Below, we list statistics of the number of duplicates per article in the resulting test set.   ```python import math  # Create a sample from the dataset SAMPLE_FRACTION = 0.002 test_documents = (     df.groupby(df.labelled_duplicates.map(len))     .apply(lambda x: x.head(math.ceil(len(x) * SAMPLE_FRACTION)))     .reset_index(drop=True) )  print(\"Number of documents with specified number of duplicates:\") lens = test_documents.labelled_duplicates.apply(len) lens.value_counts() ```      Number of documents with specified number of duplicates:     0     100     1      73     2      16     3       7     4       3     5       2     6       1     7       1     8       1     9       1     10      1     11      1     12      1     13      1     Name: labelled_duplicates, dtype: int64     ```python # Use the model to create embeddings for test articles, which will be the query vectors query_vectors = model.encode(test_documents.combined_text.to_list()).tolist() ```   ```python # Query the vector index query_results = [] for xq in tqdm(query_vectors):     query_res = index.query(xq, top_k=100, include_metadata=True)     query_results.append(query_res) ```       100%|██████████| 209/209 [01:01<00:00, 3.54it/s]    ```python # Save all retrieval recalls into a list recalls = []  for id, res in tqdm(list(zip(test_documents.core_id.values, query_results))):     # Find document with id in labelled dataset     labeled_df = df[df.core_id.astype(str) == str(id)]     # Calculate the retrieval recall     top_k_list = set([match.id for match in res.matches])     labelled_duplicates = set(labeled_df.labelled_duplicates.values[0])     intersection = top_k_list.intersection(labelled_duplicates)     if len(labelled_duplicates) != 0:         recalls.append(len(intersection) / len(labelled_duplicates)) ```       100%|██████████| 0/209 [00:02<00:00, 104.50it/s]    ```python import statistics  print(\"Mean for the retrieval recall is \" + str(statistics.mean(recalls))) print(\"Standard Deviation is  \" + str(statistics.stdev(recalls))) ```      Mean for the retrieval recall is 0.9702529886016125     Standard Deviation is  0.16219287104729735       ### Running the Classifier   We mentioned earlier in the article that we will perform two steps for deduplication, searching to produce candidates and performing classifciation on them.  We will use Deduplication Classifier based on [LSH](https://en.wikipedia.org/wiki/Locality-sensitive_hashing) for detecting duplicates on the results from the previous step. We will run this on a sample of query results we got in the previous step. Feel free to try out the results on the entire set of query results.   ```python import pandas as pd from gensim.utils import tokenize from datasketch.minhash import MinHash from datasketch.lsh import MinHashLSH ```   ```python # Counters for correct/false predictions all_predictions = {\"Correct\": 0, \"False\": 0} predictions_per_category = {}  # From the results in the previous step, we will take a subset to test our classifier query_sample = query_results[::10] ids_sample = test_documents.core_id.to_list()[::10]  for id, res in zip(ids_sample, query_sample):          # Find document with id from the labelled dataset     labeled_df = df[df.core_id.astype(str) == str(id)]      \"\"\"     For every article in the result set, we store the scores and abstract of the articles most similar      to it, according to search in the previous step.     \"\"\"      df_result = pd.DataFrame(         {             \"id\": [match.id for match in res.matches],             \"document\": [match[\"metadata\"][\"processed_abstract\"] for match in res.matches],             \"score\": [match.score for match in res.matches],         }     )      print(df_result.head())      # We need content and labels for our classifier which we can get from the df_results     content = df_result.document.values     labels = list(df_result.id.values)          # Create MinHash for each of the documents in result set     min_hashes = {}     for label, text in zip(labels, content):         m = MinHash(num_perm=128, seed=5)         tokens = set(tokenize(text))         for d in tokens:             m.update(d.encode('utf8'))         min_hashes[label] = m          # Create LSH index     lsh = MinHashLSH(threshold=0.7, num_perm=128, )     for i, j in min_hashes.items():         lsh.insert(str(i), j)          query_minhash = min_hashes[str(id)]     duplicates = lsh.query(query_minhash)     duplicates.remove(str(id))          # Check whether prediction matches labeled duplicates. Here the groud truth is the set of duplicates from our original set     prediction = (         \"Correct\"         if set(labeled_df.labelled_duplicates.values[0]) == set(duplicates)         else \"False\"     )          # Add to all predictions     all_predictions[prediction] += 1          # Create and/or add to the specific category based on number of duplicates in original dataset     num_of_duplicates = len(labeled_df.labelled_duplicates.values[0])     if num_of_duplicates not in predictions_per_category:         predictions_per_category[num_of_duplicates] = [0, 0]      if prediction == \"Correct\":         predictions_per_category[num_of_duplicates][0] += 1     else:         predictions_per_category[num_of_duplicates][1] += 1      # Print the results for a document     print(         \"{}: expected: {}, predicted: {}, prediction: {}\".format(             id, labeled_df.labelled_duplicates.values[0], duplicates, prediction         )     ) ```               id                                           document     score     0  15080768  analyse centred methodology. discretisation so...  1.000000     1  52682462  audiencethe tissues pulses modelled compartmen...  0.787797     2  52900859  audiencethe tissues pulses modelled compartmen...  0.787797     3   2553555  multilayered illuminated acoustic electromagne...  0.781398     4  50544308  heterostructure schr dinger poisson numericall...  0.778778     15080768: expected: [], predicted: [], prediction: Correct               id                                           document     score     0   55110306  latrepirdine orally administered molecule init...  1.000000     1  188404434  cysteamine potentially numerous huntington dis...  0.903964     2   81634102  deutetrabenazine molecule deuterium attenuates...  0.880078     3   42021224  comorbidities. safe drugs available. efficacy ...  0.857741     4   78271101  promising prevent onset ultrahigh psychosis di...  0.849158     55110306: expected: [], predicted: [], prediction: Correct              id                                           document     score     0  10914205  read objectives schoolchildren sunscreen morni...  1.000000     1  77409456  overeating harmful alcohol tobacco aetiology c...  0.669037     2  10896024  sunlight cutaneous vitamin production. highlig...  0.633516     3  15070865  drink heavily nonstudent peers unaware drinkin...  0.633497     4  52131855  dette siste tekst versjon artikkelen inneholde...  0.627933     10914205: expected: [], predicted: [], prediction: Correct              id                                           document     score     0  43096919  publishedcomparative studymulticenter tcontext...  1.000000     1  77165332  cerebral amyloid aggregation pathological alzh...  0.871247     2  70343569  neurodegenerative heterogeneous disorders prog...  0.867806     3  18448676  beta amyloid beta deposition hallmarks alzheim...  0.855655     4  46964510  alzheimer unexplained. sought loci detect robu...  0.855137     43096919: expected: [], predicted: [], prediction: Correct              id                                           document     score     0  12203626  hypernatremia recipients homografts postoperat...  1.000000     1  82542813  abstractobjectivesto intravenous maintenance f...  0.800283     2  81206306  uromodulin tamm–horsfall abundant excreted uri...  0.794892     3  36026525  drinking sodium bicarbonated mineral cardiovas...  0.793452     4  83567081  drinking sodium bicarbonated mineral cardiovas...  0.793252     12203626: expected: [], predicted: [], prediction: Correct               id                                           document     score     0   15070865  drink heavily nonstudent peers unaware drinkin...  1.000000     1  154671698  updated alcohol suicidal level. searches retri...  0.889408     2   52132897  updated alcohol suicidal level. searches retri...  0.889408     3   43606482  fulltext .pdf publisher effectiveness drinking...  0.883402     4   82484980  abstractthe effectiveness drinking motive tail...  0.883145     15070865: expected: [], predicted: [], prediction: Correct               id                                           document     score     0   80341690  potentially inappropriate medicines pims older...  1.000000     1   39320843  elderly receive medications adverse effects. e...  0.807533     2   82162292  abstractbackgroundrisk assessments widely pred...  0.780006     3   77027179  assessments widely predict opioid disorder unc...  0.779406     4  153514317  yesbackground challenging person dementia. beh...  0.757255     80341690: expected: [], predicted: [], prediction: Correct              id                                           document     score     0   9066821  commotio retinae opacification retina blunt oc...  1.000000     1  78051578  neovascular macular degeneration anti–vascular...  0.731147     2  86422032  automated lesions challenging diagnostic lesio...  0.703925     3  48174418  audiencewe propose voxelwise images. relies ge...  0.699708     4  52434306  audiencewe propose voxelwise images. relies ge...  0.699708     9066821: expected: [], predicted: [], prediction: Correct               id                                           document     score     0   15052827  indirect schizophrenia australia incidence cos...  1.000000     1  154860392  illness schizophrenia bipolar disorder depress...  0.795662     2   51964867  audiencebackground cholesterol lowering jupite...  0.791904     3   75913230  thesis characterize burden cardiovascular deme...  0.775635     4  154672015  aims depression anxiety myocardial infarction ...  0.765936     15052827: expected: [], predicted: [], prediction: Correct              id                                           document     score     0  12203661  glomerulonephritis serious hemoptysis. antiglo...  1.000000     1  12204810  twenty alagille syndrome underwent transplanta...  0.811871     2  52198725  audiencepatients autoimmune polyendocrine synd...  0.810457     3  47112592  audiencepatients autoimmune polyendocrine synd...  0.810457     4  52460385  audiencepatients autoimmune polyendocrine synd...  0.810457     12203661: expected: [], predicted: [], prediction: Correct              id                                           document     score     0  11251086  unobstructed vision refractive lens differenti...  1.000000     1  82332306  unobstructed vision refractive lens differenti...  1.000000     2  61371524  aims osmotic oxidative progression advancement...  0.839048     3  59036307  aims osmotic oxidative progression advancement...  0.839048     4  11249430  dysfunction cilia nearly ubiquitously solitary...  0.796622     11251086: expected: ['82332306'], predicted: ['82332306'], prediction: Correct               id                                           document     score     0   12001088  presents vision successfully discriminates wee...  1.000000     1  148662402  presents vision successfully discriminates wee...  1.000000     2  148666025  proposes oriented crop maize weed pressure. vi...  0.904243     3   18424329  proposes oriented crop maize weed pressure. vi...  0.904243     4   18424394  proposes oriented identifying crop rows maize ...  0.861464     12001088: expected: ['148662402'], predicted: ['148662402'], prediction: Correct               id                                           document     score     0   11307919  reflectance exciton–polariton film polycrystal...  1.000000     1  147595688  reflectance exciton–polariton film polycrystal...  1.000000     2  147595695  photoluminescence reflectance oriented polycry...  0.816958     3   11307922  photoluminescence reflectance oriented polycry...  0.816958     4   33106913  macroscopic dielectric polycrystalline commonl...  0.804686     147595688: expected: ['11307919'], predicted: ['11307919'], prediction: Correct               id                                           document     score     0   12002296  thanks inherent probabilistic graphical prime ...  1.000000     1  148663921  thanks inherent probabilistic graphical prime ...  1.000000     2   52634130  audienceobject oriented brms platform automati...  0.869993     3   52294731  audienceobject oriented brms platform automati...  0.869993     4   34403460  acceptance artificial intelligence aims learn ...  0.865814     148663921: expected: ['12002296'], predicted: ['12002296'], prediction: Correct               id                                           document     score     0  151641478  stabilised soems unstable aircraft presented. ...  1.000000     1   11874260  stabilised soems unstable aircraft presented. ...  1.000000     2   29528077  projection snapshot balanced truncation unstab...  0.724496     3   77005252  projection snapshot balanced truncation unstab...  0.724496     4  148663435  ideas robust computationally amenable industri...  0.722027     151641478: expected: ['11874260'], predicted: ['11874260'], prediction: Correct               id                                           document     score     0  188365084  installed rapidly decade deployments deeper wa...  1.000000     1  158351487  installed rapidly decade deployments deeper wa...  1.000000     2  158370190  offshore turbine reliability biggest paper. un...  0.853790     3   83926778  offshore turbine reliability biggest paper. un...  0.853790     4   74226591  investigates overruns underruns occurring onsh...  0.834363     188365084: expected: ['158351487'], predicted: ['158351487'], prediction: Correct              id                                           document     score     0   2097371  propose vulnerability network. analogy balls l...  1.000000     1   9030380  propose vulnerability network. analogy balls l...  1.000000     2  49270269  audiencethis introduces validates sensor propa...  0.754055     3  43094896  peer reviewed brownjohn displacement sensor co...  0.745553     4  49271868  audiencea predictive giving displacement digit...  0.734554     2097371: expected: ['9030380'], predicted: ['9030380'], prediction: Correct               id                                           document     score     0  148674298  race segments swimmers. analysed finals sessio...  1.000000     1   33176265  race segments swimmers. analysed finals sessio...  1.000000     2  148674300  swimming race parameters. hundred fifty eight ...  0.886608     3   33176267  swimming race parameters. hundred fifty eight ...  0.886608     4  143900637  swimmers swimmers coaches trainers. video sens...  0.736030     33176265: expected: ['148674298'], predicted: ['148674298'], prediction: Correct              id                                           document     score     0  52844591  audiencehere geochemical lopevi volcano volcan...  1.000000     1  52308905  audiencehere geochemical lopevi volcano volcan...  1.000000     2  52722823  audiencehere geochemical lopevi volcano volcan...  1.000000     3  52717537  audiencethe volcanism cameroon volcanic mantle...  0.893717     4  52840980  audiencethe volcanism cameroon volcanic mantle...  0.893717     52308905: expected: ['52722823' '52844591'], predicted: ['52722823', '52844591'], prediction: Correct              id                                           document     score     0  44119402  lagrangian formalism supermembrane supergravit...  1.000000     1  35093363  lagrangian formalism supermembrane supergravit...  1.000000     2   2531039  lagrangian formalism supermembrane supergravit...  1.000000     3  35078501  lagrangian formalism supermembrane supergravit...  1.000000     4  35089833  supergravity correlators worldsheet analogous ...  0.847565     44119402: expected: ['2531039' '35078501' '35093363'], predicted: ['2531039', '35078501', '35093363'], prediction: Correct               id                                           document  score     0   52739626  microlensing surveys tens millions stars. unpr...    1.0     1   52456923  microlensing surveys tens millions stars. unpr...    1.0     2   47110549  microlensing surveys tens millions stars. unpr...    1.0     3   52695218  microlensing surveys tens millions stars. unpr...    1.0     4  152091185  microlensing surveys tens millions stars. unpr...    1.0     47110549: expected: ['46770666' '52456923' '152091185' '52695218' '52739626'], predicted: ['52456923', '52695218', '52739626', '152091185', '46770666'], prediction: Correct        ```python all_predictions ```         {'Correct': 21, 'False': 0}     ```python # Overall accuracy on a test accuracy = round(     all_predictions[\"Correct\"]     / (all_predictions[\"Correct\"] + all_predictions[\"False\"]),     4, ) accuracy ```         1.0     ```python # Print the prediction count for each class depending on the number of duplicates in labeled dataset pd.DataFrame.from_dict(     predictions_per_category, orient=\"index\", columns=[\"Correct\", \"False\"] ) ```      <div id=\"df-fc5ae990-df9a-48b2-8269-ff3efe85e71f\">   <div class=\"colab-df-container\">     <style scoped>         .dataframe tbody tr th:only-of-type {             vertical-align: middle;         }         .dataframe tbody tr th {             vertical-align: top;         }         .dataframe thead th {             text-align: right;         }     </style>     <table class=\"dataframe\">       <thead>         <tr style=\"text-align: right;\">           <th></th>           <th>Correct</th>           <th>False</th>         </tr>       </thead>       <tbody>         <tr>           <th>0</th>           <td>10</td>           <td>0</td>         </tr>         <tr>           <th>1</th>           <td>8</td>           <td>0</td>         </tr>         <tr>           <th>2</th>           <td>1</td>           <td>0</td>         </tr>         <tr>           <th>3</th>           <td>1</td>           <td>0</td>         </tr>         <tr>           <th>5</th>           <td>1</td>           <td>0</td>         </tr>       </tbody>     </table>   </div> </div>      ## Delete the Index Delete the index once you are sure that you do not want to use it anymore. Once the index is deleted, you cannot use it again.     ```python # Delete the index if it's not going to be used anymore pinecone.delete_index(index_name) ```  ## Summary  In this notebook we demonstrate how to perform a deduplication task of over 100,000 articles using Pinecone. With articles embedded as vectors, you can use Pinecone's vector index to find similar articles. For each query article, we then use an LSH classifier on the similar articles to identify duplicate articles. Overall, we show that it is ease to incorporate Pinecone wtih article embedding models and duplication classifiers to build a deduplication service.  ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e253"
  },
  "title": "Product Recommender",
  "category": "630fc5235d91a70054705fb7",
  "content": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/recommendation/product-recommender/product_recommender.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/recommendation/product-recommender/product_recommender.ipynb) [![Open github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/pinecone-io/examples/blob/master/recommendation/product-recommender/product_recommender.ipynb)  Learn how to build a product recommendation engine using collaborative filtering and Pinecone.  In this example, we will generate product recommendations for ecommerce customers based on previous orders and trending items. This example covers preparing the vector embeddings, creating and deploying the Pinecone service, writing data to Pinecone, and finally querying Pinecone to receive a ranked list of recommended products.  ## Data Preparation  **Import Python Libraries**   ```python import os import time import random import numpy as np import pandas as pd import scipy.sparse as sparse import itertools ```  **Load the (Example) Instacart Data**  We are going to use the [Instacart Market Basket Analysis](https://www.kaggle.com/c/instacart-market-basket-analysis/data) dataset for this task.  The data used throughout this example is a set of files describing customers' orders over time. The main focus is on the *orders.csv* file, where each line represents a relation between a user and the order. In other words, each line has information on *userid* (user who made the order) and *orderid*. Note there is no information about products in this table. Product information related to specific orders is stored in the *order_product__*.csv* dataset.   ```python order_products_train = pd.read_csv('data/order_products__train.csv') order_products_prior = pd.read_csv('data/order_products__prior.csv') products = pd.read_csv('data/products.csv') orders = pd.read_csv('data/orders.csv')  order_products = order_products_train.append(order_products_prior) ```  **Preparing data for the model**   The Collaborative Filtering model used in this example requires only users’ historical preferences on a set of items. As there is no explicit rating in the data we are using, the purchase quantity can represent a “confidence” in terms of how strong the interaction was between the user and the products.  The dataframe data will store this data and will be the base for the model.   ```python customer_order_products = pd.merge(orders, order_products, how='inner',on='order_id')  # creating a table with \"confidences\" data = customer_order_products.groupby(['user_id', 'product_id'])[['order_id']].count().reset_index() data.columns=[\"user_id\", \"product_id\", \"total_orders\"] data.product_id = data.product_id.astype('int64')  # Create a lookup frame so we can get the product names back in readable form later. products_lookup = products[['product_id', 'product_name']].drop_duplicates() products_lookup['product_id'] = products_lookup.product_id.astype('int64') ```  We will create three prototype users here and add them to our data dataframe. Each user will be buying only a specific product: - The first user will be buying only **Mineral Water** - The second user will be buying baby products: **No More Tears Baby Shampoo** and **Baby Wash & Shampoo**  These users will be later used for querying and examination of the model results.   ```python data_new = pd.DataFrame([[data.user_id.max() + 1, 22802, 97],                          [data.user_id.max() + 2, 26834, 89],                          [data.user_id.max() + 2, 12590, 77]                         ], columns=['user_id', 'product_id', 'total_orders']) data_new ```  <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     } </style> <table  class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>user_id</th>       <th>product_id</th>       <th>total_orders</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>206210</td>       <td>22802</td>       <td>97</td>     </tr>     <tr>       <th>1</th>       <td>206211</td>       <td>26834</td>       <td>89</td>     </tr>     <tr>       <th>2</th>       <td>206211</td>       <td>12590</td>       <td>77</td>     </tr>   </tbody> </table> </div>     ```python data = data.append(data_new).reset_index(drop = True) data.tail() ```     <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     } </style> <table  class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>user_id</th>       <th>product_id</th>       <th>total_orders</th>     </tr>   </thead>   <tbody>     <tr>       <th>13863744</th>       <td>206209</td>       <td>48697</td>       <td>1</td>     </tr>     <tr>       <th>13863745</th>       <td>206209</td>       <td>48742</td>       <td>2</td>     </tr>     <tr>       <th>13863746</th>       <td>206210</td>       <td>22802</td>       <td>97</td>     </tr>     <tr>       <th>13863747</th>       <td>206211</td>       <td>26834</td>       <td>89</td>     </tr>     <tr>       <th>13863748</th>       <td>206211</td>       <td>12590</td>       <td>77</td>     </tr>   </tbody> </table> </div>    In the next step, we will first extract user and item unique ids, in order to create a CSR (Compressed Sparse Row) matrix.     ```python users = list(np.sort(data.user_id.unique())) items = list(np.sort(products.product_id.unique())) purchases = list(data.total_orders)  # create zero-based index position <-> user/item ID mappings index_to_user = pd.Series(users)  # create reverse mappings from user/item ID to index positions user_to_index = pd.Series(data=index_to_user.index + 1, index=index_to_user.values)  # create zero-based index position <-> item/user ID mappings index_to_item = pd.Series(items)  # create reverse mapping from item/user ID to index positions item_to_index = pd.Series(data=index_to_item.index, index=index_to_item.values)  # Get the rows and columns for our new matrix products_rows = data.product_id.astype(int) users_cols = data.user_id.astype(int)  # Create a sparse matrix for our users and products containing number of purchases sparse_product_user = sparse.csr_matrix((purchases, (products_rows, users_cols)), shape=(len(items) + 1, len(users) + 1)) sparse_product_user.data = np.nan_to_num(sparse_product_user.data, copy=False)  sparse_user_product = sparse.csr_matrix((purchases, (users_cols, products_rows)), shape=(len(users) + 1, len(items) + 1)) sparse_user_product.data = np.nan_to_num(sparse_user_product.data, copy=False) ```  ## Implicit Model  In this section we will demonstrate creation and training of a recommender model using the **implicit** library. The recommendation model is based off the algorithms described in the paper [Collaborative Filtering for Implicit Feedback Datasets](https://www.researchgate.net/publication/220765111_Collaborative_Filtering_for_Implicit_Feedback_Datasets) with performance optimizations described in [Applications of the Conjugate Gradient Method for Implicit Feedback Collaborative Filtering](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.379.6473&rep=rep1&type=pdf).    ```python !pip install --quiet -U implicit ```   ```python import implicit from implicit import evaluation  #split data into train and test sets train_set, test_set = evaluation.train_test_split(sparse_product_user, train_percentage=0.9)  # initialize a model model = implicit.als.AlternatingLeastSquares(factors=100,                                              regularization=0.05,                                              iterations=50,                                              num_threads=1)  alpha_val = 15 train_set = (train_set * alpha_val).astype('double')  # train the model on a sparse matrix of item/user/confidence weights model.fit(train_set, show_progress = True) ```      WARNING:root:OpenBLAS detected. Its highly recommend to set the environment variable 'export OPENBLAS_NUM_THREADS=1' to disable its internal multithreading          0%|          | 0/50 [00:00<?, ?it/s]   We will evaluate the model using the inbuilt library function   ```python test_set = (test_set * alpha_val).astype('double') evaluation.ranking_metrics_at_k(model, train_set.T, test_set.T, K=100,                          show_progress=True, num_threads=1) ```         0%|          | 0/206212 [00:00<?, ?it/s]          {'precision': 0.27489359984895717,      'map': 0.04460861877969595,      'ndcg': 0.14436536385146576,      'auc': 0.6551648380086259}    This is what item and user factors look like. These vectors will be stored in our vector index later and used for recommendation.   ```python model.item_factors[1:3] ```         array([[ 0.01009897,  0.00260342,  0.00165942,  0.01748168,  0.00649343,             -0.01647822,  0.01860397, -0.01009837,  0.01125452,  0.01987451,             -0.00579512,  0.00421128,  0.01707346, -0.00212536,  0.01915585,              0.03640049, -0.01142028,  0.01023709,  0.00446458, -0.00143529,             -0.00024208,  0.00909473, -0.01408565,  0.02619351,  0.00210135,             -0.00378899,  0.01231347,  0.00278133,  0.00071992,  0.00915809,              0.01640408,  0.00880539, -0.00648519, -0.01160682,  0.00664212,             -0.00406996,  0.01543106,  0.00690582,  0.00898032,  0.00277333,              0.00626428, -0.01610408,  0.01018737,  0.0008459 ,  0.02026955,             -0.01055363, -0.00107795,  0.01484767,  0.01800155, -0.00275021,             -0.0018283 , -0.00346971,  0.00077051, -0.01080908,  0.00037001,             -0.00290308,  0.00491365, -0.01362148, -0.00129594,  0.00192484,              0.00101756, -0.00051836,  0.00603317,  0.01611738,  0.00511096,             -0.0053055 ,  0.01907502,  0.01232757,  0.01042075,  0.01301588,              0.00567376,  0.0152219 ,  0.02414433,  0.01395251,  0.00916175,              0.01294622,  0.00187435,  0.01768819,  0.01806206,  0.01500281,              0.01065951,  0.02733074,  0.00765102,  0.00435439, -0.01976543,              0.01680202,  0.00840835,  0.00042277, -0.00216795,  0.00113048,             -0.00012699,  0.01142939,  0.01374972, -0.00985129,  0.00935802,              0.00541372,  0.01037668,  0.02024015, -0.00793628, -0.00261189],            [ 0.00088747,  0.00581244,  0.00074211,  0.00428396,  0.00124957,              0.00699728,  0.00304013,  0.00676518,  0.00414387,  0.00205417,              0.0029335 ,  0.00505301,  0.00522107,  0.00404108,  0.00236721,              0.00406507,  0.00101947,  0.00298186,  0.00049156,  0.00279067,              0.00343525,  0.00175488,  0.00907208,  0.00276436,  0.00414505,              0.00458229,  0.00363405,  0.00375954,  0.00198171,  0.00270804,              0.00479605,  0.00120687,  0.00249341,  0.00051512, -0.00110135,              0.00844493,  0.00641403,  0.00101385,  0.00484058,  0.00632413,              0.00334539,  0.00232208,  0.00288551,  0.00755766,  0.00279979,              0.00587453,  0.00742234,  0.00580525,  0.00412665,  0.00347631,              0.00433106,  0.00427196,  0.00670939,  0.00304596,  0.00385384,              0.00222394,  0.00511582,  0.00354225,  0.00200116,  0.00717725,              0.00186237,  0.00434178,  0.00102088,  0.00222063,  0.00230367,              0.00420666,  0.00698098,  0.00549557,  0.00345657,  0.00642341,              0.00036   ,  0.00464778,  0.00284442,  0.00530352,  0.00218676,              0.00493103,  0.00179086,  0.0041003 ,  0.00497837,  0.0068793 ,              0.00429972,  0.00396508,  0.00451153,  0.00486684,  0.00272128,              0.00467645,  0.00423267,  0.00388015,  0.00339444,  0.00115735,              0.00807636,  0.00298532,  0.00143811,  0.00293057,  0.00590145,              0.00418158,  0.00488713,  0.00097365, -0.00083799,  0.00363581]],           dtype=float32)     ```python model.user_factors[1:3] ```         array([[ 7.24285245e-01,  5.59004486e-01,  4.96992081e-01,             -4.15437818e-01, -1.94785964e+00, -2.23764396e+00,             -1.76767483e-02, -2.21530461e+00, -6.52559578e-01,              2.78620571e-01,  6.03808701e-01,  1.27670407e-01,              3.06052566e-01, -9.93388355e-01, -5.34315288e-01,              1.20948291e+00, -2.11217976e+00,  1.67127061e+00,              1.03314137e+00,  8.54326487e-01,  1.85733151e+00,              5.69297194e-01, -8.93577933e-01,  1.76394248e+00,              1.28939009e+00,  3.32375497e-01, -2.60327369e-01,              4.21450347e-01, -1.72091925e+00,  1.10491872e+00,             -1.86411276e-01, -3.51959467e-02, -1.41517222e+00,             -9.19971287e-01,  4.63204056e-01, -4.07809407e-01,              1.23038590e+00, -8.25872004e-01, -1.50579488e+00,              8.65903348e-02, -7.29649186e-01, -5.21384776e-01,              1.59157085e+00, -8.51297379e-01,  2.81686401e+00,             -8.55669677e-01, -3.48052949e-01, -5.16085029e-01,              8.01080287e-01,  1.04207866e-01, -2.72860657e-02,             -5.18645883e-01, -1.77561533e+00, -1.22266948e+00,             -1.74415603e-01,  3.58568132e-01, -8.37117255e-01,             -1.45265543e+00,  2.43810445e-01,  5.80842435e-01,             -5.91480255e-01,  1.29645097e+00,  1.47483099e+00,             -6.84086800e-01, -7.20921755e-01, -1.11399984e+00,              2.38089368e-01,  2.19725475e-01,  3.29073220e-01,             -6.45937538e-03,  2.44079873e-01,  1.26761782e+00,              7.07967520e-01,  1.21964478e+00,  1.10735869e+00,              1.02583379e-01, -2.92189389e-01,  5.52688181e-01,              1.61700773e+00,  5.11932790e-01, -2.67194122e-01,              1.47362947e+00, -1.13380539e+00,  1.40330446e+00,              4.91484731e-01,  1.36100423e+00,  1.80482656e-01,              9.14917171e-01,  6.22740746e-01, -1.88607132e+00,             -1.34071469e+00, -2.27820247e-01,  1.15018475e+00,             -1.23491549e+00, -4.78476077e-01, -4.65549737e-01,              9.11170244e-01,  2.07606936e+00,  1.04314007e-01,              1.81862903e+00],            [ 8.30793440e-01,  3.86868089e-01, -1.63957000e-01,              6.93703368e-02,  1.53786719e+00, -5.87535620e-01,              3.72619987e+00,  1.22163899e-01, -8.54973614e-01,              1.11186251e-01, -1.42095876e+00, -8.75619590e-01,             -1.81247914e+00, -9.44502056e-01,  8.14570427e-01,             -5.43736219e-01, -6.02845371e-01,  2.01962996e+00,              1.60777140e+00,  2.20254612e+00,  2.08239055e+00,              8.16642225e-01, -4.42571700e-01,  6.22263908e-01,              6.29432023e-01, -1.16571808e+00,  2.32731175e+00,             -1.12640738e+00,  1.60938001e+00,  4.67458010e+00,             -1.46235943e+00,  1.46000063e+00,  1.11922979e-01,             -2.55218220e+00,  7.85077095e-01,  8.50843608e-01,             -1.10671151e+00, -6.06540870e-03,  2.76003122e-01,             -9.57318366e-01, -1.30121040e+00, -3.81188631e-01,              2.17489243e+00,  8.48001361e-01,  2.24089599e+00,             -1.32857335e+00,  9.44799244e-01,  2.29169533e-01,              1.10746622e+00, -3.48530680e-01, -2.12854624e+00,              4.96270150e-01, -1.30754066e+00,  1.41697776e+00,              2.73206377e+00,  1.48888981e+00, -1.58728147e+00,              1.58903934e-03,  1.66406441e+00, -1.75263867e-01,              2.02891684e+00, -1.95949566e+00,  1.52711666e+00,              8.71322572e-01,  1.82597125e+00,  1.37408182e-01,             -1.81464672e+00, -1.04905093e+00, -2.37590694e+00,              8.15740228e-01,  1.64217085e-01,  1.99734032e+00,             -1.54955173e+00, -5.57012379e-01,  1.32525837e+00,             -1.30014801e+00,  1.32985008e+00, -3.50400567e+00,              2.45490909e-01, -2.43037295e+00, -2.74685884e+00,             -2.12384558e+00, -1.42703640e+00, -6.69254959e-01,              1.30702591e+00, -2.15909433e+00,  1.44703603e+00,             -2.29611732e-02,  1.82583869e+00,  1.57409739e+00,             -3.97216320e-01, -6.94107652e-01,  2.89623165e+00,              2.33722359e-01, -5.27708590e-01,  1.04344904e+00,              8.51706207e-01, -4.50546294e-01,  1.38413882e+00,              2.07552814e+00]], dtype=float32)    ## Configure Pinecone  Install and setup Pinecone   ```python !pip install --quiet -U pinecone-client ```   ```python import pinecone ```   ```python # Load Pinecone API key api_key = os.getenv('PINECONE_API_KEY') or 'YOUR_API_KEY' # Set Pinecone environment. env = os.getenv('PINECONE_ENVIRONMENT') or 'YOUR_ENVIRONMENT'  pinecone.init(api_key=api_key, environment=env) ```  [Get a Pinecone API key](http://app.pinecone.io/) if you don't have one.   ```python #List all present indexes associated with your key, should be empty on the first run pinecone.list_indexes() ```         []    **Create an Index**   ```python # Set a name for your index index_name = 'shopping-cart-demo' ```   ```python # Make sure service with the same name does not exist if index_name in pinecone.list_indexes():     pinecone.delete_index(index_name) pinecone.create_index(name=index_name, dimension=100) ```  **Connect to the new index**   ```python index = pinecone.Index(index_name=index_name) ```  ## Load Data  Uploading all items (products that one can buy) and displaying some examples of products and their vector representations.    ```python # Get all of the items all_items = [title for title in products_lookup['product_name']]  # Transform items into factors items_factors = model.item_factors  # Prepare item factors for upload items_to_insert = list(zip(all_items, items_factors[1:].tolist())) display(items_to_insert[:2]) ```       [('Chocolate Sandwich Cookies',       [0.010098974220454693,        0.0026034200564026833,        0.0016594183398410678,        0.017481675371527672,        0.006493427790701389,        -0.016478220000863075,        0.018603969365358353,        -0.010098369792103767,        0.01125451922416687,        0.019874505698680878,        -0.005795117933303118,        0.00421128049492836,        0.017073458060622215,        -0.0021253626327961683,        0.019155845046043396,        0.036400485783815384,        -0.01142028160393238,        0.010237086564302444,        0.004464581608772278,        -0.0014352924190461636,        -0.00024208369723055512,        0.009094727225601673,        -0.014085653237998486,        0.02619350701570511,        0.002101349411532283,        -0.0037889881059527397,        0.012313470244407654,        0.002781332703307271,        0.0007199185783974826,        0.009158086962997913,        0.016404075548052788,        0.008805392310023308,        -0.006485185585916042,        -0.01160681527107954,        0.006642122287303209,        -0.004069960676133633,        0.015431062318384647,        0.006905817426741123,        0.008980315178632736,        0.002773326588794589,        0.0062642814591526985,        -0.0161040760576725,        0.010187366977334023,        0.0008458984084427357,        0.02026955410838127,        -0.010553630068898201,        -0.0010779497679322958,        0.014847667887806892,        0.018001552671194077,        -0.0027502067387104034,        -0.0018282983219251037,        -0.0034697114024311304,        0.000770510989241302,        -0.010809078812599182,        0.0003700107627082616,        -0.002903081476688385,        0.004913648124784231,        -0.01362148392945528,        -0.001295942347496748,        0.0019248360767960548,        0.0010175565257668495,        -0.0005183601751923561,        0.006033174227923155,        0.016117379069328308,        0.005110959522426128,        -0.00530549930408597,        0.019075021147727966,        0.012327569536864758,        0.01042074803262949,        0.01301588024944067,        0.005673760548233986,        0.015221904963254929,        0.024144325405359268,        0.01395251415669918,        0.009161749854683876,        0.012946223840117455,        0.0018743481487035751,        0.017688188701868057,        0.018062060698866844,        0.015002812258899212,        0.010659514926373959,        0.02733074128627777,        0.0076510170474648476,        0.0043543861247599125,        -0.019765431061387062,        0.016802024096250534,        0.008408350870013237,        0.0004227694298606366,        -0.002167945960536599,        0.0011304811341688037,        -0.0001269889180548489,        0.01142938993871212,        0.013749724254012108,        -0.00985129363834858,        0.009358019568026066,        0.0054137222468853,        0.010376684367656708,        0.020240148529410362,        -0.007936276495456696,        -0.0026118927635252476]),      ('All-Seasons Salt',       [0.0008874664781615138,        0.0058124433271586895,        0.0007421106565743685,        0.00428396463394165,        0.001249574706889689,        0.006997276097536087,        0.0030401344411075115,        0.006765175145119429,        0.004143866710364819,        0.0020541702397167683,        0.002933498937636614,        0.005053007043898106,        0.00522107258439064,        0.004041083622723818,        0.002367211040109396,        0.004065068904310465,        0.0010194696951657534,        0.0029818632174283266,        0.0004915563040412962,        0.0027906731702387333,        0.0034352506045252085,        0.0017548849573358893,        0.009072077460587025,        0.002764355158433318,        0.004145053215324879,        0.004582288675010204,        0.003634049789980054,        0.0037595359608531,        0.00198170798830688,        0.002708042971789837,        0.004796050023287535,        0.0012068713549524546,        0.0024934052489697933,        0.0005151224322617054,        -0.001101348432712257,        0.00844493042677641,        0.006414031144231558,        0.001013854518532753,        0.0048405807465314865,        0.006324129644781351,        0.0033453928772360086,        0.0023220758885145187,        0.002885512774810195,        0.007557660341262817,        0.002799794776365161,        0.005874533671885729,        0.007422335911542177,        0.0058052497915923595,        0.004126648418605328,        0.0034763067960739136,        0.004331058822572231,        0.004271955695003271,        0.00670938566327095,        0.0030459642875939608,        0.0038538381922990084,        0.0022239401005208492,        0.005115816835314035,        0.003542253514751792,        0.002001164946705103,        0.007177253719419241,        0.0018623704090714455,        0.004341782070696354,        0.0010208759922534227,        0.0022206329740583897,        0.002303670858964324,        0.004206661134958267,        0.006980976089835167,        0.005495565943419933,        0.003456572536379099,        0.006423408165574074,        0.0003599990450311452,        0.004647782538086176,        0.0028444179333746433,        0.005303522571921349,        0.0021867596078664064,        0.004931030794978142,        0.0017908598529174924,        0.0041002980433404446,        0.004978368990123272,        0.006879299879074097,        0.004299724940210581,        0.0039650811813771725,        0.004511528182774782,        0.00486684450879693,        0.0027212793938815594,        0.004676445387303829,        0.0042326669208705425,        0.003880152478814125,        0.003394442144781351,        0.0011573455994948745,        0.008076360449194908,        0.0029853193555027246,        0.0014381115324795246,        0.0029305710922926664,        0.005901449825614691,        0.004181584343314171,        0.004887125454843044,        0.0009736462379805744,        -0.0008379911305382848,        0.0036358062643557787])]   **Insert items into the index**   ```python def chunks(iterable, batch_size=100):     it = iter(iterable)     chunk = tuple(itertools.islice(it, batch_size))     while chunk:         yield chunk         chunk = tuple(itertools.islice(it, batch_size)) ```   ```python print('Index statistics before upsert:', index.describe_index_stats())  for e, batch in enumerate(chunks([(ii[:64],x) for ii,x in items_to_insert])):     index.upsert(vectors=batch)  print('Index statistics after upsert:', index.describe_index_stats()) ```      Index statistics before upsert: {'dimension': 0, 'namespaces': {}}     Index statistics after upsert: {'dimension': 100, 'namespaces': {'': {'vector_count': 49677}}}   This is a helper method for analysing recommendations later. This method returns top N products that someone bought in the past (based on product quantity).   ```python def products_bought_by_user_in_the_past(user_id: int, top: int = 10):      selected = data[data.user_id == user_id].sort_values(by=['total_orders'], ascending=False)      selected['product_name'] = selected['product_id'].map(products_lookup.set_index('product_id')['product_name'])     selected = selected[['product_id', 'product_name', 'total_orders']].reset_index(drop=True)     if selected.shape[0] < top:         return selected      return selected[:top] ```   ```python data.tail() ```     <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     } </style> <table  class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>user_id</th>       <th>product_id</th>       <th>total_orders</th>     </tr>   </thead>   <tbody>     <tr>       <th>13863744</th>       <td>206209</td>       <td>48697</td>       <td>1</td>     </tr>     <tr>       <th>13863745</th>       <td>206209</td>       <td>48742</td>       <td>2</td>     </tr>     <tr>       <th>13863746</th>       <td>206210</td>       <td>22802</td>       <td>97</td>     </tr>     <tr>       <th>13863747</th>       <td>206211</td>       <td>26834</td>       <td>89</td>     </tr>     <tr>       <th>13863748</th>       <td>206211</td>       <td>12590</td>       <td>77</td>     </tr>   </tbody> </table> </div>    ## Query for Recommendations  We are now retrieving user factors for users that we have manually created before for testing purposes. Besides these users, we are adding a random existing user. We are also displaying these users so you can see what these factors look like.   ```python user_ids = [206210, 206211, 103593] user_factors = model.user_factors[user_to_index[user_ids]]  display(user_factors[1:]) ```       array([[-2.446773  , -0.62870413, -0.9166386 , -1.0933994 ,  0.9897131 ,             -2.166681  ,  0.09873585,  1.1049409 ,  1.6753025 ,  1.5794269 ,              1.8142459 ,  1.5048354 ,  0.7157051 , -0.7888281 ,  0.06156079,             -1.6539581 , -0.15790005,  0.5999737 , -1.4803663 , -0.03179923,              0.91451246,  0.14260213, -1.1541293 , -0.01566206, -1.3449577 ,             -2.232925  , -0.88052607,  0.19183849,  0.3109626 ,  1.32479   ,              0.16483077, -0.8045166 ,  1.36922   ,  0.81774026,  1.3368418 ,              2.8871357 ,  2.4540865 , -1.908394  ,  2.8208447 , -1.3499558 ,             -0.90089166,  1.0632626 ,  1.8107275 , -0.83986664,  1.1764408 ,             -1.6621239 , -1.4636188 , -2.3367987 , -1.2510904 ,  0.4349534 ,              0.08233324,  1.0688674 , -0.41190436,  1.6045849 , -2.3667567 ,             -1.8557758 , -0.1931467 ,  0.10383442,  1.3932719 ,  1.3465406 ,             -0.17274773,  0.41542327, -1.0992794 ,  1.7954347 , -0.9157203 ,             -0.3183454 ,  0.7724282 , -0.5658835 ,  1.0758705 , -1.7377888 ,              2.0294137 , -2.1382923 ,  1.0606468 ,  1.800927  , -1.3713943 ,              1.0659586 ,  0.31013912, -0.5963934 ,  0.69738954,  1.383554  ,              1.0078012 , -2.7117298 , -1.7087    ,  0.4050448 ,  3.548174  ,              0.27247337, -0.16570352, -0.92676795, -1.2243328 ,  0.63455725,             -1.5337977 , -2.8735108 ,  1.2812912 , -0.11600056,  1.2358317 ,              0.5591759 , -0.63913107,  1.2325013 ,  1.3712876 , -1.3370212 ],            [ 1.70396   , -1.5320156 ,  2.8847353 ,  0.32170388,  1.3340172 ,             -1.1947397 ,  1.9013127 , -0.4816413 , -2.0899863 , -1.2761233 ,             -1.8430734 , -0.6221577 ,  0.8063771 ,  1.2961249 ,  0.18268324,             -3.2958453 , -0.31202024,  3.8049164 ,  0.73393685,  1.7682556 ,              0.372242  ,  1.002703  ,  0.32070097,  0.2046866 ,  0.9008953 ,              1.3807229 ,  1.1176021 ,  0.1957425 , -1.3196671 ,  2.1180258 ,              0.48846507,  0.76666814, -0.30274457, -2.5167181 ,  0.3489467 ,              2.0131872 , -1.5119745 , -0.91736513,  1.3228838 , -1.5192536 ,             -1.1463904 , -1.0334512 ,  1.2355485 , -0.21977787,  2.3017268 ,             -1.4751832 , -0.6216355 ,  0.3089897 , -0.85497165, -0.31444585,             -3.100829  ,  2.390458  ,  0.07399248, -0.09938905, -1.0162137 ,              1.9475894 , -0.9248195 , -1.084834  ,  0.39212215,  0.6491842 ,              1.2028612 , -1.0323097 ,  2.6522071 , -0.8172474 ,  1.0873827 ,             -2.9416876 , -0.06957518, -0.7316911 , -0.7430743 ,  0.319504  ,             -0.9984044 ,  0.06710945, -3.003772  ,  0.6744962 ,  2.1210036 ,             -0.4559903 ,  0.6154137 , -1.7743443 ,  0.5672013 ,  1.004357  ,             -1.8588076 ,  0.05864619,  0.01209994,  2.0575655 , -1.1680491 ,              0.3783967 ,  1.6527759 ,  1.5397102 , -0.2965242 ,  2.5335467 ,             -0.40009058, -0.66989446, -1.6143844 ,  0.7761751 , -1.0538983 ,              0.48226374,  1.2432365 ,  2.1671696 ,  1.7070205 ,  0.2968687 ]],           dtype=float32)   ### Model recommendations  We will now retrieve recommendations from our model directly, just to have these results as a baseline.   ```python print(\"Model recommendations\\n\")  start_time = time.process_time() recommendations0 = model.recommend(userid=user_ids[0], user_items=sparse_user_product) recommendations1 = model.recommend(userid=user_ids[1], user_items=sparse_user_product) recommendations2 = model.recommend(userid=user_ids[2], user_items=sparse_user_product) print(\"Time needed for retrieving recommended products: \" + str(time.process_time() - start_time) + ' seconds.\\n')  print('\\nRecommendations for person 0:') for recommendation in recommendations0:     product_id = recommendation[0]     print(products_lookup[products_lookup.product_id == product_id]['product_name'].values)  print('\\nRecommendations for person 1:') for recommendation in recommendations1:     product_id = recommendation[0]     print(products_lookup[products_lookup.product_id == product_id]['product_name'].values)  print('\\nRecommendations for person 2:') for recommendation in recommendations2:     product_id = recommendation[0]     print(products_lookup[products_lookup.product_id == product_id]['product_name'].values) ```      Model recommendations          Time needed for retrieving recommended products: 0.0625 seconds.               Recommendations for person 0:     ['Sparkling Water']     ['Soda']     ['Smartwater']     ['Zero Calorie Cola']     ['Natural Artesian Water']     ['Natural Spring Water']     ['Distilled Water']     ['Sparkling Natural Mineral Water']     ['Spring Water']     ['Drinking Water']          Recommendations for person 1:     ['Baby Wipes Sensitive']     ['YoKids Squeezers Organic Low-Fat Yogurt, Strawberry']     ['Organic Blackberries']     ['Organic Whole Milk']     ['Eggo Pancakes Minis']     ['Natural California Raisins Mini Snack Boxes']     ['100% Raw Coconut Water']     ['White Buttermints']     ['Danimals Strawberry Explosion Flavored Smoothie']     ['Strawberry Explosion/Banana Split Smoothie']          Recommendations for person 2:     ['Organic Golden Delicious Apple']     ['Organic Red Delicious Apple']     ['Bartlett Pears']     ['Organic Blackberries']     ['Bag of Organic Bananas']     ['Black Seedless Grapes']     ['Organic Braeburn Apple']     ['Organic Blueberries']     [\"Organic D'Anjou Pears\"]     ['White Peach']   ### Query the index  Let's now query the index to check how quickly we retrieve results. Please note that query speed depends in part on your internet connection.   ```python # Query by user factors  start_time = time.process_time() query_results = index.query(queries=user_factors[:-1].tolist(), top_k=10) print(\"Time needed for retrieving recommended products using Pinecone: \" + str(time.process_time() - start_time) + ' seconds.\\n')  for _id, res in zip(user_ids, query_results.results):     print(f'user_id={_id}')     df = pd.DataFrame(         {             'products': [match.id for match in res.matches],             'scores': [match.score for match in res.matches]         }     )     print(\"Recommendation: \")     display(df)     print(\"Top buys from the past: \")     display(products_bought_by_user_in_the_past(_id, top=15)) ```      Time needed for retrieving recommended products using Pinecone: 0.03125 seconds.          user_id=206210     Recommendation:     <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     } </style> <table  class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>products</th>       <th>scores</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>Mineral Water</td>       <td>0.919242</td>     </tr>     <tr>       <th>1</th>       <td>Zero Calorie Cola</td>       <td>0.716640</td>     </tr>     <tr>       <th>2</th>       <td>Orange &amp; Lemon Flavor Variety Pack Sparkling F...</td>       <td>0.631119</td>     </tr>     <tr>       <th>3</th>       <td>Sparkling Water</td>       <td>0.603575</td>     </tr>     <tr>       <th>4</th>       <td>Milk Chocolate Almonds</td>       <td>0.577868</td>     </tr>     <tr>       <th>5</th>       <td>Extra Fancy Unsalted Mixed Nuts</td>       <td>0.577714</td>     </tr>     <tr>       <th>6</th>       <td>Popcorn</td>       <td>0.565397</td>     </tr>     <tr>       <th>7</th>       <td>Organic Coconut Water</td>       <td>0.547605</td>     </tr>     <tr>       <th>8</th>       <td>Drinking Water</td>       <td>0.542832</td>     </tr>     <tr>       <th>9</th>       <td>Tall Kitchen Bag With Febreze Odor Shield</td>       <td>0.538533</td>     </tr>   </tbody> </table> </div>       Top buys from the past:     <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     } </style> <table  class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>product_id</th>       <th>product_name</th>       <th>total_orders</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>22802</td>       <td>Mineral Water</td>       <td>97</td>     </tr>   </tbody> </table> </div>       user_id=206211     Recommendation:     <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     } </style> <table  class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>products</th>       <th>scores</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>Baby Wash &amp; Shampoo</td>       <td>0.731054</td>     </tr>     <tr>       <th>1</th>       <td>No More Tears Baby Shampoo</td>       <td>0.695655</td>     </tr>     <tr>       <th>2</th>       <td>Size 6 Baby Dry Diapers</td>       <td>0.526953</td>     </tr>     <tr>       <th>3</th>       <td>Natural Applesauce Snack &amp; Go Pouches</td>       <td>0.478145</td>     </tr>     <tr>       <th>4</th>       <td>White Buttermints</td>       <td>0.475006</td>     </tr>     <tr>       <th>5</th>       <td>Size 5 Cruisers Diapers Super Pack</td>       <td>0.474203</td>     </tr>     <tr>       <th>6</th>       <td>Go-Gurt SpongeBob SquarePants Strawberry Ripti...</td>       <td>0.461982</td>     </tr>     <tr>       <th>7</th>       <td>Baby Wipes Sensitive</td>       <td>0.461840</td>     </tr>     <tr>       <th>8</th>       <td>Original Detergent</td>       <td>0.456813</td>     </tr>     <tr>       <th>9</th>       <td>Stage 1 Newborn Hypoallergenic Liquid Detergent</td>       <td>0.456143</td>     </tr>   </tbody> </table> </div>       Top buys from the past:     <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     } </style> <table  class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>product_id</th>       <th>product_name</th>       <th>total_orders</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>26834</td>       <td>No More Tears Baby Shampoo</td>       <td>89</td>     </tr>     <tr>       <th>1</th>       <td>12590</td>       <td>Baby Wash &amp; Shampoo</td>       <td>77</td>     </tr>   </tbody> </table> </div>   *Note* The inference using Pinecone is much faster compared to retrieving recommendations from a model directly. Please note that this result depends on your internet connection as well.   All that’s left to do is surface these recommendations on the shopping site, or feed them into other applications.  ## Clean up  Delete the index once you are sure that you do not want to use it anymore. Once it is deleted, you cannot reuse it.   ```python pinecone.delete_index(index_name) ```  ## Summary  In this example we used [Pinecone](https://www.pinecone.io/) to build and deploy a product recommendation engine that uses collaborative filtering, relatively quickly.  Once deployed, the product recommendation engine can index new data, retrieve recommendations in milliseconds, and send results to production applications. ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e255"
  },
  "title": "Search result diversification with post-processors",
  "category": "630fc5235d91a70054705fb7",
  "content": "This notebook demonstrates how Pinecone's API lets you control the way your service handles requests. Here, we demonstrate Pinecone's powerful Post-processing API and define a post-processing function that will perform search result diversification.  For some search applications, you may want to exclude results that are near-duplicates to the query. For example, in a product search application, we often want to retrieve a diverse set of products instead of the same product with slight variations.  This demo notebook will walk you through building an image search application that achieves that. We will use [Pinecone](/) to tie everything together and expose the image search as a real-time service that will take any fashion article image and return a diverse set of similar fashion article images.  We will, 1. implement a simple diversification filter as a Pinecone's post-processing function; 2. upload the post-processing function to Pinecone's Model Hub; 3. launch an image search service that includes a vector index backend and a diversification filter post-processor function; 4. upload and index our image vectors; 5. query our deployed service; 6. and compare the service with a baseline service that does not contain a diversification functionality.  ## Install and Setup Pinecone Client First, let's install the [Pinecone client](https://docs.beta.pinecone.io/en/latest/intro/installation.html) and set up its API key. [Here](/start/) you can obtain an API key.   ```python !pip install --quiet -U numpy pinecone-client python-mnist matplotlib progressbar2 pandas ipywidgets ```   ```python import pinecone.graph import pinecone.service import pinecone.connector import pinecone.hub  pinecone.init(api_key='FILL-YOUR-API-KEY') ```  ## Define a Search Result Diversification Postprocessor  The following code computes a heterogeneous \"top-five\" subset out of a query result set. This is done simply by clustering the data into five clusters and choosing from each cluster a representative. We use the [k-means](https://en.wikipedia.org/wiki/K-means_clustering) clustering algorithm to minimize inner cluster distance variance while maximizing in-between clusters variance.  Recall that our demo focus is on Pinecone's post-processing API. Therefore, we apply a simple diversification idea and assess it subjectively. For more rigorous search results diversification ideas, see for example this [work](https://www.microsoft.com/en-us/research/wp-content/uploads/2009/02/diversifying-wsdm09.pdf).  Our diversification postprocessor is a python class that follows Pinecone Model Hub's [Postprocessor API](https://docs.beta.pinecone.io/en/latest/python_client/hub.html#pinecone.hub.postprocessor). In short, we should implement a `transform` function that receives the query results and manipulates them.  Note that we save the code as a file because we will later package it as a [docker image](https://www.docker.com/), and upload it to [Pinecone's Model Hub](https://docs.beta.pinecone.io/en/latest/python_client/hub.html). This way, we will be able to define a search service with built-in search result diversification functionality.   ```python %%writefile diversity_postprocessor.py  import numpy as np from sklearn.cluster import KMeans  from pinecone.hub import postprocessor, QueryResult  @postprocessor class DiversityPostprocessor:     def __init__(self):         self._k = 5  # top k      def _diversity_filter(self, data):         kmeans = KMeans(n_clusters=self._k, random_state=0).fit(data)          inxs_per_cluster = [[i for i, value in enumerate(kmeans.labels_) if value == c] for c in range(self._k)]   # group cluster indices         results = set([inxs[ int(len(inxs)/2) ] for inxs in inxs_per_cluster]) # from each cluster take the \"median\" index          return results      def transform(self, queries, matches):         \"\"\"This is the postprocessor relevant function\"\"\"         output = []         for q, match in zip(queries, matches):             # Filter data             res = self._diversity_filter(match.data)              # Then rearrange results             new_scores = [s for i,s in enumerate(match.scores) if i in res]             new_ids = [id_ for i,id_ in enumerate(match.ids) if i in res]             new_data = np.array([x for i,x in enumerate(match.data) if i in res])             output.append(QueryResult(ids=new_ids, scores=new_scores, data=new_data))          return output ```      Overwriting diversity_postprocessor.py   ### Create the Post-Processor Docker Image And Push It to Pinecone's Model Hub   ```python diversity_filter_image_builder = pinecone.hub.ImageBuilder(     image=\"diversity_filter:v1\",  # The name of the docker image (you should also tag the image     build_path=\"./docker_build/diversity_filter/v1\",  # Path to which docker build artifacts are saved     model_path='./diversity_postprocessor.py', # Main model file     pip=['numpy', 'scikit-learn'],  # Additional pip packages needed     data_paths=[],  # Additional files or directories needed )  # Log into Pinecone's Model Hub login_cmd = pinecone.hub.get_login_cmd() !{login_cmd} ```      WARNING! Using --password via the CLI is insecure. Use --password-stdin.     WARNING! Your password will be stored unencrypted in /home/jupyter/.docker/config.json.     Configure a credential helper to remove this warning. See     https://docs.docker.com/engine/reference/commandline/login/#credentials-store          Login Succeeded    ```python diversity_filter_image_builder.package(exist_ok=True) !{diversity_filter_image_builder.get_build_cmd()} !{diversity_filter_image_builder.get_push_cmd()} ```      ~/docker_build/diversity_filter/v1 ~     Sending build context to Docker daemon  4.096kB     Step 1/4 : FROM hub.beta.pinecone.io/pinecone/base:0.8.34      ---> e988c545396e     Step 2/4 : RUN pip3 install --quiet --upgrade pip      ---> Using cache      ---> 5ac13fbcd300     Step 3/4 : RUN pip3 install --quiet --no-cache-dir numpy scikit-learn      ---> Using cache      ---> f9b753cff6cd     Step 4/4 : COPY model.py ./model.py      ---> Using cache      ---> 1129e677325b     Successfully built 1129e677325b     Successfully tagged diversity_filter:v1     ~     /bin/bash: -c: line 0: syntax error near unexpected token `}'     /bin/bash: -c: line 0: `{diversity_filter_image_builder.get_push_cmd()}'   ## Set Up a Pinecone Service  ### Define How the Service Handles Requests Here we define how the service handles requests. We want to store and retrieve a diverse set of images. We store the vector embeddings in Pinecone's [vector index](https://www.pinecone.io/learn/what-is-a-vector-index/). We rank and retrieve them using the [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) measure. Finally, we apply our search results diversification function on the top matched vectors and retrieve the index's final results.  Let's deploy these computation steps using Pinecone's [Index Graph](https://docs.beta.pinecone.io/en/latest/intro/concepts.html#graph). Observe that we initiate a Vector Index with Euclidean distance and attach our postprocessor function. The resulting graph defines how we set (i.e., `write`) or retrieve (i.e., `read`) data.   ```python graph = pinecone.graph.IndexGraph(metric='euclidean')  # Name of the hub images diversity_filter_image_name = pinecone.hub.as_user_image(diversity_filter_image_builder.image)  # Add to the graph function that will deduplicate results diversity_filter_postprocessor = pinecone.hub.HubFunction(name='diversity-postprocessor', image=diversity_filter_image_name)  graph.add_postprocessor(fn=diversity_filter_postprocessor)  # View the updated graph graph.view() ```        ### Deploy the Service and Set a Connection   ```python service_name = \"diversity-postprocessor-demo\" pinecone.service.deploy(service_name, graph, timeout=300) conn = pinecone.connector.connect(service_name) conn.info() ```         InfoResult(index_size=0)    ## Upload Vectors to the Service Let's upload real image [vector embeddings](https://www.pinecone.io/learn/what-are-vectors-embeddings/) into the service!  We are using [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset that contains fashion item images. For the sake of simplicity, we will use the raw grayscale pixel values as our vector embedding. Note that this choice is not optimal. Therefore, we expect it to produce reasonable search results only. (Recall, the focus of the demo is on the pre-processing API.)  First, let's download the dataset.   ```python !wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz !wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz !gunzip -f train-images-idx3-ubyte.gz !gunzip -f train-labels-idx1-ubyte.gz ```      --2021-03-21 11:28:42--  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz     Resolving fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)... 52.219.75.48     Connecting to fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)|52.219.75.48|:80... connected.     HTTP request sent, awaiting response... 200 OK     Length: 26421880 (25M) [binary/octet-stream]     Saving to: ‘train-images-idx3-ubyte.gz’          train-images-idx3-u 100%[===================>]  25.20M  11.8MB/s    in 2.1s          2021-03-21 11:28:45 (11.8 MB/s) - ‘train-images-idx3-ubyte.gz’ saved [26421880/26421880]          --2021-03-21 11:28:45--  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz     Resolving fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)... 52.219.75.48     Connecting to fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)|52.219.75.48|:80... connected.     HTTP request sent, awaiting response... 200 OK     Length: 29515 (29K) [binary/octet-stream]     Saving to: ‘train-labels-idx1-ubyte.gz’          train-labels-idx1-u 100%[===================>]  28.82K  --.-KB/s    in 0.1s          2021-03-21 11:28:45 (199 KB/s) - ‘train-labels-idx1-ubyte.gz’ saved [29515/29515]     ```python from mnist import MNIST import numpy as np images, labels = MNIST('.').load_training() images = np.array(images) np.array(images).shape ```         (60000, 784)    #### Upload the Vectors   ```python import progressbar  upsert_acks = conn.upsert(items=( (f\"img-{i}\", img) for i,img in progressbar.progressbar(enumerate(images)))).collect() ```      59999 Elapsed Time: 0:00:03   #### Count the Index's New Size   ```python conn.info() ```         InfoResult(index_size=60000)    ## Search Example Let's try our new service! We query the service with an arbitrary image. Observe that we set the desired number of matches to be 30. Our search results diversification filter will reduce these matches into five results only. Hence, our service will retrieve five final results.   ```python import matplotlib.pyplot as plt ```  ### Choose an Arbitrary Query Image Here we choose our query vector. This is just an arbitrary image from our dataset.   ```python query_id = np.random.randint(images.shape[0]) ```   ```python query = images[ query_id ]  fig = plt.figure(figsize=(3, 3)) img = query.reshape([28, 28]) plt.imshow(img, cmap='gray') plt.axis('off') plt.tight_layout() plt.show() ```    ![png](https://pinecone.io/images/similarity-search-diversification-example-25_0.png)    ### Query the Service Here we query the service. Observe that we set the required (maximal) number of matches to be 30; we expect to retrieve five results. Also, note that we require that the results would contain the vectors data.  Recall that this is required by our post-processor function that clusters these vectors.   ```python res = conn.query(queries=[query], top_k=30, include_data=True).collect()[0] ```  ### Retrieved Items Let's visualize the retrieved items.   ```python columns = 5 rows = int(np.ceil(len(res.ids)/columns))  fig = plt.figure(figsize=(8, 8))  for i in range(1, len(res.ids)+1):     data_idx = int(res.ids[i-1].split('-')[-1])     img = images[data_idx].reshape([28, 28])     lbl = labels[data_idx]     fig.add_subplot(rows, columns, i)     plt.imshow(img, cmap='gray')     plt.axis('off') plt.tight_layout() plt.show() ```    ![png](https://pinecone.io/images/similarity-search-diversification-example-29_0.png)    --- ## Compare With a Service Without Search Results Diversification Functionality Does our search results diversification filter work alright? Let's compare it with a service that does not contain such a post-processing functionality.  First, let's deploy a service with vector index only. This is the most basic Pinecone service functionality. Observe how simple and easy it is to deploy such a service.   ```python graph = pinecone.graph.IndexGraph(metric='cosine') graph.view() ```      ![svg](/images/similarity-search-diversification-example-31_0.svg)     #### Deploy the Baseline Service  Here we deploy, fill in, and query the simple baseline service. We use the same query image as above. Then, we compare this baseline service vs. our search results diversification service.   ```python baseline_service_name = \"diversity-postprocessor-demo-baseline\" pinecone.service.deploy(baseline_service_name, graph, timeout=300) baseline_conn = pinecone.connector.connect(baseline_service_name)  upsert_acks = baseline_conn.upsert(items=( (f\"img-{i}\", img) for i,img in progressbar.progressbar(enumerate(images)))).collect() ```      59999 Elapsed Time: 0:00:03    ```python def show_results(res):     columns = 5     rows = int(np.ceil(len(res.ids)/columns))      fig = plt.figure(figsize=(5, 5))      for i in range(1, len(res.ids)+1):         data_idx = int(res.ids[i-1].split('-')[-1])         img = images[data_idx].reshape([28, 28])         lbl = labels[data_idx]         fig.add_subplot(rows, columns, i)         plt.imshow(img, cmap='gray')         plt.axis('off')     plt.tight_layout()     plt.show()  def compare(query):     print(\"Query\")     fig = plt.figure(figsize=(2, 2))     img = query.reshape([28, 28])     plt.imshow(img, cmap='gray')     plt.axis('off')     plt.tight_layout()     plt.show()     print()      print(\"Baseline without a Diversirty Filter\")     res = baseline_conn.query(queries=[query], top_k=5, include_data=True).collect()[0]     show_results(res)     print(\"-\"*20)     print(\"Service with a Diversity Filter\")     res = conn.query(queries=[query], top_k=30, include_data=True).collect()[0]     show_results(res)     print(\"\\n\\n\")  ```  ### Cherry-Picked Examples Let's examine a few cherry-picked query examples.  #### Diversity in Action Observe that all the baseline results (upper row) are near-duplicates, while our diversification-service results exhibit variance.   ```python compare(images[27697]) ```      Query     ![png](https://pinecone.io/images/similarity-search-diversification-example-36_1.png)         Baseline without a Diversirty Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-36_3.png)        --------------------     Service with a Diversity Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-36_5.png)         #### Diversity Adds Noise Adding variance to the results comes with the risk of adding non-relevant results. In this example, the lower row's last match is less relevant. (Although it indeed eliminates a baseline duplicate match.)   ```python compare(images[4647]) ```      Query     ![png](https://pinecone.io/images/similarity-search-diversification-example-38_1.png)         Baseline without a Diversirty Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-38_3.png)        --------------------     Service with a Diversity Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-38_5.png)         #### Baseline Results are Already Diverse Sometimes it is hard to declare which of the options is best.   ```python compare(images[6999]) ```      Query     ![png](https://pinecone.io/images/similarity-search-diversification-example-40_1.png)         Baseline without a Diversirty Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-40_3.png)        --------------------     Service with a Diversity Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-40_5.png)         #### Try It Yourself Let's run a quick subjective comparison. We pick ten query images at random (i.i.d.) and compare the diversification-filter vs. the baseline results.   ```python for query_id in range(10):     query_id = np.random.randint(images.shape[0])     print(f\"qid {query_id}\")     compare(images[ query_id ]) ```      qid 34671     Query     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_1.png)         Baseline without a Diversirty Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_3.png)        --------------------     Service with a Diversity Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_5.png)           qid 42726     Query     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_7.png)         Baseline without a Diversirty Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_9.png)        --------------------     Service with a Diversity Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_11.png)           qid 54686     Query     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_13.png)         Baseline without a Diversirty Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_15.png)        --------------------     Service with a Diversity Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_17.png)           qid 17412     Query     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_19.png)         Baseline without a Diversirty Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_21.png)        --------------------     Service with a Diversity Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_23.png)           qid 49222     Query     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_25.png)         Baseline without a Diversirty Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_27.png)        --------------------     Service with a Diversity Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_29.png)           qid 36067     Query     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_31.png)         Baseline without a Diversirty Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_33.png)        --------------------     Service with a Diversity Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_35.png)           qid 22519     Query     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_37.png)         Baseline without a Diversirty Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_39.png)        --------------------     Service with a Diversity Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_41.png)           qid 56056     Query     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_43.png)         Baseline without a Diversirty Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_45.png)        --------------------     Service with a Diversity Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_47.png)           qid 21216     Query     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_49.png)         Baseline without a Diversirty Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_51.png)        --------------------     Service with a Diversity Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_53.png)           qid 54265     Query     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_55.png)         Baseline without a Diversirty Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_57.png)        --------------------     Service with a Diversity Filter     ![png](https://pinecone.io/images/similarity-search-diversification-example-42_59.png)         ### Conclusion Although we defined a simple search result diversification function and utilized a basically raw vector embedding technique, the cherry-picked examples demonstrate diversification's usefulness. Search results diversification is an active research area. If you seek more rigorous search results diversification ideas and evaluation methods, then [this book](https://www.nowpublishers.com/article/Details/INR-040) might be a good starting point.  Besides that, the comparison demonstrates the ease-of-use and flexibility of Pinecone's Graph API. Observe how easily we could launch two different service flavors and compare them with live data. This gives a gist of what you could do with Pinecone. For example, rapid model development, live experiments (e.g., A/B tests), complex ETL and post-processing steps, and more.  --- ## Shut down the Services We will not use the services anymore. Let's shut them down.  Note that this permanently shuts the service. You will not be able to restart the service, and all resources need to be recreated. We suggest that you only stop a service if no application is using it.   ```python for srv in pinecone.service.ls():     pinecone.service.stop(srv) ``` ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e257"
  },
  "title": "Image Search",
  "category": "630fc5235d91a70054705fb7",
  "content": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/search/image/image-search/image-search.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/search/image/image-search/image-search.ipynb) [![Open Github](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/github-shield.svg)](https://github.com/pinecone-io/examples/blob/master/search/image/image-search/image-search.ipynb)   ## Background  ### What is Image Search and how will we use it?  One may find themselves with an image, looking for similar images among a large image corpus. The difficult part of this requirement is instantly retrieving, at scale, similar images, especially when there are tens of millions or billions of images from which to choose.  In this example, we will walk you through the mechanics of how to solve this problem using an off-the-shelf, pretrained, neural network to generate data structures known as [vector embeddings](https://www.pinecone.io/learn/vector-embeddings/). We will use Pinecone's vector database offering to find images with similar vector embeddings to an _query image_.  ### Learning Goals and Estimated Reading Time  _By the end of this 15 minute demo (on a recent MacBook Pro, or up to an hour on Google Colab), you will have:_  1. Learned about Pinecone's value for solving realtime image search requirements!  2. Stored and retrieved vectors from Pinecone your very-own Pinecone Vector Database.  3. Encoded images as vectors using a pretrained neural network (i.e. no model training necessary).  4. Queried Pinecone's Vector Database to find similar images to the query in question.   Once all data is encoded as vectors, and is in your Pinecone Index, results of Pinecone queries are returned, on average, in tens of milliseconds.  ## Setup: Prerequisites and Image Data  ### Python 3.7+  This code has been tested with Python 3.7. It is recommended to run this code in a virtual environment or Google Colab.  ### Acquiring your Pinecone API Key  A Pinecone API key is required. You can obtain a complimentary key on our [our website](https://app.pinecone.io/). Either add `PINECONE_EXAMPLE_API_KEY` to your list of environmental variables, or manually enter it after running the below cell (a prompt will pop up requesting the API key, storing the result within this kernel (session)).  ### Installing and Importing Prerequisite Libraries: All prerequisites are installed and listed in the next cell.  #### Installing via `pip`   ```python !pip install -qU pinecone-client \\                  torchvision \\                  seaborn \\                  tqdm \\                  httpimport \\                  requests ```  #### Importing and Defining Constants   ```python import os import requests  import tqdm import httpimport import pinecone import numpy as np from PIL import Image  import torch import torchvision  DATA_DIRECTORY = 'tmp' INDEX_NAME = 'image-search' INDEX_DIMENSION = 1000 BATCH_SIZE=200 ```  ### Helper Module  This helper module will be imported and will enable this notebook to be self-contained.   ```python # There is a helper module required for this notebook to run. # When not present with this notebook, it will be streamed in from Pinecone's Example Repository. # You can find the module at https://github.com/pinecone-io/examples/tree/master/image_search  if os.path.isfile('helper.py'):     import helper as h else:     print('importing `helper.py` from https://github.com/pinecone-io')     with httpimport.github_repo(         username='startakovsky',          repo='pinecone-examples-fork',         module=['image_search'],         branch='may-2022-image-search-refresh'):         from image_search import helper as h ```   Extracting API Key from environmental variable `PINECONE_EXAMPLE_API_KEY`...    Pinecone API Key available at `h.pinecone_api_key`   ### Downloading Data  To demonstrate image search using Pinecone, we will download 100,000 small images using [built-in datasets](https://pytorch.org/vision/stable/datasets.html) available with the `torchvision` library.   ```python datasets = {     'CIFAR10': torchvision.datasets.CIFAR10(DATA_DIRECTORY, transform=h.preprocess, download=True),     'CIFAR100': torchvision.datasets.CIFAR100(DATA_DIRECTORY, transform=h.preprocess, download=True) } ```      Files already downloaded and verified     Files already downloaded and verified       ### Inspecting Images These are some of the images from what was just downloaded. If interested, read about the CIFAR image dataset [here](https://www.cs.toronto.edu/~kriz/cifar.html).   ```python h.show_random_images_from_full_dataset(datasets['CIFAR100']) ```        ![png](https://raw.githubusercontent.com/pinecone-io/img/main/image-search-1.png)        ## Generating Embeddings and Sending them to Pinecone  ### Loading a Pretrained Computer Vision Model  We will use a pretrained model that, like the dataset above, is shipped with PyTorch. This model will create a 1000-dimensional sequence of floats for each input image. We will use this output as an embedding associated with an image.   ```python model = torchvision.models.squeezenet1_1(pretrained=True).eval() ```  ### Why SqueezeNet?  We chose the [SqueezeNet](https://en.wikipedia.org/wiki/SqueezeNet) model because it is a very small model and basic model that has been trained on [millions of images](https://en.wikipedia.org/wiki/ImageNet) across 1000 classes. It is easy to instantiate with one line of code and generates embeddings quite a bit faster than deeper models.  ### On Comparing Embeddings  Two embeddings might look like something like this:  - \\[-0.02, 0.06, 0.0, 0.01, 0.08, -0.03, 0.01, 0.02, 0.01, 0.02, -0.07, -0.11, -0.01, 0.08, -0.04\\] - \\[-0.04, -0.09, 0.04, -0.1, -0.05, -0.01, -0.06, -0.04, -0.02, -0.04, -0.04, 0.07, 0.03, 0.02, 0.03\\]  In order to determine how similar they are, we use a [simple](https://towardsdatascience.com/importance-of-distance-metrics-in-machine-learning-modelling-e51395ffe60d) formula that takes a very short time to compute. Similarity scores are, in general, an excellent proxy for image similarity.  ### Creating Our Pinecone Index  The process for creating a Pinecone Index requires your Pinecone API key, the name of your index, and the number of dimensions of each vector (1000).  In this example, to compare embeddings, we will use the [cosine similarity score](https://en.wikipedia.org/wiki/Cosine_similarity) because this model generates un-normalized probability vectors. While this calculation is trivial when comparing two vectors, it will take quite a long time when needing to compare a query vector against millions or billions of vectors and determine those most similar with the query vector.  ### What is Pinecone for?  There is often a technical requirement to compare one vector to tens or hundreds of millions or more vectors, to do so with low latency (less than 50ms) and a high throughput. Pinecone solves this problem with its managed vector database service, and we will demonstrate this below.   ```python # authenticate with Pinecone API, keys and environment available at your project at https://app.pinecone.io pinecone.init(h.pinecone_api_key, environment='YOUR_ENVIRONMENT') # if the index does not already exist, we create it if INDEX_NAME not in pinecone.list_indexes():     pinecone.create_index(name=INDEX_NAME, dimension=INDEX_DIMENSION) # instantiate connection to your Pinecone index index = pinecone.Index(INDEX_NAME) ```  ### Preparing Vector Embeddings  We will encode the downloaded images for upload to Pinecone, and store the associated class of each image as metadata.  #### Creating Vector IDs Each vector ID will have a prefix corresponding to _CIFAR10_ or _CIFAR100_.   ```python def get_vector_ids(batch_number, batch_size, prefix):     \"\"\"Return vector ids.\"\"\"     start_index = batch_number * batch_size     end_index = start_index + batch_size     ids = np.arange(start_index, end_index)     # create id based on prefix      # eg. if id == 5, prefix == 'CIFAR10', then create 'CIFAR10.5' as vector id.     ids_with_prefix = map(lambda x: f'{prefix}.{str(x)}', ids)     return ids_with_prefix ```  #### Creating metadata for each vector containing class label   ```python def get_vector_metadata(label_indices, class_list):     \"\"\"Return list of {'label': <class name>}.\"\"\"     get_class_name = lambda index: {'label': class_list[index]}     return map(get_class_name, label_indices) ```  #### Constructing Vector Embeddings  In a Pinecone Vector Database, there are three components to every Pinecone vector embedding:   - a vector ID  - a sequence of floats of a user-defined, fixed dimension  - vector metadata (a key-value mapping, used for filtering at runtime)   ```python def get_vectors_from_batch(preprocessed_data, label_indices, batch_number, dataset):     \"\"\"Return list of tuples like (vector_id, vector_values, vector_metadata).\"\"\"     num_records = len(preprocessed_data)     prefix = dataset.__class__.__name__     with torch.no_grad():         # generate image embeddings with PyTorch model         vector_values = model(preprocessed_data).tolist()     # return respective IDs/metadata for each image embedding     vector_metadata = get_vector_metadata(label_indices, dataset.classes)     vector_ids = get_vector_ids(batch_number, num_records, prefix)     return list(zip(vector_ids, vector_values, vector_metadata)) ```  #### Example Vector Embedding The below code is an example of a vector embedding, showing just the first 3 components of the associated vector.   ```python dataset = datasets['CIFAR100'] list_of_preprocessed_tensors, label_indices = list(zip(*[dataset[i] for i in range(BATCH_SIZE)])) preprocessed_data = torch.stack(list_of_preprocessed_tensors) vectors = get_vectors_from_batch(preprocessed_data, label_indices, 0, dataset) id_, embedding, metadata = vectors[123] print(id_, embedding[:3], metadata, sep=', ') ```      CIFAR100.123, [4.237038612365723, 11.179943084716797, 1.3662679195404053], {'label': 'orange'}       #### Upsert Vectors to Pinecone This function iterates through a dataset in batches, generates a list of vector embeddings (as in the the above example) and upserts in batches to Pinecone.   ```python def upsert_image_embeddings(dataset, pinecone_index, batch_size=BATCH_SIZE, num_rows=None):     \"\"\"Iterate through dataset, generate embeddings and upsert in batches to Pinecone index.          Args:      - dataset: a PyTorch Dataset      - pinecone_index: your Pinecone index      - batch_size: batch size      - num_rows: Number of initial rows to use of dataset, use all rows if None.      \"\"\"     if num_rows > len(dataset):         raise ValueError(f'`num_rows` should not exceed length of dataset: {len(dataset)}')     if num_rows:         sampler = range(num_rows)     else:         sampler = None     dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=sampler)     tqdm_kwargs = h.get_tqdm_kwargs(dataloader)     for batch_number, (data, label_indices) in tqdm.notebook.tqdm(enumerate(dataloader), **tqdm_kwargs):         vectors = get_vectors_from_batch(             data,              label_indices,              batch_number,              dataloader.dataset)         pinecone_index.upsert(vectors) ```  ### Begin Upsert for all 100,000 Images One progress bar is generated per dataset. Truncate number of rows in each dataset by modifying `num_rows` parameter value in the cell below. Each of the CIFAR datasets have 50,000 rows.   ```python for dataset in datasets.values():     upsert_image_embeddings(dataset, index, num_rows=50_000) ```         0%|          | 0/250 [00:00<?, ?chunk of 200 CIFAR10 vectors/s]          0%|          | 0/250 [00:00<?, ?chunk of 200 CIFAR100 vectors/s]   ### View Progress On The [Pinecone Console](https://app.pinecone.io) (sample screenshot below)  ![pinecone-console](https://raw.githubusercontent.com/pinecone-io/img/main/image-search-5.png)  ## Querying Pinecone  Now that all the embeddings of the images are on Pinecone's database, it's time to demonstrate Pinecone's lightning fast query capabilities.  ###  Pinecone Example Usage  In the below example we query Pinecone's API with an embedding of a query image to return the vector embeddings that have the highest similarity score. Pinecone effeciently estimates which of the uploaded vector embeddings have the highest similarity when paired with the query term's embedding, and the database will scale to billions of embeddings maintaining low-latency and high throughput. In this example we have upserted 100,000 embeddings. Our starter plan supports up to one million.  #### Example: Pinecone API Request and Response  Let's find images similar to the `query_image` variable, shown below.  #### Example Query Image   ```python url = 'https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog4.png' r = requests.get(url, stream=True) query_image = Image.open(r.raw) h.printmd(\"#### A sample image\") query_image.resize((125,125)) ```   #### A sample image           ![png](https://raw.githubusercontent.com/pinecone-io/img/main/image-search-2.png)          ```python query_embedding = model(h.preprocess(query_image).unsqueeze(0)).tolist() response = index.query(query_embedding, top_k=4, include_metadata=True) h.printmd(f\"#### A sample response from Pinecone \\n ==============\\n \\n\") h.printmd(f\"```python\\n{response}\\n```\") ```   #### A sample response from Pinecone   ==============       ```python {'matches': [{'id': 'CIFAR10.11519',               'metadata': {'label': 'dog'},               'score': 1.00000012,               'values': []},              {'id': 'CIFAR10.21059',               'metadata': {'label': 'dog'},               'score': 0.982942224,               'values': []},              {'id': 'CIFAR10.48510',               'metadata': {'label': 'dog'},               'score': 0.982879698,               'values': []},              {'id': 'CIFAR100.32560',               'metadata': {'label': 'seal'},               'score': 0.982618093,               'values': []}],  'namespace': ''} ```   #### Enriched Response In the next few lines, we look up the actual images associated to the vector embeddings.   ```python h.show_response_as_grid(response, datasets, 1, 4, figsize=(10, 10)) ```        ![png](https://raw.githubusercontent.com/pinecone-io/img/main/image-search-3.png)        #### Results  We invite the reader to explore various queries to see how they come up. In the one above, we chose one of the CIFAR-10 images as the query image. Note that the query image embedding need not exist in your Pinecone index in order to find similar images. Additionally, the search results are only as good as the embeddings, which are based on the quality and quantity of the images as well as how expressive the model used is. There are plenty of other out of the box, pretrained models in PyTorch and elsewhere!  ### Pinecone Example Usage with Metadata  Extensive predicate logic can be applied to metadata filtering, just like the [WHERE clause](https://www.pinecone.io/learn/vector-search-filtering/) in SQL! Pinecone's [metadata feature](https://www.pinecone.io/docs/metadata-filtering/) provides easy-to-implement filtering.  #### Example using Metadata  For demonstration, let's use metadata to find all images classified as a _seal_ that look like the `query_image` variable shown above.   ```python response = index.query(     query_embedding,      top_k=25,      filter={\"label\": {\"$eq\": \"seal\"}},     include_metadata=True ) h.show_response_as_grid(response, datasets, 5, 5, figsize=(10, 10)) ```        ![png](https://raw.githubusercontent.com/pinecone-io/img/main/image-search-4.png)        #### Results  All of the results returned are indeed seals, and many of them do look like the query image! Note how the cosine similarity scores are returned in descending order.  #### Additional Note On Querying Pinecone  In this example, you queried your Pinecone index with an embedding that was already in the index, however that is not necessary at all. For this index, _any 1000-dimensional embedding_ can be used to query Pinecone.  ## Conclusion  In this example, we demonstrated how Pinecone makes it possible to do realtime image similarity search using a pre-trained computer vision model! We also demonstrated the use of metadata filtering with querying Pinecone's vector database.  ### Like what you see? Explore our [community](https://www.pinecone.io/community/)   Learn more about semantic search and the rich, performant, and production-level feature set of Pinecone's Vector Database by visiting https://pinecone.io, connecting with us [here](https://www.pinecone.io/contact/) and following us on [LinkedIn](https://www.linkedin.com/company/pinecone-io). If interested in some of the algorithms that allow for effecient estimation of similar vectors, visit our Algorithms and Libraries section of our [Learning Center](https://www.pinecone.io/learn/). ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e259"
  },
  "title": "Haystack integration",
  "category": "630fc5235d91a70054705fb7",
  "content": " > ⚠️  Warning > > This guide is intended for older versions of Haystack (`<1.3`). For newer versions refer to the [updated page](/integrations/haystack).  In this guide we will see how to integrate Pinecone and the popular [Haystack library](https://github.com/deepset-ai/haystack) for *Question-Answering*.  ## Installing Haystack  We start by installing the latest version of Haystack with all dependencies required for the `PineconeDocumentStore`.  ```bash !pip install -U 'farm-haystack[pinecone]' ```  ## Initializing the PineconeDocumentStore  We initialize a `PineconeDocumentStore` by providing an API key and environment name. ([Create an account](https://app.pinecone.io) to get your API key. You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**.)  ```python from haystack.document_stores import PineconeDocumentStore  document_store = PineconeDocumentStore(     api_key='<YOUR_API_KEY>',     environment='YOUR_ENVIRONMENT' ) ```  > ⚠️  Warning > > If you see a **ModuleNotFoundError** or **ImportError**, try installing the > Pinecone client manually using `pip install -U pinecone-client`.  ## Data Preparation  Before adding data to the document store, we must download and convert data into the Document format that Haystack uses.  We download pages from the Game of Thrones wiki.  ```python from haystack.utils import clean_wiki_text, convert_files_to_dicts, fetch_archive_from_http, print_answers  doc_dir = \"data/article_txt_got\" s3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt.zip\" fetch_archive_from_http(url=s3_url, output_dir=doc_dir) ```  Then convert these files into the Document format.  ```python dicts = convert_files_to_dicts(     dir_path=doc_dir,     clean_func=clean_wiki_text,     split_paragraphs=True ) ```  This Document format contains two fields; *'content'* for the text content or paragraphs, and *'meta'* where we can place any additional information that can later be used to apply metadata filtering in our search. Here is an example of the Document format:  ```json {'content': \"'''David Benioff''' (; né '''Friedman''' ; September 25, 1970) is \"             'an American screenwriter and television producer, writer, and '             'director. Along with his collaborator D. B. Weiss, he is best '             \"known as co-creator, showrunner, and writer of ''Game of \"             \"Thrones'' (2011–2019), the HBO adaptation of George R. R. \"             \"Martin's series of books ''A Song of Ice and Fire''. He is also \"             \"known for writing ''Troy'' (2004) and co-writing ''X-Men Origins: \"             \"Wolverine'' (2009).\",  'meta': {'name': '33_David_Benioff.txt'}} ```  ## Indexing Documents  To index the documents we use the `PineconeDocumentStore.write_documents` method.  ```python document_store.write_documents(dicts) ```  ## Creating and Upserting Embeddings  To create embeddings for our documents we must initialize a `DensePassageRetriever` model.  ```python from haystack.nodes import DensePassageRetriever retriever = DensePassageRetriever(     document_store=document_store,     query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",     passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",     max_seq_len_query=64,     max_seq_len_passage=256,     batch_size=2,     use_gpu=True,     embed_title=True,     use_fast_tokenizers=True ) ```  Then we run the `PineconeDocumentStore.update_embeddings` method with the `retriever` provided as an argument. GPU acceleration can greatly reduce the time required for this step.  ```python document_store.update_embeddings(     retriever,     batch_size=16 ) ```  ## Inspect Documents and Embeddings  We can get documents by their ID with the `PineconeDocumentStore.get_documents_by_id` method.  ```python d = document_store.get_documents_by_id(ids=['49091c797d2236e73fab510b1e9c7f6b'], return_embedding=True)[0] ```  From here we return can view document content with `d.content` and the document embedding with `d.embedding`.  ## Initializing an Extractive QA Pipeline  An `ExtractiveQAPipeline` contains three key components by default:  * a document store (`PineconeDocumentStore`) * a retriever model * a reader model  We can initialize a reader model from the HuggingFace Model Hub named `deepset/roberta-base-squad2`.  ```python from haystack.nodes import FARMReader  reader = FARMReader(     model_name_or_path=\"deepset/roberta-base-squad2\",      use_gpu=True ) ```  We are now ready to initialize the `ExtractiveQAPipeline`.  ```python from haystack.pipelines import ExtractiveQAPipeline  pipe = ExtractiveQAPipeline(reader, retriever) ```  ## Asking Questions  Using our QA pipeline we can begin querying with `pipe.run`.  ```python prediction = pipe.run(     query=\"Who created the Dothraki vocabulary?\",     params={         \"Retriever\": {\"top_k\": 10},         \"Reader\": {\"top_k\": 5}     } ) ```  We are also passing two `top_k` values, the retriever `top_k` defines how many records to retrieve from Pinecone. These records are then passed to the reader model which identifies a specific answer from each content paragraph and reranks the returned records. The reader `top_k` defines how many of these reranked records to return.  To view the answers we use `haystack.utils.print_answers`.  ```python from haystack.utils import print_answers print_answers(prediction, details=\"minimum\") ```  This will return:  ```json Query: Who created the Dothraki vocabulary? Answers: [   {   'answer': 'David J. Peterson',         'context': 'orld. The language was developed for the TV series by the '                    'linguist David J. Peterson, working off the Dothraki words '                    \"and phrases in Martin's novels.\\n\"                    ','},     {   'answer': 'David J. Peterson',         'context': '\\n'                    '===Valyrian===\\n'                    'David J. Peterson, who created the Dothraki language for '                    'the first season of the show, was entrusted by the '                    'producers to design a new '},     {   'answer': 'David J. Peterson',         'context': \"age for ''Game of Thrones''\\n\"                    'The Dothraki vocabulary was created by David J. Peterson '                    'well in advance of the adaptation. HBO hired the Language '                    'Creatio'},     {   'answer': 'D. B. Weiss and David Benioff',         'context': '\\n'                    '===Conception and development===\\n'                    'Showrunners D. B. Weiss and David Benioff created the '                    'series, wrote most of its episodes and directed several.\\n'                    'In Ja'},     {   'answer': 'books',         'context': 'ints.  First, the language had to match the uses already '                    'put down in the books. Secondly, it had to be easily '                    'pronounceable or learnable by the actors'}] ```  We can view more details including the score of each answer by specifying `details=\"all\"`.  ```python print_answers(prediction, details=\"all\") ```  Which returns:  ```python Query: Who created the Dothraki vocabulary? Answers: [   <Answer {'answer': 'David J. Peterson', 'type': 'extractive', 'score': 0.9532108306884766, 'context': \"orld. The language was developed for the TV series by the linguist David J. Peterson, working off the Dothraki words and phrases in Martin's novels.\\n,\", 'offsets_in_document': [{'start': 329, 'end': 346}], 'offsets_in_context': [{'start': 67, 'end': 84}], 'document_id': '308dca876f94e5e839187f1463693015', 'meta': {'name': '214_Dothraki_language.txt'}}>,     <Answer {'answer': 'David J. Peterson', 'type': 'extractive', 'score': 0.8807850480079651, 'context': '\\n===Valyrian===\\nDavid J. Peterson, who created the Dothraki language for the first season of the show, was entrusted by the producers to design a new ', 'offsets_in_document': [{'start': 16, 'end': 33}], 'offsets_in_context': [{'start': 16, 'end': 33}], 'document_id': 'b368200c210d555625bd409b0dc27be1', 'meta': {'name': '87_Valar_Dohaeris.txt'}}>,     <Answer {'answer': 'David J. Peterson', 'type': 'extractive', 'score': 0.8687494099140167, 'context': \"age for ''Game of Thrones''\\nThe Dothraki vocabulary was created by David J. Peterson well in advance of the adaptation. HBO hired the Language Creatio\", 'offsets_in_document': [{'start': 139, 'end': 156}], 'offsets_in_context': [{'start': 67, 'end': 84}], 'document_id': '27baa56e5aab6b04d38f19e97e078bc6', 'meta': {'name': '214_Dothraki_language.txt'}}>,     <Answer {'answer': 'D. B. Weiss and David Benioff', 'type': 'extractive', 'score': 0.10197015851736069, 'context': '\\n===Conception and development===\\nShowrunners D. B. Weiss and David Benioff created the series, wrote most of its episodes and directed several.\\nIn Ja', 'offsets_in_document': [{'start': 46, 'end': 75}], 'offsets_in_context': [{'start': 46, 'end': 75}], 'document_id': 'd8b7f165cc64c549532b74249cc692dd', 'meta': {'name': '229_Game_of_Thrones.txt'}}>,     <Answer {'answer': 'books', 'type': 'extractive', 'score': 0.0460672490298748, 'context': 'ints.  First, the language had to match the uses already put down in the books. Secondly, it had to be easily pronounceable or learnable by the actors', 'offsets_in_document': [{'start': 166, 'end': 171}], 'offsets_in_context': [{'start': 73, 'end': 78}], 'document_id': '8767e85c7a9bcec61f95e13bb61f3e98', 'meta': {'name': '214_Dothraki_language.txt'}}>] ```  ## Metadata Filtering  The `PineconeDocumentStore` gives us access to Pinecone's powerful metadata filtering functionality. When performing filtering with Haystack we use a slightly different filter syntax to that used by Pinecone.  Using the Game of Thrones dataset we can filter by filename.  ```python prediction = pipe.run(     query=\"Who created the Dothraki vocabulary?\",     params={\"Retriever\": {         \"top_k\": 10,         \"filters\": {             \"name\": {\"$eq\": \"368_Jaime_Lannister.txt\"}         }     }, \"Reader\": {\"top_k\": 5}} ) ```  ### Haystack Filter Examples  Here are a few more examples of Haystack filtering syntax.  ```python filters = {     \"$and\": {         \"type\": {\"$eq\": \"article\"},         \"date\": {\"$gte\": \"2015-01-01\", \"$lt\": \"2021-01-01\"},         \"rating\": {\"$gte\": 3},         \"$or\": {             \"genre\": {\"$in\": [\"economy\", \"politics\"]},             \"publisher\": {\"$eq\": \"nytimes\"}         }     } } # or simpler using default operators filters = {     \"type\": \"article\",     \"date\": {\"$gte\": \"2015-01-01\", \"$lt\": \"2021-01-01\"},     \"rating\": {\"$gte\": 3},     \"$or\": {         \"genre\": [\"economy\", \"politics\"],         \"publisher\": \"nytimes\"     } } ``` ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e25b"
  },
  "title": "Facial Similarity Search",
  "category": "630fc5235d91a70054705fb7",
  "content": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/search/image/facial-similarity-search/facial-similarity-search.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/search/image/facial-similarity-search/facial-similarity-search.ipynb) [![Open github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/pinecone-io/examples/blob/master/search/image/facial-similarity-search/facial-similarity-search.ipynb)   In this notebook, we will demonstrate how to use Pinecone to build an image-based vector search application to discover people with similar facial features. We will:  1. Extract faces from a celebrity image dataset 2. Convert the faces to embeddings and store them in a Pinecone index (alongside metadata related to the celebrities) 3. Query the Pinecone index with an image of a person and find the most similar celebrities   # Install Dependencies   ```python !pip install datasets pinecone-client[grpc] facenet-pytorch requests Pillow ```  # Load Dataset  We will use a dataset containing photos of ~115K most popular people on The Movie Database (TMDB). This dataset can be loaded from Huggingface as follows:   ```python from datasets import load_dataset  # load the dataset celeb_faces = load_dataset(\"ashraq/tmdb-people-image\", split=\"train\") celeb_faces ```      Dataset({         features: ['adult', 'also_known_as', 'biography', 'birthday', 'deathday', 'gender', 'homepage', 'id', 'imdb_id', 'known_for_department', 'name', 'place_of_birth', 'popularity', 'profile_path', 'image'],         num_rows: 116404     })    We have got few metadata about the people and their image in the dataset. Let's take a look:   ```python celeb = celeb_faces[10] celeb ```         {'adult': False,      'also_known_as': \"['Thomas Stanley Holland', 'Том Холланд', 'トム・ホランド', '톰 홀랜드', 'توم هولاند', 'ทอม ฮอลแลนด์', '汤姆·赫兰德', 'Τομ Χόλαντ', 'Том Голланд', '湯姆·霍蘭德', 'טום הולנד', 'תומאס סטנלי הולנד', 'Nhện Đệ Tam', 'ტომ ჰოლანდი']\",      'biography': 'Thomas \"Tom\" Stanley Holland is an English actor and dancer. He is best known for playing Peter Parker / Spider-Man in the Marvel Cinematic Universe and has appeared as the character in six films: Captain America: Civil War (2016), Spider-Man: Homecoming (2017), Avengers: Infinity War (2018), Avengers: Endgame (2019), Spider-Man: Far From Home (2019), and Spider-Man: No Way Home (2021). He is also known for playing the title role in Billy Elliot the Musical at the Victoria Palace Theatre, London, as well as for starring in the 2012 film The Impossible.',      'birthday': '1996-06-01',      'deathday': None,      'gender': 2,      'homepage': None,      'id': 1136406,      'imdb_id': 'nm4043618',      'known_for_department': 'Acting',      'name': 'Tom Holland',      'place_of_birth': 'Surrey, England, UK',      'popularity': 104.302,      'profile_path': 'bBRlrpJm9XkNSg0YT5LCaxqoFMX.jpg',      'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=421x632 at 0x7FA4FC04CB50>}     ```python celeb[\"image\"].resize((200, 300)) ```          ![png](https://raw.githubusercontent.com/pinecone-io/img/main/facial-similarity-search_9_0.png)         We do not need all these metadata fields. So we will remove the ones we do not need and convert the rest into a pandas dataframe.   ```python # remove metadata fields not needed, convert into a pandas dataframe metadata = celeb_faces.remove_columns(['adult', 'also_known_as', 'biography', 'deathday', 'gender', 'homepage', 'id', 'imdb_id', 'known_for_department', 'image']).to_pandas() # replace any empty fields with None metadata = metadata.fillna(\"None\") ```  # Embedding Model  We will use two models: one for extracting faces and another for generating vector embeddings of the face. We're focusing on faces only because using full images would introduce too much noise and result in poor results.  For face extraction, we will use MTCNN, which is a popular choice due to its ability to accurately detect and align faces in images despite variations in pose and appearance. We can use a Pytorch implementation of MTCNN with the ``facenet-pytorch`` package. Since the images in our dataset are already in PIL format, we can directly test the MTCNN model, which expects PIL image objects as inputs, as shown below:   ```python from facenet_pytorch import MTCNN  # initialize the MTCNN model mtcnn = MTCNN() # create a copy of the face img = celeb[\"image\"].copy() # detect face and get coordinates of the face with probability boxes, prob = mtcnn.detect(img) boxes, prob ```         (array([[ 91.4824  , 112.335335, 316.80338 , 409.37723 ]], dtype=float32),      array([0.9999924], dtype=float32))    The detect method in MTCNN gives us the coordinates of the face and how confident it was in detecting the face, in this case, with 99% accuracy. Let's draw a rectangle on the image using these coordinates to see if it correctly detected the face.   ```python from PIL import Image, ImageDraw  # draw a rectangle on the image using coordinates returned by the MTCNN model draw = ImageDraw.Draw(img) draw.rectangle(boxes.reshape((2,2)), width=3) # resize the image to display a smaller size img.resize((200, 290)) ```          ![png](https://raw.githubusercontent.com/pinecone-io/img/main/facial-similarity-search_16_0.png)         As we can see, the model has successfully identified the face. To extract the face, we can crop the image to only include the area within the rectangle, using either ``opencv`` or another package. Alternatively, the ``facenet-pytorch`` package has a function that does this for us and returns the result as Pytorch tensors that can be used as input for the embedding model directly. This can be done as follows:   ```python # pass the image or batch of images directly through mtcnn model face = mtcnn(img) face.shape ```         torch.Size([3, 160, 160])    To generate embeddings, we will use VGGFace2, which is a deep learning model for facial recognition that was trained on the VGGFace2 dataset, which includes more than 3 million images of over 9000 people. The model can be loaded and used as follows:   ```python from facenet_pytorch import InceptionResnetV1 import torch  # initialize VGGFace2 model resnet = InceptionResnetV1(pretrained=\"vggface2\").eval() # generate embedding for the face extracted using mtcnn above embedding = resnet(torch.stack([face])) embedding.shape ```       torch.Size([1, 512])    We can now generated vector embedding for the face. Let's write a pipeline to easy do all of this in batches.   ```python import numpy as np   class FacenetEmbedder:     def __init__(self):         # set device to use GPU if available         self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')         # initialize MTCNN model         self.mtcnn = MTCNN(device=self.device)         # initialize VGGFace2 model         self.resnet = InceptionResnetV1(pretrained='vggface2', device=self.device).eval()      def detect_face(self, batch):         # get coordinates of the face         faces = self.mtcnn.detect(batch)         return faces      def encode(self, batch):         # pass the batch of images directly through mtcnn model         face_batch = self.mtcnn(batch)         # remove any images that does not contain a face         face_batch = [i for i in face_batch if i is not None]         # concatenate face batch to form a single tensor         aligned = torch.stack(face_batch)         # if using gpu move the input batch to gpu         if self.device.type == \"cuda\":              aligned = aligned.to(self.device)         # generate embedding         embeddings = self.resnet(aligned).detach().cpu()         return embeddings.tolist() ```   ```python # initialize the embedding pipeline facenet = FacenetEmbedder() ```   ```python # test the pipeline using a small image batch batch = celeb_faces[10:20][\"image\"] len(facenet.encode(batch)) ```         10    We can now simply call the ``encode`` method in the ``FacenetEmbedder`` with a batch of PIL images and it would extract the faces and generate embedding for us. Keep in mind that batch encoding only works if all the images in the batch have the same shape. We can use the following function to reshape a batch of PIL images to ensure it always works.   ```python def reshape(batch):     batch = [image.convert(\"RGB\").resize((421, 632)) for image in batch]     return batch ```  # Initialize Pinecone Index  Now we need to set up the Pinecone index, which stores vector representations of our images that can be retrieved using the embedding of another image (called the query vector). Before we can do this, we have to establish a connection to Pinecone using an API key. You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**. This connection is initialized as follows:   ```python import pinecone  # connect to pinecone environment pinecone.init(     api_key=\"YOUR_API_KEY\",     environment=\"YOUR_ENVIRONMENT\" ) ```  Now, we can create our vector index and name it \"tmdb-people\" (although you can choose any name you like). We specify the metric type as cosine and the dimension as 512, as these are the vector space and dimensionality of the vectors produced by the embedding model we use.   ```python index_name = \"tmdb-people\"  # check if the tmdb-people index exists if index_name not in pinecone.list_indexes():     # create the index if it does not exist     pinecone.create_index(         index_name,         dimension=512,         metric=\"cosine\"     )  # connect to tmdb-people index we created index = pinecone.GRPCIndex(index_name) ```  # Generate Embeddings and Upsert  Next, we need to generate embeddings for the celebrity faces and upload them into the Pinecone index. To do this efficiently, we will process them in batches and upload the resulting embeddings to the Pinecone index. For each celebrity in the dataset, we need to provide Pinecone with a unique id, the corresponding embedding, and metadata. The metadata is a collection of information related to the celebrities, including their name, profile image url, date of birth, etc.   ```python from tqdm.auto import tqdm  # we will use batches of 64 batch_size = 64  for i in tqdm(range(0, len(celeb_faces), batch_size)):     # find end of batch     i_end = min(i+batch_size, len(celeb_faces))     # extract batch     batch = celeb_faces[i:i_end][\"image\"]     # reshape the images to ensure they all have same shape     batch = reshape(batch)     # generate embeddings for batch     emb = facenet.encode(batch)     # create unique IDs     ids = [f\"{idx}\" for idx in range(i, i_end)]     # add metadata     meta = metadata[i:i_end].to_dict(orient=\"records\")     # add all to upsert list     to_upsert = list(zip(ids, emb, meta))     # upsert/insert these records to pinecone     _ = index.upsert(vectors=to_upsert)  # # check that we have all vectors in index index.describe_index_stats() ```  We have successfully added everything we need to the Pinecone index.  # Find Similar Celebrities  Now we can query the Pinecone index with an embedding of a face and instantly get the celebrities that are most similar. First, let's write a helper functions to query pinecone and display the results.   ```python from IPython.core.display import HTML   def display_result(metadata):     figures = []     for m in metadata:         figures.append(f'''             <figure style=\"margin: 5px !important;\">                 <img src=\"https://image.tmdb.org/t/p/h632/{m[\"profile_path\"]}\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >                 <figcaption>{m[\"name\"]}</figcaption>             </figure>         ''')     return HTML(data=f'''         <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">         {''.join(figures)}         </div>     ''') ```   ```python def find_similar_faces(face, top_k=10):     # pass the image through the embedding pipeline     emb = facenet.encode([face])     # query pinecone with the face embedding     result = index.query(emb[0], top_k=6, include_metadata=True)     # extract metadata from the search results and display results      r = [x[\"metadata\"] for x in result[\"matches\"]]     return display_result(r) ```  Let's run some test queries using celebrity images from the dataset to find other celebrities who look alike.   ```python celeb = celeb_faces[40][\"image\"] celeb.resize((190,240)) ```          ![png](https://raw.githubusercontent.com/pinecone-io/img/main/facial-similarity-search_41_0.png)          ```python find_similar_faces(celeb) ```      <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/dSK2BBupETZYcsO0DfP2OD1AMnT.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Claudia Harrison</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/bHGtm29qlx6OYsgwOq84xd6MRpF.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Sonya Walger</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/6TSe26ctGPvmZRKDgul1Gos6old.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Bettina Mittendorfer</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/2RJ30pPSQQxteuoMjhN1FWTwxlZ.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Julianne Nicholson</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/8Jju93UvSPszRzZ9Glvjp9z2anF.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Aglaia Szyszkowitz</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/duMdWlldESIqFYDpaJNAcLFzTTq.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Hannah Barefoot</figcaption>     </figure> </div>     The search result looks good as we can definately see some celebrities with similar facial features. Let's run more queries.   ```python celeb = celeb_faces[35][\"image\"] find_similar_faces(celeb) ```      <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/jpurJ9jAcLCYjgHHfYF32m3zJYm.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Chris Hemsworth</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/As5ujeSSJRQN4TE3odg3kDk0b5o.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Liam Hemsworth</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/jcDCpX5WiH0AkLDpR6ELreIn2Ta.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Ed Hendrik</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/yvo81AU4zUKbcNtWEQjaSTvLTPS.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Chris Reid</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/lLsbZLO0JQPIiaRZTcVYbVxfv4m.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Kenny Doughty</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/6lqQsR2Ot7nYthCGxfKBzmPzyV3.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Christopher Russell</figcaption>     </figure> </div>      ```python celeb = celeb_faces[1][\"image\"] find_similar_faces(celeb) ```      <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/8dCFK8FDFQbYFZvzAE9IIeaDMKo.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Seung Ha</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/bsSM5aG1T5oXwpcL3glOf0ViNy4.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Shara Lin</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/cVA40E5Pus66fLzq2mWLJDNNtNh.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Yura</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/3JTUzCh3PrbamSBf5M5yxMqbxGY.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Napasorn Weerayuttwilai</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/ksilFsztYt9Ojp2cbMk75nxumOS.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Go Joon-hee</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/kpwv7ozzwUmBaCCgx412CmHqofb.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>An Danwei</figcaption>     </figure> </div>      ```python celeb = celeb_faces[12][\"image\"] find_similar_faces(celeb) ```      <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/whNwkEQYWLFJA8ij0WyOOAD5xhQ.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Jason Statham</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/2v1MXtsjWJHqx0U0ATZ5VrBh2dD.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Vladimir Raiman</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/nJXxgwcGdAL5P1KfgcGrjDaCbsY.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Brendan Kelly</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/tnDgEU5q47anoWZJDzzudC678m7.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Scott C. Brown</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/pAkKjbzxP6Z6Y5zAGpwmWQo1iji.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Michael Chiklis</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/8XBU3bsvmPSDTf6d0edNb2eSDpb.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Huw Garmon</figcaption>     </figure> </div>      ```python celeb = celeb_faces[17][\"image\"] find_similar_faces(celeb) ```      <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/kL0lWLJA6lbvmPM3YL0ISE6rVr6.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Joey King</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/gdTcDujkkPIoKtDZrGcbQ2voFI3.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Elva Trill</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/iF9rvJzqepJCxzqQC07OG4bTxEf.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Rosalind Halstead</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/AeYvjBU8swVC9zKS933fX8YsnKt.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Svetlana Svetlichnaya</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/caXilGBPQtpF4nZEPUlDb0UfVnL.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Megan Parkinson</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/IgGOiwSg4qq3OEzXG1LROXTsel.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Clara Ponsot</figcaption>     </figure> </div>      ```python celeb = celeb_faces[29][\"image\"] find_similar_faces(celeb) ```      <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/52KsHFCu0LToakebnxqC4VeRixl.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Luke Grimes</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/bkAEduLHkQg1AZmempKPLpoqibA.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Philip Ettinger</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/aE8WEoNAs2PtgUzF6AcGodwLQvy.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Nick Thune</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/fHSGEllSO8O7p5StwFbWr1du7Bw.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Vicente Alves do Ó</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/iR0ElrLlagBaGSsOZVphTLbU8TE.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Thomas McDonell</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/yxOUD7RXqwp1EokJqslrJgzI38t.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Irhad Mutic</figcaption>     </figure> </div>      ```python celeb = celeb_faces[64][\"image\"] find_similar_faces(celeb) ```      <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/m8bdrmh6ExDCGQ64E83mHg002YV.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Jeffrey Dean Morgan</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/5By3t8kAp5Iiu9aULc3qryyTOWO.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Pompeu José</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/btBOmL6PvUMzBjmALO9x0R44ncf.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Özcan Varaylı</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/j7AFVjuiU0h0K0Bke7FJnZQpcTz.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Selahattin Taşdöğen</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/9Shbe0mLDItn2Is7QloNIu3G6x1.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Vagelis Rokos</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/nyLa6LmV9iGv4h9Zqbh1xMp7T26.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Darrell D'Silva</figcaption>     </figure> </div>     The search results look excellent. To further test our system, let's try using images that are not in the dataset. The following function, ``get_image``, can be utilized to load an image as a PIL object using a URL:   ```python from PIL import Image import requests  def get_image(url):   img = Image.open(requests.get(url, stream=True).raw)   return img    ```   ```python url = \"https://live.staticflickr.com/7442/9509564504_21d2dc42e1_z.jpg\" # load the image as PIL object from url celeb = get_image(url) celeb.resize((190,240)) ```          ![png](https://raw.githubusercontent.com/pinecone-io/img/main/facial-similarity-search_52_0.png)          ```python find_similar_faces(celeb) ```      <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/f7cyXplSWuYFX1e7JxcNMiRfbaH.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Jennifer Lawrence</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/qpSUiChjYsgsFhaBNtPRNATkqx3.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Aislyn Watson</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/8ZYFDTg7qVripncGatvG7ufxeyx.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Carrie Underwood</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/1midcRjbkFrMX3km1290Jlwf6xi.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Carissa Capobianco</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/djzcylhUn3FzpKFrwHQbhbZqMpn.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Kimberley Klaver</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/l3wEP7cqUYQs9qzbWkx05zTpMnN.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Esti Ginzburg</figcaption>     </figure> </div>      ```python url = \"https://live.staticflickr.com/3563/3304692615_bc67db2606_z.jpg\" # load the image as PIL object from url celeb = get_image(url) celeb.resize((190,240)) ```          ![png](https://raw.githubusercontent.com/pinecone-io/img/main/facial-similarity-search_54_0.png)          ```python find_similar_faces(celeb) ```      <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/ajNaPmXVVMJFg9GWmu6MJzTaXdV.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Brad Pitt</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/boqqxkt8GcjCUmUsGRFzr3rSTDh.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Peter M. Lenkov</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/wWRbCMgZnG6ItMBcMHsJ495FrU8.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Michał Lewandowski</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/b19aJzhUAfyQXd0mtmDvLAyOth8.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Luke Arnold</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/hiuSdXFjI7EIMv8Y83zh2KtGbbk.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>David Berry</figcaption>     </figure>     <figure style=\"margin: 5px !important;\">         <img src=\"https://image.tmdb.org/t/p/h632/w2oRqYtVa8B3ALOrBqajwp72YQf.jpg\" style=\"width: 190px; height: 240px; border-radius: 10px;\" >         <figcaption>Marco Quaglia</figcaption>     </figure> </div>     As we can see, the search result correctly identifies the celebrity in the picture as the top match and also finds other celebrities with similar facial features.  # Example Application  Are you curious if you share a resemblance with a famous celebrity? Try this [demo app](https://huggingface.co/spaces/pinecone/find-your-celebrity-match), which has been built based on this notebook, to find out. ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e25d"
  },
  "title": "Generative QA with OpenAI",
  "category": "630fc5235d91a70054705fb7",
  "content": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/generation/generative-qa/openai/gen-qa-openai/gen-qa-openai.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/generation/generative-qa/openai/gen-qa-openai/gen-qa-openai.ipynb) [![Open Github](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/github-shield.svg)](https://github.com/pinecone-io/examples/blob/master/generation/generative-qa/openai/gen-qa-openai/gen-qa-openai.ipynb)   In this notebook we will learn how to build a **retrieval enhanced generative question-answering system** with Pinecone and OpenAI. This will allow us to retrieve relevant contexts to our queries from Pinecone, and pass these to a generative OpenAI model to generate an answer backed by real data sources. Required installs for this notebook are:   ```python !pip install -qU openai pinecone-client datasets ```  Initialize OpenAI connection with:  ```python import openai  # get API key from top-right dropdown on OpenAI website openai.api_key = \"OPENAI_API_KEY\" ```  For many questions *state-of-the-art (SOTA)* LLMs are more than capable of answering correctly.   ```python query = \"who was the 12th person on the moon and when did they land?\"  # now query text-davinci-003 WITHOUT context res = openai.Completion.create(     engine='text-davinci-003',     prompt=query,     temperature=0,     max_tokens=400,     top_p=1,     frequency_penalty=0,     presence_penalty=0,     stop=None )  res['choices'][0]['text'].strip() ```         'The 12th person on the moon was Harrison Schmitt, and he landed on December 11, 1972.'    However, that isn't always the case. Let's first rewrite the above into a simple function so we're not rewriting this every time.   ```python def complete(prompt):     # query text-davinci-003     res = openai.Completion.create(         engine='text-davinci-003',         prompt=prompt,         temperature=0,         max_tokens=400,         top_p=1,         frequency_penalty=0,         presence_penalty=0,         stop=None     )     return res['choices'][0]['text'].strip() ```  Now let's ask a more specific question about training a specific type of transformer model called a *sentence-transformer*. The ideal answer we'd be looking for is _\"Multiple Negatives Ranking (MNR) loss\"_.  Don't worry if this is a new term to you, it isn't required to understand what we're doing or demoing here.   ```python query = (     \"Which training method should I use for sentence transformers when \" +     \"I only have pairs of related sentences?\" )  complete(query) ```         'If you only have pairs of related sentences, then the best training method to use for sentence transformers is the supervised learning approach. This approach involves providing the model with labeled data, such as pairs of related sentences, and then training the model to learn the relationships between the sentences. This approach is often used for tasks such as natural language inference, semantic similarity, and paraphrase identification.'    Another common answer produced by the LLM is:  ``` The best training method to use for fine-tuning a pre-trained model with sentence transformers is the Masked Language Model (MLM) training. MLM training involves randomly masking some of the words in a sentence and then training the model to predict the masked words. This helps the model to learn the context of the sentence and better understand the relationships between words. ```  Both answers seem convincing. Yet, they're both wrong. For the former about `supervised learning approach` being the most suitable. This is completely true, but it's not specific and doesn't answer the question.  For the latter, MLM is typically used in the pretraining step of a transformer model but *cannot* be used to fine-tune a sentence-transformer, and has nothing to do with having _\"pairs of related sentences\"_.  We have two options for enabling our LLM in understanding and correctly answering this question:  1. We fine-tune the LLM on text data covering the topic mentioned, likely on articles and papers talking about sentence transformers, semantic search training methods, etc.  2. We use **R**etrieval **A**ugmented **G**eneration (RAG), a technique that implements an information retrieval component to the generation process. Allowing us to retrieve relevant information and feed this information into the generation model as a *secondary* source of information.  We will demonstrate option **2**.  ---  ## Building a Knowledge Base  With open **2** the retrieval of relevant information requires an external _\"Knowledge Base\"_, a place where we can store and use to efficiently retrieve information. We can think of this as the external _long-term memory_ of our LLM.  We will need to retrieve information that is semantically related to our queries, to do this we need to use _\"dense vector embeddings\"_. These can be thought of as numerical representations of the *meaning* behind our sentences.  There are many options for creating these dense vectors, like open source [sentence transformers](https://pinecone.io/learn/nlp/) or OpenAI's [ada-002 model](https://youtu.be/ocxq84ocYi0). We will use OpenAI's offering in this example.  We have already authenticated our OpenAI connection, to create an embedding we just do:   ```python embed_model = \"text-embedding-ada-002\"  res = openai.Embedding.create(     input=[         \"Sample document text goes here\",         \"there will be several phrases in each batch\"     ], engine=embed_model ) ```  In the response `res` we will find a JSON-like object containing our new embeddings within the `'data'` field.   ```python res.keys() ```         dict_keys(['object', 'data', 'model', 'usage'])    Inside `'data'` we will find two records, one for each of the two sentences we just embedded. Each vector embedding contains `1536` dimensions (the output dimensionality of the `text-embedding-ada-002` model.   ```python len(res['data']) ```         2     ```python len(res['data'][0]['embedding']), len(res['data'][1]['embedding']) ```         (1536, 1536)    We will apply this same embedding logic to a dataset containing information relevant to our query (and many other queries on the topics of ML and AI).  ### Data Preparation  The dataset we will be using is the `jamescalam/youtube-transcriptions` from Hugging Face _Datasets_. It contains transcribed audio from several ML and tech YouTube channels. We download it with:   ```python from datasets import load_dataset  data = load_dataset('jamescalam/youtube-transcriptions', split='train') data ```      Using custom data configuration jamescalam--youtube-transcriptions-6a482f3df0aedcdb     Reusing dataset json (/Users/jamesbriggs/.cache/huggingface/datasets/jamescalam___json/jamescalam--youtube-transcriptions-6a482f3df0aedcdb/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)          Dataset({         features: ['title', 'published', 'url', 'video_id', 'channel_id', 'id', 'text', 'start', 'end'],         num_rows: 208619     })     ```python data[0] ```         {'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4',      'published': '2021-07-06 13:00:03 UTC',      'url': 'https://youtu.be/35Pdoyi6ZoQ',      'video_id': '35Pdoyi6ZoQ',      'channel_id': 'UCv83tO5cePwHMt1952IVVHw',      'id': '35Pdoyi6ZoQ-t0.0',      'text': 'Hi, welcome to the video.',      'start': 0.0,      'end': 9.36}    The dataset contains many small snippets of text data. We will need to merge many snippets from each video to create more substantial chunks of text that contain more information.   ```python from tqdm.auto import tqdm  new_data = []  window = 20  # number of sentences to combine stride = 4  # number of sentences to 'stride' over, used to create overlap  for i in tqdm(range(0, len(data), stride)):     i_end = min(len(data)-1, i+window)     if data[i]['title'] != data[i_end]['title']:         # in this case we skip this entry as we have start/end of two videos         continue     text = ' '.join(data[i:i_end]['text'])     # create the new merged dataset     new_data.append({         'start': data[i]['start'],         'end': data[i_end]['end'],         'title': data[i]['title'],         'text': text,         'id': data[i]['id'],         'url': data[i]['url'],         'published': data[i]['published'],         'channel_id': data[i]['channel_id']     }) ```         0%|          | 0/52155 [00:00<?, ?it/s]    ```python new_data[0] ```         {'start': 0.0,      'end': 74.12,      'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4',      'text': \"Hi, welcome to the video. So this is the fourth video in a Transformers from Scratch mini series. So if you haven't been following along, we've essentially covered what you can see on the screen. So we got some data. We built a tokenizer with it. And then we've set up our input pipeline ready to begin actually training our model, which is what we're going to cover in this video. So let's move over to the code. And we see here that we have essentially everything we've done so far. So we've built our input data, our input pipeline. And we're now at a point where we have a data loader, PyTorch data loader, ready. And we can begin training a model with it. So there are a few things to be aware of. So I mean, first, let's just have a quick look at the structure of our data.\",      'id': '35Pdoyi6ZoQ-t0.0',      'url': 'https://youtu.be/35Pdoyi6ZoQ',      'published': '2021-07-06 13:00:03 UTC',      'channel_id': 'UCv83tO5cePwHMt1952IVVHw'}  ### Indexing Data in Vector DB  Now we need a place to store these embeddings and enable a efficient _vector search_ through them all. To do that we use Pinecone, we can get a [free API key](https://app.pinecone.io) and enter it below where we will initialize our connection to Pinecone and create a new index. You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**.   ```python import pinecone  index_name = 'openai-youtube-transcriptions'  # initialize connection to pinecone (get API key at app.pinecone.io) pinecone.init(     api_key=\"PINECONE_API_KEY\",     environment=\"YOUR_ENVIRONMENT\" )  # check if index already exists (it shouldn't if this is first time) if index_name not in pinecone.list_indexes():     # if does not exist, create index     pinecone.create_index(         index_name,         dimension=len(res['data'][0]['embedding']),         metric='cosine',         metadata_config={'indexed': ['channel_id', 'published']}     ) # connect to index index = pinecone.Index(index_name) # view index stats index.describe_index_stats() ```         {'dimension': 1536,      'index_fullness': 0.0,      'namespaces': {},      'total_vector_count': 0}    We can see the index is currently empty with a `total_vector_count` of `0`. We can begin populating it with OpenAI `text-embedding-ada-002` built embeddings like so:   ```python from tqdm.auto import tqdm import datetime from time import sleep  batch_size = 100  # how many embeddings we create and insert at once  for i in tqdm(range(0, len(new_data), batch_size)):     # find end of batch     i_end = min(len(new_data), i+batch_size)     meta_batch = new_data[i:i_end]     # get ids     ids_batch = [x['id'] for x in meta_batch]     # get texts to encode     texts = [x['text'] for x in meta_batch]     # create embeddings (try-except added to avoid RateLimitError)     try:         res = openai.Embedding.create(input=texts, engine=embed_model)     except:         done = False         while not done:             sleep(5)             try:                 res = openai.Embedding.create(input=texts, engine=embed_model)                 done = True             except:                 pass     embeds = [record['embedding'] for record in res['data']]     # cleanup metadata     meta_batch = [{         'start': x['start'],         'end': x['end'],         'title': x['title'],         'text': x['text'],         'url': x['url'],         'published': x['published'],         'channel_id': x['channel_id']     } for x in meta_batch]     to_upsert = list(zip(ids_batch, embeds, meta_batch))     # upsert to Pinecone     index.upsert(vectors=to_upsert) ```         0%|          | 0/487 [00:00<?, ?it/s]  ### Making Queries  Now we search, for this we need to create a _query vector_ `xq`:   ```python res = openai.Embedding.create(     input=[query],     engine=embed_model )  # retrieve from Pinecone xq = res['data'][0]['embedding']  # get relevant contexts (including the questions) res = index.query(xq, top_k=2, include_metadata=True) ```   ```python res ```         {'matches': [{'id': 'pNvujJ1XyeQ-t418.88',                   'metadata': {'channel_id': 'UCv83tO5cePwHMt1952IVVHw',                                'end': 568.4,                                'published': datetime.date(2021, 11, 24),                                'start': 418.88,                                'text': 'pairs of related sentences you can go '                                        'ahead and actually try training or '                                        'fine-tuning using NLI with multiple '                                        \"negative ranking loss. If you don't have \"                                        'that fine. Another option is that you have '                                        'a semantic textual similarity data set or '                                        'STS and what this is is you have so you '                                        'have sentence A here, sentence B here and '                                        'then you have a score from from 0 to 1 '                                        'that tells you the similarity between '                                        'those two scores and you would train this '                                        'using something like cosine similarity '                                        \"loss. Now if that's not an option and your \"                                        'focus or use case is on building a '                                        'sentence transformer for another language '                                        'where there is no current sentence '                                        'transformer you can use multilingual '                                        'parallel data. So what I mean by that is '                                        'so parallel data just means translation '                                        'pairs so if you have for example a English '                                        'sentence and then you have another '                                        'language here so it can it can be anything '                                        \"I'm just going to put XX and that XX is \"                                        'your target language you can fine-tune a '                                        'model using something called multilingual '                                        'knowledge distillation and what that does '                                        'is takes a monolingual model for example '                                        'in English and using those translation '                                        'pairs it distills the knowledge the '                                        'semantic similarity knowledge from that '                                        'monolingual English model into a '                                        'multilingual model which can handle both '                                        'English and your target language. So '                                        \"they're three options quite popular very \"                                        'common that you can go for and as a '                                        'supervised methods the chances are that '                                        'probably going to outperform anything you '                                        'do with unsupervised training at least for '                                        'now. So if none of those sound like '                                        'something',                                'title': 'Today Unsupervised Sentence Transformers, '                                         'Tomorrow Skynet (how TSDAE works)',                                'url': 'https://youtu.be/pNvujJ1XyeQ'},                   'score': 0.865277052,                   'sparseValues': {},                   'values': []},                  {'id': 'WS1uVMGhlWQ-t737.28',                   'metadata': {'channel_id': 'UCv83tO5cePwHMt1952IVVHw',                                'end': 900.72,                                'published': datetime.date(2021, 10, 20),                                'start': 737.28,                                'text': \"were actually more accurate. So we can't \"                                        \"really do that. We can't use this what is \"                                        'called a mean pooling approach. Or we '                                        \"can't use it in its current form. Now the \"                                        'solution to this problem was introduced by '                                        'two people in 2019 Nils Reimers and Irenia '                                        'Gurevich. They introduced what is the '                                        'first sentence transformer or sentence '                                        'BERT. And it was found that sentence BERT '                                        'or S BERT outformed all of the previous '                                        'Save the Art models on pretty much all '                                        'benchmarks. Not all of them but most of '                                        'them. And it did it in a very quick time. '                                        'So if we compare it to BERT, if we wanted '                                        'to find the most similar sentence pair '                                        'from 10,000 sentences in that 2019 paper '                                        'they found that with BERT that took 65 '                                        'hours. With S BERT embeddings they could '                                        'create all the embeddings in just around '                                        'five seconds. And then they could compare '                                        'all those with cosine similarity in 0.01 '                                        \"seconds. So it's a lot faster. We go from \"                                        '65 hours to just over five seconds which '                                        'is I think pretty incredible. Now I think '                                        \"that's pretty much all the context we need \"                                        'behind sentence transformers. And what we '                                        'do now is dive into a little bit of how '                                        'they actually work. Now we said before we '                                        'have the core transform models and what S '                                        'BERT does is fine tunes on sentence pairs '                                        'using what is called a Siamese '                                        'architecture or Siamese network. What we '                                        'mean by a Siamese network is that we have '                                        'what we can see, what can view as two BERT '                                        'models that are identical and the weights '                                        'between those two models are tied. Now in '                                        'reality when implementing this we just use '                                        'a single BERT model. And what we do is we '                                        'process one sentence, a sentence A through '                                        'the model and then we process another '                                        'sentence, sentence B through the model. '                                        \"And that's the sentence pair. So with our \"                                        'cross-linked we were processing the '                                        'sentence pair together. We were putting '                                        'them both together, processing them all at '                                        'once. This time we process them '                                        'separately. And during training what '                                        'happens is the weights',                                'title': 'Intro to Sentence Embeddings with '                                         'Transformers',                                'url': 'https://youtu.be/WS1uVMGhlWQ'},                   'score': 0.85855335,                   'sparseValues': {},                   'values': []}],      'namespace': ''}  To make this and the next step of building an expanded query simpler, we pack everything into a function named `retrieve`.   ```python limit = 3750  def retrieve(query):     res = openai.Embedding.create(         input=[query],         engine=embed_model     )      # retrieve from Pinecone     xq = res['data'][0]['embedding']      # get relevant contexts     res = index.query(xq, top_k=3, include_metadata=True)     contexts = [         x['metadata']['text'] for x in res['matches']     ]      # build our prompt with the retrieved contexts included     prompt_start = (         \"Answer the question based on the context below.\\n\\n\"+         \"Context:\\n\"     )     prompt_end = (         f\"\\n\\nQuestion: {query}\\nAnswer:\"     )     # append contexts until hitting limit     for i in range(1, len(contexts)):         if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) >= limit:             prompt = (                 prompt_start +                 \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +                 prompt_end             )             break         elif i == len(contexts)-1:             prompt = (                 prompt_start +                 \"\\n\\n---\\n\\n\".join(contexts) +                 prompt_end             )     return prompt ```  Using `retrieve` we return the expanded query:   ```python # first we retrieve relevant items from Pinecone query_with_contexts = retrieve(query) query_with_contexts ```         \"Answer the question based on the context below.\\n\\nContext:\\npairs of related sentences you can go ahead and actually try training or fine-tuning using NLI with multiple negative ranking loss. If you don't have that fine. Another option is that you have a semantic textual similarity data set or STS and what this is is you have so you have sentence A here, sentence B here and then you have a score from from 0 to 1 that tells you the similarity between those two scores and you would train this using something like cosine similarity loss. Now if that's not an option and your focus or use case is on building a sentence transformer for another language where there is no current sentence transformer you can use multilingual parallel data. So what I mean by that is so parallel data just means translation pairs so if you have for example a English sentence and then you have another language here so it can it can be anything I'm just going to put XX and that XX is your target language you can fine-tune a model using something called multilingual knowledge distillation and what that does is takes a monolingual model for example in English and using those translation pairs it distills the knowledge the semantic similarity knowledge from that monolingual English model into a multilingual model which can handle both English and your target language. So they're three options quite popular very common that you can go for and as a supervised methods the chances are that probably going to outperform anything you do with unsupervised training at least for now. So if none of those sound like something\\n\\n---\\n\\nwere actually more accurate. So we can't really do that. We can't use this what is called a mean pooling approach. Or we can't use it in its current form. Now the solution to this problem was introduced by two people in 2019 Nils Reimers and Irenia Gurevich. They introduced what is the first sentence transformer or sentence BERT. And it was found that sentence BERT or S BERT outformed all of the previous Save the Art models on pretty much all benchmarks. Not all of them but most of them. And it did it in a very quick time. So if we compare it to BERT, if we wanted to find the most similar sentence pair from 10,000 sentences in that 2019 paper they found that with BERT that took 65 hours. With S BERT embeddings they could create all the embeddings in just around five seconds. And then they could compare all those with cosine similarity in 0.01 seconds. So it's a lot faster. We go from 65 hours to just over five seconds which is I think pretty incredible. Now I think that's pretty much all the context we need behind sentence transformers. And what we do now is dive into a little bit of how they actually work. Now we said before we have the core transform models and what S BERT does is fine tunes on sentence pairs using what is called a Siamese architecture or Siamese network. What we mean by a Siamese network is that we have what we can see, what can view as two BERT models that are identical and the weights between those two models are tied. Now in reality when implementing this we just use a single BERT model. And what we do is we process one sentence, a sentence A through the model and then we process another sentence, sentence B through the model. And that's the sentence pair. So with our cross-linked we were processing the sentence pair together. We were putting them both together, processing them all at once. This time we process them separately. And during training what happens is the weights\\n\\n---\\n\\nTransformer-based Sequential Denoising Autoencoder. So what we'll do is jump straight into it and take a look at where we might want to use this training approach and and how we can actually implement it. So the first question we need to ask is do we really need to resort to unsupervised training? Now what we're going to do here is just have a look at a few of the most popular training approaches and what sort of data we need for that. So the first one we're looking at here is Natural Language Inference or NLI and NLI requires that we have pairs of sentences that are labeled as either contradictory, neutral which means they're not necessarily related or as entailing or as inferring each other. So you have pairs that entail each other so they are both very similar pairs that are neutral and also pairs that are contradictory. And this is the traditional NLI data. Now using another version of fine-tuning with NLI called a multiple negatives ranking loss you can get by with only entailment pairs so pairs that are related to each other or positive pairs and it can also use contradictory pairs to improve the performance of training as well but you don't need it. So if you have positive pairs of related sentences you can go ahead and actually try training or fine-tuning using NLI with multiple negative ranking loss. If you don't have that fine. Another option is that you have a semantic textual similarity data set or STS and what this is is you have so you have sentence A here, sentence B\\n\\nQuestion: Which training method should I use for sentence transformers when I only have pairs of related sentences?\\nAnswer:\"   Now we pass our expanded query to the LLM:  ```python # then we complete the context-infused query complete(query_with_contexts) ```         'You should use Natural Language Inference (NLI) with multiple negative ranking loss.'    And we get a pretty great answer straight away, specifying to use _multiple-rankings loss_ (also called _multiple negatives ranking loss_). ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e25f"
  },
  "title": "Abstractive Question Answering",
  "category": "630fc5235d91a70054705fb7",
  "content": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/search/question-answering/abstractive-question-answering.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/search/question-answering/abstractive-question-answering.ipynb) [![Open Github](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/github-shield.svg)](https://github.com/pinecone-io/examples/tree/master/search/question-answering/abstractive-question-answering.ipynb)  Abstractive question-answering focuses on the generation of multi-sentence answers to open-ended questions. It usually works by searching massive document stores for relevant information and then using this information to synthetically generate answers. This notebook demonstrates how Pinecone helps you build an abstractive question-answering system. We need three main components:  - A vector index to store and run semantic search - A retriever model for embedding context passages - A generator model to generate answers  # Install Dependencies   ```python !pip install -qU datasets pinecone-client sentence-transformers torch ```       # Load and Prepare Dataset  Our source data will be taken from the Wiki Snippets dataset, which contains over 17 million passages from Wikipedia. But, since indexing the entire dataset may take some time, we will only utilize 50,000 passages in this demo that include \"History\" in the \"section title\" column. If you want, you may utilize the complete dataset. Pinecone vector database can effortlessly manage millions of documents for you.   ```python from datasets import load_dataset  # load the dataset from huggingface in streaming mode and shuffle it wiki_data = load_dataset(     'vblagoje/wikipedia_snippets_streamed',     split='train',     streaming=True ).shuffle(seed=960) ```   We are loading the dataset in the streaming mode so that we don't have to wait for the whole dataset to download (which is over 9GB). Instead, we iteratively download records one at a time.   ```python # show the contents of a single document in the dataset next(iter(wiki_data)) ```         {'wiki_id': 'Q7649565',      'start_paragraph': 20,      'start_character': 272,      'end_paragraph': 24,      'end_character': 380,      'article_title': 'Sustainable Agriculture Research and Education',      'section_title': \"2000s & Evaluation of the program's effectiveness\",      'passage_text': \"preserving the surrounding prairies. It ran until March 31, 2001.\\nIn 2008, SARE celebrated its 20th anniversary. To that date, the program had funded 3,700 projects and was operating with an annual budget of approximately $19 million. Evaluation of the program's effectiveness As of 2008, 64% of farmers who had received SARE grants stated that they had been able to earn increased profits as a result of the funding they received and utilization of sustainable agriculture methods. Additionally, 79% of grantees said that they had experienced a significant improvement in soil quality though the environmentally friendly, sustainable methods that they were\"}     ```python # filter only documents with History as section_title history = wiki_data.filter(     lambda d: d['section_title'].startswith('History') ) ```  Let's iterate through the dataset and apply our filter to select the 50,000 historical passages. We will extract `article_title`, `section_title` and `passage_text` from each document.   ```python from tqdm.auto import tqdm  # progress bar  total_doc_count = 50000  counter = 0 docs = [] # iterate through the dataset and apply our filter for d in tqdm(history, total=total_doc_count):     # extract the fields we need     doc = {         \"article_title\": d[\"article_title\"],         \"section_title\": d[\"section_title\"],         \"passage_text\": d[\"passage_text\"]     }     # add the dict containing fields we need to docs list     docs.append(doc)      # stop iteration once we reach 50k     if counter == total_doc_count:         break      # increase the counter on every iteration     counter += 1 ```         100%|██████████| 50000/50000 [05:18<00:00, 145.03it/s]    ```python import pandas as pd  # create a pandas dataframe with the documents we extracted df = pd.DataFrame(docs) df.head() ```      <div id=\"df-7daab3f7-5df6-4a4c-bc59-831df588cd12\">   <div class=\"colab-df-container\">     <div>       <style scoped>           .dataframe tbody tr th:only-of-type {               vertical-align: middle;           }           .dataframe tbody tr th {               vertical-align: top;           }           .dataframe thead th {               text-align: right;           }       </style>       <table border=\"1\" class=\"dataframe\">         <thead>           <tr style=\"text-align: right;\">             <th></th>             <th>article_title</th>             <th>section_title</th>             <th>passage_text</th>           </tr>         </thead>         <tbody>           <tr>             <th>0</th>             <td>Taupo District</td>             <td>History</td>             <td>was not until the 1950s that the region starte...</td>           </tr>           <tr>             <th>1</th>             <td>Sutarfeni</td>             <td>History &amp; Western asian analogues</td>             <td>Sutarfeni History strand-like pheni were Phena...</td>           </tr>           <tr>             <th>2</th>             <td>The Bishop Wand Church of England School</td>             <td>History</td>             <td>The Bishop Wand Church of England School Histo...</td>           </tr>           <tr>             <th>3</th>             <td>Teufelsmoor</td>             <td>History &amp; Situation today</td>             <td>made to preserve the original landscape, altho...</td>           </tr>           <tr>             <th>4</th>             <td>Surface Hill Uniting Church</td>             <td>History</td>             <td>in perpetual reminder that work and worship go...</td>           </tr>         </tbody>       </table>     </div>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>   <script>         const buttonEl =           document.querySelector('#df-7daab3f7-5df6-4a4c-bc59-831df588cd12 button.colab-df-convert');         buttonEl.style.display =           google.colab.kernel.accessAllowed ? 'block' : 'none';          async function convertToInteractive(key) {           const element = document.querySelector('#df-7daab3f7-5df6-4a4c-bc59-831df588cd12');           const dataTable =             await google.colab.kernel.invokeFunction('convertToInteractive',                                                     [key], {});           if (!dataTable) return;            const docLinkHtml = 'Like what you see? Visit the ' +             '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'             + ' to learn more about interactive tables.';           element.innerHTML = '';           dataTable['output_type'] = 'display_data';           await google.colab.output.renderOutput(dataTable, element);           const docLink = document.createElement('div');           docLink.innerHTML = docLinkHtml;           element.appendChild(docLink);         }   </script>   </div>   </div>     # Initialize Pinecone Index  The Pinecone index stores vector representations of our historical passages which we can retrieve later using another vector (query vector). To build our vector index, we must first establish a connection with Pinecone. For this, we need an API from Pinecone. You can get one for free from [here](https://app.pinecone.io/). You also need to know the environment for your index; for new accounts, the default environment is `us-east1-gcp`.   We initialize the connection as follows:   ```python import pinecone  # connect to pinecone environment pinecone.init(     api_key=\"<<YOUR_API_KEY>>\",     environment=\"<<YOUR_ENVIRONMENT>>\" ) ```  Now we create a new index. We will name it \"abstractive-question-answering\" — you can name it anything we want. We specify the metric type as \"cosine\" and dimension as 768 because the retriever we use to generate context embeddings is optimized for cosine similarity and outputs 768-dimension vectors.   ```python index_name = \"abstractive-question-answering\"  # check if the abstractive-question-answering index exists if index_name not in pinecone.list_indexes():     # create the index if it does not exist     pinecone.create_index(         index_name,         dimension=768,         metric=\"cosine\"     )  # connect to abstractive-question-answering index we created index = pinecone.Index(index_name) ```  # Initialize Retriever  Next, we need to initialize our retriever. The retriever will mainly do two things:  - Generate embeddings for all historical passages (context vectors/embeddings) - Generate embeddings for our questions (query vector/embedding)  The retriever will create embeddings such that the questions and passages that hold the answers to our queries are close to one another in the vector space. We will use a SentenceTransformer model based on Microsoft's MPNet as our retriever. This model performs quite well for comparing the similarity between queries and documents. We can use Cosine Similarity to compute the similarity between query and context vectors generated by this model (Pinecone automatically does this for us).   ```python import torch from sentence_transformers import SentenceTransformer  # set device to GPU if available device = 'cuda' if torch.cuda.is_available() else 'cpu' # load the retriever model from huggingface model hub retriever = SentenceTransformer(\"flax-sentence-embeddings/all_datasets_v3_mpnet-base\", device=device) retriever ```       SentenceTransformer(       (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: MPNetModel        (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})       (2): Normalize()     )    # Generate Embeddings and Upsert  Next, we need to generate embeddings for the context passages. We will do this in batches to help us more quickly generate embeddings and upload them to the Pinecone index. When passing the documents to Pinecone, we need an id (a unique value), context embedding, and metadata for each document representing context passages in the dataset. The metadata is a dictionary containing data relevant to our embeddings, such as the article title, section title, passage text, etc.   ```python # we will use batches of 64 batch_size = 64  for i in tqdm(range(0, len(df), batch_size)):     # find end of batch     i_end = min(i+batch_size, len(df))     # extract batch     batch = df.iloc[i:i_end]     # generate embeddings for batch     emb = retriever.encode(batch[\"passage_text\"].tolist()).tolist()     # get metadata     meta = batch.to_dict(orient=\"records\")     # create unique IDs     ids = [f\"{idx}\" for idx in range(i, i_end)]     # add all to upsert list     to_upsert = list(zip(ids, emb, meta))     # upsert/insert these records to pinecone     _ = index.upsert(vectors=to_upsert)  # check that we have all vectors in index index.describe_index_stats() ```         100%|██████████| 782/782 [13:56<00:00, 1.13it/s]      {'dimension': 768,      'index_fullness': 0.1,      'namespaces': {'': {'vector_count': 50001}},      'total_vector_count': 50001}    # Initialize Generator  We will use ELI5 BART for the generator which is a Sequence-To-Sequence model trained using the ‘Explain Like I’m 5’ (ELI5) dataset. Sequence-To-Sequence models can take a text sequence as input and produce a different text sequence as output.  The input to the ELI5 BART model is a single string which is a concatenation of the query and the relevant documents providing the context for the answer. The documents are separated by a special token &lt;P>, so the input string will look as follows:  >question: What is a sonic boom? context: &lt;P> A sonic boom is a sound associated with shock waves created when an object travels through the air faster than the speed of sound. &lt;P> Sonic booms generate enormous amounts of sound energy, sounding similar to an explosion or a thunderclap to the human ear. &lt;P> Sonic booms due to large supersonic aircraft can be particularly loud and startling, tend to awaken people, and may cause minor damage to some structures. This led to prohibition of routine supersonic flight overland.  More detail on how the ELI5 dataset was built is available [here](https://arxiv.org/abs/1907.09190) and how ELI5 BART model was trained is available [here](https://yjernite.github.io/lfqa.html).  Let's initialize the BART model using transformers.   ```python from transformers import BartTokenizer, BartForConditionalGeneration  # load bart tokenizer and model from huggingface tokenizer = BartTokenizer.from_pretrained('vblagoje/bart_lfqa') generator = BartForConditionalGeneration.from_pretrained('vblagoje/bart_lfqa').to(device) ```   All the components of our abstract QA system are complete and ready to be queried. But first, let's write some helper functions to retrieve context passages from Pinecone index and to format the query in the way the generator expects the input.   ```python def query_pinecone(query, top_k):     # generate embeddings for the query     xq = retriever.encode([query]).tolist()     # search pinecone index for context passage with the answer     xc = index.query(xq, top_k=top_k, include_metadata=True)     return xc ```   ```python def format_query(query, context):     # extract passage_text from Pinecone search result and add the <P> tag     context = [f\"<P> {m['metadata']['passage_text']}\" for m in context]     # concatinate all context passages     context = \" \".join(context)     # contcatinate the query and context passages     query = f\"question: {query} context: {context}\"     return query ```  Let's test the helper functions. We will query the Pinecone index function we created earlier with the `query_pinecone` to get context passages and pass them to the `format_query` function.   ```python query = \"when was the first electric power system built?\" result = query_pinecone(query, top_k=1) result ```         {'matches': [{'id': '3593',                   'metadata': {'article_title': 'Electric power system',                                'passage_text': 'Electric power system History In '                                                '1881, two electricians built the '                                                \"world's first power system at \"                                                'Godalming in England. It was '                                                'powered by two waterwheels and '                                                'produced an alternating current '                                                'that in turn supplied seven '                                                'Siemens arc lamps at 250 volts and '                                                '34 incandescent lamps at 40 volts. '                                                'However, supply to the lamps was '                                                'intermittent and in 1882 Thomas '                                                'Edison and his company, The Edison '                                                'Electric Light Company, developed '                                                'the first steam-powered electric '                                                'power station on Pearl Street in '                                                'New York City. The Pearl Street '                                                'Station initially powered around '                                                '3,000 lamps for 59 customers. The '                                                'power station generated direct '                                                'current and',                                'section_title': 'History'},                   'score': 0.69118017,                   'values': []}],      'namespace': ''}     ```python from pprint import pprint ```   ```python # format the query in the form generator expects the input query = format_query(query, result[\"matches\"]) pprint(query) ```      ('question: when was the first electric power system built? context: <P> '      \"Electric power system History In 1881, two electricians built the world's \"      'first power system at Godalming in England. It was powered by two '      'waterwheels and produced an alternating current that in turn supplied seven '      'Siemens arc lamps at 250 volts and 34 incandescent lamps at 40 volts. '      'However, supply to the lamps was intermittent and in 1882 Thomas Edison and '      'his company, The Edison Electric Light Company, developed the first '      'steam-powered electric power station on Pearl Street in New York City. The '      'Pearl Street Station initially powered around 3,000 lamps for 59 customers. '      'The power station generated direct current and')       The output looks great. Now let's write a function to generate answers.   ```python def generate_answer(query):     # tokenize the query to get input_ids     inputs = tokenizer([query], max_length=1024, return_tensors=\"pt\")     # use generator to predict output ids     ids = generator.generate(inputs[\"input_ids\"], num_beams=2, min_length=20, max_length=40)     # use tokenizer to decode the output ids     answer = tokenizer.batch_decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]     return pprint(answer) ```   ```python generate_answer(query) ```      ('The first electric power system was built in 1881 at Godalming in England. '      'It was powered by two waterwheels and produced alternating current that in '      'turn supplied seven Siemens arc lamps')       As we can see, the generator used the provided context to answer our question. Let's run some more queries.   ```python query = \"How was the first wireless message sent?\" context = query_pinecone(query, top_k=5) query = format_query(query, context[\"matches\"]) generate_answer(query) ```      ('The first wireless message was sent in 1866 by Mahlon Loomis, who had a kite '      'on a mountaintop 14 miles apart. The kite was connected to a cable')       To confirm that this answer is correct, we can check the contexts used to generate the answer.   ```python for doc in context[\"matches\"]:     print(doc[\"metadata\"][\"passage_text\"], end='\\n---\\n') ```      by electrostatic induction or electromagnetic induction, which had too short a range to be practical. In 1866 Mahlon Loomis claimed to have transmitted an electrical signal through the atmosphere between two 600 foot wires held aloft by kites on mountaintops 14 miles apart. Thomas Edison had come close to discovering radio in 1875; he had generated and detected radio waves which he called \"etheric currents\" experimenting with high-voltage spark circuits, but due to lack of time did not pursue the matter. David Edward Hughes in 1879 had also stumbled on radio wave transmission which he received with his carbon microphone     ---     the east coast of India, then on to Penang, Malacca, Singapore, Batavia (current Jakarta), to finally reach Darwin, Australia. It was the first direct link between Australia and Great Britain. The company that laid the first part of the cable took the name of Falmouth, Gibraltar and Malta Telegraph Company and had been founded in 1869. This company later operated as the Eastern Telegraph Company from Mount Pleasant in Gibraltar and eventually became Cable & Wireless.     The first telephones were introduced to Gibraltar in 1886 by a private company which was later taken over by the colonial authorities. The first wireless     ---     audio distance records, and were heard as far west as Hawaii. They were also received in Paris, France, which marked the first transmission of speech across the Atlantic.     With the entrance of the United States into World War I in April 1917 the federal government took over full control of the radio industry, and it became illegal for civilians to possess an operational radio receiver. However NAA continued to operate during the conflict. In addition to time signals and weather reports, it also broadcast news summaries received by troops on land and aboard ships in the Atlantic. Effective April 15, 1919     ---     Message from space (science fiction) For other uses, see Message from Space (disambiguation).     \"Message from space\" is a type of \"first contact\" theme in science fiction . Stories of this type involve receiving an interstellar message which reveals the existence of other intelligent life in the universe. History An early short story, A Message from Space (Joseph Schlossel, Weird Tales, March 1926) tells of an amateur who builds a ham TV set and suddenly sees an alien, The latter one realises it is being watched and tells its soap opera story. The verdict of Everett Franklin Bleiler: \"original ideas, but clumsy     ---     radio operators, interested in a practical benefit from their hobby, and jewelers, who previously had been reliant on time services transmitted over telegraph wires, which had a reputation for being both expensive and of questionable reliability, especially compared to the free and very accurate NAA transmissions.     NAA's original transmitters were only capable of producing the dots-and-dashes of Morse code. The later development of vacuum tube transmitters made audio transmissions practical, and in 1915 the American Telephone and Telegraph Company (AT&T) received permission from the Navy to conduct a series of tests at the NAA facility. These experimental transmissions set impressive new     ---       In this case, the answer looks correct. If we ask a question and no relevant contexts are retrieved, the generator will typically return nonsensical or false answers, like with this question about COVID-19:   ```python query = \"where did COVID-19 originate?\" context = query_pinecone(query, top_k=3) query = format_query(query, context[\"matches\"]) generate_answer(query) ```      ('COVID-19 is a zoonotic disease, which means that it is a virus that is '      'transmitted from one animal to another. It is not a virus that can be '      'transmitted from person')        ```python for doc in context[\"matches\"]:     print(doc[\"metadata\"][\"passage_text\"], end='\\n---\\n') ```      to establish with certainty which diseases jumped from other animals to humans, but there is increasing evidence from DNA and RNA sequencing, that measles, smallpox, influenza, HIV, and diphtheria came to humans this way. Various forms of the common cold and tuberculosis also are adaptations of strains originating in other species.     Zoonoses are of interest because they are often previously unrecognized diseases or have increased virulence in populations lacking immunity. The West Nile virus appeared in the United States in 1999 in the New York City area, and moved through the country in the summer of 2002, causing much distress. Bubonic     ---     plague is a zoonotic disease, as are salmonellosis, Rocky Mountain spotted fever, and Lyme disease.     A major factor contributing to the appearance of new zoonotic pathogens in human populations is increased contact between humans and wildlife. This can be caused either by encroachment of human activity into wilderness areas or by movement of wild animals into areas of human activity. An example of this is the outbreak of Nipah virus in peninsular Malaysia in 1999, when intensive pig farming began on the habitat of infected fruit bats. Unidentified infection of the pigs amplified the force of infection, eventually transmitting the virus     ---     man killed and twenty-nine died of disease.     ---       Let’s finish with a final few questions.   ```python query = \"what was the war of currents?\" context = query_pinecone(query, top_k=5) query = format_query(query, context[\"matches\"]) generate_answer(query) ```      ('The War of Currents was a series of events in the early 1900s between Edison '      'and Westinghouse. The two companies were competing for the market share of '      'electric power in the United States')        ```python query = \"who was the first person on the moon?\" context = query_pinecone(query, top_k=10) query = format_query(query, context[\"matches\"]) generate_answer(query) ```      ('The first person to walk on the moon was Neil Armstrong in 1969. He walked '      'on the moon in 1969. He was the first person to walk on the moon.')        ```python query = \"what was NASAs most expensive project?\" context = query_pinecone(query, top_k=3) query = format_query(query, context[\"matches\"]) generate_answer(query) ```      ('The Space Shuttle was the most expensive project in the history of NASA. It '      'cost about $10 billion to build.')       As we can see, the model can generate some great answers.  # Example Application  To try out an example application of abstractive QA, see this [demo app](https://huggingface.co/spaces/pinecone/abstractive-question-answering). ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e261"
  },
  "title": "Operationalize vector search with Pinecone and Feast Feature Store",
  "category": "630fc5235d91a70054705fb7",
  "content": "Vector embeddings are the key ingredient that makes [similarity search](https://www.pinecone.io/learn/what-is-similarity-search/) possible. Raw data goes from a data store or data stream, through an embedding model to be converted into a [vector embedding](https://www.pinecone.io/learn/vector-embeddings/), and finally into the [vector search index](https://www.pinecone.io/learn/vector-database/).  ![Vector search with Pinecone](https://raw.githubusercontent.com/pinecone-io/img/main/pinecone-feast-vector-search.png)  If you have multiple data sources, frequent data updates, and are constantly experimenting with different models, then it becomes harder to maintain an accurate and up-to-date search index. That could lead to subpar results in your recommender systems, search applications, or wherever you are using vector search.  How you store and manage the assets — vector embeddings — is crucial to the accuracy and freshness of your vector search results. This is where “feature stores” come in. [Features stores](https://www.tecton.ai/blog/what-is-a-feature-store/) provide a centralized place for managing vector embeddings within organizations with sprawling data sources and frequently updated models. They enable efficient feature engineering and management, feature reuse, and consistency between online and batch embedding models.  Combining a feature store with a similarity search service leads to more accurate and reliable retrieval within your AI/ML applications. In this article, we will build a [question-answering application](question-answering.md) to demonstrate how the [Feast feature store](https://feast.dev/) can be used alongside the Pinecone vector search solution.  ![Vector search with Feast feature store and Pinecone](https://raw.githubusercontent.com/pinecone-io/img/main/pinecone-feast-question-answering.png)   The steps are:  1. Create a catalog of questions with known answers by loading the raw text and their vector embeddings into Feast. 1. Index vector embeddings of those questions in Pinecone so we can search through them by [semantic similarity](https://www.pinecone.io/learn/semantic-search/). 1. Transform new, incoming questions into vector embeddings and catalog them in Feast, then query Pinecone for the IDs of the most similar known questions, and finally fetch the text of those questions from Feast and display results to the user.  Let’s begin! You can also [view the source code on GitHub](https://github.com/pinecone-io/examples/blob/master/feature-store/feast_feature_store_notebook.ipynb).  ## Setup  Let's install and load necessary Python packages in your preferred cloud environment, like Google Colab.  ```python !pip install -qU feast !pip install -qU sentence-transformers --no-cache-dir !pip install -qU pinecone-client ```  > If you are using Google Colab, restart the runtime after the installation. >  ```python import os import pandas as pd import numpy as np ```  ## Dataset and Model  We use the [Quora Question Pairs Dataset](https://www.kaggle.com/c/quora-question-pairs) that enables a question-answering application. We index a set of questions that can be associated with answers. The application utilizes a new question's vector embedding to retrieve the top relevant stored question and its associated answer.   The embeddings stored in the feature store are created using the [Average Word Embeddings Models](https://www.sbert.net/docs/pretrained_models.html#average-word-embeddings-models). Since we want to query new questions and find the most similar match among questions in the feature store, we need to create a comparable vector. This means that once we define a new question, we will transform it into a vector embedding using the same model.   ```python from sentence_transformers import SentenceTransformer  model = SentenceTransformer('average_word_embeddings_komninos') ```   ## Feast Feature Store  It's time to set up our Feast feature store. We will follow the [tutorial for creating a Feast feature store](https://docs.feast.dev/quickstart). We intend to use Feast for storing questions and their vector embeddings.    For each question, we will store the following information: * A question identifier number. We will store these ids along with the corresponding embeddings in the similarity search index.  * The question's text.  * The question's vector embeddings. Here, the features being learned using a deep neural network and don't have an intuitive meaning. Thus, we denote them with their index number `e0 ... e300`.  Note that if our data contained the answers, we should have stored them along with this information.   ```python # Initialize feast feature store !feast init feature_repo os.chdir('feature_repo') ```  Choose one of these two options to include the necessary file.  *   You can find the file *questions.parquet* as part of the example. This file contains pre-computed embeddings for each question from the past. We will load this data into our feature store. Please add the *questions.parquet* file to the /feature_repo/data path.  *   You can run the code from the section **Create parquet file yourself (Optional)**. Using this code, you can control the number of questions you include in the example.  Once we created the feature store and placed the parquet file where necessary, we have to overwrite the default *example.py* file. This file defines the file source, the entity definition, and the feature view to serve once online.   We will define another file - *test_example.py*, which will contain the feature view definition for the test questions. Test questions will be defined later, saved into a new parquet file, and loaded into a feature store.  > *Note: We added a one-day expiration to the feature view (notice the TTL field).   ```python %%writefile ./example.py   from feast import Entity, Feature, FeatureView, ValueType from feast.data_source import FileSource import os import platform  path = os.getcwd() + \"/data/questions.parquet\" source = FileSource(     path= path if platform.system() != 'Windows' else path.replace('/', '\\\\'),     event_timestamp_column=\"datetime\", )  question = Entity(name=\"qid1\", value_type=ValueType.INT64)  question_feature = Feature(     name=\"question1\",     dtype=ValueType.STRING )  embedding_features = [         Feature(name=f\"e_{i}\", dtype=ValueType.FLOAT)         for i in range(300)       ]  questions_view = FeatureView(     name=\"questions\",     entities=[\"qid1\"],     ttl=timedelta(days=1),     features= [question_feature, *embedding_features],          input=source,      )  ```      Overwriting ./example.py    ```python %%writefile ./test_example.py  from google.protobuf.duration_pb2 import Duration  from feast import Entity, Feature, FeatureView, ValueType from feast.data_source import FileSource import os import platform  path = os.getcwd() + \"/data/test_questions.parquet\" source = FileSource(     path= path if platform.system() != 'Windows' else path.replace('/', '\\\\'),     event_timestamp_column=\"datetime\",     created_timestamp_column=\"created\", )  test_question = Entity(name=\"qid1\", value_type=ValueType.INT64, description=\"question id\",)  question_feature = Feature(     name=\"question1\",     dtype=ValueType.STRING )  embedding_features = [         Feature(name=f\"e_{i}\", dtype=ValueType.FLOAT)         for i in range(300)       ]  test_questions_view = FeatureView(     name=\"test_questions\",     entities=[\"qid1\"],     ttl=Duration(seconds=86400 * 1),     features= [question_feature, *embedding_features],     online=True,     input=source,     tags={}, ) ```      Overwriting ./test_example.py   To deploy our infrastructure, we need to run the following command.   ```python # Register the features !feast apply ```  Finally, we need to populate the online store with the most recent features from the offline store. We can do that with the following command.   ```python !feast materialize 2021-06-02T00:00:00 2021-07-10T00:00:00 --views questions ```  > Note: Don't forget to change the end date if you created the parquet file yourself!   ## Uploading Vectors into Pinecone   After setting up our feature store, we are ready to index our question vectors within Pinecone's similarity search service. Let's start by defining a Pinecone index, and then uploading the stored vectors into Pinecone.   ### Pinecone Setup  ```python import pinecone ```  Use your API key to connect to Pinecone. In case you don't have one, [get your API key here](https://www.pinecone.io/start/).   ```python # Load Pinecone API key api_key = os.getenv(\"PINECONE_API_KEY\") or '<YOUR API KEY>' pinecone.init(api_key=api_key) pinecone.list_indexes() ```  Create a new vector index.  ```python # Pick a name for the new index index_name = 'feast-questions' ```   ```python if index_name in pinecone.list_indexes():     pinecone.delete_index(index_name) ```   ```python # Create a new vector index pinecone.create_index(name=index_name, metric='cosine', shards=1) ``` ### Upload from Feature Store  We fetch the questions' vectors from the feature store in batches and upload them into Pinecone's vector index.   ```python # Get question ids from the file question_ids = pd.read_parquet('./data/questions.parquet', columns=['qid1']) ```   ```python # Define a batch size to read from Feast BATCH_SIZE = 1000 ```   ```python # Connect to the created index index = pinecone.Index(name = index_name, response_timeout=300) ```   ```python # Print info index.info() ```      InfoResult(index_size=0)  ```python from feast import FeatureStore  store = FeatureStore(repo_path=\".\")  for i in range(0, len(question_ids), BATCH_SIZE):     batch = question_ids[i: i+BATCH_SIZE]      feature_vectors = store.get_online_features(         feature_refs=[f'questions:e_{i}'                       for i in range(300)                      ],         entity_rows=[{\"qid1\":_id} for _id in batch.qid1.to_list()]     ).to_dict()      # Prepare list of items to upload into Pinecone's index     items_to_insert = []      for e in range(len(feature_vectors['qid1'])):         l = [feature_vectors[f'questions__e_{i}'][e] for i in range(300)]         items_to_insert.append((feature_vectors['qid1'][e], np.array(l)))          # Upsert batch data     index.upsert(items=items_to_insert)   ```   ```python # Print index info index.info() ```      InfoResult(index_size=10000)   ## Query  We are now all set to start querying our similarity search index. Our queries are questions in text format. We will transform the question into a vector embedding, serve this query vector into Pinecone's service, and retrieve a set of top-matched stored question IDs. Since Feast acts as the centralized source of truth for feature vectors, we will store the transformed question vectors in Feast and materialize query vectors before forwarding them to Pinecone.   This section describes how to:  * Define new questions and create their embeddings * Manage these embeddings in Feast:   * Load these embeddings into Feast   * Fetch test question embeddings from Feast * Query Pinecone with the fetched vector embeddings   ### Define New questions and Create their embeddings  Let's define new questions first.  ```python df_new_questions = pd.DataFrame([[1000001, 'How can I make money using Youtube?'],                                   [1000002, 'What is the best book for learning Python?']], columns=['qid1', 'question1']) df_new_questions ```   <div> <table class=\"table table-responsive\">   <thead>     <tr>       <th></th>       <th>qid1</th>       <th>question1</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>1000001</td>       <td>How can I make money using Youtube?</td>     </tr>     <tr>       <th>1</th>       <td>1000002</td>       <td>What is the best book for learning Python?</td>     </tr>   </tbody> </table> </div>   Then, we create embeddings for these questions and save them in a new parquet file.  ```python # Create embedding for each question df_new_questions['question_vector'] = df_new_questions.question1.apply(lambda x: model.encode(str(x), show_progress_bar=False))  # Create timestamps  df_new_questions['created'] = datetime.datetime.utcnow() df_new_questions['datetime'] = df_new_questions['created'].dt.floor('h')  # Generate columns for vector elements df_new_questions2 = df_new_questions.question_vector.apply(pd.Series) df_new_questions2.columns = [f'e_{i}' for i in range(300)] result = pd.concat([df_new_questions, df_new_questions2], axis=1)  # Exclude some columns result = result.drop(['question_vector'], axis=1)  # Change directory if needed if os.getcwd().split('/')[-1] != 'feature_repo':     os.chdir('feature_repo')  # Save to parquet file result.to_parquet('./data/test_questions.parquet') ```  ### Manage the Embeddings in Feast  Recall that we created and deployed a feature view called **test_questions** earlier that loads the file we have just created.  We will make these questions accessible when querying the feature store online.   ```python !feast materialize 2021-06-02T00:00:00 2021-07-10T00:00:00 --views test_questions ```  Now that we have their embeddings in the feature store, we will show how you can fetch the questions using the ids.   ```python # Fetch the feature store and get feature vectors for the query questions store = FeatureStore(repo_path=\".\")  feature_vectors = store.get_online_features(     feature_refs=[f'test_questions:question1',                   *[f'test_questions:e_{i}'                     for i in range(300)                   ]],     entity_rows=[{\"qid1\":_id} for _id in df_new_questions.qid1.tolist()] ).to_dict()  # Prepare list of vectors to query Pinecone query_vectors = []  for e in range(len(feature_vectors['qid1'])):     l = [feature_vectors[f'test_questions__e_{i}'][e] for i in range(300)]     query_vectors.append(np.array(l)) ```  ### Query Pinecone  Next, we query Pinecone and show the most similar questions (from the sample dataset).  ```python # Query Pinecone's index query_results = index.query(queries=query_vectors, top_k=5)  # Show results for e, res in enumerate(query_results):     print(e)     print('\\n\\n\\n Original question: ' + feature_vectors['test_questions__question1'][e])     print('\\n Most similar questions based on Pinecone vector search: \\n')      # Fetch from Feast to get question text     result_feature_vectors = store.get_online_features(         feature_refs=[f'questions:question1'],         entity_rows=[{\"qid1\":int(_id)} for _id in res.ids]     ).to_dict()      # Prepare and display table     df_result = pd.DataFrame({'id':res.ids,                               'question': result_feature_vectors['questions__question1'],                               'score':res.scores})     display(df_result) ```       Original question: How can I make money using Youtube?           Most similar questions based on Pinecone vector search:   <div> <table class=\"table table-responsive\">   <thead>     <tr>       <th></th>       <th>id</th>       <th>question</th>       <th>score</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>1292</td>       <td>How do I make money with YouTube?</td>       <td>0.944259</td>     </tr>     <tr>       <th>1</th>       <td>14375</td>       <td>How do I make money using Instagram?</td>       <td>0.936641</td>     </tr>     <tr>       <th>2</th>       <td>1126</td>       <td>How can I earn money from YouTube?</td>       <td>0.866271</td>     </tr>     <tr>       <th>3</th>       <td>3759</td>       <td>How do you make money giving through a app?</td>       <td>0.864226</td>     </tr>     <tr>       <th>4</th>       <td>157</td>       <td>How can I make money through the Internet?</td>       <td>0.858337</td>     </tr>   </tbody> </table> </div>         Original question: What is the best book for learning Python?           Most similar questions based on Pinecone vector search:      <div> <table class=\"table table-responsive\">   <thead>     <tr>       <th></th>       <th>id</th>       <th>question</th>       <th>score</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>10033</td>       <td>What is the best Python learning book for beginners?</td>       <td>0.945661</td>     </tr>     <tr>       <th>1</th>       <td>16072</td>       <td>Which is the best book for learning Python 3 for absolute beginners?</td>       <td>0.872750</td>     </tr>     <tr>       <th>2</th>       <td>13142</td>       <td>What's the best way to learn python on my own?</td>       <td>0.847575</td>     </tr>     <tr>       <th>3</th>       <td>8939</td>       <td>Which is the best book for learning android programming from sratch?</td>       <td>0.845041</td>     </tr>     <tr>       <th>4</th>       <td>7023</td>       <td>What is the best beginner friendly book on python?</td>       <td>0.829327</td>     </tr>   </tbody> </table> </div> ## Turn off the Pinecone Service  Turn off the service once you are sure that you do not want to use it anymore. Once the service is stopped, you cannot use it again.  ```python pinecone.delete_index(index_name) ```  ## Summary  We demonstrated the integration between two emerging core ML/AI infrastructure technologies, feature stores and vector similarity search engines.  These technologies deal with feature vectors, the core information unit of any AI/ML application. Feature stores are responsible for all operational aspects of feature vectors, while similarity search engines enable numerous applications relying on semantic retrieval of those vectors.    --- ## Optional: Create Your Parquet File  This section presents the code for creating a *questions.parquet* file for the feature store. We used a sample of 10,000 questions in the default parquet file that we showed. Using the following code, you can create a *questions.parquet* file with a different number of questions. That way, you can try out what happens once you have fewer/more questions.   ```python # Download dataset import requests, os, zipfile  DATA_DIR = \"tmp\" QA_DIR = f\"{DATA_DIR}/quora_duplicate_questions\" QA_FILE = f\"{DATA_DIR}/quora_duplicate_questions.tsv\" QA_URL = \"https://qim.fs.quoracdn.net/quora_duplicate_questions.tsv\"   def download_data():     os.makedirs(DATA_DIR, exist_ok=True)      if not os.path.exists(QA_DIR):         if not os.path.exists(QA_FILE):             r = requests.get(QA_URL)              with open(QA_FILE, \"wb\") as f:                 f.write(r.content)  download_data() ```   ```python pd.set_option('display.max_colwidth', 500) df = pd.read_csv(QA_FILE, sep='\\t',  usecols=[\"qid1\", \"question1\"], index_col=False) df = df.reset_index(drop=True) df.drop_duplicates(inplace=True) df.head() ``` <div> <table class=\"table table-responsive\">   <thead>     <tr>       <th></th>       <th>qid1</th>       <th>question1</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>1</td>       <td>What is the step by step guide to invest in share market in india?</td>     </tr>     <tr>       <th>1</th>       <td>3</td>       <td>What is the story of Kohinoor (Koh-i-Noor) Diamond?</td>     </tr>     <tr>       <th>2</th>       <td>5</td>       <td>How can I increase the speed of my internet connection while using a VPN?</td>     </tr>     <tr>       <th>3</th>       <td>7</td>       <td>Why am I mentally very lonely? How can I solve it?</td>     </tr>     <tr>       <th>4</th>       <td>9</td>       <td>Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?</td>     </tr>   </tbody> </table> </div>   ```python # Set any value for number of questions NUM_OF_QUESTIONS = 10000 # Or select the complete dataset #NUM_OF_QUESTIONS = len(df) ```   ```python import datetime  # Use only defined number of rows df = df[:NUM_OF_QUESTIONS]  # Create embedding for each question df['question_vector'] = df.question1.apply(lambda x: model.encode(str(x)))  # Create timestamps  df['created'] = datetime.datetime.utcnow() df['datetime'] = df['created'].dt.floor('h')  # Generate columns for vector elements df2 = df.question_vector.apply(pd.Series) df2.columns = [f'e_{i}' for i in range(300)] result = pd.concat([df, df2], axis=1)  # Exclude some columns result = result.drop(['question_vector'], axis=1)  # Change directory if needed if os.getcwd().split('/')[-1] != 'feature_repo':     os.chdir('feature_repo')      # Save to parquet file result[:NUM_OF_QUESTIONS].to_parquet('./data/questions.parquet') ``` ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e263"
  },
  "title": "Table Question Answering",
  "category": "630fc5235d91a70054705fb7",
  "content": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/search/question-answering/table-qa.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/search/question-answering/table-qa.ipynb) [![Open Github](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/github-shield.svg)](https://github.com/pinecone-io/examples/tree/master/search/question-answering/table-qa.ipynb)  Table Question Answering (Table QA) refers to providing precise answers from tables to answer a user's question. With recent works on Table QA, is it now possible to answer natural language queries from tabular data. This notebook demonstrates how you can build a Table QA system that can answer your natural language queries using the Pinecone vector database.   We need three main components to build the Table QA system:  - A vector index to store table embeddings - A retriever model for embedding queries and tables - A reader model to read the tables and extract answers  # Install Dependencies   ```python # torch-scatter may take few minutes to install !pip install datasets pinecone-client sentence_transformers torch-scatter ```  # Load the Dataset  We will work with a subset of the Open Table-and-Text Question Answering ([OTT-QA](https://github.com/wenhuchen/OTT-QA)) dataset, consisting of texts and tables from Wikipedia. The subset contains 20,000 tables, and it can be loaded from the Huggigface Datasets hub as follows:   ```python from datasets import load_dataset  # load the dataset from huggingface datasets hub data = load_dataset(\"ashraq/ott-qa-20k\", split=\"train\") data ```      Dataset({         features: ['url', 'title', 'header', 'data', 'section_title', 'section_text', 'uid', 'intro'],         num_rows: 20000     })     ```python data[2] ```         {'url': 'https://en.wikipedia.org/wiki/1976_New_York_Mets_season',      'title': '1976 New York Mets season',      'header': ['Level', 'Team', 'League', 'Manager'],      'data': [['AAA', 'Tidewater Tides', 'International League', 'Tom Burgess'],       ['AA', 'Jackson Mets', 'Texas League', 'John Antonelli'],       ['A', 'Lynchburg Mets', 'Carolina League', 'Jack Aker'],       ['A', 'Wausau Mets', 'Midwest League', 'Bill Monbouquette'],       ['Rookie', 'Marion Mets', 'Appalachian League', 'Al Jackson']],      'section_title': 'Farm system',      'section_text': 'See also : Minor League Baseball',      'uid': '1976_New_York_Mets_season_7',      'intro': 'The New York Mets season was the 15th regular season for the Mets, who played home games at Shea Stadium. Led by manager Joe Frazier, the team had an 86-76 record and finished in third place in the National League East.'}    As we can see, the dataset includes both textual and tabular data that are related to one another. Let's extract and transform the dataset's tables into pandas dataframes as we will only be using the tables in this example.   ```python import pandas as pd  # store all tables in the tables list tables = [] # loop through the dataset and convert tabular data to pandas dataframes for doc in data:     table = pd.DataFrame(doc[\"data\"], columns=doc[\"header\"])     tables.append(table) ```   ```python tables[2] ```      <div id=\"df-5746ac5e-ff5f-42a9-9fb9-48b8dd8bebd9\">   <div class=\"colab-df-container\">     <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     } </style> <table border=\"1\" class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>Level</th>       <th>Team</th>       <th>League</th>       <th>Manager</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>AAA</td>       <td>Tidewater Tides</td>       <td>International League</td>       <td>Tom Burgess</td>     </tr>     <tr>       <th>1</th>       <td>AA</td>       <td>Jackson Mets</td>       <td>Texas League</td>       <td>John Antonelli</td>     </tr>     <tr>       <th>2</th>       <td>A</td>       <td>Lynchburg Mets</td>       <td>Carolina League</td>       <td>Jack Aker</td>     </tr>     <tr>       <th>3</th>       <td>A</td>       <td>Wausau Mets</td>       <td>Midwest League</td>       <td>Bill Monbouquette</td>     </tr>     <tr>       <th>4</th>       <td>Rookie</td>       <td>Marion Mets</td>       <td>Appalachian League</td>       <td>Al Jackson</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5746ac5e-ff5f-42a9-9fb9-48b8dd8bebd9')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\">     <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>     <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>   </svg>       </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>    <script>     const buttonEl =       document.querySelector('#df-5746ac5e-ff5f-42a9-9fb9-48b8dd8bebd9 button.colab-df-convert');     buttonEl.style.display =       google.colab.kernel.accessAllowed ? 'block' : 'none';      async function convertToInteractive(key) {       const element = document.querySelector('#df-5746ac5e-ff5f-42a9-9fb9-48b8dd8bebd9');       const dataTable =         await google.colab.kernel.invokeFunction('convertToInteractive',                                                   [key], {});       if (!dataTable) return;        const docLinkHtml = 'Like what you see? Visit the ' +         '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'         + ' to learn more about interactive tables.';       element.innerHTML = '';       dataTable['output_type'] = 'display_data';       await google.colab.output.renderOutput(dataTable, element);       const docLink = document.createElement('div');       docLink.innerHTML = docLinkHtml;       element.appendChild(docLink);     }   </script> </div>   </div>     # Initialize Retriever  The retriever transforms natural language queries and tabular data into embeddings/vectors. It will generate embeddings in a way that the natural language questions and tables containing answers to our questions are nearby in the vector space.  We will use a SentenceTransformer model trained specifically for embedding tabular data for retrieval tasks. The model can be loaded from the Huggingface Models hub as follows:   ```python import torch from sentence_transformers import SentenceTransformer  # set device to GPU if available device = 'cuda' if torch.cuda.is_available() else 'cpu' # load the table embedding model from huggingface models hub retriever = SentenceTransformer(\"deepset/all-mpnet-base-v2-table\", device=device) retriever ```      SentenceTransformer(       (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel        (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})       (2): Normalize()     )    The retriever expects tables to be in a particular format. Let's write a function to convert the tables to this format.   ```python def _preprocess_tables(tables: list):     processed = []     # loop through all tables     for table in tables:         # convert the table to csv and          processed_table = \"\\n\".join([table.to_csv(index=False)])         # add the processed table to processed list         processed.append(processed_table)     return processed  ```  Notice that we are only using tables here. However, if you want the retriever to take the metadata into account while retrieving the tables, you can join any metadata strings, such as title, section_title, etc., separated by new line characters at the beginning of the processed table.  Let's take a look at the formatted tables.   ```python # format all the dataframes in the tables list processed_tables = _preprocess_tables(tables) # display the formatted table processed_tables[2] ```         'Level,Team,League,Manager\\nAAA,Tidewater Tides,International League,Tom Burgess\\nAA,Jackson Mets,Texas League,John Antonelli\\nA,Lynchburg Mets,Carolina League,Jack Aker\\nA,Wausau Mets,Midwest League,Bill Monbouquette\\nRookie,Marion Mets,Appalachian League,Al Jackson\\n'    The formatted table may not make sense to us, but the embedding model is trained to understand it and generate accurate embeddings.  # Initialize Pinecone Index  We will use the Pinecone vector database as our vector index. The Pinecone index stores vector representations of our tables which we can retrieve using a natural language query (query vector). Pinecone does this by computing the similarity between the query vector and the embedded tables stored in the vector index.   To use Pinecone, we first need to initialize a connection to Pinecone. For this, we need a [free API key](https://app.pinecone.io/). You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**. We initialize the connection like so:   ```python import pinecone  # connect to pinecone environment pinecone.init(     api_key=\"YOUR API KEY\",     environment=\"YOUR_ENVIRONMENT\" ) ```  Now we create a new index. We specify the metric type as \"cosine\" and dimension as 768 because the retriever we use to generate context embeddings outputs 768-dimension vectors. Pinecone will use cosine similarity to compute the similarity between the query and table embeddings.   ```python # you can choose any name for the index index_name = \"table-qa\"  # check if the table-qa index exists if index_name not in pinecone.list_indexes():     # create the index if it does not exist     pinecone.create_index(         index_name,         dimension=768,         metric=\"cosine\"     )  # connect to table-qa index we created index = pinecone.Index(index_name) ```  # Generate Embeddings and Upsert   Next we need to generate the table embeddings and upload it to the Pinecone index. We can easily do that as follows:   ```python from tqdm.auto import tqdm  # we will use batches of 64 batch_size = 64  for i in tqdm(range(0, len(processed_tables), batch_size)):     # find end of batch     i_end = min(i+batch_size, len(processed_tables))     # extract batch     batch = processed_tables[i:i_end]     # generate embeddings for batch     emb = retriever.encode(batch).tolist()     # create unique IDs ranging from zero to the total number of tables in the dataset     ids = [f\"{idx}\" for idx in range(i, i_end)]     # add all to upsert list     to_upsert = list(zip(ids, emb))     # upsert/insert these records to pinecone     _ = index.upsert(vectors=to_upsert)  # check that we have all vectors in index index.describe_index_stats() ```         100%|██████████| 313/313 [09:12<00:00, 1.49s/it]      {'dimension': 768,      'index_fullness': 0.0,      'namespaces': {'': {'vector_count': 20000}},      'total_vector_count': 20000}    Now the Pinecone index is ready for querying. Let's test to see if it returns tables relevant to our queries.   ```python query = \"which country has the highest GDP in 2020?\" # generate embedding for the query xq = retriever.encode([query]).tolist() # query pinecone index to find the table containing answer to the query result = index.query(xq, top_k=1) result  ```         {'matches': [{'id': '19931', 'score': 0.822087, 'values': []}], 'namespace': ''}    The Pinecone index has returned the ```id``` of a table that would contain the answer to our query with 82.2% confidence. Let's see if this table actually contains the answer. We can use the returned ```id``` as an index to get the relevant pandas dataframe from the ```tables``` list.   ```python id = int(result[\"matches\"][0][\"id\"]) tables[id].head() ```        <div id=\"df-74ed8f27-1770-4db1-9983-2633ef78b0c5\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     } </style> <table border=\"1\" class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>Rank</th>       <th>Country</th>       <th>GDP ( PPP , Peak Year ) millions of USD</th>       <th>Peak Year</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>1</td>       <td>China</td>       <td>27,804,953</td>       <td>2020</td>     </tr>     <tr>       <th>1</th>       <td>2</td>       <td>India</td>       <td>11,321,280</td>       <td>2020</td>     </tr>     <tr>       <th>2</th>       <td>3</td>       <td>Russia</td>       <td>4,389,960</td>       <td>2019</td>     </tr>     <tr>       <th>3</th>       <td>4</td>       <td>Indonesia</td>       <td>3,778,134</td>       <td>2020</td>     </tr>     <tr>       <th>4</th>       <td>5</td>       <td>Brazil</td>       <td>3,596,841</td>       <td>2020</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-74ed8f27-1770-4db1-9983-2633ef78b0c5')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\">     <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>     <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>   </svg>       </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>    <script>     const buttonEl =       document.querySelector('#df-74ed8f27-1770-4db1-9983-2633ef78b0c5 button.colab-df-convert');     buttonEl.style.display =       google.colab.kernel.accessAllowed ? 'block' : 'none';      async function convertToInteractive(key) {       const element = document.querySelector('#df-74ed8f27-1770-4db1-9983-2633ef78b0c5');       const dataTable =         await google.colab.kernel.invokeFunction('convertToInteractive',                                                   [key], {});       if (!dataTable) return;        const docLinkHtml = 'Like what you see? Visit the ' +         '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'         + ' to learn more about interactive tables.';       element.innerHTML = '';       dataTable['output_type'] = 'display_data';       await google.colab.output.renderOutput(dataTable, element);       const docLink = document.createElement('div');       docLink.innerHTML = docLinkHtml;       element.appendChild(docLink);     }   </script> </div>   </div>     The table returned by the Pinecone index indeed has the answer to our query. Now we need a model that can read this table and extract the precise answer.  # Initialize Table Reader  As the reader, we will use a TAPAS model fine-tuned for the Table QA task. TAPAS is a BERT-like Transformer model pretrained in a self-supervised manner on a large corpus of English language data from Wikipedia. We load the model and tokenizer from the Huggingface model hub into a question-answering pipeline.   ```python from transformers import pipeline, TapasTokenizer, TapasForQuestionAnswering  model_name = \"google/tapas-base-finetuned-wtq\" # load the tokenizer and the model from huggingface model hub tokenizer = TapasTokenizer.from_pretrained(model_name) model = TapasForQuestionAnswering.from_pretrained(model_name, local_files_only=False) # load the model and tokenizer into a question-answering pipeline pipe = pipeline(\"table-question-answering\",  model=model, tokenizer=tokenizer, device=device) ```   Let's run the table returned by the Pinecone index and the query we used before into the question-answering pipeline to extract the answer.   ```python pipe(table=tables[id], query=query) ```         {'answer': 'China',      'coordinates': [(0, 1)],      'cells': ['China'],      'aggregator': 'NONE'}    The model has precisely answered our query. Let's run some more queries.  # Querying  First, we will define two function to handle our queries and extract answers from tables.   ```python def query_pinecone(query):     # generate embedding for the query     xq = retriever.encode([query]).tolist()     # query pinecone index to find the table containing answer to the query     result = index.query(xq, top_k=1)     # return the relevant table from the tables list     return tables[int(result[\"matches\"][0][\"id\"])] ```   ```python def get_answer_from_table(table, query):     # run the table and query through the question-answering pipeline     answers = pipe(table=table, query=query)     return answers ```   ```python query = \"which car manufacturers produce cars with a top speed of above 180 kph?\" table = query_pinecone(query) table ```        <div id=\"df-dc62f5db-6acf-4ae5-80e4-3e6233b55474\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     } </style> <table border=\"1\" class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>Manufacturer</th>       <th>Model</th>       <th>Engine</th>       <th>Power Output</th>       <th>Max . Speed ( kph )</th>       <th>Dry Weight ( kg )</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>Fiat</td>       <td>805-405</td>       <td>FIAT 1979cc S6 supercharged</td>       <td>130 bhp</td>       <td>220</td>       <td>680</td>     </tr>     <tr>       <th>1</th>       <td>Alfa Romeo</td>       <td>GPR ( P1 )</td>       <td>Alfa Romeo 1990cc S6</td>       <td>95 bhp</td>       <td>180</td>       <td>850</td>     </tr>     <tr>       <th>2</th>       <td>Diatto</td>       <td>Tipo 20 S</td>       <td>Diatto 1997cc S4</td>       <td>75 bhp</td>       <td>155</td>       <td>700</td>     </tr>     <tr>       <th>3</th>       <td>Bugatti</td>       <td>Type 32</td>       <td>Bugatti 1991cc S8</td>       <td>100 bhp</td>       <td>190</td>       <td>660</td>     </tr>     <tr>       <th>4</th>       <td>Voisin</td>       <td>C6 Laboratoire</td>       <td>Voisin 1978cc S6</td>       <td>90 bhp</td>       <td>175</td>       <td>710</td>     </tr>     <tr>       <th>5</th>       <td>Sunbeam</td>       <td></td>       <td>Sunbeam 1988cc S6</td>       <td>108 bhp</td>       <td>180</td>       <td>675</td>     </tr>     <tr>       <th>6</th>       <td>Mercedes</td>       <td>M7294</td>       <td>Mercedes 1990cc S4 supercharged</td>       <td>120 bhp</td>       <td>180</td>       <td>750</td>     </tr>     <tr>       <th>7</th>       <td>Benz</td>       <td>RH Tropfenwagen</td>       <td>Benz 1998cc S6</td>       <td>95 bhp</td>       <td>185</td>       <td>745</td>     </tr>     <tr>       <th>8</th>       <td>Miller</td>       <td>122</td>       <td>Miller 1978cc S8</td>       <td>120 bhp</td>       <td>186</td>       <td>850</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc62f5db-6acf-4ae5-80e4-3e6233b55474')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\">     <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>     <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>   </svg>       </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>    <script>     const buttonEl =       document.querySelector('#df-dc62f5db-6acf-4ae5-80e4-3e6233b55474 button.colab-df-convert');     buttonEl.style.display =       google.colab.kernel.accessAllowed ? 'block' : 'none';      async function convertToInteractive(key) {       const element = document.querySelector('#df-dc62f5db-6acf-4ae5-80e4-3e6233b55474');       const dataTable =         await google.colab.kernel.invokeFunction('convertToInteractive',                                                   [key], {});       if (!dataTable) return;        const docLinkHtml = 'Like what you see? Visit the ' +         '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'         + ' to learn more about interactive tables.';       element.innerHTML = '';       dataTable['output_type'] = 'display_data';       await google.colab.output.renderOutput(dataTable, element);       const docLink = document.createElement('div');       docLink.innerHTML = docLinkHtml;       element.appendChild(docLink);     }   </script> </div>   </div>      ```python get_answer_from_table(table, query) ```         {'answer': 'Fiat, Bugatti, Benz, Miller',      'coordinates': [(0, 0), (3, 0), (7, 0), (8, 0)],      'cells': ['Fiat', 'Bugatti', 'Benz', 'Miller'],      'aggregator': 'NONE'}     ```python query = \"which scientist is known for improving the steam engine?\" table = query_pinecone(query) table.head() ```        <div id=\"df-985a94cf-fc9c-4e3a-969f-e0c6498f57bd\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     } </style> <table border=\"1\" class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>Year</th>       <th>Name</th>       <th>Location</th>       <th>Rationale</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>1839</td>       <td>Robert Hare</td>       <td>Philadelphia , Pennsylvania</td>       <td>Inventor of the oxy-hydrogen blowpipe</td>     </tr>     <tr>       <th>1</th>       <td>1862</td>       <td>John Ericsson</td>       <td>New York , New York</td>       <td>His work improved the field of heat management...</td>     </tr>     <tr>       <th>2</th>       <td>1865</td>       <td>Daniel Treadwell</td>       <td>Cambridge , Massachusetts</td>       <td>Heat management . He was awarded especially fo...</td>     </tr>     <tr>       <th>3</th>       <td>1866</td>       <td>Alvan Clark</td>       <td>Cambridge , Massachusetts</td>       <td>Improved refracting telescopes</td>     </tr>     <tr>       <th>4</th>       <td>1869</td>       <td>George Henry Corliss</td>       <td>Providence , Rhode Island</td>       <td>For improving the steam engine</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-985a94cf-fc9c-4e3a-969f-e0c6498f57bd')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\">     <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>     <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>   </svg>       </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>    <script>     const buttonEl =       document.querySelector('#df-985a94cf-fc9c-4e3a-969f-e0c6498f57bd button.colab-df-convert');     buttonEl.style.display =       google.colab.kernel.accessAllowed ? 'block' : 'none';      async function convertToInteractive(key) {       const element = document.querySelector('#df-985a94cf-fc9c-4e3a-969f-e0c6498f57bd');       const dataTable =         await google.colab.kernel.invokeFunction('convertToInteractive',                                                   [key], {});       if (!dataTable) return;        const docLinkHtml = 'Like what you see? Visit the ' +         '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'         + ' to learn more about interactive tables.';       element.innerHTML = '';       dataTable['output_type'] = 'display_data';       await google.colab.output.renderOutput(dataTable, element);       const docLink = document.createElement('div');       docLink.innerHTML = docLinkHtml;       element.appendChild(docLink);     }   </script> </div>   </div>      ```python get_answer_from_table(table, query) ```         {'answer': 'George Henry Corliss',      'coordinates': [(4, 1)],      'cells': ['George Henry Corliss'],      'aggregator': 'NONE'}     ```python query = \"What is the Maldivian island name for Oblu Select at Sangeli\tresort?\" table = query_pinecone(query) table.head() ```        <div id=\"df-a83c0554-e109-4f76-907a-43a909cb8156\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     } </style> <table border=\"1\" class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>Name</th>       <th>Resort Name</th>       <th>Geographic Atoll</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>Asdhoo</td>       <td>Asdu Sun Island Resort</td>       <td>North Male Atoll</td>     </tr>     <tr>       <th>1</th>       <td>Akirifushi</td>       <td>Oblu Select at Sangeli</td>       <td>North Male Atoll</td>     </tr>     <tr>       <th>2</th>       <td>Baros</td>       <td>Baros Island Resort</td>       <td>North Male Atoll</td>     </tr>     <tr>       <th>3</th>       <td>Biyaadhoo</td>       <td>Biyadhoo Island Resort</td>       <td>South Male Atoll</td>     </tr>     <tr>       <th>4</th>       <td>Bodubandos</td>       <td>Bandos Maldives Resort</td>       <td>North Male Atoll</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a83c0554-e109-4f76-907a-43a909cb8156')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\">     <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>     <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>   </svg>       </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>    <script>     const buttonEl =       document.querySelector('#df-a83c0554-e109-4f76-907a-43a909cb8156 button.colab-df-convert');     buttonEl.style.display =       google.colab.kernel.accessAllowed ? 'block' : 'none';      async function convertToInteractive(key) {       const element = document.querySelector('#df-a83c0554-e109-4f76-907a-43a909cb8156');       const dataTable =         await google.colab.kernel.invokeFunction('convertToInteractive',                                                   [key], {});       if (!dataTable) return;        const docLinkHtml = 'Like what you see? Visit the ' +         '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'         + ' to learn more about interactive tables.';       element.innerHTML = '';       dataTable['output_type'] = 'display_data';       await google.colab.output.renderOutput(dataTable, element);       const docLink = document.createElement('div');       docLink.innerHTML = docLinkHtml;       element.appendChild(docLink);     }   </script> </div>   </div>      ```python get_answer_from_table(table, query) ```         {'answer': 'Akirifushi',      'coordinates': [(1, 0)],      'cells': ['Akirifushi'],      'aggregator': 'NONE'}    As we can see, our Table QA system can retrieve the correct table from the Pinecone index and extract precise answers from the table. The TAPAS model we use supports more advanced queries. It has an aggregation head which indicates whether we need to count, sum, or average cells to answer the questions. Let's run some advanced queries that require aggregation to answer.   ```python query = \"what was the total GDP of China and Indonesia in 2020?\" table = query_pinecone(query) table.head() ```        <div id=\"df-a20fc2ac-c471-4bc5-8526-26ab08545922\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     } </style> <table border=\"1\" class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>Rank</th>       <th>Country</th>       <th>GDP ( PPP , Peak Year ) millions of USD</th>       <th>Peak Year</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>1</td>       <td>China</td>       <td>27,804,953</td>       <td>2020</td>     </tr>     <tr>       <th>1</th>       <td>2</td>       <td>India</td>       <td>11,321,280</td>       <td>2020</td>     </tr>     <tr>       <th>2</th>       <td>3</td>       <td>Russia</td>       <td>4,389,960</td>       <td>2019</td>     </tr>     <tr>       <th>3</th>       <td>4</td>       <td>Indonesia</td>       <td>3,778,134</td>       <td>2020</td>     </tr>     <tr>       <th>4</th>       <td>5</td>       <td>Brazil</td>       <td>3,596,841</td>       <td>2020</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a20fc2ac-c471-4bc5-8526-26ab08545922')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\">     <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>     <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>   </svg>       </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>    <script>     const buttonEl =       document.querySelector('#df-a20fc2ac-c471-4bc5-8526-26ab08545922 button.colab-df-convert');     buttonEl.style.display =       google.colab.kernel.accessAllowed ? 'block' : 'none';      async function convertToInteractive(key) {       const element = document.querySelector('#df-a20fc2ac-c471-4bc5-8526-26ab08545922');       const dataTable =         await google.colab.kernel.invokeFunction('convertToInteractive',                                                   [key], {});       if (!dataTable) return;        const docLinkHtml = 'Like what you see? Visit the ' +         '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'         + ' to learn more about interactive tables.';       element.innerHTML = '';       dataTable['output_type'] = 'display_data';       await google.colab.output.renderOutput(dataTable, element);       const docLink = document.createElement('div');       docLink.innerHTML = docLinkHtml;       element.appendChild(docLink);     }   </script> </div>   </div>      ```python get_answer_from_table(table, query) ```         {'answer': 'SUM > 27,804,953, 3,778,134',      'coordinates': [(0, 2), (3, 2)],      'cells': ['27,804,953', '3,778,134'],      'aggregator': 'SUM'}    Here the QA system suggests the correct cells to add in order to get the total GDP of China and Indonesia in 2020.   ```python query = \"what is the average carbon emission of power stations in australia, canada and germany?\" table = query_pinecone(query) table.head() ```        <div id=\"df-85612087-ccea-4ebf-9fdb-f783d2e968d9\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     } </style> <table border=\"1\" class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>CO 2 intensity ( kg/kWh )</th>       <th>Power station</th>       <th>Country</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>1.58</td>       <td>Hazelwood Power Station , Victoria closed 31 M...</td>       <td>Australia</td>     </tr>     <tr>       <th>1</th>       <td>1.56</td>       <td>Edwardsport IGCC , Edwardsport , Indiana , clo...</td>       <td>United States</td>     </tr>     <tr>       <th>2</th>       <td>1.27</td>       <td>Frimmersdorf power plant , Grevenbroich</td>       <td>Germany</td>     </tr>     <tr>       <th>3</th>       <td>1.25</td>       <td>HR Milner Generating Station , Grande Cache , ...</td>       <td>Canada</td>     </tr>     <tr>       <th>4</th>       <td>1.18</td>       <td>C. TG . Portes Gil , Río Bravo</td>       <td>Mexico</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85612087-ccea-4ebf-9fdb-f783d2e968d9')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\">     <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>     <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>   </svg>       </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>    <script>     const buttonEl =       document.querySelector('#df-85612087-ccea-4ebf-9fdb-f783d2e968d9 button.colab-df-convert');     buttonEl.style.display =       google.colab.kernel.accessAllowed ? 'block' : 'none';      async function convertToInteractive(key) {       const element = document.querySelector('#df-85612087-ccea-4ebf-9fdb-f783d2e968d9');       const dataTable =         await google.colab.kernel.invokeFunction('convertToInteractive',                                                   [key], {});       if (!dataTable) return;        const docLinkHtml = 'Like what you see? Visit the ' +         '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'         + ' to learn more about interactive tables.';       element.innerHTML = '';       dataTable['output_type'] = 'display_data';       await google.colab.output.renderOutput(dataTable, element);       const docLink = document.createElement('div');       docLink.innerHTML = docLinkHtml;       element.appendChild(docLink);     }   </script> </div>   </div>      ```python get_answer_from_table(table, query) ```         {'answer': 'AVERAGE > 1.58, 1.27, 1.25',      'coordinates': [(0, 0), (2, 0), (3, 0)],      'cells': ['1.58', '1.27', '1.25'],      'aggregator': 'AVERAGE'}    As we can see, the QA system correctly identified which cells to average to answer our question. ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e265"
  },
  "title": "Article Recommender",
  "category": "630fc5235d91a70054705fb7",
  "content": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/recommendation/article-recommender/article_recommendations.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/recommendation/article-recommender/article_recommendations.ipynb) [![Open github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/pinecone-io/examples/blob/master/recommendation/article-recommender/article_recommendations.ipynb)  This notebook demonstrates how to use Pinecone's similarity search to create a simple personalized article or content recommender.  The goal is to create a recommendation engine that retrieves the best article recommendations for each user. When making recommendations with content-based filtering, we evaluate the user’s past behavior and the content items themselves. So in this example, users will be recommended articles that are similar to those they've already read.  ## Install and Import Python Packages  ```python !pip install --quiet wordcloud pandas !pip install --quiet sentence-transformers --no-cache-dir ```   ```python import os import pandas as pd import numpy as np import time import re from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator import matplotlib.pyplot as plt from statistics import mean %matplotlib inline ```  In the following sections, we will use Pinecone to easily build an article recommendation engine. Pinecone will be responsible for storing embeddings for articles, maintaining a live index of those vectors, and returning recommended articles on-demand.  ## Pinecone Setup  ```python !pip install --quiet -U pinecone-client ```   ```python import pinecone ```   ```python # Load Pinecone API key api_key = os.getenv('PINECONE_API_KEY') or 'YOUR_API_KEY' # Set Pinecone environment. Default environment is YOUR_ENVIRONMENT env = os.getenv('PINECONE_ENVIRONMENT') or 'YOUR_ENVIRONMENT' pinecone.init(api_key=api_key, environment=env) ```  [Get a Pinecone API key](https://www.pinecone.io/start/) if you don’t have one already. You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**.  ```python index_name = 'articles-recommendation' ```   ```python # If index of the same name exists, then delete it if index_name in pinecone.list_indexes():     pinecone.delete_index(index_name) ```  Create an index.  ```python pinecone.create_index(index_name, dimension=300) ```  Connect to the new index.  ```python index = pinecone.Index(index_name) index.describe_index_stats() ```      {'dimension': 300, 'namespaces': {}}  ## Upload Articles  Next, we will prepare data for the Pinecone vector index, and insert it in batches.  The [dataset](https://components.one/datasets/all-the-news-2-news-articles-dataset/) used throughout this example contains 2.7 million news articles and essays from 27 American publications.  Let's download the dataset.  ```python !rm all-the-news-2-1.zip !rm all-the-news-2-1.csv !wget https://www.dropbox.com/s/cn2utnr5ipathhh/all-the-news-2-1.zip -q --show-progress !unzip -q all-the-news-2-1.zip ```      rm: cannot remove 'all-the-news-2-1.zip': No such file or directory     rm: cannot remove 'all-the-news-2-1.csv': No such file or directory     all-the-news-2-1.zi     [             <=>    ]   3.13G  82.7MB/s    in 39s  ### Create Vector Embeddings  The model used in this example is the [Average Word Embeddings Models](https://www.sbert.net/docs/pretrained_models.html#average-word-embeddings-models). This model allows us to create vector embeddings for each article, using the content and title of each.  ```python import torch from sentence_transformers import SentenceTransformer  # set device to GPU if available device = 'cuda' if torch.cuda.is_available() else 'cpu' model = SentenceTransformer('average_word_embeddings_komninos', device=device) ```  Using the complete dataset may require more time for the model to generate vector embeddings. We will use only a sample, but if you want to try uploading the whole dataset, set the **NROWS** flag to **None**.  ```python NROWS = 200000      # number of rows to be loaded from the csv, set to None for loading all rows, reduce if you have a low amount of RAM or want a faster execution BATCH_SIZE = 500    # batch size for upserting ```  Let's prepare data for upload.  Uploading the data may take a while, and depends on the network you use.  ```python #%%time       def prepare_data(data) -> pd.DataFrame:     'Preprocesses data and prepares it for upsert.'          # add an id column     print(\"Preparing data...\")     data[\"id\"] = range(len(data))      # extract only first few sentences of each article for quicker vector calculations     data['article'] = data['article'].fillna('')     data['article'] = data.article.apply(lambda x: ' '.join(re.split(r'(?<=[.:;])\\s', x)[:4]))     data['title_article'] = data['title'] + data['article']          # create a vector embedding based on title and article columns     print('Encoding articles...')     encoded_articles = model.encode(data['title_article'])     data['article_vector'] = pd.Series(encoded_articles.tolist())          return data   def upload_items(data):     'Uploads data in batches.'     print(\"Uploading items...\")          # create a list of items for upload     items_to_upload = [(str(row.id), row.article_vector) for i,row in data.iterrows()]          # upsert     for i in range(0, len(items_to_upload), BATCH_SIZE):         index.upsert(vectors=items_to_upload[i:i+BATCH_SIZE])       def process_file(filename: str) -> pd.DataFrame:     'Reads csv files in chunks, prepares and uploads data.'          data = pd.read_csv(filename, nrows=NROWS)     data = prepare_data(data)     upload_items(data)     return data              uploaded_data = process_file(filename='all-the-news-2-1.csv') ```        Preparing data...     Encoding articles...     Uploading items...  ```python # Print index statistics index.describe_index_stats() ```      {'dimension': 300, 'namespaces': {'': {'vector_count': 200000}}}  ## Query the Pinecone Index  We will query the index for the specific users. The users are defined as a set of the articles that they previously read. More specifically, we will define 10 articles for each user, and based on the article embeddings, we will define a unique embedding for the user.  We will create three users and query Pinecone for each of them:  - User who likes to read Sport News - User who likes to read Entertainment News - User who likes to read Business News  Let's define mappings for titles, sections, and publications for each article.  ```python titles_mapped = dict(zip(uploaded_data.id, uploaded_data.title)) sections_mapped = dict(zip(uploaded_data.id, uploaded_data.section)) publications_mapped = dict(zip(uploaded_data.id, uploaded_data.publication)) ```  Also, we will define a function that uses _wordcloud_ to visualize results.  ```python def get_wordcloud_for_user(recommendations):      stopwords = set(STOPWORDS).union([np.nan, 'NaN', 'S'])      wordcloud = WordCloud(                    max_words=50000,                     min_font_size =12,                     max_font_size=50,                     relative_scaling = 0.9,                     stopwords=set(STOPWORDS),                    normalize_plurals= True     )      clean_titles = [word for word in recommendations.title.values if word not in stopwords]     title_wordcloud = wordcloud.generate(' '.join(clean_titles))      plt.imshow(title_wordcloud, interpolation='bilinear')     plt.axis(\"off\")     plt.show() ```  Let's query the Pinecone index using three users.  ### Query Sports User  ```python # first create a user who likes to read sport news about tennis sport_user = uploaded_data.loc[((uploaded_data['section'] == 'Sports News' ) |                                  (uploaded_data['section'] == 'Sports')) &                                 (uploaded_data['article'].str.contains('Tennis'))][:10]  print('\\nHere is the example of previously read articles by this user:\\n') display(sport_user[['title', 'article', 'section', 'publication']])  # then create a vector for this user a = sport_user['article_vector'] sport_user_vector = [*map(mean, zip(*a))]  # query the pinecone res = index.query(sport_user_vector, top_k=10)  # print results ids = [match.id for match in res.matches] scores = [match.score for match in res.matches] df = pd.DataFrame({'id': ids,                     'score': scores,                    'title': [titles_mapped[int(_id)] for _id in ids],                    'section': [sections_mapped[int(_id)] for _id in ids],                    'publication': [publications_mapped[int(_id)] for _id in ids]                     })  print(\"\\nThis table contains recommended articles for the user:\\n\") display(df) print(\"\\nA word-cloud representing the results:\\n\") get_wordcloud_for_user(df) ```      Here is the example of previously read articles by this user:    <div id=\"df-f22bbca3-ad57-487f-8fff-30873403b02f\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     }  </style> <table class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>title</th>       <th>article</th>       <th>section</th>       <th>publication</th>     </tr>   </thead>   <tbody>     <tr>       <th>2261</th>       <td>Son of Borg makes quiet debut on London grassc...</td>       <td>LONDON (Reuters) - A blonde-haired, blue-eyed ...</td>       <td>Sports News</td>       <td>Reuters</td>     </tr>     <tr>       <th>12373</th>       <td>Cilic offers Nadal a Wimbledon reality check</td>       <td>LONDON (Reuters) - Spaniard Rafael Nadal got a...</td>       <td>Sports News</td>       <td>Reuters</td>     </tr>     <tr>       <th>17124</th>       <td>Perth confirmed as host for Fed Cup final</td>       <td>(Reuters) - Perth has been named host city for...</td>       <td>Sports News</td>       <td>Reuters</td>     </tr>     <tr>       <th>18411</th>       <td>Fed Cup gets revamp with 12-nation Finals in B...</td>       <td>LONDON (Reuters) - The Fed Cup’s existing form...</td>       <td>Sports News</td>       <td>Reuters</td>     </tr>     <tr>       <th>26574</th>       <td>Nadal to prepare for Wimbledon at Hurlingham e...</td>       <td>(Reuters) - World number two Rafa Nadal has en...</td>       <td>Sports News</td>       <td>Reuters</td>     </tr>     <tr>       <th>34957</th>       <td>Tennis Legend Margaret Court Went Off the Rail...</td>       <td>Margaret Court, the most decorated tennis play...</td>       <td>Sports</td>       <td>Vice</td>     </tr>     <tr>       <th>35508</th>       <td>Puck City: The Enduring Success of Ice Hockey ...</td>       <td>This article originally appeared on VICE Sport...</td>       <td>Sports</td>       <td>Vice</td>     </tr>     <tr>       <th>38393</th>       <td>As if by royal command, seven Britons make it ...</td>       <td>LONDON (Reuters) - Tennis fan the Duchess of C...</td>       <td>Sports News</td>       <td>Reuters</td>     </tr>     <tr>       <th>62445</th>       <td>Williams fined $17,000 for U.S. Open code viol...</td>       <td>NEW YORK (Reuters) - Serena Williams has been ...</td>       <td>Sports News</td>       <td>Reuters</td>     </tr>     <tr>       <th>84122</th>       <td>Kyrgios still wrestling with his tennis soul a...</td>       <td>LONDON (Reuters) - Timothy Gallwey’s million-s...</td>       <td>Sports News</td>       <td>Reuters</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f22bbca3-ad57-487f-8fff-30873403b02f')\"               title=\"Convert this dataframe to an interactive table.\" style=\"display:none;\"               >  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 0 24 24\"        width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg> </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>  <script> const buttonEl =     document.querySelector('#df-f22bbca3-ad57-487f-8fff-30873403b02f button.colab-df-convert'); buttonEl.style.display =     google.colab.kernel.accessAllowed ? 'block' : 'none';  async function convertToInteractive(key) {     const element = document.querySelector('#df-f22bbca3-ad57-487f-8fff-30873403b02f');     const dataTable =     await google.colab.kernel.invokeFunction('convertToInteractive',                                                 [key], {});     if (!dataTable) return;      const docLinkHtml = 'Like what you see? Visit the ' +     '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'     + ' to learn more about interactive tables.';     element.innerHTML = '';     dataTable['output_type'] = 'display_data';     await google.colab.output.renderOutput(dataTable, element);     const docLink = document.createElement('div');     docLink.innerHTML = docLinkHtml;     element.appendChild(docLink); } </script> </div>   </div>      This table contains recommended articles for the user:    <div id=\"df-2e1273af-dda4-4c1c-91a7-5c6c54a03777\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     }  </style> <table class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>id</th>       <th>score</th>       <th>title</th>       <th>section</th>       <th>publication</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>138865</td>       <td>0.966407</td>       <td>Federer survives first-set wobble to down Wimb...</td>       <td>Sports News</td>       <td>Reuters</td>     </tr>     <tr>       <th>1</th>       <td>26574</td>       <td>0.965867</td>       <td>Nadal to prepare for Wimbledon at Hurlingham e...</td>       <td>Sports News</td>       <td>Reuters</td>     </tr>     <tr>       <th>2</th>       <td>12373</td>       <td>0.965307</td>       <td>Cilic offers Nadal a Wimbledon reality check</td>       <td>Sports News</td>       <td>Reuters</td>     </tr>     <tr>       <th>3</th>       <td>155913</td>       <td>0.963684</td>       <td>U.S. men likely to wander Wimbledon wilderness...</td>       <td>Sports News</td>       <td>Reuters</td>     </tr>     <tr>       <th>4</th>       <td>60613</td>       <td>0.962414</td>       <td>Auger-Aliassime powers past Tsitsipas into Que...</td>       <td>Sports News</td>       <td>Reuters</td>     </tr>     <tr>       <th>5</th>       <td>22764</td>       <td>0.962373</td>       <td>Serena headed to Wimbledon seeking return to form</td>       <td>Sports News</td>       <td>Reuters</td>     </tr>     <tr>       <th>6</th>       <td>71768</td>       <td>0.962168</td>       <td>Venus, Serena, and the Power of Believing</td>       <td>Sports</td>       <td>Vice</td>     </tr>     <tr>       <th>7</th>       <td>2261</td>       <td>0.961590</td>       <td>Son of Borg makes quiet debut on London grassc...</td>       <td>Sports News</td>       <td>Reuters</td>     </tr>     <tr>       <th>8</th>       <td>45469</td>       <td>0.961451</td>       <td>Tennis: Barty a win away from world number one</td>       <td>Sports News</td>       <td>Reuters</td>     </tr>     <tr>       <th>9</th>       <td>55061</td>       <td>0.960677</td>       <td>Warrior on court, diplomat off it, classy Bart...</td>       <td>Sports News</td>       <td>Reuters</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e1273af-dda4-4c1c-91a7-5c6c54a03777')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 0 24 24\"        width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg> </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>  <script> const buttonEl =     document.querySelector('#df-2e1273af-dda4-4c1c-91a7-5c6c54a03777 button.colab-df-convert'); buttonEl.style.display =     google.colab.kernel.accessAllowed ? 'block' : 'none';  async function convertToInteractive(key) {     const element = document.querySelector('#df-2e1273af-dda4-4c1c-91a7-5c6c54a03777');     const dataTable =     await google.colab.kernel.invokeFunction('convertToInteractive',                                                 [key], {});     if (!dataTable) return;      const docLinkHtml = 'Like what you see? Visit the ' +     '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'     + ' to learn more about interactive tables.';     element.innerHTML = '';     dataTable['output_type'] = 'display_data';     await google.colab.output.renderOutput(dataTable, element);     const docLink = document.createElement('div');     docLink.innerHTML = docLinkHtml;     element.appendChild(docLink); } </script> </div>    </div>      A word-cloud representing the results:  ![Wordcloud of recommended sports articles](https://raw.githubusercontent.com/pinecone-io/img/main/personalized-content-recommendations-example-1.png)  ### Query Entertainment User  ```python # first create a user who likes to read news about Xbox entertainment_user = uploaded_data.loc[((uploaded_data['section'] == 'Entertainment') |                                         (uploaded_data['section'] == 'Games') |                                         (uploaded_data['section'] == 'Tech by VICE')) &                                         (uploaded_data['article'].str.contains('Xbox'))][:10]  print('\\nHere is the example of previously read articles by this user:\\n') display(entertainment_user[['title', 'article', 'section', 'publication']])  # then create a vector for this user a = entertainment_user['article_vector'] entertainment_user_vector = [*map(mean, zip(*a))]  # query the pinecone res = index.query(entertainment_user_vector, top_k=10)  # print results ids = [match.id for match in res.matches] scores = [match.score for match in res.matches] df = pd.DataFrame({'id': ids,                     'score': scores,                    'title': [titles_mapped[int(_id)] for _id in ids],                    'section': [sections_mapped[int(_id)] for _id in ids],                    'publication': [publications_mapped[int(_id)] for _id in ids]                     })  print(\"\\nThis table contains recommended articles for the user:\\n\") display(df) print(\"\\nA word-cloud representing the results:\\n\") get_wordcloud_for_user(df) ```      Here is the example of previously read articles by this user:    <div id=\"df-40de5ea3-0e1d-48b5-b827-718415441735\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     }  </style> <table class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>title</th>       <th>article</th>       <th>section</th>       <th>publication</th>     </tr>   </thead>   <tbody>     <tr>       <th>4977</th>       <td>A Canadian Man Is Pissed That His Son Ran Up a...</td>       <td>A Pembroke, Ontario, gun shop owner is \"mad as...</td>       <td>Games</td>       <td>Vice</td>     </tr>     <tr>       <th>12016</th>       <td>'I Expect You to Die' is One of Virtual Realit...</td>       <td>The reason I bought a Vive over and Oculus ear...</td>       <td>Games</td>       <td>Vice</td>     </tr>     <tr>       <th>16078</th>       <td>Windows 10's Killer App? Xbox One Games</td>       <td>Microsoft's crusade to get the world to instal...</td>       <td>Tech by VICE</td>       <td>Vice</td>     </tr>     <tr>       <th>20318</th>       <td>Black Friday Not Your Thing? Play These Free G...</td>       <td>It's Black Friday, the oh-so-American shopping...</td>       <td>Games</td>       <td>Vice</td>     </tr>     <tr>       <th>25785</th>       <td>Nintendo’s Win at E3 Shows That It's a Console...</td>       <td>​ E3 has come and gone for 2016, the LA expo o...</td>       <td>Games</td>       <td>Vice</td>     </tr>     <tr>       <th>29653</th>       <td>You Can Smell Like a Gamer With Lynx’s New Xbo...</td>       <td>Gamers in Australia and New Zealand will soon ...</td>       <td>Games</td>       <td>Vice</td>     </tr>     <tr>       <th>33234</th>       <td>It’s Old and It’s Clunky, But You Really Must ...</td>       <td>When Dragon's Dogma first popped up in 2012, t...</td>       <td>Games</td>       <td>Vice</td>     </tr>     <tr>       <th>34617</th>       <td>Nintendo’s Win at E3 Shows That It's a Console...</td>       <td>E3 has come and gone for 2016, the LA expo of ...</td>       <td>Games</td>       <td>Vice</td>     </tr>     <tr>       <th>38608</th>       <td>PC Gaming Is Still Way Too Hard</td>       <td>Here's Motherboard's super simple guide to bui...</td>       <td>Tech by VICE</td>       <td>Vice</td>     </tr>     <tr>       <th>41444</th>       <td>Here’s Everything That Happened at the Xbox E3...</td>       <td>That's Xbox's Big Show for E3 2016 over and do...</td>       <td>Games</td>       <td>Vice</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-40de5ea3-0e1d-48b5-b827-718415441735')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 0 24 24\"        width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg> </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>  <script> const buttonEl =     document.querySelector('#df-40de5ea3-0e1d-48b5-b827-718415441735 button.colab-df-convert'); buttonEl.style.display =     google.colab.kernel.accessAllowed ? 'block' : 'none';  async function convertToInteractive(key) {     const element = document.querySelector('#df-40de5ea3-0e1d-48b5-b827-718415441735');     const dataTable =     await google.colab.kernel.invokeFunction('convertToInteractive',                                                 [key], {});     if (!dataTable) return;      const docLinkHtml = 'Like what you see? Visit the ' +     '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'     + ' to learn more about interactive tables.';     element.innerHTML = '';     dataTable['output_type'] = 'display_data';     await google.colab.output.renderOutput(dataTable, element);     const docLink = document.createElement('div');     docLink.innerHTML = docLinkHtml;     element.appendChild(docLink); } </script> </div>    </div>      This table contains recommended articles for the user:    <div id=\"df-ad60df1b-7a3e-439a-9304-5cdca611d5f4\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     }  </style> <table class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>id</th>       <th>score</th>       <th>title</th>       <th>section</th>       <th>publication</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>34617</td>       <td>0.966389</td>       <td>Nintendo’s Win at E3 Shows That It's a Console...</td>       <td>Games</td>       <td>Vice</td>     </tr>     <tr>       <th>1</th>       <td>63293</td>       <td>0.965053</td>       <td>A Title Card vs Six Teraflops: How Metroid Sto...</td>       <td>Games</td>       <td>Vice</td>     </tr>     <tr>       <th>2</th>       <td>25785</td>       <td>0.964193</td>       <td>Nintendo’s Win at E3 Shows That It's a Console...</td>       <td>Games</td>       <td>Vice</td>     </tr>     <tr>       <th>3</th>       <td>16771</td>       <td>0.963487</td>       <td>The Lo-Fi Flaws That Define Our Favorite Old G...</td>       <td>Games</td>       <td>Vice</td>     </tr>     <tr>       <th>4</th>       <td>38608</td>       <td>0.960349</td>       <td>PC Gaming Is Still Way Too Hard</td>       <td>Tech by VICE</td>       <td>Vice</td>     </tr>     <tr>       <th>5</th>       <td>121140</td>       <td>0.960174</td>       <td>Microsoft’s New Direction All Started With the...</td>       <td>Tech by VICE</td>       <td>Vice</td>     </tr>     <tr>       <th>6</th>       <td>160409</td>       <td>0.959802</td>       <td>Sometimes a David Bowie Song Gets Your Favorit...</td>       <td>Tech by VICE</td>       <td>Vice</td>     </tr>     <tr>       <th>7</th>       <td>29653</td>       <td>0.959628</td>       <td>You Can Smell Like a Gamer With Lynx’s New Xbo...</td>       <td>Games</td>       <td>Vice</td>     </tr>     <tr>       <th>8</th>       <td>156585</td>       <td>0.959380</td>       <td>Google Takes Aim at PlayStation, Xbox With Gam...</td>       <td>Games</td>       <td>Vice</td>     </tr>     <tr>       <th>9</th>       <td>185864</td>       <td>0.958856</td>       <td>The Switch Succeeds on Nintendo's Historic \"To...</td>       <td>Games</td>       <td>Vice</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad60df1b-7a3e-439a-9304-5cdca611d5f4')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 0 24 24\"        width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg> </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>  <script> const buttonEl =     document.querySelector('#df-ad60df1b-7a3e-439a-9304-5cdca611d5f4 button.colab-df-convert'); buttonEl.style.display =     google.colab.kernel.accessAllowed ? 'block' : 'none';  async function convertToInteractive(key) {     const element = document.querySelector('#df-ad60df1b-7a3e-439a-9304-5cdca611d5f4');     const dataTable =     await google.colab.kernel.invokeFunction('convertToInteractive',                                                 [key], {});     if (!dataTable) return;      const docLinkHtml = 'Like what you see? Visit the ' +     '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'     + ' to learn more about interactive tables.';     element.innerHTML = '';     dataTable['output_type'] = 'display_data';     await google.colab.output.renderOutput(dataTable, element);     const docLink = document.createElement('div');     docLink.innerHTML = docLinkHtml;     element.appendChild(docLink); } </script> </div>    </div>      A word-cloud representing the results:  ![Wordcloud of recommended entertainment articles](https://raw.githubusercontent.com/pinecone-io/img/main/personalized-content-recommendations-example-2.png)  ### Query Business User  ```python # first create a user who likes to read about Wall Street business news business_user = uploaded_data.loc[((uploaded_data['section'] == 'Business News')|                                    (uploaded_data['section'] == 'business')) &                                    (uploaded_data['article'].str.contains('Wall Street'))][:10]  print('\\nHere is the example of previously read articles by this user:\\n') display(business_user[['title', 'article', 'section', 'publication']])  # then create a vector for this user a = business_user['article_vector'] business_user_vector = [*map(mean, zip(*a))]  # query the pinecone res = index.query(business_user_vector, top_k=10)  # print results ids = [match.id for match in res.matches] scores = [match.score for match in res.matches] df = pd.DataFrame({'id': ids,                     'score': scores,                    'title': [titles_mapped[int(_id)] for _id in ids],                    'section': [sections_mapped[int(_id)] for _id in ids],                    'publication': [publications_mapped[int(_id)] for _id in ids]                     })  print(\"\\nThis table contains recommended articles for the user:\\n\") display(df) print(\"\\nA word-cloud representing the results:\\n\") get_wordcloud_for_user(df) ```      Here is the example of previously read articles by this user:    <div id=\"df-e8d6ebdc-61ac-4489-87a5-ac67cd1e8dfc\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     }  </style> <table class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>title</th>       <th>article</th>       <th>section</th>       <th>publication</th>     </tr>   </thead>   <tbody>     <tr>       <th>370</th>       <td>Wall St. falls as investors eye a united hawki...</td>       <td>NEW YORK (Reuters) - Wall Street’s major index...</td>       <td>Business News</td>       <td>Reuters</td>     </tr>     <tr>       <th>809</th>       <td>Oil surges on tanker attacks; stocks rise on F...</td>       <td>NEW YORK (Reuters) - Oil futures rose on Thurs...</td>       <td>Business News</td>       <td>Reuters</td>     </tr>     <tr>       <th>885</th>       <td>A look at Tesla's nine-member board</td>       <td>(Reuters) - Tesla Inc’s board has named a spec...</td>       <td>Business News</td>       <td>Reuters</td>     </tr>     <tr>       <th>1049</th>       <td>Home Depot posts rare sales miss as delayed sp...</td>       <td>(Reuters) - Home Depot Inc (HD.N) on Tuesday m...</td>       <td>Business News</td>       <td>Reuters</td>     </tr>     <tr>       <th>1555</th>       <td>PepsiCo's mini-sized sodas boost quarterly res...</td>       <td>(Reuters) - PepsiCo Inc’s (PEP.O) quarterly re...</td>       <td>Business News</td>       <td>Reuters</td>     </tr>     <tr>       <th>1638</th>       <td>Wall Street extends rally on U.S.-China trade ...</td>       <td>NEW YORK (Reuters) - U.S. stocks rallied on Fr...</td>       <td>Business News</td>       <td>Reuters</td>     </tr>     <tr>       <th>1900</th>       <td>U.S. plans limits on Chinese investment in U.S...</td>       <td>WASHINGTON (Reuters) - The U.S. Treasury Depar...</td>       <td>Business News</td>       <td>Reuters</td>     </tr>     <tr>       <th>2109</th>       <td>Exxon Mobil, Chevron dogged by refining, chemi...</td>       <td>HOUSTON (Reuters) - Exxon Mobil Corp and Chevr...</td>       <td>Business News</td>       <td>Reuters</td>     </tr>     <tr>       <th>2286</th>       <td>Wall Street soars on U.S. rate cut hopes</td>       <td>NEW YORK (Reuters) - Wall Street’s three major...</td>       <td>Business News</td>       <td>Reuters</td>     </tr>     <tr>       <th>2563</th>       <td>Apple shares drop on iPhone suppliers' warnings</td>       <td>(Reuters) - Apple Inc (AAPL.O) shares fell to ...</td>       <td>Business News</td>       <td>Reuters</td>     </tr>   </tbody> </table> </div>   <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e8d6ebdc-61ac-4489-87a5-ac67cd1e8dfc')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">     <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 0 24 24\"        width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg>   </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>  <script> const buttonEl =     document.querySelector('#df-e8d6ebdc-61ac-4489-87a5-ac67cd1e8dfc button.colab-df-convert'); buttonEl.style.display =     google.colab.kernel.accessAllowed ? 'block' : 'none';  async function convertToInteractive(key) {     const element = document.querySelector('#df-e8d6ebdc-61ac-4489-87a5-ac67cd1e8dfc');     const dataTable =     await google.colab.kernel.invokeFunction('convertToInteractive',                                                 [key], {});     if (!dataTable) return;      const docLinkHtml = 'Like what you see? Visit the ' +     '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'     + ' to learn more about interactive tables.';     element.innerHTML = '';     dataTable['output_type'] = 'display_data';     await google.colab.output.renderOutput(dataTable, element);     const docLink = document.createElement('div');     docLink.innerHTML = docLinkHtml;     element.appendChild(docLink); } </script> </div>    </div>      This table contains recommended articles for the user:    <div id=\"df-533066f8-08a5-4e0e-88fe-a60ad0d5ad3a\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     }  </style> <table class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>id</th>       <th>score</th>       <th>title</th>       <th>section</th>       <th>publication</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>131603</td>       <td>0.970929</td>       <td>US STOCKS-Wall Street muted as rate cut bets t...</td>       <td>Market News</td>       <td>Reuters</td>     </tr>     <tr>       <th>1</th>       <td>93287</td>       <td>0.970408</td>       <td>MONEY MARKETS-U.S. rate-cut bets in June slip ...</td>       <td>Bonds News</td>       <td>Reuters</td>     </tr>     <tr>       <th>2</th>       <td>159587</td>       <td>0.970357</td>       <td>Wall Street ekes out gain, Apple cuts revenue ...</td>       <td>Business News</td>       <td>Reuters</td>     </tr>     <tr>       <th>3</th>       <td>53602</td>       <td>0.969963</td>       <td>US STOCKS-Wall St drops on trade worries, Fed ...</td>       <td>Market News</td>       <td>Reuters</td>     </tr>     <tr>       <th>4</th>       <td>45533</td>       <td>0.969199</td>       <td>Wall Street wavers as tech gives ground and in...</td>       <td>Business News</td>       <td>Reuters</td>     </tr>     <tr>       <th>5</th>       <td>147320</td>       <td>0.968576</td>       <td>Dented Fed rate cut hopes drag on stocks; doll...</td>       <td>Davos</td>       <td>Reuters</td>     </tr>     <tr>       <th>6</th>       <td>152313</td>       <td>0.968503</td>       <td>MIDEAST - Factors to watch - July 9</td>       <td>Earnings Season</td>       <td>Reuters</td>     </tr>     <tr>       <th>7</th>       <td>34583</td>       <td>0.968178</td>       <td>Global stocks rally after speech by Fed's Powe...</td>       <td>Business News</td>       <td>Reuters</td>     </tr>     <tr>       <th>8</th>       <td>89976</td>       <td>0.968087</td>       <td>Stocks, yields rise after deal announced to en...</td>       <td>Business News</td>       <td>Reuters</td>     </tr>     <tr>       <th>9</th>       <td>96107</td>       <td>0.968018</td>       <td>Wall Street surges on higher oil after U.S. qu...</td>       <td>Business News</td>       <td>Reuters</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-533066f8-08a5-4e0e-88fe-a60ad0d5ad3a')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 0 24 24\"        width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg> </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>  <script> const buttonEl =     document.querySelector('#df-533066f8-08a5-4e0e-88fe-a60ad0d5ad3a button.colab-df-convert'); buttonEl.style.display =     google.colab.kernel.accessAllowed ? 'block' : 'none';  async function convertToInteractive(key) {     const element = document.querySelector('#df-533066f8-08a5-4e0e-88fe-a60ad0d5ad3a');     const dataTable =     await google.colab.kernel.invokeFunction('convertToInteractive',                                                 [key], {});     if (!dataTable) return;      const docLinkHtml = 'Like what you see? Visit the ' +     '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'     + ' to learn more about interactive tables.';     element.innerHTML = '';     dataTable['output_type'] = 'display_data';     await google.colab.output.renderOutput(dataTable, element);     const docLink = document.createElement('div');     docLink.innerHTML = docLinkHtml;     element.appendChild(docLink); } </script> </div>    </div>      A word-cloud representing the results:  ![Wordcloud of recommended business articles](https://raw.githubusercontent.com/pinecone-io/img/main/personalized-content-recommendations-example-3.png)  ### Query Results  We can see that each user's recommendations have a high similarity to what the user actually reads. A user who likes Tennis news has plenty of Tennis news recommendations. A user who likes to read about Xbox has that kind of news. And a business user has plenty of Wall Street news that he/she enjoys.  From the word-cloud, you can see the most frequent words that appear in the recommended articles' titles.  Since we used only the title and the content of the article to define the embeddings, and we did not take publications and sections into account, a user may get recommendations from a publication/section that he does not regularly read. You may try adding this information when creating embeddings as well and check your query results then!  Also, you may notice that some articles appear in the recommendations, although the user has already read them. These articles could be removed as part of postprocessing the query results, in case you prefer not to see them in the recommendations.  ## Delete the index  Delete the index once you are sure that you do not want to use it anymore. Once it is deleted, you cannot use it again.  ```python pinecone.delete_index(index_name) ``` ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e267"
  },
  "title": "Movie Recommender",
  "category": "630fc5235d91a70054705fb7",
  "content": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/recommendation/movie-recommender/00_movie_recommender.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/recommendation/movie-recommender/00_movie_recommender.ipynb) [![Open github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/pinecone-io/examples/blob/master/recommendation/movie-recommender/00_movie_recommender.ipynb)  This notebook demonstrates how Pinecone helps you build a simple Movie Recommender System. There are three parts to this recommender system:  - A dataset containing movie ratings - Two neural network models for embedding movies and users - A vector index to perform similarity search on those embeddings  The architecture of our recommender system is shown below. We have two models, a user model and a movie model, which generate embedding for users and movies. The two models are trained such that the proximity between a user and a movie in the multi-dimensional vector space depends on the rating given by the user for that movie. This means if a user gives a high rating to a movie, the movie will be closer to the user in the multi-dimensional vector space and vice versa. The result is that users with similar movie preferences and the movies they rated highly become closer in the vector space. A similarity search in this vector space for a user would give new recommendations based on the shared movie preference with other users.  ![Network Architecture Diagram](https://raw.githubusercontent.com/pinecone-io/img/main/movie-recommender.png)  ## Install Dependencies   ```python !pip install datasets transformers pinecone-client tensorflow ```  ## Load the Dataset  We will use a subset of the [MovieLens 25M Dataset](https://grouplens.org/datasets/movielens/25m/) in this project. This dataset contains ~1M user ratings provided by over 30k unique users for the most recent ~10k movies from the [MovieLens 25M Dataset](https://grouplens.org/datasets/movielens/25m/). The subset is available [here](https://huggingface.co/datasets/pinecone/movielens-recent-ratings) on HuggingFace datasets.   ```python from datasets import load_dataset  # load the dataset into a pandas datafame movies = load_dataset(\"pinecone/movielens-recent-ratings\", split=\"train\").to_pandas() ```   ```python # drop duplicates to return only unique movies unique_movies = movies.drop_duplicates(subset=\"imdb_id\") unique_movies.head() ```   <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     } </style> <table border=\"1\" class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>imdb_id</th>       <th>movie_id</th>       <th>user_id</th>       <th>rating</th>       <th>title</th>       <th>poster</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>tt5027774</td>       <td>6705</td>       <td>4556</td>       <td>4.0</td>       <td>Three Billboards Outside Ebbing, Missouri (2017)</td>       <td>https://m.media-amazon.com/images/M/MV5BMjI0OD...</td>     </tr>     <tr>       <th>1</th>       <td>tt5463162</td>       <td>7966</td>       <td>20798</td>       <td>3.5</td>       <td>Deadpool 2 (2018)</td>       <td>https://m.media-amazon.com/images/M/MV5BMDkzNm...</td>     </tr>     <tr>       <th>2</th>       <td>tt4007502</td>       <td>1614</td>       <td>26543</td>       <td>4.5</td>       <td>Frozen Fever (2015)</td>       <td>https://m.media-amazon.com/images/M/MV5BMjY3YT...</td>     </tr>     <tr>       <th>3</th>       <td>tt4209788</td>       <td>7022</td>       <td>4106</td>       <td>4.0</td>       <td>Molly's Game (2017)</td>       <td>https://m.media-amazon.com/images/M/MV5BNTkzMz...</td>     </tr>     <tr>       <th>4</th>       <td>tt2948356</td>       <td>3571</td>       <td>15259</td>       <td>4.0</td>       <td>Zootopia (2016)</td>       <td>https://m.media-amazon.com/images/M/MV5BOTMyMj...</td>     </tr>   </tbody> </table> </div>    ## Initialize Embedding Models  The `user_model` and `movie_model` are trained using Tensorflow Keras. The `user_model` transforms a given `user_id` into a 32-dimensional embedding in the same vector space as the movies, representing the user’s movie preference. The movie recommendations are then fetched based on proximity to the user’s location in the multi-dimensional space.  Similarly, the `movie_model` transforms a given `movie_id` into a 32-dimensional embedding in the same vector space as other similar movies — making it possible to find movies similar to a given movie.   ```python from huggingface_hub import from_pretrained_keras  # load the user model and movie model from huggingface user_model = from_pretrained_keras(\"pinecone/movie-recommender-user-model\") movie_model = from_pretrained_keras(\"pinecone/movie-recommender-movie-model\") ```   ## Create Pinecone Index  To create our vector index, we first need to initialize our connection to Pinecone. For this we need a [free API key](https://app.pinecone.io/). You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**. Once we have those, we initialize the connection like so:   ```python import pinecone  # connect to pinecone environment pinecone.init(     api_key=\"<<YOUR_API_KEY>>\",     environment=\"YOUR_ENVIRONMENT\" ) ```  Now we create a new index called `\"movie-emb\"`. What we name this isn't important.   ```python index_name = 'movie-emb'  # check if the movie-emb index exists if index_name not in pinecone.list_indexes():     # create the index if it does not exist     pinecone.create_index(         index_name,         dimension=32,         metric=\"cosine\"     )  # connect to movie-emb index we created index = pinecone.Index(index_name) ```  ## Create Movie Embeddings  We will be creating movie embeddings using the pretrained `movie_model`. All of the movie embeddings will be upserted to the new `\"movie-emb\"` index in Pinecone.   ```python from tqdm.auto import tqdm  # we will use batches of 64 batch_size = 64  for i in tqdm(range(0, len(unique_movies), batch_size)):     # find end of batch     i_end = min(i+batch_size, len(unique_movies))     # extract batch     batch = unique_movies.iloc[i:i_end]     # generate embeddings for batch     emb = movie_model.predict(batch['movie_id']).tolist()     # get metadata     meta = batch.to_dict(orient='records')     # create IDs     ids = batch[\"imdb_id\"].values.tolist()     # add all to upsert list     to_upsert = list(zip(ids, emb, meta))     # upsert/insert these records to pinecone     _ = index.upsert(vectors=to_upsert)  # check that we have all vectors in index index.describe_index_stats() ```      {'dimension': 32,      'index_fullness': 0.0,      'namespaces': {'': {'vector_count': 10269}},      'total_vector_count': 10269}    ## Get Recommendations  We now have movie embeddings stored in Pinecone. To get recommendations, we can do one of two things:  + Get a user embedding via a user embedding model and our `user_id`s, and retrieve movie embeddings that are most similar from Pinecone. + Use an existing movie embedding to retrieve other similar movies.  Both of these options use the same approach; the only difference is the source of data (user vs. movie) and the embedding model (user vs. movie).  We will start with the strategy of getting recommendations for a user embedding.   ### Get recommendations for a user ```python # we do this to display movie posters in a jupyter notebook from IPython.core.display import HTML ```  We will start by looking at a user's top rated movies. We can find this information inside the `movies` dataframe by filtering for movie ratings by a specific user (as per their `user_id`) and ordering these by the rating score.   ```python def top_movies_user_rated(user):     # get list of movies that the user has rated     user_movies = movies[movies[\"user_id\"] == user]     # order by their top rated movies     top_rated = user_movies.sort_values(by=['rating'], ascending=False)     # return the top 14 movies     return top_rated['poster'].tolist()[:14], top_rated['rating'].tolist()[:14] ```  After this, we can define a function called `display_posters` that will take a list of movie posters (like those returned by `top_movies_user_rated`) and display them in the notebook.   ```python def display_posters(posters):     figures = []     for poster in posters:         figures.append(f'''             <figure style=\"margin: 5px !important;\">               <img src=\"{poster}\" style=\"width: 120px; height: 150px\" >             </figure>         ''')     return HTML(data=f'''         <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">         {''.join(figures)}         </div>     ''') ```  Let's take a look at user `3`'s top rated movies:   ```python user = 3 top_rated, scores = top_movies_user_rated(user) display_posters(top_rated) ```   <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMDliOTIzNmUtOTllOC00NDU3LWFiNjYtMGM0NDc1YTMxNjYxXkEyXkFqcGdeQXVyNTM3NzExMDQ@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMjQ0MTgyNjAxMV5BMl5BanBnXkFtZTgwNjUzMDkyODE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTM4OGJmNWMtOTM4Ni00NTE3LTg3MDItZmQxYjc4N2JhNmUxXkEyXkFqcGdeQXVyNTgzMDMzMTg@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTExMzU0ODcxNDheQTJeQWpwZ15BbWU4MDE1OTI4MzAy._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTc2MTQ3MDA1Nl5BMl5BanBnXkFtZTgwODA3OTI4NjE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  </div>   ```python print(scores) ```      [4.5, 4.0, 4.0, 2.5, 2.5]   User `3` has rated these five movies, with *Big Hero 6*, *Civil War*, and *Avengers* being given good scores. They seem less enthusiastic about more sci-fi films like *Arrival* and *The Martian*.  Now let's see how to make some movie recommendations for this user.  Start by defining the `get_recommendations` function. Given a specific `user_id`, this uses the `user_model` to create a user embedding (`xq`). It then retrieves the most similar movie vectors from Pinecone (`xc`), and extracts the relevant movie posters so we can display them later.   ```python def get_recommendations(user):     # generate embeddings for the user     xq = user_model([user]).numpy().tolist()     # compute cosine similarity between user and movie vectors and return top k movies     xc = index.query(xq, top_k=14,                     include_metadata=True)     result = []     # iterate through results and extract movie posters     for match in xc['matches']:         poster = match['metadata']['poster']         result.append(poster)     return result ```  Now we can retrieve recommendations for the user.   ```python urls = get_recommendations(user) display_posters(urls) ```  <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMDliOTIzNmUtOTllOC00NDU3LWFiNjYtMGM0NDc1YTMxNjYxXkEyXkFqcGdeQXVyNTM3NzExMDQ@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMjQ0MTgyNjAxMV5BMl5BanBnXkFtZTgwNjUzMDkyODE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTM4OGJmNWMtOTM4Ni00NTE3LTg3MDItZmQxYjc4N2JhNmUxXkEyXkFqcGdeQXVyNTgzMDMzMTg@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMGQzN2Y0NDYtOGNlOS00OTVjLTkzMGUtZjYzNzdlMjQxMzgzXkEyXkFqcGdeQXVyNTY4NTYzMDM@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMjMxNjY2MDU1OV5BMl5BanBnXkFtZTgwNzY1MTUwNTM@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BNTk4ODQ1MzgzNl5BMl5BanBnXkFtZTgwMTMyMzM4MTI@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMjE0ODYxMzI2M15BMl5BanBnXkFtZTgwMDczODA2MDE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BNjM0NTc0NzItM2FlYS00YzEwLWE0YmUtNTA2ZWIzODc2OTgxXkEyXkFqcGdeQXVyNTgwNzIyNzg@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTU0NDE0Nzk1NF5BMl5BanBnXkFtZTgwMTY1NTAxMzE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMGZlNTY1ZWUtYTMzNC00ZjUyLWE0MjQtMTMxN2E3ODYxMWVmXkEyXkFqcGdeQXVyMDM2NDM2MQ@@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTAwMjU5OTgxNjZeQTJeQWpwZ15BbWU4MDUxNDYxODEx._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMjMyNDkzMzI1OF5BMl5BanBnXkFtZTgwODcxODg5MjI@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTc5MDE2ODcwNV5BMl5BanBnXkFtZTgwMzI2NzQ2NzM@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BYjhlNDljNTgtZjc4My00NmZmLTk2YzAtYWE5MDYwYjM4MTkzXkEyXkFqcGdeQXVyODE5NzE3OTE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  </div>   That looks good: the top results actually match the user's three favorite results. Following this, we see a lot of Marvel superhero films, which user `3` is probably going to enjoy, judging from their current ratings.  Let's see another user. This time, we choose user `128`.   ```python user = 128 top_rated, scores = top_movies_user_rated(user) display_posters(top_rated) ```   <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTAwMjU5OTgxNjZeQTJeQWpwZ15BbWU4MDUxNDYxODEx._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BOTgwMzFiMWYtZDhlNS00ODNkLWJiODAtZDVhNzgyNzJhYjQ4L2ltYWdlXkEyXkFqcGdeQXVyNzEzOTYxNTQ@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMjQ0MTgyNjAxMV5BMl5BanBnXkFtZTgwNjUzMDkyODE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTk0MDQ3MzAzOV5BMl5BanBnXkFtZTgwNzU1NzE3MjE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BYmI5ZGIxOGMtMjcwMS00Yzk3LWE0YWUtMzc5YTFhNGQ4OWZmXkEyXkFqcGdeQXVyNTIzOTk5ODM@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BOTc2OTA1MDM4M15BMl5BanBnXkFtZTgwNjczMDk5MjE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTQ1MjQwMTE5OF5BMl5BanBnXkFtZTgwNjk3MTcyMDE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BOTgxMDQwMDk0OF5BMl5BanBnXkFtZTgwNjU5OTg2NDE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BOTAzODEzNDAzMl5BMl5BanBnXkFtZTgwMDU1MTgzNzE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTUxMjQ2NjI4OV5BMl5BanBnXkFtZTgwODc2NjUwNDE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BOTMyMjEyNzIzMV5BMl5BanBnXkFtZTgwNzIyNjU0NzE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTM4OGJmNWMtOTM4Ni00NTE3LTg3MDItZmQxYjc4N2JhNmUxXkEyXkFqcGdeQXVyNTgzMDMzMTg@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BNzQ1MjQzMzM3OF5BMl5BanBnXkFtZTcwMzg3NzQ3OQ@@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTc2MTQ3MDA1Nl5BMl5BanBnXkFtZTgwODA3OTI4NjE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  </div>  ```python print(scores) ```      [4.5, 4.5, 4.5, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0]   Because this user seems to like everything, they also get recommended a mix of different things:   ```python urls = get_recommendations(user) display_posters(urls) ```   <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BYjFhOWY0OTgtNDkzMC00YWJkLTk1NGEtYWUxNjhmMmQ5ZjYyXkEyXkFqcGdeQXVyMjMxOTE0ODA@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BNjIwYjg1ZTEtOWRjYy00MTY3LWIyYTktMTI1Zjk2YzZkNDhiL2ltYWdlL2ltYWdlXkEyXkFqcGdeQXVyMjExNjgyMTc@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BNDEwZjU4ZmYtNjk0Ny00ZjVjLWE4OGUtNWE5NzFhNDI0MjgyXkEyXkFqcGdeQXVyNjU2NTIyOTE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTUzMjAxMzg5M15BMl5BanBnXkFtZTgwNjIxNjk5NzE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTQ4NzI2OTg5NV5BMl5BanBnXkFtZTgwNjQ3MDgyMjE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BNDQ4YTAyNDktMDhhYi00MzgyLWI0ZTktMjNiMGQ4MGU0NDQyXkEyXkFqcGdeQXVyNDY5MTUyNjU@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMjk4NGZiMzAtODU1NS00MmQ4LWJiNmQtNWU5ZWU4Y2VmNWI0XkEyXkFqcGdeQXVyODE5NzE3OTE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BYWE1NWFhZWEtOTYxZS00NTZmLWE5OWItMGQ2MTYzODNiNjQxXkEyXkFqcGdeQXVyNDM1ODc2NzE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BZWVlZmQ5N2EtZTQ2My00ZDUzLThkMmQtMDgyYTgwZWZlMjA0XkEyXkFqcGdeQXVyMjQ5NjMxNDA@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BYjkzZWIyZTctN2U3Ny00MDZlLTkzZTYtMTI2MWI5YTFiZWZkXkEyXkFqcGdeQXVyNTM2NTg3Nzg@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BYmJhZmJlYTItZmZlNy00MGY0LTg0ZGMtNWFkYWU5NTA1YTNhXkEyXkFqcGdeQXVyODE5NzE3OTE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMDNjZjkyNjQtNWMyMC00ODA5LTgyODctOGRiOWUwYTAzOWVjXkEyXkFqcGdeQXVyODE5NzE3OTE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTQzMjE5NDQwMl5BMl5BanBnXkFtZTgwMjI2NzA2MDE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BY2ViYjZiYTktZmJmMS00MGU5LTkxYjgtZWNkYzIyMGFjNWU4XkEyXkFqcGdeQXVyNTMzOTU3NzA@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  </div>   ```python user = 20000 top_rated, scores = top_movies_user_rated(user) display_posters(top_rated) ```  <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTU2NjA1ODgzMF5BMl5BanBnXkFtZTgwMTM2MTI4MjE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BN2U1YzdhYWMtZWUzMi00OWI1LWFkM2ItNWVjM2YxMGQ2MmNhXkEyXkFqcGdeQXVyNjU0OTQ0OTY@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMzM5NjUxOTEyMl5BMl5BanBnXkFtZTgwNjEyMDM0MDE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTc2MTQ3MDA1Nl5BMl5BanBnXkFtZTgwODA3OTI4NjE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMDliOTIzNmUtOTllOC00NDU3LWFiNjYtMGM0NDc1YTMxNjYxXkEyXkFqcGdeQXVyNTM3NzExMDQ@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BOTgwMzFiMWYtZDhlNS00ODNkLWJiODAtZDVhNzgyNzJhYjQ4L2ltYWdlXkEyXkFqcGdeQXVyNzEzOTYxNTQ@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BOTMyMjEyNzIzMV5BMl5BanBnXkFtZTgwNzIyNjU0NzE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  </div>   ```python print(scores) ```      [5.0, 4.0, 3.5, 3.5, 3.5, 3.0, 1.0]   We can see more of a trend towards action films with this user, so we can expect to see similar action-focused recommendations.   ```python urls = get_recommendations(user) display_posters(urls) ```   <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMjE2NDkxNTY2M15BMl5BanBnXkFtZTgwMDc2NzE0MTI@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTU2NjA1ODgzMF5BMl5BanBnXkFtZTgwMTM2MTI4MjE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTUyODU0ODU1Ml5BMl5BanBnXkFtZTgwNzM1MjIyMDE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMjIzMzA5NDk0NF5BMl5BanBnXkFtZTgwMDY2OTE2OTE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTg5MTE2NjA4OV5BMl5BanBnXkFtZTgwMTUyMjczMTE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BOGQyYTZjMDktMWQwNS00Y2NiLTg5MDctMjE3NjU5MjhmZDdiXkEyXkFqcGdeQXVyNTE0MDY4Mjk@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BNDA0MTlkYWQtNzNiMS00ZWE3LTg2ODUtZTQwNmVkN2E3M2NhXkEyXkFqcGdeQXVyNjUzNjY0NTE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTk1MjUzNzM0OF5BMl5BanBnXkFtZTgwMTg2MzIxMTI@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BYzc5MTU4N2EtYTkyMi00NjdhLTg3NWEtMTY4OTEyMzJhZTAzXkEyXkFqcGdeQXVyNjc1NTYyMjg@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMjMyNDkzMzI1OF5BMl5BanBnXkFtZTgwODcxODg5MjI@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BODg2OTVhZGQtYTU3Yi00NDg3LTljNzQtMjZhNDBhZjNlOGEyXkEyXkFqcGdeQXVyNjU1OTg4OTM@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BOTQyODc5MTAwM15BMl5BanBnXkFtZTgwNjMwMjA1MjE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BYmY0NGNiNzQtYmQ2Yi00OWEyLThmMWMtZjUzM2UwNDg1YjUxXkEyXkFqcGdeQXVyNTg4MTExMTg@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BM2Y3ZGM1OGItMTNjZS00MzI3LThkOGEtMDA2MmFlOTVlMTVhXkEyXkFqcGdeQXVyMTMxODk2OTU@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  </div>  ### Find Similar Movies  Now let's see how to find some similar movies.  Start by defining the `get_similar_movies` function. Given a specific `imdb_id`, we query directly using the pre-existing embedding for that ID stored in Pinecone.   ```python # search for similar movies in pinecone index def get_similar_movies(imdb_id):     # compute cosine similarity between movie and embedding vectors and return top k movies     xc = index.query(id=imdb_id, top_k=14, include_metadata=True)     result = []     # iterate through results and extract movie posters     for match in xc['matches']:         poster = match['metadata']['poster']         result.append(poster)     return result ```   ```python # imdbid of Avengers Infinity War imdb_id = \"tt4154756\" # filter the imdbid from the unique_movies movie = unique_movies[unique_movies[\"imdb_id\"] == imdb_id] movie ```     <div id=\"df-ab0e361d-f5c4-46bb-a2b9-b79d18186f20\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     } </style> <table border=\"1\" class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>imdb_id</th>       <th>movie_id</th>       <th>user_id</th>       <th>rating</th>       <th>title</th>       <th>poster</th>     </tr>   </thead>   <tbody>     <tr>       <th>11</th>       <td>tt4154756</td>       <td>1263</td>       <td>153</td>       <td>4.0</td>       <td>Avengers: Infinity War - Part I (2018)</td>       <td>https://m.media-amazon.com/images/M/MV5BMjMxNj...</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab0e361d-f5c4-46bb-a2b9-b79d18186f20')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\">     <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>     <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>   </svg>       </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>    <script>     const buttonEl =       document.querySelector('#df-ab0e361d-f5c4-46bb-a2b9-b79d18186f20 button.colab-df-convert');     buttonEl.style.display =       google.colab.kernel.accessAllowed ? 'block' : 'none';      async function convertToInteractive(key) {       const element = document.querySelector('#df-ab0e361d-f5c4-46bb-a2b9-b79d18186f20');       const dataTable =         await google.colab.kernel.invokeFunction('convertToInteractive',                                                   [key], {});       if (!dataTable) return;        const docLinkHtml = 'Like what you see? Visit the ' +         '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'         + ' to learn more about interactive tables.';       element.innerHTML = '';       dataTable['output_type'] = 'display_data';       await google.colab.output.renderOutput(dataTable, element);       const docLink = document.createElement('div');       docLink.innerHTML = docLinkHtml;       element.appendChild(docLink);     }   </script> </div>   </div>   ```python # display the poster of the movie display_posters(movie[\"poster\"]) ```   <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMjMxNjY2MDU1OV5BMl5BanBnXkFtZTgwNzY1MTUwNTM@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  </div>  Now we have *Avengers: Infinity War*. Let's find movies that are similar to this movie.   ```python similar_movies = get_similar_movies(imdb_id) display_posters(similar_movies) ```   <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMjMxNjY2MDU1OV5BMl5BanBnXkFtZTgwNzY1MTUwNTM@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTc5MDE2ODcwNV5BMl5BanBnXkFtZTgwMzI2NzQ2NzM@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMjMyNDkzMzI1OF5BMl5BanBnXkFtZTgwODcxODg5MjI@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMjQ0MTgyNjAxMV5BMl5BanBnXkFtZTgwNjUzMDkyODE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTAwMjU5OTgxNjZeQTJeQWpwZ15BbWU4MDUxNDYxODEx._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BNjM0NTc0NzItM2FlYS00YzEwLWE0YmUtNTA2ZWIzODc2OTgxXkEyXkFqcGdeQXVyNTgwNzIyNzg@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTM4OGJmNWMtOTM4Ni00NTE3LTg3MDItZmQxYjc4N2JhNmUxXkEyXkFqcGdeQXVyNTgzMDMzMTg@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BNTk4ODQ1MzgzNl5BMl5BanBnXkFtZTgwMTMyMzM4MTI@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BYzc5MTU4N2EtYTkyMi00NjdhLTg3NWEtMTY4OTEyMzJhZTAzXkEyXkFqcGdeQXVyNjc1NTYyMjg@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BYzFhMGM5ZTMtOGEyMC00ZTY5LWE1ZDUtNjQzM2NjZDdiMjg3XkEyXkFqcGdeQXVyNzIzMzE0NDY@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BNmUyMzU3YjgtZTliNS00NWM2LWI5ODgtYWE3ZjAzODgyNjNhXkEyXkFqcGdeQXVyNjY1MTg4Mzc@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMjMwNDkxMTgzOF5BMl5BanBnXkFtZTgwNTkwNTQ3NjM@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMDkzNmRhNTMtZDI4NC00Zjg1LTgxM2QtMjYxZDQ3OWJlMDRlXkEyXkFqcGdeQXVyNTU5MjkzMTU@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTc2MTQ3MDA1Nl5BMl5BanBnXkFtZTgwODA3OTI4NjE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  </div>  The top results closely match *Avengers: Infinity War*, the most similar movie being that movie itself. Following this, we see a lot of other Marvel superhero films.   Let's try another movie, this time a cartoon.   ```python # imdbid of Moana imdb_id = \"tt3521164\" # filter the imdbid from the unique_movies movie = unique_movies[unique_movies[\"imdb_id\"] == imdb_id] movie ```     <div id=\"df-8d5ad3ee-589c-4ab9-8835-80dd6049362e\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     } </style> <table border=\"1\" class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>imdb_id</th>       <th>movie_id</th>       <th>user_id</th>       <th>rating</th>       <th>title</th>       <th>poster</th>     </tr>   </thead>   <tbody>     <tr>       <th>97</th>       <td>tt3521164</td>       <td>5138</td>       <td>24875</td>       <td>5.0</td>       <td>Moana (2016)</td>       <td>https://m.media-amazon.com/images/M/MV5BMjI4Mz...</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d5ad3ee-589c-4ab9-8835-80dd6049362e')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\">     <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>     <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>   </svg>       </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>    <script>     const buttonEl =       document.querySelector('#df-8d5ad3ee-589c-4ab9-8835-80dd6049362e button.colab-df-convert');     buttonEl.style.display =       google.colab.kernel.accessAllowed ? 'block' : 'none';      async function convertToInteractive(key) {       const element = document.querySelector('#df-8d5ad3ee-589c-4ab9-8835-80dd6049362e');       const dataTable =         await google.colab.kernel.invokeFunction('convertToInteractive',                                                   [key], {});       if (!dataTable) return;        const docLinkHtml = 'Like what you see? Visit the ' +         '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'         + ' to learn more about interactive tables.';       element.innerHTML = '';       dataTable['output_type'] = 'display_data';       await google.colab.output.renderOutput(dataTable, element);       const docLink = document.createElement('div');       docLink.innerHTML = docLinkHtml;       element.appendChild(docLink);     }   </script> </div> </div>      ```python # display the poster of the movie display_posters(movie[\"poster\"]) ```      <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMjI4MzU5NTExNF5BMl5BanBnXkFtZTgwNzY1MTEwMDI@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  </div>      ```python similar_movies = get_similar_movies(imdb_id) display_posters(similar_movies) ```      <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMjI4MzU5NTExNF5BMl5BanBnXkFtZTgwNzY1MTEwMDI@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMzJmNGFmYmMtMmZhOC00MGM2LTk5NWItYzMzZmM1MzgzMTgxXkEyXkFqcGdeQXVyNTM3MDMyMDQ@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMTM4ODg0MzM0MV5BMl5BanBnXkFtZTcwNDY2MTc3Nw@@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BYjQ5NjM0Y2YtNjZkNC00ZDhkLWJjMWItN2QyNzFkMDE3ZjAxXkEyXkFqcGdeQXVyODIxMzk5NjA@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BZGUxZGMzYTYtNjJlMS00OGQ5LTg5YjItN2JjM2Y2NjQzMzdkL2ltYWdlXkEyXkFqcGdeQXVyNTAyODkwOQ@@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BNDMyZDc3YzktNWI3Yy00ZDM1LWJjYTMtY2I2YzRmZGQ5MTU4XkEyXkFqcGdeQXVyMTY5Nzc4MDY@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMjA2Mzg2NDMzNl5BMl5BanBnXkFtZTgwMjcwODUzOTE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMjAyODEwOTE4OV5BMl5BanBnXkFtZTgwNDIzMDc3ODE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BNDk3MzYyMjU5NF5BMl5BanBnXkFtZTgwNzQ5MDkzMzE@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BZDUyNzhjMTAtNGI5OC00MjYzLWFlNDUtMTQzYTdhZjliMDk0XkEyXkFqcGdeQXVyNTc5OTMwOTQ@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BOGY5ZDA4MDEtNWIzNi00YjkxLWE3Y2EtNmJiNzBhOWEyMWVjXkEyXkFqcGdeQXVyNTE1NjY5Mg@@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BYzMzZWM0NjYtMmNjMi00MzUzLThlNTAtZWQ1NjQzM2QyNzIwXkEyXkFqcGdeQXVyNDQ5MDYzMTk@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BMzg2Mzg4YmUtNDdkNy00NWY1LWE3NmEtZWMwNGNlMzE5YzU3XkEyXkFqcGdeQXVyMjA5MTIzMjQ@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  <figure style=\"margin: 5px !important;\">   <img src=\"https://m.media-amazon.com/images/M/MV5BNDc3YTEzZDItNjE2Yy00Nzg2LTgxMDAtNWMxOTJiMWQxZmNiXkEyXkFqcGdeQXVyMjExNjgyMTc@._V1_SX300.jpg\" style=\"width: 120px; height: 150px\" > </figure>  </div>   This result quality is good again. The top results include plenty of cartoons.  With that, we have built a recommendation system able to recommend movies based both on user movie ratings *and* similar movies. ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e269"
  },
  "title": "Video Transcript Search",
  "category": "630fc5235d91a70054705fb7",
  "content": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/search/semantic-search/yt-search/00-data-build.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/search/semantic-search/yt-search/00-data-build.ipynb) [![Open Github](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/github-shield.svg)](https://github.com/pinecone-io/examples/tree/master/search/semantic-search/yt-search)  We will work through an example of indexing and querying YouTube video transcriptions data. The prerequisite packages can be installed with:  ```python !pip install -U datasets sentence-transformers pinecone-client tqdm ```  We start by loading the dataset.   ```python from datasets import load_dataset  ytt = load_dataset(     \"pinecone/yt-transcriptions\",     split=\"train\",     revision=\"926a45\" ) ytt ```      Dataset({         features: ['video_id', 'text', 'start_second', 'end_second', 'url', 'title', 'thumbnail'],         num_rows: 11298     })    Each sample includes video-level information (ID, title, url and thumbnail) and snippet-level information (text, start_second, end_second).   ```python for x in ytt:     print(x)     break ```      {'video_id': 'ZPewmEu7644', 'text': \" hi this is Jeff Dean welcome to applications of deep neural networks of Washington University in this video we're going to look at how we can use ganz to generate additional training data for the latest on my a I course and projects click subscribe in the bell next to it to be notified of every new video Dan's have a wide array of uses beyond just the face generation that you\", 'start_second': 0, 'end_second': 20, 'url': 'https://www.youtube.com/watch?v=ZPewmEu7644&t=0s', 'title': 'GANS for Semi-Supervised Learning in Keras (7.4)', 'thumbnail': 'https://i.ytimg.com/vi/ZPewmEu7644/maxresdefault.jpg'}   ## Inserting Documents to Pinecone Index  The next step is indexing this dataset in Pinecone. For this, we need a sentence transformer model to encode the text into embeddings and a Pinecone index.  We will initialize the sentence transformer first.   ```python from sentence_transformers import SentenceTransformer  retriever = SentenceTransformer('flax-sentence-embeddings/all_datasets_v3_mpnet-base') retriever ```       SentenceTransformer(       (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: MPNetModel        (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})       (2): Normalize()     )    We can see the embedding dimension of `768` above. We will need this when creating our Pinecone index.   ```python embed_dim = retriever.get_sentence_embedding_dimension() embed_dim ```       768    Now we can initialize our index.   ```python import pinecone  # get api key from app.pinecone.io pinecone.init(     api_key=\"<<YOUR_API_KEY>>\",     environment=\"YOUR_ENVIRONMENT\" )  # create index pinecone.create_index(     \"youtube-search\",     dimension=embed_dim,     metric=\"cosine\" )  # connect to new index index = pinecone.Index(\"youtube-search\") ```  You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**.  We will index our data in batches of `64`. The data we insert into our index will contain records (here, *documents*) containing a unique document/snippet ID, embedding, and metadata in the following format:  ```json {     'doc-id',     [0.0, 0.3, 0.1, ...],     {'title': '???', 'start_seconds': 12, ...} } ```  To create these *documents* and insert them to Pinecone, we run the following loop:  ```python from tqdm.auto import tqdm  docs = []  # this will store IDs, embeddings, and metadata  batch_size = 64  for i in tqdm(range(0, len(ytt), batch_size)):     i_end = min(i+batch_size, len(ytt))     # extract batch from YT transactions data     batch = ytt[i:i_end]     # encode batch of text     embeds = retriever.encode(batch['text']).tolist()     # each snippet needs a unique ID     # we will merge video ID and start_seconds for this     ids = [f\"{x[0]}-{x[1]}\" for x in zip(batch['video_id'], batch['start_second'])]     # create metadata records     meta = [{         'video_id': x[0],         'title': x[1],         'text': x[2],         'start_second': x[3],         'end_second': x[4],         'url': x[5],         'thumbnail': x[6]     } for x in zip(         batch['video_id'],         batch['title'],         batch['text'],         batch['start_second'],         batch['end_second'],         batch['url'],         batch['thumbnail']     )]     # create list of (IDs, vectors, metadata) to upsert     to_upsert = list(zip(ids, embeds, meta))     # add to pinecone     index.upsert(vectors=to_upsert) index.describe_index_stats() ```       {'dimension': 768,      'index_fullness': 0.01,      'namespaces': {'': {'vector_count': 11298}}}  Using `index.describe_index_stats()` we can see that the index now contains *11'298* vectors, the full `pinecone/yt-transcriptions` dataset.   ## Querying  When query we encode our text with the same retriever model and pass it to the Pinecone `query` endpoint.   ```python query = \"What is deep learning?\"  xq = retriever.encode(query).tolist() ```   ```python xc = index.query(xq, top_k=5,                  include_metadata=True) for context in xc['matches']:     print(context['metadata']['text'], end=\"\\n---\\n\") ```       terms of optimization but what's the algorithm for updating the parameters or updating whatever the state of the network is and then the the last part is the the data set like how do you actually represent the world as it comes into your machine learning system so I think of deep learning as telling us something about what does the model look like and basically to qualify as deep I     ---      any theoretical components any theoretical things that you need to understand about deep learning can be sick later for that link again just watched the word doc file again in that I mentioned the link also the second channel is my channel because deep learning might be complete deep learning playlist that I have created is completely in order okay to the other     ---      under a rock for the last few years you have heard of the deep networks and how they have revolutionised computer vision and kind of the standard classic way of doing this is it's basically a classic supervised learning problem you are giving a network which you can think of as a big black box a pairs of input images and output labels XY pairs okay and this big black box essentially you     ---      do the task at hand. Now deep learning is just a subset of machine learning which takes this idea even a step further and says how can we automatically extract the useful pieces of information needed to inform those future predictions or make a decision And that's what this class is all about teaching algorithms how to learn a task directly from raw data. We want to     ---      algorithm and yelled at everybody in a good way that nobody was answering it correctly everybody knew what the alkyl it was graduate course everybody knew what an algorithm was but they weren't able to answer it well let me ask you in that same spirit what is deep learning I would say deep learning is any kind of machine learning that involves learning parameters of more than one consecutive     ---  ## Example application  To try out an application like this one, see this [example application](https://huggingface.co/spaces/pinecone/yt-search). ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e26b"
  },
  "title": "Semantic Search",
  "category": "630fc5235d91a70054705fb7",
  "content": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/semantic_text_search/semantic_text_search.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/semantic_text_search/semantic_text_search.ipynb) [![Open github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/pinecone-io/examples/blob/master/semantic_text_search/semantic_text_search.ipynb)   This notebook demonstrates how to create a simple semantic text search using Pinecone’s similarity search service.  The goal is to create a search application that retrieves news articles based on short description queries (e.g., article titles). To achieve that, we will store vector representations of the articles in Pinecone's index. These vectors and their proximity capture semantic relations. Nearby vectors indicate similar content, and contents from faraway vectors are dissimilar.  Semantic textual search is a technique used for solving other text-based applications. For example, our deduplication, question-answering and personalized article recommendation [demos](https://www.pinecone.io/docs/examples/) were solved using semantic textual search.  ## Pinecone Setup   ```python !pip install -qU pinecone-client ipywidgets ```   ```python import pinecone ```   ```python # Load Pinecone API key import os api_key = os.getenv(\"PINECONE_API_KEY\") or \"YOUR-API-KEY\" pinecone.init(api_key=api_key, environment='YOUR_ENVIRONMENT')  # List all indexes currently present for your key pinecone.list_indexes() ```      []   [Get a Pinecone API key](https://www.pinecone.io/start/) if you don’t have one already. You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**.  ## Install and Import Python Packages   ```python !pip install -qU wordcloud pandas-profiling !pip install -qU sentence-transformers --no-cache-dir ```   ```python import pandas as pd import numpy as np import time import re from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator import matplotlib.pyplot as plt import sqlite3  pd.set_option('display.max_colwidth', 200) ```  ## Create a New Service   ```python # Pick a name for the new index index_name = 'semantic-text-search' ```   ```python # Check whether the index with the same name already exists if index_name in pinecone.list_indexes():     pinecone.delete_index(index_name) ```   ```python pinecone.create_index(name=index_name, dimension=300) ```   ```python index = pinecone.Index(index_name=index_name) ```  ## Upload  We will define two separate sub-indexes using Pinecone's namespace feature. One for indexing articles by **content**, and the other by **title**. At query time, we will return an aggregation of the results from the content and title indexes.    First, we will load data and the model, and then create embeddings and upsert them into the namespaces.   ###  Load data  The [dataset](https://components.one/datasets/all-the-news-articles-dataset) used throughout this example contains 204,135 articles from 18 American publications.  Let's download the dataset and load data.   ```python import requests, os DATA_DIR = 'tmp' URL = \"https://www.dropbox.com/s/b2cyb85ib17s7zo/all-the-news.db?dl=1\" FILE = f\"{DATA_DIR}/all-the-news.db\"  def download_data():     os.makedirs(DATA_DIR, exist_ok=True)      if not os.path.exists(FILE):         r = requests.get(URL)  # create HTTP response object         with open(FILE, \"wb\") as f:                         f.write(r.content)  download_data() ```   ```python cnx = sqlite3.connect(FILE) data = pd.read_sql_query(\"SELECT * FROM longform\", cnx) data.set_index('id', inplace=True) data.head() ```        <div id=\"df-0ad3a18a-9598-42a1-83a0-f51c54cd3e5e\">     <div class=\"colab-df-container\"> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     } </style> <table class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>title</th>       <th>author</th>       <th>date</th>       <th>content</th>       <th>year</th>       <th>month</th>       <th>publication</th>       <th>category</th>       <th>digital</th>       <th>section</th>       <th>url</th>     </tr>     <tr>       <th>id</th>       <th></th>       <th></th>       <th></th>       <th></th>       <th></th>       <th></th>       <th></th>       <th></th>       <th></th>       <th></th>       <th></th>     </tr>   </thead>   <tbody>     <tr>       <th>1</th>       <td>Agent Cooper in Twin Peaks is the audience: once delighted, now disintegrating</td>       <td>\\nTasha Robinson\\n</td>       <td>2017-05-31</td>       <td>And never more so than in Showtime’s new series revival Some spoilers ahead through episode 4 of season 3 of Twin Peaks. On May 21st, Showtime brought back David Lynch’s groundbreaking TV se...</td>       <td>2017</td>       <td>5</td>       <td>Verge</td>       <td>Longform</td>       <td>1.0</td>       <td>None</td>       <td>None</td>     </tr>     <tr>       <th>2</th>       <td>AI, the humanity!</td>       <td>\\nSam Byford\\n</td>       <td>2017-05-30</td>       <td>AlphaGo’s victory isn’t a defeat for humans — it’s an opportunity A loss for humanity! Man succumbs to machine! If you heard about AlphaGo’s latest exploits last week — crushing the world’s ...</td>       <td>2017</td>       <td>5</td>       <td>Verge</td>       <td>Longform</td>       <td>1.0</td>       <td>None</td>       <td>None</td>     </tr>     <tr>       <th>3</th>       <td>The Viral Machine</td>       <td>\\nKaitlyn Tiffany\\n</td>       <td>2017-05-25</td>       <td>Super Deluxe built a weird internet empire. Can it succeed on TV? When Wolfgang Hammer talks about the future of entertainment, people listen. Hammer is the mastermind behind the American re...</td>       <td>2017</td>       <td>5</td>       <td>Verge</td>       <td>Longform</td>       <td>1.0</td>       <td>None</td>       <td>None</td>     </tr>     <tr>       <th>4</th>       <td>How Anker is beating Apple and Samsung at their own accessory game</td>       <td>\\nNick Statt\\n</td>       <td>2017-05-22</td>       <td>Steven Yang quit his job at Google in the summer of 2011 to build the products he felt the world needed: a line of reasonably priced accessories that would be better than the ones you could ...</td>       <td>2017</td>       <td>5</td>       <td>Verge</td>       <td>Longform</td>       <td>1.0</td>       <td>None</td>       <td>None</td>     </tr>     <tr>       <th>5</th>       <td>Tour Black Panther’s reimagined homeland with Ta-Nehisi Coates</td>       <td>\\nKwame Opam\\n</td>       <td>2017-05-15</td>       <td>Ahead of Black Panther’s 2018 theatrical release, Marvel turned to Ta-Nehisi Coates to breathe new life into the nation of Wakanda. “I made most of my career analyzing the forces of racism a...</td>       <td>2017</td>       <td>5</td>       <td>Verge</td>       <td>Longform</td>       <td>1.0</td>       <td>None</td>       <td>None</td>     </tr>   </tbody> </table> </div>  ```python # Define number of test articles NUM_OF_TEST_ARTICLES = 2  # Remove test articles from data and keep them in separate dataframe test_articles = data[['title','content']][97::81][:NUM_OF_TEST_ARTICLES] data.drop(list(test_articles.index), inplace=True) ```  ### Use Ready Made Vector Embedding Model  We will use an [Average Word Embeddings Model](https://www.sbert.net/docs/pretrained_models.html#average-word-embeddings-models) to create both title and content embeddings. Pinecone allows you to create paritions in the index that we call namespaces. This will allow us to maintain separate embeddings for the data that can be used for different tasks.   ```python from sentence_transformers import SentenceTransformer  model = SentenceTransformer('average_word_embeddings_komninos') ```  ### Upload Vectors of Titles  Here we index articles by title only. You can notice we create a *title* namespace for this purpose.   ```python from typing import Iterator  class BatchGenerator:     \"\"\" Models a simple batch generator that make chunks out of an input DataFrame. \"\"\"          def __init__(self, batch_size: int = 10) -> None:         self.batch_size = batch_size          def to_batches(self, df: pd.DataFrame) -> Iterator[pd.DataFrame]:         \"\"\" Makes chunks out of an input DataFrame. \"\"\"         splits = self.splits_num(df.shape[0])         if splits <= 1:             yield df         else:             for chunk in np.array_split(df, splits):                 yield chunk          def splits_num(self, elements: int) -> int:         \"\"\" Determines how many chunks DataFrame contians. \"\"\"         return round(elements / self.batch_size)          __call__ = to_batches  df_batcher = BatchGenerator(300) ```   ```python # Fill missing and remove redundant data data['title'] = data['title'].fillna('')  # Create vector embeddings based on the title column print('Encoding titles...') encoded_titles = model.encode(data['title'].tolist(), show_progress_bar=True) data['title_vector'] = encoded_titles.tolist() ```      Encoding titles...        Batches:   0%|          | 0/6380 [00:00<?, ?it/s]    ```python data['vector_id'] = data.index data['vector_id'] = data['vector_id'].apply(str) ```   ```python # Upsert title vectors in title namespace print(\"Uploading vectors to title namespace..\") for batch_df in df_batcher(data):     index.upsert(vectors=zip(batch_df.vector_id, batch_df.title_vector), namespace='title') ```      Uploading vectors to title namespace..   ### Upload Vectors of Content  Now we index articles by their content. We want to separately maintain embeddings for both title and content hence we use a separate namespace in the same index.   ```python # Fill missing data data['content'] = data['content'].fillna('')  # Extract only first few sentences of each article for quicker vector calculations data['content'] = data.content.apply(lambda x: ' '.join(re.split(r'(?<=[.:;])\\s', x)[:10]))  # Create vector embeddings based on the content column print('Encoding content...') encoded_content = model.encode(data['content'].tolist(), show_progress_bar=True) data['content_vector'] = encoded_content.tolist() ```      Encoding content...        Batches:   0%|          | 0/6380 [00:00<?, ?it/s]    ```python # Upsert title vectors in content namespace print(\"Uploading vectors to content namespace..\") for batch_df in df_batcher(data):     index.upsert(vectors=zip(batch_df.vector_id, batch_df.content_vector), namespace='content') ```      Uploading vectors to content namespace..   Now that we have upserted data, we can check the size of each namespace.     ```python # Check index size for each namespace index.describe_index_stats() ```         {'dimension': 300,      'index_fullness': 0.06,      'namespaces': {'content': {'vector_count': 204133},                     'title': {'vector_count': 204133}}}    ## Query  Let's see what our test articles look like first.   ```python # Print test articles display(test_articles) ```      <div id=\"df-746d8ca7-7405-4dda-84b2-c12520410b09\">     <div class=\"colab-df-container\"> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     } </style> <table class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>title</th>       <th>content</th>     </tr>     <tr>       <th>id</th>       <th></th>       <th></th>     </tr>   </thead>   <tbody>     <tr>       <th>111</th>       <td>The Rise and Fall and Rise of Virtual Reality</td>       <td>In the wake of Facebook's purchase of Oculus VR, can this revolutionary technology triumph anew?</td>     </tr>     <tr>       <th>6467</th>       <td>Who should go to Mars?</td>       <td>Elon Musk laid out his plan to colonize Mars at a conference on Tuesday, but it was during the Q&amp;ampampA session that a woman asked one of the key questions: who will be chosen to embark on ...</td>     </tr>   </tbody> </table> </div>  The following utility functions help us process and present the results.   ```python titles_mapped = dict(zip(data.index, data.title)) content_mapped = dict(zip(data.index, data.content)) ```   ```python def get_wordcloud_for_article(recommendations, namespace):     'Generates word cloud for the recommendations (titles or content).'      stopwords = set(STOPWORDS).union([np.nan, 'NaN', 'S'])     wordcloud = WordCloud(                    max_words=50000,                     min_font_size =12,                     max_font_size=50,                     relative_scaling = 0.9,                     stopwords=set(STOPWORDS),                    normalize_plurals= True                   )          if namespace == 'title':         clean_titles = [word for word in recommendations.title.values if word not in stopwords]         wordcloud = wordcloud.generate(' '.join(clean_titles))     else:         clean_content = [word for word in recommendations.content.values if word not in stopwords]         wordcloud = wordcloud.generate(' '.join(clean_content))      plt.imshow(wordcloud, interpolation='bilinear')     plt.axis(\"off\")     plt.show()   def print_query_results(query_result, query, namespace, show_options={'wordcloud':True, 'tabular':True}):     'Prints query result with wordcloud.'            print(f'\\nMost similar results querying {query} in \"{namespace}\" namespace:\\n')     if not query_result.matches:         print('no query result')          matches = query_result.matches     ids = [res.id for res in matches]     scores = [res.score for res in matches]     df = pd.DataFrame({'id':ids,                         'score':scores,                        'title': [titles_mapped[int(_id)] if int(_id) in titles_mapped else ' '  for _id in ids],                        'content': [content_mapped[int(_id)] if int(_id) in content_mapped else ' '  for _id in ids],                        })     if show_options['tabular']:         display(df.head(5))     if show_options['wordcloud']:         get_wordcloud_for_article(df, namespace)     print('\\n') ```  The following two functions we use to query the test article's title or content in either of the namespaces we created. This means we can query the title in the \"title\" namespace or the \"content\" namespace. The same is with the article content.   ```python def query_article_title(test_article, namespace, top_k=5, show_options={'wordcloud':True, 'tabular':True}):     '''Queries an article using its title in the specified      namespace and prints results.'''      # Create vector embeddings based on the title column     encoded_titles = model.encode(test_article['title'],                                    show_progress_bar=False)     test_article['title_vector'] = encoded_titles.tolist()      # Query namespace passed as parameter using title vector     query_result_titles = index.query(test_article.title_vector,                                        namespace=namespace,                                        top_k=top_k)      # Print query results      if show_options['wordcloud'] or show_options['tabular']:         print_query_results(query_result_titles, query='title',                              namespace=namespace,                              show_options=show_options)      return query_result_titles ```  When querying content, we will first create the article content vector and search for the most similar vectors in the \"title\" or the \"content\" namespace.   ```python def query_article_content(test_article, namespace, top_k=5, show_options={'wordcloud':True, 'tabular':True}):     '''Queries an article using its content in the specified      namespace and prints results.'''      # Create vector embeddings based on the content column     encoded_content = model.encode(test_article['content'],                                     show_progress_bar=False)     test_article['content_vector'] = encoded_content.tolist()      # Query content namespace using content vector     query_result_content = index.query(test_article.content_vector,                                         namespace=namespace,                                         top_k=top_k)      # Print query results      if show_options['wordcloud'] or show_options['tabular']:         print_query_results(query_result_content,                              query='content',                              namespace=namespace,                              show_options=show_options)      return query_result_content ```  Now it's time to do the cross namespace querying and aggregate the results. The following functions query for four combinations (title/content only and title-content, content-title). Then aggregates the results of the four queries to calculate the total occurrence of the articles and their average scores. They are ranked accordingly and the most similar articles are returned.   ```python def aggregate_results(article):     '''Aggregates results after querying both namespaces        for both the article's title and content.'''      results = []          results.append(query_article_title(article, namespace='title', top_k=30, show_options={'wordcloud':False, 'tabular':False}))     results.append(query_article_title(article, namespace='content', top_k=30, show_options={'wordcloud':False, 'tabular':False}))     results.append(query_article_content(article, namespace='title', top_k=30, show_options={'wordcloud':False, 'tabular':False}))     results.append(query_article_content(article, namespace='content', top_k=30, show_options={'wordcloud':False, 'tabular':False}))      articles_scores = {}     articles_count = {}      for res in results:         ids = [r.id for r in res.matches]         scores = [r.score for r in res.matches]         for id, score in zip(ids, scores):             if id not in articles_scores:                 articles_scores[id] = score                 articles_count[id] = 1             else:                 articles_scores[id] += score                 articles_count[id] += 1          return articles_scores , articles_count  def show_aggregated_results(results_dict, counts, show_options={'wordcloud':True, 'tabular':True}):     '''Shows results after aggregation. Values are sorted based     on the number of queries they appear (1-4) and based on their     average score.'''          df = pd.DataFrame({'id':results_dict.keys(),                         'count': counts.values(),                        'average_score':[round(r/c, 3) for r, c in zip(results_dict.values(),counts.values())],                        'title': [titles_mapped[int(_id)] if int(_id) in titles_mapped else ' '  for _id in results_dict.keys()],                        'content': [content_mapped[int(_id)] if int(_id) in content_mapped else ' '  for _id in results_dict.keys()],                        })     df.sort_values(by=['count', 'average_score'], ascending=False, inplace=True)          if show_options['tabular']:         print('\\nMost similar results after aggregation:\\n')         display(df.head(5))     if show_options['wordcloud']:         print('\\nWordcloud for titles and content after aggregation:')         print('-Titles:')         get_wordcloud_for_article(df[:10], 'title')         print('-Content:')         get_wordcloud_for_article(df[:10], 'content')     print('\\n') ```  ### Query by Aggregation We are ready to query our service! We will use all the above auxiliary functions to query the test articles. We will be using our cross-namespace approach that combines four query results into one.  Note that you can add the tabular data results for each query by changing the ```show_options``` flags below.   ```python # Query index using simple and cross namespace approach for e, (_, test_article) in enumerate(test_articles.iterrows()):     print(f'\\nArticle {e+1}')     print(f'\\n Title: {test_article.title}')     print(f' Content: {test_article.content[:200].strip()}' + ('...' if len(test_article.content) > 200 else ''))          # Uncomment to query the titles in title namespace     # query_article_title(test_article, 'title',  show_options={'wordcloud':True, 'tabular':False})      # Uncomment to query the content in content namespace     # query_article_content(test_article, namespace='content', show_options={'wordcloud':True, 'tabular':False})      # Cross namespace query     aggregated_results, counts = aggregate_results(test_article)     show_aggregated_results(aggregated_results, counts, show_options={'wordcloud':False, 'tabular':True}) ```           Article 1           Title: The Rise and Fall and Rise of Virtual Reality      Content: In the wake of Facebook's purchase of Oculus VR, can this revolutionary technology triumph anew?          Most similar results after aggregation:            <div id=\"df-95f8bb0e-16a0-4668-b9e8-2cefcd42975d\">     <div class=\"colab-df-container\"> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     } </style> <table class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>id</th>       <th>count</th>       <th>average_score</th>       <th>title</th>       <th>content</th>     </tr>   </thead>   <tbody>     <tr>       <th>59</th>       <td>19720</td>       <td>2</td>       <td>0.809</td>       <td>Oculus Founder, at Center of Legal Battle Over VR, Departs Facebook - The New York Times</td>       <td>SAN FRANCISCO — Palmer Luckey, a founder of the virtual-reality technology company Oculus, has left Facebook three years after the social network acquired his company for close to $3 billion. Mr. ...</td>     </tr>     <tr>       <th>61</th>       <td>7201</td>       <td>2</td>       <td>0.808</td>       <td>Flush with cash, Oculus plans ambitious new VR headset</td>       <td>According to Oculus Rift inventor Palmer Luckey, virtual reality is near and dear to Marc Andreessen&amp;amprsquos heart. Twenty years ago&amp;ampnbsp&amp;ampmdash&amp;ampnbspbefore he created the Mosaic we...</td>     </tr>     <tr>       <th>63</th>       <td>34611</td>       <td>2</td>       <td>0.806</td>       <td>Microsoft Introducing VR Headsets at Half the Price of Oculus Rift - Breitbart</td>       <td>On October 26, Microsoft doubled down on virtual reality by announcing their own VR headsets at the Windows 10 event.[Unless you’ve got $599 for the Oculus Rift, or $799 for Valve’s HTC Vive, your...</td>     </tr>     <tr>       <th>62</th>       <td>13199</td>       <td>2</td>       <td>0.800</td>       <td>Oculus VR founder Palmer Luckey talks GoPro, 'Minecraft' and eSports - LA Times</td>       <td>Oculus VR founder Palmer Luckey answers questions at the Loews Hollywood Hotel on Sept. 24. ', 'A few years ago, journalism major Palmer Luckey dropped out of Cal State Long Beach to work on a dev...</td>     </tr>     <tr>       <th>64</th>       <td>9533</td>       <td>2</td>       <td>0.800</td>       <td>Virtual reality visionary Palmer Luckey leaves Facebook 3 years after $2-billion Oculus deal - LA Times</td>       <td>Palmer Luckey, the Long Beach entrepreneur whose zeal for virtual reality kickstarted mass investment in the technology, has left Facebook three years after selling his start-up Oculus VR to the s...</td>     </tr>   </tbody> </table> </div>          Article 2           Title: Who should go to Mars?      Content: Elon Musk laid out his plan to colonize Mars at a conference on Tuesday, but it was during the Q&ampampA session that a woman asked one of the key questions: who will be chosen to embark on a ri...          Most similar results after aggregation:            <div id=\"df-2e92eccd-3e99-49bd-ab18-72dbc0e3adb4\">     <div class=\"colab-df-container\"> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     } </style> <table class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>id</th>       <th>count</th>       <th>average_score</th>       <th>title</th>       <th>content</th>     </tr>   </thead>   <tbody>     <tr>       <th>3</th>       <td>206370</td>       <td>2</td>       <td>0.703</td>       <td>How Mars lost its atmosphere, and why Earth didn’t</td>       <td>Mars was once wetter and warmer, and very possibly a congenial environment for life as we know it. Today it looks mighty dead, with all due respect. If there's life, it's cryptic. Mars ju...</td>     </tr>     <tr>       <th>6</th>       <td>83533</td>       <td>2</td>       <td>0.679</td>       <td>Mars Reconnaissance Orbiter celebrates 10 years at red planet</td>       <td>[Sign in to comment!, NASA’s Mars Reconnaissance Orbiter (MRO) arrived at the red planet 10 years ago today and has since completed 45,000 orbits and generated a vast amount of scientific data., O...</td>     </tr>     <tr>       <th>11</th>       <td>83531</td>       <td>2</td>       <td>0.664</td>       <td>Buzz Aldrin eyes 2040 for manned Mars mission</td>       <td>[Sign in to comment!, Former astronaut Buzz Aldrin is eyeing 2040 for the first manned mission to Mars, noting that the red planet’s moon Phobos could play a vital role for astronauts., “I think t...</td>     </tr>     <tr>       <th>27</th>       <td>16615</td>       <td>2</td>       <td>0.641</td>       <td>NASA orbiters watch as comet flies safely past Mars - LA Times</td>       <td>Comet Siding Spring sailed past Mars on Sunday, coming 10 times closer to the Red Planet than any comet on record has come to Earth.', \"At the time of the comet's closest approach at 11:27 a.m., i...</td>     </tr>     <tr>       <th>29</th>       <td>158293</td>       <td>2</td>       <td>0.640</td>       <td>Mars makes closest approach to Earth for 11 years</td>       <td>Mars reaches its closest approach to Earth for 11 years this evening at 21:35 GMT. The red planet will be just 75 million kilometres away., Mars has been steadily approaching, tripling its apparen...</td>     </tr>   </tbody> </table> </div>  ## Summary We demonstrated a simple textual semantic search approach that aggregates results from two different news article representations:  for titles only and content only. We do that by utilizing Pinecone's namespace feature to create two namespaced indexes. The aggregation mechanism is simple. We use the query's title and content representations to query both namespaces and weight results by their occurrences. Our example queries illustrate the effectiveness of this approach.   We encourage you to try the code with your data. You might want to try other embedding or aggregation mechanisms. Working with a similarity search service makes such experimentations easy. Have fun, and [let us know](https://www.pinecone.io/contact/) if you have any questions or interesting findings.   ## Delete the index  Delete the index once you are sure that you do not want to use it anymore. Once the index is deleted, you cannot use it again. Use it as a cleanup step if you are done working with a specific index.   ```python pinecone.delete_index(index_name) ``` ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e26d"
  },
  "title": "Time Series Search",
  "category": "630fc5235d91a70054705fb7",
  "content": " [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/analytics-and-ml/time-series/time-series-stocks-pattern-example.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/analytics-and-ml/time-series/time-series-stocks-pattern-example.ipynb) [![Open github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/pinecone-io/examples/blob/master/analytics-and-ml/time-series/time-series-stocks-pattern-example.ipynb)   Time series, a sequence of values ordered by time, is one of the fundamental data forms. Consequently, there are plentiful time-series analysis methods and tools, ranging from forecasting to anomaly detection.   Here we demonstrate how to perform time-series \"pattern\" matching using a similarity search service. Wherein we want to retrieve all historical time series that match a particular pattern. Such matching capability serves as a core ingredient for time series applications such as clustering, labeling, and recommendation. For example, consider a time series describing web page visitors and a need to retrieve all historical peak surges, drops, or trends.   We will walk you through a simple approach that utilizes the time series raw data as-is. In other words, it does not require any modeling heavy lifting. Such an approach is very appealing because it does not require any domain-specific technical knowledge nor extra model generation resources. Sounds too good to be true?  Our demo indicates that this simple approach provides satisfying results. We will show you how to index and search a set of stock market daily prices time series. Then we will compare the simple approach with an alternative that utilizes a comprehensive time-series library recently published by Facebook AI.   What we'll cover: * Prerequisites * Simple Time-Series Embeddings     * Prepare data     * Index     * Search * Facebook's [Kats](https://engineering.fb.com/2021/06/21/open-source/kats/) Time-Series Embeddings     * Index     * Search * Conclusion  ## Prerequisites Install and import relevant python packages   ```python !pip install -qU convertdate kaggle matplotlib==3.1.3  !pip install -q git+https://github.com/facebookresearch/Kats.git !pip install -qU pinecone-client ```  *If you are using Google Colab, please restart the runtime in order to use newly installed package versions.*   ```python import os import pandas as pd import numpy as np import matplotlib.pyplot as plt import pprint from sklearn.preprocessing import MinMaxScaler from kats.consts import TimeSeriesData from kats.tsfeatures.tsfeatures import TsFeatures import itertools from decimal import Decimal from IPython.display import clear_output  import warnings warnings.simplefilter(action='ignore') ```  ## Simple Time-Series Embeddings  ### Upload Time Series to Pinecone's Similarity Search Service In the steps below how to set up Pinecone's similarity search service and upload the time series into the service's *Index* data structure. Pinecone stores and searches [vector embeddings](https://www.pinecone.io/learn/vector-embeddings/). These embeddings or feature vectors are a numerical representation of raw data semantics.   Recall that we want to create two indexes:  * An index that contains vectors representing the raw data of historical prices of different stocks. In other words, vector embedding is simply the time-series sequence of numbers.  * An index that stores feature embeddings calculated using Facebook's [Kats toolkit](https://engineering.fb.com/2021/06/21/open-source/kats/). Kats is a powerful time-series analysis tool that includes a time-series [embedding functionality](https://github.com/facebookresearch/Kats#tsfeatures).   ### Configure Pinecone Let's start by configuring the Pinecone service.   #### Pinecone Setup   ```python import pinecone ```   ```python # Load Pinecone API key api_key = os.getenv('PINECONE_API_KEY') or 'YOUR_API_KEY' # Set Pinecone environment. Default environment is YOUR_ENVIRONMENT env = os.getenv('PINECONE_ENVIRONMENT') or 'YOUR_ENVIRONMENT' pinecone.init(api_key=api_key, environment=env) ```  [Get your API key](https://www.pinecone.io/start/) and try this example yourself! You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**.  #### Create a New Service  Let's start with the simple approach and create an index.   ```python # Pick a name for the new index simple_index_name = 'stocks-trends' ```   ```python # Check whether the index with the same name already exists if simple_index_name in pinecone.list_indexes():     pinecone.delete_index(simple_index_name) ```   ```python # Create a new index pinecone.create_index(name=simple_index_name, dimension=128) ```   ```python # Establish a connection simple_index = pinecone.Index(index_name=simple_index_name) ```  ### Prepare data Starting with the simple embedding approach described earlier. Wherein we represent a time series as a vector of the time series sequence of numbers.   Throughout the demo, we use a [Stock Market Dataset](https://www.kaggle.com/jacksoncrow/stock-market-dataset). This dataset contains historical daily prices for all tickers trading on NASDAQ, up to April 2020. The dataset is defined in Kaggle and requires either a manual download or a Kaggle API to download it.  The data processing (i.e., [ETL part](https://en.wikipedia.org/wiki/Extract,_transform,_load)) here is heavy lifting and includes: * Downloading the data from [Kaggle](https://www.kaggle.com/jacksoncrow/stock-market-dataset). (Recall, you will need a [Kaggle API key](https://www.kaggle.com/docs/api).)  * Define the time series raw data.  * Extract the time series from the relevant files.  * Transform the raw data into vectors and upload the vectors into Pinecone's service.  #### Download Kaggle Stock Market Dataset  In order to use the Kaggle’s public API, you must first authenticate using an API token. Please replace the username and key values in the following cell with the values from your [Kaggle API token](https://www.kaggle.com/docs/api#getting-started-installation-&-authentication).   ```python %%writefile kaggle.json {\"username\":\"KAGGLE_USERNAME\",\"key\":\"KAGGLE_KEY\"} ```      Writing kaggle.json    ```python #Check Kaggle username and key ! cat ./kaggle.json ```   ```python !mkdir ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json !kaggle datasets download -d jacksoncrow/stock-market-dataset !unzip -q stock-market-dataset.zip -d data ```      Downloading stock-market-dataset.zip to /content     100% 521M/522M [00:04<00:00, 126MB/s]     100% 522M/522M [00:04<00:00, 125MB/s]   #### Set up Time Series Hyperparameters We set two hyperparameters defining how we extract the time series: * sliding window, which controls the length of the time series in consecutive day periods. * step size, which defines a gap in the start dates of two consecutive vectors.  Feel free to set the window or step size to a different value.    ```python # Define sliding window and step size SLIDING_WINDOW_SIZE = 64 STEP = 10 ```  #### Define Extract-Transform-Load Functions Before we do all of the other steps, we will define utility functions to help us extract, transform, and upload the time series data.  Note that we will: * Work only with the stock prices and disregard the ETF data folder. * Load data for a set of symbols from the stock folder for the simplicity of the example. * When creating vectors, we will include daily *Open* and *Close* prices. This way, our vectors will be double the size of a sliding window. Feel free to try different prices when creating vectors.   ```python def windows(data, window_size, step):     r = np.arange(len(data))     s = r[::step]     z = list(zip(s, s + window_size))     f = '{0[0]}:{0[1]}'.format     g = lambda t: data.iloc[t[0]:t[1]]     return pd.concat(map(g, z), keys=map(f, z))  def get_feature_embedding_for_window(df, stock):     ts_name = f\"{stock.strip('.csv')}_{str(df.Date.min())}_{str(df.Date.max())}\"     scaler=MinMaxScaler()     df[['Open', 'Close']] = scaler.fit_transform(df[['Open', 'Close']])     prices = df[['Open', 'Close']].values.tolist()     flat_values = [item for sublist in prices for item in sublist]     df = df.rename(columns={\"Date\":\"time\"})      ts_df = pd.DataFrame({'time':df.time.repeat(2),                            'price':flat_values})     ts_df.drop_duplicates(keep='first', inplace=True)        # Use Kats to extract features for the time window     try:         if not (len(np.unique(ts_df.price.tolist())) == 1 \\             or len(np.unique(ts_df.price.tolist())) == 0):             timeseries = TimeSeriesData(ts_df)             features = TsFeatures().transform(timeseries)             feature_list = [float(v) if not pd.isnull(v) else float(0) for _, v in features.items()]             if Decimal('Infinity') in feature_list or Decimal('-Infinity') in feature_list:                 return None             return (ts_name, feature_list)     except np.linalg.LinAlgError as e:         print(f\"Can't process {ts_name}:{e}\")     return None  def get_simple_pair_for_window(df, stock):     ts_name = f\"{stock.strip('.csv')}_{str(df.Date.min())}_{str(df.Date.max())}\"     prices = df[['Open', 'Close']].values.tolist()     flat_values = [item for sublist in prices for item in sublist]     return (ts_name, flat_values)  def chunks(iterable, batch_size=100):     it = iter(iterable)     chunk = tuple(itertools.islice(it, batch_size))     while chunk:         yield chunk         chunk = tuple(itertools.islice(it, batch_size)) ```   ```python def upload_data_to_index(index, create_pair_func, verbose=False):     # Define path to the folder     stocks = sorted(os.listdir('./data/stocks'))          # Iterate over files, create vectors and upload data     for stock in stocks[::50]:         print(stock.strip('.csv'))         data = pd.read_csv(os.path.join('./data/stocks', stock))         data = data.sort_index(axis=0, ascending=True)         data[\"Date\"] = pd.to_datetime(data[\"Date\"]).dt.date          # Interpolate data for missing dates         data.set_index('Date', inplace=True)         data = data.reindex(pd.date_range(start=data.index.min(),                                           end=data.index.max(),                                           freq='1D'))         data = data.interpolate(method='linear')         data = data.reset_index().rename(columns={'index': 'Date'})         data[\"Date\"] = pd.to_datetime(data[\"Date\"]).dt.date                  # Create sliding windows dataset         wdf = windows(data, SLIDING_WINDOW_SIZE, STEP)                  # Prepare sequences for upload          items_to_upload = []         for window, new_df in wdf.groupby(level=0):             if new_df.shape[0] == SLIDING_WINDOW_SIZE:                 pair = create_pair_func(new_df, stock)                 if pair:                     items_to_upload.append(pair)          # Upload data for the symbol         for batch in chunks(items_to_upload, 500):             index.upsert(vectors=batch) ```  ### Index Let's upsert data into the simple index.   ```python upload_data_to_index(simple_index, get_simple_pair_for_window) clear_output() ```   ```python # Check the index size simple_index.describe_index_stats() ```         {'dimension': 128, 'namespaces': {'': {'vector_count': 61212}}}    ### Search Now that we have uploaded the items into the vector index, it is time to check the similarities between vectors.  In this section, we will: * Define stocks and their windows for the query. * Fetch these query items from the index to retrieve their vectors. * Query the index using these vectors. Pinecone will return top K most similar vectors for each query item. * Show the results.  Below we define utility functions for data preparation and display.   ```python def prepare_items_for_graph(data):         scaler = MinMaxScaler()     result_list = []          for _, row in data.iterrows():         id = row['id']         vec = row['values']         scaled_vec = scaler.fit_transform(np.array(vec).reshape(-1,1))         result_list.append((id, (vec, scaled_vec)))     return result_list ```   ```python def show_query_results(query_item, data):     data_prepared = prepare_items_for_graph(data)     graph_index = pd.Float64Index(np.arange(start=0, stop=SLIDING_WINDOW_SIZE, step=0.5))      print('\\n The most similar items from the vector index:')     data.reset_index(inplace=True, drop=True)     display(data)            fig = plt.figure(figsize=(20,7))     for item in data_prepared:         _id, vectors = item         ax1 = plt.subplot(1, 2, 1)         graph = plt.plot(graph_index, vectors[0], label = _id, marker='o' if _id == query_item else None)         ax2 = plt.subplot(1, 2, 2)         graph = plt.plot(graph_index, vectors[1], label = _id, marker='o' if _id == query_item else None)         ax1.set_xlabel(\"Days in time window\")     ax2.set_xlabel(\"Days in time window\")     ax1.set_ylabel(\"Stock values\")     ax2.set_ylabel(\"Normalized Stock Values\")     ax1.title.set_text(f'Similar stock patterns and their market values')     ax2.title.set_text(f'Similar stock patterns and their normalized market values')     plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))     plt.show() ```  Note that we will filter the retrieved results to make sure we present a diverse set of stocks. Otherwise, we might get consecutive time windows for the same stock.   ```python def filter_results(query_item, data, historical_only=False):     already_present = []          # Remove symbol that is already included     for i, row in data.iterrows():         check_name = row.id.split('_')[0]         if check_name not in already_present:             already_present.append(check_name)         else:             data.drop(i,axis=0,inplace=True)                  # Include only data prior to query interval     if historical_only:         _, start_dt, end_dt = query_item.split('_')         start_dt = pd.to_datetime(start_dt).date()         data['final_date'] = data.id.apply(lambda x: x.split('_')[2])         data['final_date'] =  data.final_date.apply(lambda x: pd.to_datetime(x).date())         data = data[data.final_date <= start_dt]         del data['final_date']             return data ```  #### Numerical Examples Let's examine a few interesting price patterns and their corresponding best matches.  Here we define the query items, fetch them and prepare vectors for the query.   ```python # Define query examples items_to_query = ['BORR_2019-10-18_2019-12-20', 'HCCO_2020-01-28_2020-03-31', 'PUMP_2019-11-22_2020-01-24']  # Fetch vectors from the index fetch_res = simple_index.fetch(ids=items_to_query)  # Create a list of ids and vectors for the fetched items query_ids = [res.id for res in fetch_res.vectors.values()] query_vectors = [res.values for res in fetch_res.vectors.values()] ```  The next step is to perform the query for the query vectors.    ```python # Query the pinecone index query_results = [] for xq in query_vectors:     res = simple_index.query(xq, top_k=100, include_values=True)     query_results.append(res) ```  Finally, iterate over the results, get all vectors needed for the graphs and display them.  Note that graphs on the left show the absolute price values for the query items selected, while graphs on the right show each vector on a 0-1 scale. The price normalization ignores the magnitude of stock prices and thus focuses on the time series pattern only.   It is more likely that similar trends appear in the same time interval. There is a flag **historical_only** that lets you choose whether you want to look only at the time intervals prior to query time interval or any time interval that exists.   ```python # Iterate and show query results for each query item for query_item, q_res in zip(query_ids, query_results):     print(f'\\nQueried: {query_item}')     res_df = pd.DataFrame(         {             'id': [res.id for res in q_res.matches],              'score': [res.score for res in q_res.matches],             'values': [res.values for res in q_res.matches]          }     )     res_df = filter_results(query_item, res_df, historical_only=False)     show_query_results(query_item, res_df.head(6)) ```           Queried: BORR_2019-10-18_2019-12-20           The most similar items from the vector index:       <div id=\"df-fe076651-f3f5-4592-a734-74801a96d394\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     } </style> <table class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>id</th>       <th>score</th>       <th>values</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>BORR_2019-10-18_2019-12-20</td>       <td>1.000000</td>       <td>[6.22, 6.4, 6.45, 6.4666667, 6.68, 6.5333333, ...</td>     </tr>     <tr>       <th>1</th>       <td>OSK_1990-11-25_1991-01-27</td>       <td>0.999467</td>       <td>[1.23611116, 1.25, 1.25, 1.25, 1.25, 1.2916666...</td>     </tr>     <tr>       <th>2</th>       <td>BH_2009-10-21_2009-12-23</td>       <td>0.999386</td>       <td>[197.943146, 192.080719, 207.771332, 206.90921...</td>     </tr>     <tr>       <th>3</th>       <td>MLAB_2007-03-30_2007-06-01</td>       <td>0.999307</td>       <td>[19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19....</td>     </tr>     <tr>       <th>4</th>       <td>SORL_2008-03-19_2008-05-21</td>       <td>0.999293</td>       <td>[5.1, 4.93, 4.9, 4.93, 4.9425, 4.9525, 4.985, ...</td>     </tr>     <tr>       <th>5</th>       <td>PKBK_2012-12-02_2013-02-03</td>       <td>0.999265</td>       <td>[3.78662658, 3.77160025, 3.77160025, 3.7716002...</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe076651-f3f5-4592-a734-74801a96d394')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 0 24 24\"        width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg>       </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>    <script>     const buttonEl =       document.querySelector('#df-fe076651-f3f5-4592-a734-74801a96d394 button.colab-df-convert');     buttonEl.style.display =       google.colab.kernel.accessAllowed ? 'block' : 'none';      async function convertToInteractive(key) {       const element = document.querySelector('#df-fe076651-f3f5-4592-a734-74801a96d394');       const dataTable =         await google.colab.kernel.invokeFunction('convertToInteractive',                                                   [key], {});       if (!dataTable) return;        const docLinkHtml = 'Like what you see? Visit the ' +         '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'         + ' to learn more about interactive tables.';       element.innerHTML = '';       dataTable['output_type'] = 'display_data';       await google.colab.output.renderOutput(dataTable, element);       const docLink = document.createElement('div');       docLink.innerHTML = docLinkHtml;       element.appendChild(docLink);     }   </script>   </div> </div>          ![Example of vector similarity search for time series data](https://raw.githubusercontent.com/pinecone-io/img/main/time-series-example-40_2.png)            <div id=\"df-8a2668ba-c4ce-4306-af59-ef0251c55690\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     } </style> <table class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>id</th>       <th>score</th>       <th>values</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>HCCO_2020-01-28_2020-03-31</td>       <td>1.000000</td>       <td>[9.95, 10.0, 10.05, 10.05, 10.05, 10.05, 10.05...</td>     </tr>     <tr>       <th>1</th>       <td>TEI_2016-09-22_2016-11-24</td>       <td>0.999958</td>       <td>[10.98, 11.0, 10.99, 10.96, 10.9933329, 10.956...</td>     </tr>     <tr>       <th>2</th>       <td>BSD_2002-01-31_2002-04-04</td>       <td>0.999945</td>       <td>[14.04, 14.0, 14.04, 14.0, 14.04, 13.9933329, ...</td>     </tr>     <tr>       <th>3</th>       <td>HPI_2016-07-26_2016-09-27</td>       <td>0.999943</td>       <td>[23.21, 23.23, 23.25, 23.26, 23.2, 23.27, 23.2...</td>     </tr>     <tr>       <th>4</th>       <td>MUE_2007-09-23_2007-11-25</td>       <td>0.999943</td>       <td>[12.2566671, 12.26, 12.25, 12.26, 12.26, 12.35...</td>     </tr>     <tr>       <th>5</th>       <td>AMCI_2016-10-04_2016-12-06</td>       <td>0.999942</td>       <td>[0.488, 0.488, 0.488, 0.488, 0.488, 0.488, 0.4...</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a2668ba-c4ce-4306-af59-ef0251c55690')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 0 24 24\"        width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg>       </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>    <script>     const buttonEl =       document.querySelector('#df-8a2668ba-c4ce-4306-af59-ef0251c55690 button.colab-df-convert');     buttonEl.style.display =       google.colab.kernel.accessAllowed ? 'block' : 'none';      async function convertToInteractive(key) {       const element = document.querySelector('#df-8a2668ba-c4ce-4306-af59-ef0251c55690');       const dataTable =         await google.colab.kernel.invokeFunction('convertToInteractive',                                                   [key], {});       if (!dataTable) return;        const docLinkHtml = 'Like what you see? Visit the ' +         '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'         + ' to learn more about interactive tables.';       element.innerHTML = '';       dataTable['output_type'] = 'display_data';       await google.colab.output.renderOutput(dataTable, element);       const docLink = document.createElement('div');       docLink.innerHTML = docLinkHtml;       element.appendChild(docLink);     }   </script>   </div> </div>             Queried: HCCO_2020-01-28_2020-03-31           The most similar items from the vector index:         ![Example of vector similarity search for time series data](https://raw.githubusercontent.com/pinecone-io/img/main/time-series-example-40_5.png)                 Queried: PUMP_2019-11-22_2020-01-24           The most similar items from the vector index:       <div id=\"df-d08ca8ae-0d74-4478-9b54-73f69dec0ff6\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     } </style> <table class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>id</th>       <th>score</th>       <th>values</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>PUMP_2019-11-22_2020-01-24</td>       <td>1.000000</td>       <td>[8.34, 8.27, 8.31333351, 8.42, 8.28666687, 8.5...</td>     </tr>     <tr>       <th>1</th>       <td>AEL_2003-12-14_2004-02-15</td>       <td>0.999547</td>       <td>[9.06333351, 9.04, 9.06, 9.01, 9.06, 9.02, 9.0...</td>     </tr>     <tr>       <th>2</th>       <td>THS_2006-10-21_2006-12-23</td>       <td>0.999510</td>       <td>[24.503334, 24.376667, 24.3266659, 24.4233322,...</td>     </tr>     <tr>       <th>3</th>       <td>RUBY_2008-07-05_2008-09-06</td>       <td>0.999508</td>       <td>[5.02380943, 5.02380943, 4.97619057, 4.9761905...</td>     </tr>     <tr>       <th>4</th>       <td>SRI_2010-11-11_2011-01-13</td>       <td>0.999495</td>       <td>[12.82, 12.78, 12.61, 12.71, 12.6799994, 12.67...</td>     </tr>     <tr>       <th>5</th>       <td>BH_2017-11-18_2018-01-20</td>       <td>0.999465</td>       <td>[343.266663, 341.956665, 342.693329, 340.48333...</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d08ca8ae-0d74-4478-9b54-73f69dec0ff6')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 0 24 24\"        width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg>       </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>    <script>     const buttonEl =       document.querySelector('#df-d08ca8ae-0d74-4478-9b54-73f69dec0ff6 button.colab-df-convert');     buttonEl.style.display =       google.colab.kernel.accessAllowed ? 'block' : 'none';      async function convertToInteractive(key) {       const element = document.querySelector('#df-d08ca8ae-0d74-4478-9b54-73f69dec0ff6');       const dataTable =         await google.colab.kernel.invokeFunction('convertToInteractive',                                                   [key], {});       if (!dataTable) return;        const docLinkHtml = 'Like what you see? Visit the ' +         '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'         + ' to learn more about interactive tables.';       element.innerHTML = '';       dataTable['output_type'] = 'display_data';       await google.colab.output.renderOutput(dataTable, element);       const docLink = document.createElement('div');       docLink.innerHTML = docLinkHtml;       element.appendChild(docLink);     }   </script>   </div> </div>          ![Example of vector similarity search for time series data](https://raw.githubusercontent.com/pinecone-io/img/main/time-series-example-40_8.png)        Notice that we found patterns that look alike and are related to different stocks and different time windows.  ## Facebook's Kats Time-Series Embeddings It is time to test another approach. This time we create feature embeddings and upload them using the same stocks and windows as in our previous index. Here we utilize Facebook's [Kats toolkit](https://engineering.fb.com/2021/06/21/open-source/kats/). Kats is a powerful time-series analysis tool that includes a time-series [embedding functionality](https://github.com/facebookresearch/Kats#tsfeatures).   Let's create a new index first.  ### Create a New Pinecone Service   ```python # Pick a name for the new index kats_index_name = 'stocks-trends-with-features' ```   ```python # Check whether the index with the same name already exists if kats_index_name in pinecone.list_indexes():     pinecone.delete_index(kats_index_name) ```   ```python # Create a new index pinecone.create_index(name=kats_index_name, dimension=40) ```   ```python # Establish a connection kats_index = pinecone.Index(index_name=kats_index_name) ```  ### Index We will use Kats and its time-series feature extraction module to create feature embeddings for each stock and corresponding time window. These feature embeddings include the following types: seasonality, autocorrelation, modeling parameter, changepoints, moving statistics, and raw statistics of time series array as the ad-hoc features. We used the default set of features for our example and created 40-dimensional feature embeddings.  *Note: Ignore the Kats warning message that appears in the output.*   ```python upload_data_to_index(kats_index, get_feature_embedding_for_window) clear_output() ```   ```python kats_index.describe_index_stats() ```         {'dimension': 40, 'namespaces': {'': {'vector_count': 61105}}}    *Note* that the Kats-based index has fewer vectors compared to the simple embeddings index. It happens because Kats fails to calculate some features for some patterns. E.g., it happens if the time series has a constant value each day in a time window.   ### Search  We will use the same query item that we used to query the simple index.   ```python # Fetch vectors from the index fetch_res = kats_index.fetch(ids=items_to_query)  # Create a list of ids and vectors for the fetched items query_ids = [res.id for res in fetch_res.vectors.values()] query_vectors = [res.values for res in fetch_res.vectors.values()] ```   ```python # Query the pinecone index query_results = [] for xq in query_vectors:     res = kats_index.query(xq, top_k=100, include_values=True)     query_results.append(res) ```   ```python # Iterate and show query results for each query item for query_item, q_res in zip(query_ids, query_results):     print(f'\\nQueried: {query_item}')     res_df = pd.DataFrame(         {             'id': [res.id for res in q_res.matches],              'score': [res.score for res in q_res.matches]         }     )      # Use simple index to retrieve historical prices for query result items     res_df['values'] = res_df.id.apply(lambda x: [res.values for res in simple_index.fetch(ids=[x]).vectors.values()][0])     res_df = filter_results(query_item, res_df, historical_only=False)     show_query_results(query_item, res_df.head(6)) ```      WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /vectors/fetch?ids=BORR_2019-10-18_2019-12-20            Queried: BORR_2019-10-18_2019-12-20           The most similar items from the vector index:       <div id=\"df-322ecd0e-9264-4133-9240-e6eb1a144323\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     } </style> <table class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>id</th>       <th>score</th>       <th>values</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>BORR_2019-10-18_2019-12-20</td>       <td>1.000000</td>       <td>[6.22, 6.4, 6.45, 6.4666667, 6.68, 6.5333333, ...</td>     </tr>     <tr>       <th>1</th>       <td>NEON_2004-08-08_2004-10-10</td>       <td>0.999648</td>       <td>[4541.6665, 4500.0, 4500.0, 4512.5, 4487.5, 44...</td>     </tr>     <tr>       <th>2</th>       <td>AGTC_2019-10-27_2019-12-29</td>       <td>0.999632</td>       <td>[2.87000012, 2.92, 2.88, 2.95, 3.0, 3.03, 3.01...</td>     </tr>     <tr>       <th>3</th>       <td>DUO_2014-07-14_2014-09-15</td>       <td>0.999611</td>       <td>[1.57, 1.57, 1.57, 1.63, 1.64, 1.64, 1.64, 1.6...</td>     </tr>     <tr>       <th>4</th>       <td>MLAB_2016-04-11_2016-06-13</td>       <td>0.999604</td>       <td>[96.13, 96.93, 97.68, 97.04, 98.12, 99.29, 98....</td>     </tr>     <tr>       <th>5</th>       <td>GENC_2004-01-11_2004-03-14</td>       <td>0.999585</td>       <td>[2.0333333, 2.07777762, 2.0333333, 2.1, 2.0666...</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-322ecd0e-9264-4133-9240-e6eb1a144323')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 0 24 24\"        width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg>       </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>    <script>     const buttonEl =       document.querySelector('#df-322ecd0e-9264-4133-9240-e6eb1a144323 button.colab-df-convert');     buttonEl.style.display =       google.colab.kernel.accessAllowed ? 'block' : 'none';      async function convertToInteractive(key) {       const element = document.querySelector('#df-322ecd0e-9264-4133-9240-e6eb1a144323');       const dataTable =         await google.colab.kernel.invokeFunction('convertToInteractive',                                                   [key], {});       if (!dataTable) return;        const docLinkHtml = 'Like what you see? Visit the ' +         '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'         + ' to learn more about interactive tables.';       element.innerHTML = '';       dataTable['output_type'] = 'display_data';       await google.colab.output.renderOutput(dataTable, element);       const docLink = document.createElement('div');       docLink.innerHTML = docLinkHtml;       element.appendChild(docLink);     }   </script>   </div> </div>          ![Example of vector similarity search for time series data](https://raw.githubusercontent.com/pinecone-io/img/main/time-series-example-57_2.png)                 Queried: HCCO_2020-01-28_2020-03-31           The most similar items from the vector index:       <div id=\"df-d686a314-5075-4fd3-864b-a1b4f822f82e\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     } </style> <table class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>id</th>       <th>score</th>       <th>values</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>HCCO_2020-01-28_2020-03-31</td>       <td>1.000000</td>       <td>[9.95, 10.0, 10.05, 10.05, 10.05, 10.05, 10.05...</td>     </tr>     <tr>       <th>1</th>       <td>TEI_2002-04-29_2002-07-01</td>       <td>0.999724</td>       <td>[12.0, 12.08, 12.1, 12.1, 12.13, 12.2, 12.2, 1...</td>     </tr>     <tr>       <th>2</th>       <td>BY_2018-06-25_2018-08-27</td>       <td>0.999667</td>       <td>[23.76, 23.13, 23.13, 23.11, 23.2, 22.71, 22.7...</td>     </tr>     <tr>       <th>3</th>       <td>OSK_2016-12-18_2017-02-19</td>       <td>0.999616</td>       <td>[66.9233322, 66.8233337, 66.27, 66.97, 66.98, ...</td>     </tr>     <tr>       <th>4</th>       <td>PKBK_2019-10-27_2019-12-29</td>       <td>0.999610</td>       <td>[22.0121212, 21.9757576, 22.09091, 22.0181828,...</td>     </tr>     <tr>       <th>5</th>       <td>RGR_2012-02-08_2012-04-11</td>       <td>0.999577</td>       <td>[43.05, 42.26, 42.22, 42.41, 42.03, 41.8, 42.0...</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d686a314-5075-4fd3-864b-a1b4f822f82e')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 0 24 24\"        width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg>       </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>    <script>     const buttonEl =       document.querySelector('#df-d686a314-5075-4fd3-864b-a1b4f822f82e button.colab-df-convert');     buttonEl.style.display =       google.colab.kernel.accessAllowed ? 'block' : 'none';      async function convertToInteractive(key) {       const element = document.querySelector('#df-d686a314-5075-4fd3-864b-a1b4f822f82e');       const dataTable =         await google.colab.kernel.invokeFunction('convertToInteractive',                                                   [key], {});       if (!dataTable) return;        const docLinkHtml = 'Like what you see? Visit the ' +         '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'         + ' to learn more about interactive tables.';       element.innerHTML = '';       dataTable['output_type'] = 'display_data';       await google.colab.output.renderOutput(dataTable, element);       const docLink = document.createElement('div');       docLink.innerHTML = docLinkHtml;       element.appendChild(docLink);     }   </script>   </div> </div>          ![Example of vector similarity search for time series data](thttps://raw.githubusercontent.com/pinecone-io/img/main/ime-series-example-57_5.png)                 Queried: PUMP_2019-11-22_2020-01-24           The most similar items from the vector index:       <div id=\"df-41786a1a-e34c-42d9-addc-f42dad37f9ff\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }     .dataframe tbody tr th {         vertical-align: top;     }     .dataframe thead th {         text-align: right;     } </style> <table class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>id</th>       <th>score</th>       <th>values</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>PUMP_2019-11-22_2020-01-24</td>       <td>1.000000</td>       <td>[8.34, 8.27, 8.31333351, 8.42, 8.28666687, 8.5...</td>     </tr>     <tr>       <th>1</th>       <td>RGR_2014-07-07_2014-09-08</td>       <td>0.999919</td>       <td>[59.78, 58.95, 58.7, 58.64, 58.65, 58.46, 57.7...</td>     </tr>     <tr>       <th>2</th>       <td>TVIX_2015-09-15_2015-11-17</td>       <td>0.999819</td>       <td>[322500.0, 266500.0, 246250.0, 234500.0, 23175...</td>     </tr>     <tr>       <th>3</th>       <td>STNE_2018-12-14_2019-02-15</td>       <td>0.999745</td>       <td>[18.4, 17.71, 18.14, 17.6833324, 17.8800011, 1...</td>     </tr>     <tr>       <th>4</th>       <td>SCVL_2004-03-18_2004-05-20</td>       <td>0.999720</td>       <td>[10.373333, 10.2333336, 10.2266665, 10.2066669...</td>     </tr>     <tr>       <th>5</th>       <td>AWI_2015-12-20_2016-02-21</td>       <td>0.999673</td>       <td>[45.876667, 45.3466644, 45.77, 45.17, 45.19, 4...</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41786a1a-e34c-42d9-addc-f42dad37f9ff')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 0 24 24\"        width=\"24px\"><path d=\"M0 0h24v24H0V0z\" fill=\"none\"/><path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/></svg>       </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>    <script>     const buttonEl =       document.querySelector('#df-41786a1a-e34c-42d9-addc-f42dad37f9ff button.colab-df-convert');     buttonEl.style.display =       google.colab.kernel.accessAllowed ? 'block' : 'none';      async function convertToInteractive(key) {       const element = document.querySelector('#df-41786a1a-e34c-42d9-addc-f42dad37f9ff');       const dataTable =         await google.colab.kernel.invokeFunction('convertToInteractive',                                                   [key], {});       if (!dataTable) return;        const docLinkHtml = 'Like what you see? Visit the ' +         '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'         + ' to learn more about interactive tables.';       element.innerHTML = '';       dataTable['output_type'] = 'display_data';       await google.colab.output.renderOutput(dataTable, element);       const docLink = document.createElement('div');       docLink.innerHTML = docLinkHtml;       element.appendChild(docLink);     }   </script>   </div> </div>          ![Example of vector similarity search for time series data](thttps://raw.githubusercontent.com/pinecone-io/img/main/ime-series-example-57_8.png)        ## Conclusion Pattern matching of time series data is an important task affecting time series clustering, labeling, classification, and recommendation.   We used the similarity search to find the most similar patterns in stocks data. We tried two different approaches to create vector representations of time series. First, we used the raw data of historical prices, and then we represented time-series as a set of statistical features. In both cases, we retrieved the top 100 best matches from the Pinecone similarity search service. Then we further filtered and showed only the top 5 most similar stock trends. (We did that to make sure we retrieve a diverse set of stocks. Otherwise, we might get consecutive time windows for the same stock.)  The simple approach turned out to give good results. When using Kats' time series features, we got somehow mixed results. We noticed that the most similar feature embeddings sometimes retrieve reverse patterns.   Yet, we note that the literature has plentiful advanced time series representation techniques. Starting from sequential deep neural network approaches such as LSTMs and RNNs, combining frequency-domain representations with convolutional neural networks, and even applying deep neural graphs embeddings. We encourage you to explore this fascinating domain. Feel free to try it along with the Pinecone service, and [share](https://www.pinecone.io/contact/) your findings with us!  ## Delete indexes  Delete the indexes once you are sure that you do not want to use it anymore. Once the index is deleted, you cannot use it again.     ```python pinecone.delete_index(simple_index_name) pinecone.delete_index(kats_index_name) ``` ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e26f"
  },
  "title": "NER-Powered Semantic Search",
  "category": "630fc5235d91a70054705fb7",
  "content": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/search/semantic-search/ner-search/ner-powered-search.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/search/semantic-search/ner-search/ner-powered-search.ipynb) [![Open github](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com/pinecone-io/examples/blob/master/search/semantic-search/ner-search/ner-powered-search.ipynb)  This notebook shows how to use Named Entity Recognition (NER) for hybrid metadata + vector search with Pinecone. We will:  1. Extract named entities from text. 2. Store them in a Pinecone index as metadata (alongside respective text vectors). 3. We extract named entities from incoming queries and use them to filter and search only through records containing these named entities.  This is particularly helpful if you want to restrict the search score to records that contain information about the named entities that are also found within the query.  Let's get started.  # Install Dependencies   ```python !pip install sentence_transformers pinecone-client datasets ```  # Load and Prepare Dataset  We use a dataset containing ~190K articles scraped from Medium. We select 50K articles from the dataset as indexing all the articles may take some time. This dataset can be loaded from the HuggingFace dataset hub as follows:   ```python from datasets import load_dataset  # load the dataset and convert to pandas dataframe df = load_dataset(     \"fabiochiu/medium-articles\",     data_files=\"medium_articles.csv\",     split=\"train\" ).to_pandas() ```        ```python # drop empty rows and select 50k articles df = df.dropna().sample(50000, random_state=32) df.head() ```      <div id=\"df-4dd0304d-b1f1-4ac5-8c2c-b84168801b0a\">   <div class=\"colab-df-container\">     <div><div class=\"table-wrapper\" markdown=\"block\">  <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     } </style> <table border=\"1\" class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>title</th>       <th>text</th>       <th>url</th>       <th>authors</th>       <th>timestamp</th>       <th>tags</th>     </tr>   </thead>   <tbody>     <tr>       <th>4172</th>       <td>How the Data Stole Christmas</td>       <td>by Anonymous\\n\\nThe door sprung open and our t...</td>       <td>https://medium.com/data-ops/how-the-data-stole...</td>       <td>[]</td>       <td>2019-12-24 13:22:33.143000+00:00</td>       <td>[Data Science, Big Data, Dataops, Analytics, D...</td>     </tr>     <tr>       <th>174868</th>       <td>Automating Light Switch using the ESP32 Board ...</td>       <td>A story about how I escaped the boring task th...</td>       <td>https://python.plainenglish.io/automating-ligh...</td>       <td>['Tomas Rasymas']</td>       <td>2021-09-14 07:20:52.342000+00:00</td>       <td>[Programming, Python, Software Development, Ha...</td>     </tr>     <tr>       <th>100171</th>       <td>Keep Going Quotes Sayings for When Hope is Lost</td>       <td>It’s a very thrilling thing to achieve a goal....</td>       <td>https://medium.com/@yourselfquotes/keep-going-...</td>       <td>['Yourself Quotes']</td>       <td>2021-01-05 12:13:04.018000+00:00</td>       <td>[Quotes]</td>     </tr>     <tr>       <th>141757</th>       <td>When Will the Smoke Clear From Bay Area Skies?</td>       <td>Bay Area cities are contending with some of th...</td>       <td>https://thebolditalic.com/when-will-the-smoke-...</td>       <td>['Matt Charnock']</td>       <td>2020-09-15 22:38:33.924000+00:00</td>       <td>[Bay Area, San Francisco, California, Wildfire...</td>     </tr>     <tr>       <th>183489</th>       <td>The ABC’s of Sustainability… easy as 1, 2, 3</td>       <td>By Julia DiPrete\\n\\n(according to the Jackson ...</td>       <td>https://medium.com/sipwines/the-abcs-of-sustai...</td>       <td>['Sip Wines']</td>       <td>2021-03-02 23:39:49.948000+00:00</td>       <td>[Wine Tasting, Sustainability, Wine]</td>     </tr>   </tbody> </table> </div> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4dd0304d-b1f1-4ac5-8c2c-b84168801b0a')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\">     <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>     <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>   </svg>       </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>    <script>     const buttonEl =       document.querySelector('#df-4dd0304d-b1f1-4ac5-8c2c-b84168801b0a button.colab-df-convert');     buttonEl.style.display =       google.colab.kernel.accessAllowed ? 'block' : 'none';      async function convertToInteractive(key) {       const element = document.querySelector('#df-4dd0304d-b1f1-4ac5-8c2c-b84168801b0a');       const dataTable =         await google.colab.kernel.invokeFunction('convertToInteractive',                                                   [key], {});       if (!dataTable) return;        const docLinkHtml = 'Like what you see? Visit the ' +         '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'         + ' to learn more about interactive tables.';       element.innerHTML = '';       dataTable['output_type'] = 'display_data';       await google.colab.output.renderOutput(dataTable, element);       const docLink = document.createElement('div');       docLink.innerHTML = docLinkHtml;       element.appendChild(docLink);     }   </script> </div>   </div>     We will use the article title and its text for generating embeddings. For that, we join the article title and the first 1000 characters from the article text.   ```python # select first 1000 characters df[\"text\"] = df[\"text\"].str[:1000] # join article title and the text df[\"title_text\"] = df[\"title\"] + \". \" + df[\"text\"] ```  # Initialize NER Model  To extract named entities, we will use a NER model finetuned on a BERT-base model. The model can be loaded from the HuggingFace model hub as follows:   ```python import torch  # set device to GPU if available device = torch.cuda.current_device() if torch.cuda.is_available() else None ```   ```python from transformers import AutoTokenizer, AutoModelForTokenClassification from transformers import pipeline  model_id = \"dslim/bert-base-NER\"  # load the tokenizer from huggingface tokenizer = AutoTokenizer.from_pretrained(     model_id ) # load the NER model from huggingface model = AutoModelForTokenClassification.from_pretrained(     model_id ) # load the tokenizer and model into a NER pipeline nlp = pipeline(     \"ner\",     model=model,     tokenizer=tokenizer,     aggregation_strategy=\"max\",     device=device ) ```      ```python text = \"London is the capital of England and the United Kingdom\" # use the NER pipeline to extract named entities from the text nlp(text) ```         [{'entity_group': 'LOC',       'score': 0.9996493,       'word': 'London',       'start': 0,       'end': 6},      {'entity_group': 'LOC',       'score': 0.9997588,       'word': 'England',       'start': 25,       'end': 32},      {'entity_group': 'LOC',       'score': 0.9993923,       'word': 'United Kingdom',       'start': 41,       'end': 55}]    Our NER pipeline is working as expected and accurately extracting entities from the text.  # Initialize Retriever  A retriever model is used to embed passages (article title + first 1000 characters) and queries. It creates embeddings such that queries and passages with similar meanings are close in the vector space. We will use a sentence-transformer model as our retriever. The model can be loaded as follows:   ```python from sentence_transformers import SentenceTransformer  # load the model from huggingface retriever = SentenceTransformer(     'flax-sentence-embeddings/all_datasets_v3_mpnet-base',     device=device ) retriever ```  # Initialize Pinecone Index  Now we need to initialize our Pinecone index. The Pinecone index stores vector representations of our passages which we can retrieve using another vector (the query vector). We first need to initialize our connection to Pinecone. For this, we need a free [API key](https://app.pinecone.io/); you can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**. We initialize the connection like so:   ```python import pinecone  # connect to pinecone environment pinecone.init(     api_key=\"YOUR_API_KEY\",     environment=\"YOUR_ENVIRONMENT\" ) ```  Now we can create our vector index. We will name it `ner-search` (feel free to chose any name you prefer). We specify the metric type as `cosine` and dimension as `768` as these are the vector space and dimensionality of the vectors output by the retriever model.   ```python index_name = \"ner-search\"  # check if the ner-search index exists if index_name not in pinecone.list_indexes():     # create the index if it does not exist     pinecone.create_index(         index_name,         dimension=768,         metric=\"cosine\"     )  # connect to ner-search index we created index = pinecone.Index(index_name) ```  # Generate Embeddings and Upsert  We generate embeddings for the `title_text` column we created earlier. Alongside the embeddings, we also include the named entities in the index as metadata. Later we will apply a filter based on these named entities when executing queries.  Let's first write a helper function to extract named entities from a batch of text.   ```python def extract_named_entities(text_batch):     # extract named entities using the NER pipeline     extracted_batch = nlp(text_batch)     entities = []     # loop through the results and only select the entity names     for text in extracted_batch:         ne = [entity[\"word\"] for entity in text]         entities.append(ne)     return entities ```  Now we create the embeddings. We do this in batches of `64` to avoid overwhelming machine resources or API request limits.   ```python from tqdm.auto import tqdm  # we will use batches of 64 batch_size = 64  for i in tqdm(range(0, len(df), batch_size)):     # find end of batch     i_end = min(i+batch_size, len(df))     # extract batch     batch = df.iloc[i:i_end]     # generate embeddings for batch     emb = retriever.encode(batch[\"title_text\"].tolist()).tolist()     # extract named entities from the batch     entities = extract_named_entities(batch[\"title_text\"].tolist())     # remove duplicate entities from each record     batch[\"named_entities\"] = [list(set(entity)) for entity in entities]     batch = batch.drop('title_text', axis=1)     # get metadata     meta = batch.to_dict(orient=\"records\")     # create unique IDs     ids = [f\"{idx}\" for idx in range(i, i_end)]     # add all to upsert list     to_upsert = list(zip(ids, emb, meta))     # upsert/insert these records to pinecone     _ = index.upsert(vectors=to_upsert)   # check that we have all vectors in index index.describe_index_stats() ```         100%|██████████| 782/782 [58:24<00:00, 2.24it/s]      {'dimension': 768,      'index_fullness': 0.1,      'namespaces': {'': {'vector_count': 50000}},      'total_vector_count': 50000}    Now we have indexed the articles and relevant metadata. We can move on to querying.  # Querying  First, we will write a helper function to handle the queries.   ```python from pprint import pprint  def search_pinecone(query):     # extract named entities from the query     ne = extract_named_entities([query])[0]     # create embeddings for the query     xq = retriever.encode(query).tolist()     # query the pinecone index while applying named entity filter     xc = index.query(xq, top_k=10, include_metadata=True, filter={\"named_entities\": {\"$in\": ne}})     # extract article titles from the search result     r = [x[\"metadata\"][\"title\"] for x in xc[\"matches\"]]     return pprint({\"Extracted Named Entities\": ne, \"Result\": r}) ```  Now try a query.   ```python query = \"What are the best places to visit in Greece?\" search_pinecone(query) ```        {'Extracted Named Entities': ['Greece'],      'Result': ['Budget-Friendly Holidays: Visit The Best Summer Destinations In '                 'Greece | easyGuide',                 'Exploring Greece',                 'Santorini Island. The power of this volcanic island creates an '                 'energy that overwhelms the senses…',                 'All aboard to Greece: With lifting travel restrictions, what is '                 'the future of the Greek tourism industry?',                 'The Search for Best Villas in Greece for Rental Ends Here | '                 'Alasvillas | Greece',                 'Peripéteies in Greece — Week 31. Adventures in Greece as we '                 'pursue the…',                 '‘City of Waterfalls’ Home to Stunning Natural Scenery',                 'Skiathos — The small paradise in the Sporades, Greece.',                 'Greece has its own Dominic Cummings — and things are about to get '                 'scary',                 'One Must-Visit Attraction in Each of Europe’s Most Popular '                 'Cities']}        ```python query = \"What are the best places to visit in London?\" search_pinecone(query) ```           {'Extracted Named Entities': ['London'],      'Result': ['Historical places to visit in London',                 'Never Die Without Seeing London-1',                 'London LOOP: walk all the way around the capital',                 'To London, In London',                 'You’ll never look at London the same way again after playing '                 'Pokemon GO',                 'Recommendation system to start a restaurant business in London',                 'Primrose and Regent’s Park London Walk — Portraits in the City',                 'Parliaments, picnics and social cleansing: a walk in London',                 'Universities’ role in building back London',                 'The Building of London']}        ```python query = \"Why does SpaceX want to build a city on Mars?\" search_pinecone(query) ```      {'Extracted Named Entities': ['SpaceX', 'Mars'],      'Result': ['Elon Musk: First Mars City Will Take 1,000 Starships, 20 Years',                 'Elon Musk, SpaceX and NASA Are Taking The Second Step In The '                 'Direction Of Mars',                 'WHAT ETHICS FRAMEWORK IS NEEDED TO DEVELOP SPACE?',                 'What is the SpaceX- Starship Mission? What is the SpaceX Mars '                 'Architecture?',                 'There is a 100% chance of dying on Mars. Why Elon Musk’s plans '                 'are suicide for astronauts?',                 'The Mars Conundrum',                 'Tesla’s “Starman” or 2001’s “Star Child”—Which One Should Guide '                 'Our Species Into Space?',                 'Mars Habitat: NASA 3D Printed Habitat Challenge',                 'Reusable rockets and the robots at sea: The SpaceX story',                 'Mars Is Overrated and Going There Isn’t Progress']}        These all look like great results, making the most of Pinecone's advanced vector search capabilities while limiting search scope to relevant records only with a named entity filter. ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e271"
  },
  "title": "Extreme Classification",
  "category": "630fc5235d91a70054705fb7",
  "content": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/analytics-and-ml/extreme-classification/extreme-classification.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/analytics-and-ml/extreme-classification/extreme-classification.ipynb) [![Open Github](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/github-shield.svg)](https://github.com/pinecone-io/examples/tree/master/analytics-and-ml/extreme-classification/extreme-classification.ipynb)  This demo aims to label new texts automatically when the number of possible labels is enormous. This scenario is known as extreme classification, a supervised learning variant that deals with multi-class and multi-label problems involving many choices.   Examples for applying extreme classification are labeling a new article with Wikipedia's topical labels, matching web content with a set of relevant advertisements, classifying product descriptions with catalog labels, and classifying a resume into a collection of pertinent job titles.   Here's how we'll perform extreme classification:  1. We'll transform 250,000 labels into vector embeddings using a publicly available embedding model and upload them into a [managed vector index](https://www.pinecone.io/).  2. Then we'll take an article that requires labeling and transform it into a [vector embedding](https://www.pinecone.io/learn/vector-embeddings/) using the same model. 3. We'll use that article's vector embedding as the query to search the vector index. In effect, this will retrieve the most similar labels to the article's semantic content. 4. With the most relevant labels retrieved, we can automatically apply them to the article.  Let's get started!  ## Dependencies   ```python !pip install -qU pinecone-client ipywidgets setuptools>=36.2.1 wikitextparser unidecode !pip install -qU sentence-transformers --no-cache-dir ```   ```python import os import re import gzip import json import pandas as pd import numpy as np from wikitextparser import remove_markup, parse from sentence_transformers import SentenceTransformer from unidecode import unidecode ```  ## Setting up Pinecone's Similarity Search Service Here we set up our similarity search service. We assume you are familiar with Pinecone's [quick start tutorial](https://www.pinecone.io/docs/quickstart-python/).   ```python import pinecone ```   ```python # Load Pinecone API key api_key = os.getenv(\"PINECONE_API_KEY\") or \"YOUR_API_KEY\" pinecone.init(api_key=api_key, environment='YOUR_ENVIRONMENT')  # List all existing indices for you API key pinecone.list_indexes() ```      []       [Get a Pinecone API key](https://www.pinecone.io/start/) if you don’t have one. You can find your environment in the [Pinecone console](https://app.pinecone.io) under **API Keys**.   ```python # Pick a name for the new index index_name = 'extreme-ml' ```   ```python # Check whether the index with the same name already exists if index_name in pinecone.list_indexes():     pinecone.delete_index(index_name) ```   ```python # Create a new vector index pinecone.create_index(name=index_name, dimension=300) ```   ```python # Connect to the created index index = pinecone.Index(index_name)  # Print index statistics index.describe_index_stats() ```         {'dimension': 300, 'namespaces': {'': {'vector_count': 139500}}}    ## Data Preparation  In this demo, we classify Wikipedia articles using a standard dataset from an extreme classification benchmarking [resource](http://manikvarma.org/downloads/XC/XMLRepository.html). The data used in this example is [Wikipedia-500k](https://drive.google.com/drive/folders/12HiiGWmbLfTEEObs2Y2jiTETZfXDowrn) which contains around 500,000 labels. Here, we will download the raw data and prepare it for the classification task.    ```python # Download train dataset !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=10RBSf6nC9C38wUMwqWur2Yd8mCwtup5K' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=10RBSf6nC9C38wUMwqWur2Yd8mCwtup5K\" -O 'trn.raw.json.gz' && rm -rf /tmp/cookies.txt   # Download test dataset !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1pEyKXtkwHhinuRxmARhtwEQ39VIughDf' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1pEyKXtkwHhinuRxmARhtwEQ39VIughDf\" -O 'tst.raw.json.gz' && rm -rf /tmp/cookies.txt  # Download categories labels file !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ZYTZPlnkPBCMcNqRRO-gNx8EPgtV-GL3' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1ZYTZPlnkPBCMcNqRRO-gNx8EPgtV-GL3\" -O 'Yf.txt' && rm -rf /tmp/cookies.txt  # Create and move downloaded files to data folder !mkdir data !mv 'trn.raw.json.gz' 'tst.raw.json.gz' 'Yf.txt' data ```      --2022-02-09 17:13:45--  https://docs.google.com/uc?export=download&confirm=arfw&id=10RBSf6nC9C38wUMwqWur2Yd8mCwtup5K     Resolving docs.google.com (docs.google.com)... 142.251.107.101, 142.251.107.113, 142.251.107.100, ...     Connecting to docs.google.com (docs.google.com)|142.251.107.101|:443... connected.     HTTP request sent, awaiting response... 302 Moved Temporarily     Location: https://doc-14-5k-docs.googleusercontent.com/docs/securesc/mi75es4ss9f8cbmlkasp0714ekfl64em/oiufcg4j2mku0ucr6a89me2tql3td1v1/1644426825000/06283569454216238406/01276505903269316155Z/10RBSf6nC9C38wUMwqWur2Yd8mCwtup5K?e=download [following]     --2022-02-09 17:13:45--  https://doc-14-5k-docs.googleusercontent.com/docs/securesc/mi75es4ss9f8cbmlkasp0714ekfl64em/oiufcg4j2mku0ucr6a89me2tql3td1v1/1644426825000/06283569454216238406/01276505903269316155Z/10RBSf6nC9C38wUMwqWur2Yd8mCwtup5K?e=download     Resolving doc-14-5k-docs.googleusercontent.com (doc-14-5k-docs.googleusercontent.com)... 173.194.210.132, 2607:f8b0:400c:c0f::84     Connecting to doc-14-5k-docs.googleusercontent.com (doc-14-5k-docs.googleusercontent.com)|173.194.210.132|:443... connected.     HTTP request sent, awaiting response... 302 Found     Location: https://docs.google.com/nonceSigner?nonce=l2fmr07iln170&continue=https://doc-14-5k-docs.googleusercontent.com/docs/securesc/mi75es4ss9f8cbmlkasp0714ekfl64em/oiufcg4j2mku0ucr6a89me2tql3td1v1/1644426825000/06283569454216238406/01276505903269316155Z/10RBSf6nC9C38wUMwqWur2Yd8mCwtup5K?e%3Ddownload&hash=46dcmopdem1mp2anvp98snh4203mij96 [following]     --2022-02-09 17:13:45--  https://docs.google.com/nonceSigner?nonce=l2fmr07iln170&continue=https://doc-14-5k-docs.googleusercontent.com/docs/securesc/mi75es4ss9f8cbmlkasp0714ekfl64em/oiufcg4j2mku0ucr6a89me2tql3td1v1/1644426825000/06283569454216238406/01276505903269316155Z/10RBSf6nC9C38wUMwqWur2Yd8mCwtup5K?e%3Ddownload&hash=46dcmopdem1mp2anvp98snh4203mij96     Connecting to docs.google.com (docs.google.com)|142.251.107.101|:443... connected.     HTTP request sent, awaiting response... 302 Found     Location: https://doc-14-5k-docs.googleusercontent.com/docs/securesc/mi75es4ss9f8cbmlkasp0714ekfl64em/oiufcg4j2mku0ucr6a89me2tql3td1v1/1644426825000/06283569454216238406/01276505903269316155Z/10RBSf6nC9C38wUMwqWur2Yd8mCwtup5K?e=download&nonce=l2fmr07iln170&user=01276505903269316155Z&hash=jgf078p8n3lll6ulirsoqj13g645cf9q [following]     --2022-02-09 17:13:45--  https://doc-14-5k-docs.googleusercontent.com/docs/securesc/mi75es4ss9f8cbmlkasp0714ekfl64em/oiufcg4j2mku0ucr6a89me2tql3td1v1/1644426825000/06283569454216238406/01276505903269316155Z/10RBSf6nC9C38wUMwqWur2Yd8mCwtup5K?e=download&nonce=l2fmr07iln170&user=01276505903269316155Z&hash=jgf078p8n3lll6ulirsoqj13g645cf9q     Connecting to doc-14-5k-docs.googleusercontent.com (doc-14-5k-docs.googleusercontent.com)|173.194.210.132|:443... connected.     HTTP request sent, awaiting response... 200 OK     Length: 5292805889 (4.9G) [application/x-gzip]     Saving to: ‘trn.raw.json.gz’          trn.raw.json.gz     100%[===================>]   4.93G   120MB/s    in 57s               2022-02-09 17:14:42 (89.0 MB/s) - ‘trn.raw.json.gz’ saved [5292805889/5292805889]          --2022-02-09 17:14:43--  https://docs.google.com/uc?export=download&confirm=uE_3&id=1pEyKXtkwHhinuRxmARhtwEQ39VIughDf     Resolving docs.google.com (docs.google.com)... 142.251.107.101, 142.251.107.113, 142.251.107.100, ...     Connecting to docs.google.com (docs.google.com)|142.251.107.101|:443... connected.     HTTP request sent, awaiting response... 302 Moved Temporarily     Location: https://doc-0c-2o-docs.googleusercontent.com/docs/securesc/5m4vneh8ah734pahp3qbjncg6mmp89ta/d5qkmrodksuck3tqaeh82prkj3v26vfe/1644426825000/06283569454216238406/08808106369581203619Z/1pEyKXtkwHhinuRxmARhtwEQ39VIughDf?e=download [following]     --2022-02-09 17:14:43--  https://doc-0c-2o-docs.googleusercontent.com/docs/securesc/5m4vneh8ah734pahp3qbjncg6mmp89ta/d5qkmrodksuck3tqaeh82prkj3v26vfe/1644426825000/06283569454216238406/08808106369581203619Z/1pEyKXtkwHhinuRxmARhtwEQ39VIughDf?e=download     Resolving doc-0c-2o-docs.googleusercontent.com (doc-0c-2o-docs.googleusercontent.com)... 173.194.210.132, 2607:f8b0:400c:c0f::84     Connecting to doc-0c-2o-docs.googleusercontent.com (doc-0c-2o-docs.googleusercontent.com)|173.194.210.132|:443... connected.     HTTP request sent, awaiting response... 302 Found     Location: https://docs.google.com/nonceSigner?nonce=fv89ild8hgp3u&continue=https://doc-0c-2o-docs.googleusercontent.com/docs/securesc/5m4vneh8ah734pahp3qbjncg6mmp89ta/d5qkmrodksuck3tqaeh82prkj3v26vfe/1644426825000/06283569454216238406/08808106369581203619Z/1pEyKXtkwHhinuRxmARhtwEQ39VIughDf?e%3Ddownload&hash=algva8fi1m74v18nhdhve6o38458h8bo [following]     --2022-02-09 17:14:43--  https://docs.google.com/nonceSigner?nonce=fv89ild8hgp3u&continue=https://doc-0c-2o-docs.googleusercontent.com/docs/securesc/5m4vneh8ah734pahp3qbjncg6mmp89ta/d5qkmrodksuck3tqaeh82prkj3v26vfe/1644426825000/06283569454216238406/08808106369581203619Z/1pEyKXtkwHhinuRxmARhtwEQ39VIughDf?e%3Ddownload&hash=algva8fi1m74v18nhdhve6o38458h8bo     Connecting to docs.google.com (docs.google.com)|142.251.107.101|:443... connected.     HTTP request sent, awaiting response... 302 Found     Location: https://doc-0c-2o-docs.googleusercontent.com/docs/securesc/5m4vneh8ah734pahp3qbjncg6mmp89ta/d5qkmrodksuck3tqaeh82prkj3v26vfe/1644426825000/06283569454216238406/08808106369581203619Z/1pEyKXtkwHhinuRxmARhtwEQ39VIughDf?e=download&nonce=fv89ild8hgp3u&user=08808106369581203619Z&hash=d3m02ho8p665cjtl094bjkqk6g1qftj1 [following]     --2022-02-09 17:14:43--  https://doc-0c-2o-docs.googleusercontent.com/docs/securesc/5m4vneh8ah734pahp3qbjncg6mmp89ta/d5qkmrodksuck3tqaeh82prkj3v26vfe/1644426825000/06283569454216238406/08808106369581203619Z/1pEyKXtkwHhinuRxmARhtwEQ39VIughDf?e=download&nonce=fv89ild8hgp3u&user=08808106369581203619Z&hash=d3m02ho8p665cjtl094bjkqk6g1qftj1     Connecting to doc-0c-2o-docs.googleusercontent.com (doc-0c-2o-docs.googleusercontent.com)|173.194.210.132|:443... connected.     HTTP request sent, awaiting response... 200 OK     Length: 2297151115 (2.1G) [application/x-gzip]     Saving to: ‘tst.raw.json.gz’          tst.raw.json.gz     100%[===================>]   2.14G   130MB/s    in 15s               2022-02-09 17:14:59 (141 MB/s) - ‘tst.raw.json.gz’ saved [2297151115/2297151115]          --2022-02-09 17:15:01--  https://docs.google.com/uc?export=download&confirm=&id=1ZYTZPlnkPBCMcNqRRO-gNx8EPgtV-GL3     Resolving docs.google.com (docs.google.com)... 173.194.210.139, 173.194.210.138, 173.194.210.102, ...     Connecting to docs.google.com (docs.google.com)|173.194.210.139|:443... connected.     HTTP request sent, awaiting response... 302 Moved Temporarily     Location: https://doc-04-7s-docs.googleusercontent.com/docs/securesc/55p9v9r892nth323knj0kpu3c0fh68iu/jne4sjlttqu450ikt8fph58j505oapok/1644426900000/06283569454216238406/07409829577848409351Z/1ZYTZPlnkPBCMcNqRRO-gNx8EPgtV-GL3?e=download [following]     --2022-02-09 17:15:02--  https://doc-04-7s-docs.googleusercontent.com/docs/securesc/55p9v9r892nth323knj0kpu3c0fh68iu/jne4sjlttqu450ikt8fph58j505oapok/1644426900000/06283569454216238406/07409829577848409351Z/1ZYTZPlnkPBCMcNqRRO-gNx8EPgtV-GL3?e=download     Resolving doc-04-7s-docs.googleusercontent.com (doc-04-7s-docs.googleusercontent.com)... 173.194.210.132, 2607:f8b0:400c:c0f::84     Connecting to doc-04-7s-docs.googleusercontent.com (doc-04-7s-docs.googleusercontent.com)|173.194.210.132|:443... connected.     HTTP request sent, awaiting response... 302 Found     Location: https://docs.google.com/nonceSigner?nonce=erf8t6vb6o29s&continue=https://doc-04-7s-docs.googleusercontent.com/docs/securesc/55p9v9r892nth323knj0kpu3c0fh68iu/jne4sjlttqu450ikt8fph58j505oapok/1644426900000/06283569454216238406/07409829577848409351Z/1ZYTZPlnkPBCMcNqRRO-gNx8EPgtV-GL3?e%3Ddownload&hash=tjpqbk2dp469l934sb9627cs6d9dq9ht [following]     --2022-02-09 17:15:02--  https://docs.google.com/nonceSigner?nonce=erf8t6vb6o29s&continue=https://doc-04-7s-docs.googleusercontent.com/docs/securesc/55p9v9r892nth323knj0kpu3c0fh68iu/jne4sjlttqu450ikt8fph58j505oapok/1644426900000/06283569454216238406/07409829577848409351Z/1ZYTZPlnkPBCMcNqRRO-gNx8EPgtV-GL3?e%3Ddownload&hash=tjpqbk2dp469l934sb9627cs6d9dq9ht     Connecting to docs.google.com (docs.google.com)|173.194.210.139|:443... connected.     HTTP request sent, awaiting response... 302 Found     Location: https://doc-04-7s-docs.googleusercontent.com/docs/securesc/55p9v9r892nth323knj0kpu3c0fh68iu/jne4sjlttqu450ikt8fph58j505oapok/1644426900000/06283569454216238406/07409829577848409351Z/1ZYTZPlnkPBCMcNqRRO-gNx8EPgtV-GL3?e=download&nonce=erf8t6vb6o29s&user=07409829577848409351Z&hash=m0ja3hdvfkrvsfr6kol8k4bktej41mje [following]     --2022-02-09 17:15:02--  https://doc-04-7s-docs.googleusercontent.com/docs/securesc/55p9v9r892nth323knj0kpu3c0fh68iu/jne4sjlttqu450ikt8fph58j505oapok/1644426900000/06283569454216238406/07409829577848409351Z/1ZYTZPlnkPBCMcNqRRO-gNx8EPgtV-GL3?e=download&nonce=erf8t6vb6o29s&user=07409829577848409351Z&hash=m0ja3hdvfkrvsfr6kol8k4bktej41mje     Connecting to doc-04-7s-docs.googleusercontent.com (doc-04-7s-docs.googleusercontent.com)|173.194.210.132|:443... connected.     HTTP request sent, awaiting response... 200 OK     Length: 33740692 (32M) [text/plain]     Saving to: ‘Yf.txt’          Yf.txt              100%[===================>]  32.18M  --.-KB/s    in 0.1s              2022-02-09 17:15:02 (248 MB/s) - ‘Yf.txt’ saved [33740692/33740692]             ```python # Define paths ROOT_PATH = os.getcwd() TRAIN_DATA_PATH = (os.path.join(ROOT_PATH, 'data/trn.raw.json.gz')) TEST_DATA_PATH = (os.path.join(ROOT_PATH, 'data/tst.raw.json.gz')) ```   ```python # Load categories with open('./data/Yf.txt',  encoding='utf-8') as f:     categories = f.readlines()  # Clean values categories = [cat.split('->')[1].strip('\\n') for cat in categories]  # Show frist few categories categories[:3] ```         ['!!!_albums', '+/-_(band)_albums', '+44_(band)_songs']    ### Using a Subset of the Data For this example, we will select and use a subset of wikipedia articles. This will save time for processing and consume much less memory than the complete dataset. We will select a sample of 200,000 articles that contains around 250,000 different labels.   Feel free to run the notebook with more data.    ```python WIKI_ARTICLES_INDEX = range(0, 1000000, 5)  lines = []  with gzip.open(TRAIN_DATA_PATH) as f:     for e, line in enumerate(f):         if e >= 1000000:             break         if e in WIKI_ARTICLES_INDEX:             lines.append(json.loads(line))          df = pd.DataFrame.from_dict(lines) df = df[['title', 'content', 'target_ind']] df.head() ```        <div id=\"df-c158e1ee-532a-41cb-8be0-bc56f09f22e1\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     } </style> <table border=\"1\" class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>title</th>       <th>content</th>       <th>target_ind</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>Anarchism</td>       <td>{{redirect2|anarchist|anarchists|the fictional...</td>       <td>[81199, 83757, 83805, 193030, 368811, 368937, ...</td>     </tr>     <tr>       <th>1</th>       <td>Academy_Awards</td>       <td>{{redirect2|oscars|the oscar|the film|the osca...</td>       <td>[19080, 65864, 78208, 96051]</td>     </tr>     <tr>       <th>2</th>       <td>Anthropology</td>       <td>{{about|the social science}} {{use dmy dates|d...</td>       <td>[83605, 423943]</td>     </tr>     <tr>       <th>3</th>       <td>American_Football_Conference</td>       <td>{{refimprove|date=september 2014}} {{use dmy d...</td>       <td>[76725, 314198, 334093]</td>     </tr>     <tr>       <th>4</th>       <td>Analysis_of_variance</td>       <td>{{use dmy dates|date=june 2013}} '''analysis o...</td>       <td>[81170, 168516, 338198, 441529]</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c158e1ee-532a-41cb-8be0-bc56f09f22e1')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\">     <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>     <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>   </svg>       </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>        <script>         const buttonEl =           document.querySelector('#df-c158e1ee-532a-41cb-8be0-bc56f09f22e1 button.colab-df-convert');         buttonEl.style.display =           google.colab.kernel.accessAllowed ? 'block' : 'none';          async function convertToInteractive(key) {           const element = document.querySelector('#df-c158e1ee-532a-41cb-8be0-bc56f09f22e1');           const dataTable =             await google.colab.kernel.invokeFunction('convertToInteractive',                                                      [key], {});           if (!dataTable) return;            const docLinkHtml = 'Like what you see? Visit the ' +             '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'             + ' to learn more about interactive tables.';           element.innerHTML = '';           dataTable['output_type'] = 'display_data';           await google.colab.output.renderOutput(dataTable, element);           const docLink = document.createElement('div');           docLink.innerHTML = docLinkHtml;           element.appendChild(docLink);         }       </script>     </div>   </div>      ```python print(df.shape) ```      (200000, 3)       ### Remove Wikipedia Markup Format We are going to use only the first part of the articles to make them comparable in terms of length. Also, Wikipedia articles have a certain format that is not so readable, so we will remove the markup to make the content as clean as possible.   ```python # Reduce content to first 3000 characters df['content_short'] = df.content.apply(lambda x: x[:3000])  # Remove wiki articles markup df['content_cleaned'] = df.content_short.apply(lambda x: remove_markup(x))  # Keep only certain columns df = df[['title', 'content_cleaned', 'target_ind']]  # Show data df.head() ```        <div id=\"df-5f3fce25-2f7d-40d1-a109-335c1649ab71\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     } </style> <table border=\"1\" class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>title</th>       <th>content_cleaned</th>       <th>target_ind</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>Anarchism</td>       <td>anarchism is a political philosophy that a...</td>       <td>[81199, 83757, 83805, 193030, 368811, 368937, ...</td>     </tr>     <tr>       <th>1</th>       <td>Academy_Awards</td>       <td>the academy awards or the oscars (the offi...</td>       <td>[19080, 65864, 78208, 96051]</td>     </tr>     <tr>       <th>2</th>       <td>Anthropology</td>       <td>anthropology  is the scientific study of hu...</td>       <td>[83605, 423943]</td>     </tr>     <tr>       <th>3</th>       <td>American_Football_Conference</td>       <td>the american football conference (afc) is o...</td>       <td>[76725, 314198, 334093]</td>     </tr>     <tr>       <th>4</th>       <td>Analysis_of_variance</td>       <td>analysis of variance (anova) is a collection ...</td>       <td>[81170, 168516, 338198, 441529]</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f3fce25-2f7d-40d1-a109-335c1649ab71')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\">     <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>     <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>   </svg>       </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>        <script>         const buttonEl =           document.querySelector('#df-5f3fce25-2f7d-40d1-a109-335c1649ab71 button.colab-df-convert');         buttonEl.style.display =           google.colab.kernel.accessAllowed ? 'block' : 'none';          async function convertToInteractive(key) {           const element = document.querySelector('#df-5f3fce25-2f7d-40d1-a109-335c1649ab71');           const dataTable =             await google.colab.kernel.invokeFunction('convertToInteractive',                                                      [key], {});           if (!dataTable) return;            const docLinkHtml = 'Like what you see? Visit the ' +             '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'             + ' to learn more about interactive tables.';           element.innerHTML = '';           dataTable['output_type'] = 'display_data';           await google.colab.output.renderOutput(dataTable, element);           const docLink = document.createElement('div');           docLink.innerHTML = docLinkHtml;           element.appendChild(docLink);         }       </script>     </div>   </div>      ```python # Keep all labels in a single list all_categories = [] for i, row in df.iterrows():     all_categories.extend(row.target_ind) print('Number of labels: ',len(list(set(all_categories)))) ```      Number of labels:  256899       ### Create Article Vector Embeddings  Recall, we want to index and search all possible (250,000) *labels*. We do that by averaging, for each label, the corresponding article vector embeddings that contain that label.   Let's first create the article vector embeddings. Here we use the [Average Word Embeddings Models](https://www.sbert.net/docs/pretrained_models.html#average-word-embeddings-models).  In the next section, we will aggregate these vectors to make the final label embeddings.    ```python # Load the model model = SentenceTransformer('average_word_embeddings_komninos')  # Create embeddings encoded_articles = model.encode(df['content_cleaned'], show_progress_bar=True) df['content_vector'] = pd.Series(encoded_articles.tolist()) ```       Downloading:   0%|          | 0.00/690 [00:00<?, ?B/s]        Downloading:   0%|          | 0.00/2.13k [00:00<?, ?B/s]        Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]        Downloading:   0%|          | 0.00/248 [00:00<?, ?B/s]        Downloading:   0%|          | 0.00/267M [00:00<?, ?B/s]        Downloading:   0%|          | 0.00/2.59M [00:00<?, ?B/s]        Downloading:   0%|          | 0.00/164 [00:00<?, ?B/s]        Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]        Batches:   0%|          | 0/6250 [00:00<?, ?it/s]   ## Upload articles   It appears that using the article embeddings per se doesn't provide good enough accuracies. Therefore, we chose to index and search the labels directly.   The label embedding is simply the average of all its corresponding article embeddings.    ```python # Explode the target indicator column df_explode = df.explode('target_ind')  # Group by label and define a unique vector for each label result = df_explode.groupby('target_ind').agg(mean=('content_vector', lambda x: np.vstack(x).mean(axis=0).tolist())) result['target_ind'] = result.index result.columns = ['content_vector', 'ind']  result.head() ```        <div id=\"df-eda4ccdc-de8e-4dac-9282-b989392d0727\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     } </style> <table border=\"1\" class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>content_vector</th>       <th>ind</th>     </tr>     <tr>       <th>target_ind</th>       <th></th>       <th></th>     </tr>   </thead>   <tbody>     <tr>       <th>2</th>       <td>[0.0704750344157219, -0.007719345390796661, 0....</td>       <td>2</td>     </tr>     <tr>       <th>3</th>       <td>[0.05894148722290993, -0.03119848482310772, 0....</td>       <td>3</td>     </tr>     <tr>       <th>5</th>       <td>[0.18302207440137863, 0.061663837544620036, 0....</td>       <td>5</td>     </tr>     <tr>       <th>6</th>       <td>[0.1543595753610134, 0.03904660418629646, 0.03...</td>       <td>6</td>     </tr>     <tr>       <th>9</th>       <td>[0.22310754656791687, 0.1524289846420288, 0.09...</td>       <td>9</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eda4ccdc-de8e-4dac-9282-b989392d0727')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\">     <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>     <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>   </svg>       </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>        <script>         const buttonEl =           document.querySelector('#df-eda4ccdc-de8e-4dac-9282-b989392d0727 button.colab-df-convert');         buttonEl.style.display =           google.colab.kernel.accessAllowed ? 'block' : 'none';          async function convertToInteractive(key) {           const element = document.querySelector('#df-eda4ccdc-de8e-4dac-9282-b989392d0727');           const dataTable =             await google.colab.kernel.invokeFunction('convertToInteractive',                                                      [key], {});           if (!dataTable) return;            const docLinkHtml = 'Like what you see? Visit the ' +             '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'             + ' to learn more about interactive tables.';           element.innerHTML = '';           dataTable['output_type'] = 'display_data';           await google.colab.output.renderOutput(dataTable, element);           const docLink = document.createElement('div');           docLink.innerHTML = docLinkHtml;           element.appendChild(docLink);         }       </script>     </div>   </div>      ```python # Create a list of items to upsert items_to_upsert = [(unidecode(categories[int(row.ind)])[:64], row.content_vector) for i, row in result.iterrows()] ```   ```python import itertools  def chunks(iterable, batch_size=100):     it = iter(iterable)     chunk = tuple(itertools.islice(it, batch_size))     while chunk:         yield chunk         chunk = tuple(itertools.islice(it, batch_size)) ```   ```python # Upsert data for batch in chunks(items_to_upsert, 250):     index.upsert(vectors=batch) ```  Let's validate the number of indexed labels.   ```python index.describe_index_stats() ```         {'dimension': 300, 'namespaces': {'': {'vector_count': 256899}}}    ## Query   Now, let's test the vector index and examine the classifier results. Observe that here we retrieve a fixed number of labels. Naturally, in an actual application, you might want to calculate the size of the retrieved label set dynamically.    ```python NUM_OF_WIKI_ARTICLES = 3 WIKI_ARTICLES_INDEX = range(1111, 100000, 57)[:NUM_OF_WIKI_ARTICLES]  lines = []  with gzip.open(TEST_DATA_PATH) as f:     for e, line in enumerate(f):         if e in  WIKI_ARTICLES_INDEX:             lines.append(json.loads(line))          if e > max(WIKI_ARTICLES_INDEX):             break              df_test = pd.DataFrame.from_dict(lines) df_test = df_test[['title', 'content', 'target_ind']] df_test.head() ```        <div id=\"df-05a3b113-37f2-403c-b0e2-f5c4dd0dac99\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     } </style> <table border=\"1\" class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>title</th>       <th>content</th>       <th>target_ind</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>Discrimination</td>       <td>{{otheruses}} {{discrimination sidebar}} '''di...</td>       <td>[170479, 423902]</td>     </tr>     <tr>       <th>1</th>       <td>Erfurt</td>       <td>{{refimprove|date=june 2014}} {{use dmy dates|...</td>       <td>[142638, 187156, 219262, 294479, 329185, 38243...</td>     </tr>     <tr>       <th>2</th>       <td>ETA</td>       <td>{{about|the basque organization|other uses|eta...</td>       <td>[83681, 100838, 100849, 100868, 176034, 188979...</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-05a3b113-37f2-403c-b0e2-f5c4dd0dac99')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\">     <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>     <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>   </svg>       </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>        <script>         const buttonEl =           document.querySelector('#df-05a3b113-37f2-403c-b0e2-f5c4dd0dac99 button.colab-df-convert');         buttonEl.style.display =           google.colab.kernel.accessAllowed ? 'block' : 'none';          async function convertToInteractive(key) {           const element = document.querySelector('#df-05a3b113-37f2-403c-b0e2-f5c4dd0dac99');           const dataTable =             await google.colab.kernel.invokeFunction('convertToInteractive',                                                      [key], {});           if (!dataTable) return;            const docLinkHtml = 'Like what you see? Visit the ' +             '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'             + ' to learn more about interactive tables.';           element.innerHTML = '';           dataTable['output_type'] = 'display_data';           await google.colab.output.renderOutput(dataTable, element);           const docLink = document.createElement('div');           docLink.innerHTML = docLinkHtml;           element.appendChild(docLink);         }       </script>     </div>   </div>      ```python # Reduce content to first 3000 characters df_test['content_short'] = df_test.content.apply(lambda x: x[:3000])  # Remove wiki articles markup df_test['content_cleaned'] = df_test.content_short.apply(lambda x: remove_markup(x))  # Keep only certain columns df_test = df_test[['title', 'content_cleaned', 'target_ind']]  # Show data df_test.head() ```        <div id=\"df-bf29f1a6-0986-4c74-bb33-d82361095999\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     } </style> <table border=\"1\" class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>title</th>       <th>content_cleaned</th>       <th>target_ind</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>Discrimination</td>       <td>discrimination is action that denies social ...</td>       <td>[170479, 423902]</td>     </tr>     <tr>       <th>1</th>       <td>Erfurt</td>       <td>erfurt () is the capital city of thuringia ...</td>       <td>[142638, 187156, 219262, 294479, 329185, 38243...</td>     </tr>     <tr>       <th>2</th>       <td>ETA</td>       <td>eta (, ), an acronym for euskadi ta askatas...</td>       <td>[83681, 100838, 100849, 100868, 176034, 188979...</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf29f1a6-0986-4c74-bb33-d82361095999')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\">     <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>     <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>   </svg>       </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>        <script>         const buttonEl =           document.querySelector('#df-bf29f1a6-0986-4c74-bb33-d82361095999 button.colab-df-convert');         buttonEl.style.display =           google.colab.kernel.accessAllowed ? 'block' : 'none';          async function convertToInteractive(key) {           const element = document.querySelector('#df-bf29f1a6-0986-4c74-bb33-d82361095999');           const dataTable =             await google.colab.kernel.invokeFunction('convertToInteractive',                                                      [key], {});           if (!dataTable) return;            const docLinkHtml = 'Like what you see? Visit the ' +             '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'             + ' to learn more about interactive tables.';           element.innerHTML = '';           dataTable['output_type'] = 'display_data';           await google.colab.output.renderOutput(dataTable, element);           const docLink = document.createElement('div');           docLink.innerHTML = docLinkHtml;           element.appendChild(docLink);         }       </script>     </div>   </div>      ```python # Create embeddings for test articles test_vectors = model.encode(df_test['content_cleaned'], show_progress_bar=True).tolist() ```       Batches:   0%|          | 0/1 [00:00<?, ?it/s]    ```python # Query the vector index query_results = []  for xq in test_vectors:     query_res = index.query(xq, top_k=10)     query_results.append(query_res) ```   ```python # Show results for term, labs, res in zip(df_test.title.tolist(), df_test.target_ind.tolist(), query_results):     print()     print('Term queried: ',term)     print('Original labels: ')     for l in labs:         if l in all_categories:             print('\\t', categories[l])     print('Predicted: ')     df_result = pd.DataFrame({                 'id': [res.id for res in res.matches],                 'score': [res.score for res in res.matches],})     display(df_result) ```           Term queried:  Discrimination     Original labels:      \t Discrimination     \t Social_justice     Predicted:            <div id=\"df-98bd35a3-d003-446e-beec-909fd26b91ac\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     } </style> <table border=\"1\" class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>id</th>       <th>score</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>Discrimination</td>       <td>0.972958</td>     </tr>     <tr>       <th>1</th>       <td>Sociological_terminology</td>       <td>0.971606</td>     </tr>     <tr>       <th>2</th>       <td>Identity_politics</td>       <td>0.970097</td>     </tr>     <tr>       <th>3</th>       <td>Social_concepts</td>       <td>0.967534</td>     </tr>     <tr>       <th>4</th>       <td>Sexism</td>       <td>0.967476</td>     </tr>     <tr>       <th>5</th>       <td>Affirmative_action</td>       <td>0.967288</td>     </tr>     <tr>       <th>6</th>       <td>Political_correctness</td>       <td>0.966926</td>     </tr>     <tr>       <th>7</th>       <td>Human_behavior</td>       <td>0.966475</td>     </tr>     <tr>       <th>8</th>       <td>Persecution</td>       <td>0.965421</td>     </tr>     <tr>       <th>9</th>       <td>Social_movements</td>       <td>0.964394</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-98bd35a3-d003-446e-beec-909fd26b91ac')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\">     <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>     <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>   </svg>       </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>        <script>         const buttonEl =           document.querySelector('#df-98bd35a3-d003-446e-beec-909fd26b91ac button.colab-df-convert');         buttonEl.style.display =           google.colab.kernel.accessAllowed ? 'block' : 'none';          async function convertToInteractive(key) {           const element = document.querySelector('#df-98bd35a3-d003-446e-beec-909fd26b91ac');           const dataTable =             await google.colab.kernel.invokeFunction('convertToInteractive',                                                      [key], {});           if (!dataTable) return;            const docLinkHtml = 'Like what you see? Visit the ' +             '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'             + ' to learn more about interactive tables.';           element.innerHTML = '';           dataTable['output_type'] = 'display_data';           await google.colab.output.renderOutput(dataTable, element);           const docLink = document.createElement('div');           docLink.innerHTML = docLinkHtml;           element.appendChild(docLink);         }       </script>     </div>   </div>             Term queried:  Erfurt     Original labels:      \t Erfurt     \t German_state_capitals     \t Members_of_the_Hanseatic_League     \t Oil_Campaign_of_World_War_II     \t Province_of_Saxony     \t University_towns_in_Germany     Predicted:            <div id=\"df-6849b581-67be-451f-87c2-c297429607dc\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     } </style> <table border=\"1\" class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>id</th>       <th>score</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>University_towns_in_Germany</td>       <td>0.966058</td>     </tr>     <tr>       <th>1</th>       <td>Province_of_Saxony</td>       <td>0.959731</td>     </tr>     <tr>       <th>2</th>       <td>Populated_places_on_the_Rhine</td>       <td>0.958737</td>     </tr>     <tr>       <th>3</th>       <td>Imperial_free_cities</td>       <td>0.957159</td>     </tr>     <tr>       <th>4</th>       <td>Hildesheim_(district)</td>       <td>0.956927</td>     </tr>     <tr>       <th>5</th>       <td>History_of_the_Electoral_Palatinate</td>       <td>0.956800</td>     </tr>     <tr>       <th>6</th>       <td>Towns_in_Saxony-Anhalt</td>       <td>0.956501</td>     </tr>     <tr>       <th>7</th>       <td>Towns_in_Lower_Saxony</td>       <td>0.955259</td>     </tr>     <tr>       <th>8</th>       <td>Halle_(Saale)</td>       <td>0.954934</td>     </tr>     <tr>       <th>9</th>       <td>Cities_in_Saxony-Anhalt</td>       <td>0.954934</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6849b581-67be-451f-87c2-c297429607dc')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\">     <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>     <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>   </svg>       </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>        <script>         const buttonEl =           document.querySelector('#df-6849b581-67be-451f-87c2-c297429607dc button.colab-df-convert');         buttonEl.style.display =           google.colab.kernel.accessAllowed ? 'block' : 'none';          async function convertToInteractive(key) {           const element = document.querySelector('#df-6849b581-67be-451f-87c2-c297429607dc');           const dataTable =             await google.colab.kernel.invokeFunction('convertToInteractive',                                                      [key], {});           if (!dataTable) return;            const docLinkHtml = 'Like what you see? Visit the ' +             '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'             + ' to learn more about interactive tables.';           element.innerHTML = '';           dataTable['output_type'] = 'display_data';           await google.colab.output.renderOutput(dataTable, element);           const docLink = document.createElement('div');           docLink.innerHTML = docLinkHtml;           element.appendChild(docLink);         }       </script>     </div>   </div>             Term queried:  ETA     Original labels:      \t Anti-Francoism     \t Basque_conflict     \t Basque_history     \t Basque_politics     \t ETA     \t European_Union_designated_terrorist_organizations     \t Far-left_politics     \t Francoist_Spain     \t Government_of_Canada_designated_terrorist_organizations     \t Irregular_military     \t Military_wings_of_political_parties     \t National_liberation_movements     \t Nationalist_terrorism     \t Organizations_designated_as_terrorist_by_the_United_States_government     \t Organizations_designated_as_terrorist_in_Europe     \t Organizations_established_in_1959     \t Politics_of_Spain     \t Resistance_movements     \t Secession_in_Spain     \t Secessionist_organizations_in_Europe     \t Terrorism_in_Spain     \t United_Kingdom_Home_Office_designated_terrorist_groups     Predicted:            <div id=\"df-02b4760b-a840-4c3d-b778-64b3de6f35bf\">     <div class=\"colab-df-container\">       <div> <style scoped>     .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     } </style> <table border=\"1\" class=\"dataframe\">   <thead>     <tr style=\"text-align: right;\">       <th></th>       <th>id</th>       <th>score</th>     </tr>   </thead>   <tbody>     <tr>       <th>0</th>       <td>Organizations_designated_as_terrorist_in_Europe</td>       <td>0.948875</td>     </tr>     <tr>       <th>1</th>       <td>Terrorism_in_Spain</td>       <td>0.948431</td>     </tr>     <tr>       <th>2</th>       <td>Basque_politics</td>       <td>0.942670</td>     </tr>     <tr>       <th>3</th>       <td>Politics_of_Spain</td>       <td>0.941830</td>     </tr>     <tr>       <th>4</th>       <td>European_Union_designated_terrorist_organizations</td>       <td>0.940194</td>     </tr>     <tr>       <th>5</th>       <td>Irregular_military</td>       <td>0.938163</td>     </tr>     <tr>       <th>6</th>       <td>Political_parties_disestablished_in_1977</td>       <td>0.936437</td>     </tr>     <tr>       <th>7</th>       <td>Algerian_Civil_War</td>       <td>0.936311</td>     </tr>     <tr>       <th>8</th>       <td>Republicanism_in_Spain</td>       <td>0.935577</td>     </tr>     <tr>       <th>9</th>       <td>Guerrilla_organizations</td>       <td>0.935506</td>     </tr>   </tbody> </table> </div>       <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-02b4760b-a840-4c3d-b778-64b3de6f35bf')\"               title=\"Convert this dataframe to an interactive table.\"               style=\"display:none;\">    <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\">     <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>     <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>   </svg>       </button>    <style>     .colab-df-container {       display:flex;       flex-wrap:wrap;       gap: 12px;     }      .colab-df-convert {       background-color: #E8F0FE;       border: none;       border-radius: 50%;       cursor: pointer;       display: none;       fill: #1967D2;       height: 32px;       padding: 0 0 0 0;       width: 32px;     }      .colab-df-convert:hover {       background-color: #E2EBFA;       box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);       fill: #174EA6;     }      [theme=dark] .colab-df-convert {       background-color: #3B4455;       fill: #D2E3FC;     }      [theme=dark] .colab-df-convert:hover {       background-color: #434B5C;       box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);       filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));       fill: #FFFFFF;     }   </style>        <script>         const buttonEl =           document.querySelector('#df-02b4760b-a840-4c3d-b778-64b3de6f35bf button.colab-df-convert');         buttonEl.style.display =           google.colab.kernel.accessAllowed ? 'block' : 'none';          async function convertToInteractive(key) {           const element = document.querySelector('#df-02b4760b-a840-4c3d-b778-64b3de6f35bf');           const dataTable =             await google.colab.kernel.invokeFunction('convertToInteractive',                                                      [key], {});           if (!dataTable) return;            const docLinkHtml = 'Like what you see? Visit the ' +             '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'             + ' to learn more about interactive tables.';           element.innerHTML = '';           dataTable['output_type'] = 'display_data';           await google.colab.output.renderOutput(dataTable, element);           const docLink = document.createElement('div');           docLink.innerHTML = docLinkHtml;           element.appendChild(docLink);         }       </script>     </div>   </div>    ## Summary We demonstrated a similarity search approach for performing extreme classification of texts. We took a simple approach representing labels as the average of their corresponding texts' vector embeddings. In classification time, we match between a new article embedding and its nearest label embeddings. Our result examples indicate the usefulness of this approach.   You can take this forward by exploring advanced ideas. For example, you can utilize the hierarchical relationship between labels or improve the label representations.  Just have fun, and feel free to [share](https://www.pinecone.io/contact/) your thoughts.   ## Delete the index  Delete the index once you do not want to use it anymore. Once the index is deleted, you cannot use it again.   ```python pinecone.delete_index(index_name) ``` ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e273"
  },
  "title": "Organizations",
  "category": "630fc5235d91a70054705fb8",
  "content": "## Overview  A Pinecone organization is a set of [projects](projects) that use the same billing. Organizations allow one or more users to control billing and project permissions for all of the projects belonging to the organization. Each project belongs to an organization.   For a guide to adding users to an organization, see [Add users to a project or organization](add-users-to-projects-and-organizations/).   ## Projects in an organization  Each organization contains one or more projects that share the same organization owners and billing settings. Each project belongs to exactly one organization. If you need to move a project from one organization to another, contact [Pinecone support](https://support.pinecone.io).   ## Billing settings  All of the projects in an organization share the same billing method and settings. The billing settings for the organization are controlled by the organization owners.  ## Organization roles  There are two organization roles: organization owner and organization user.  ### Organization owners  Organization owners manage organization billing, users, and projects. Organization owners are also [project owners](projects#project-roles) for every project belonging to the organization. This means that organization owners have all permissions to manage project members, API keys, and quotas for these projects.  ### Organization users  Unlike organization owners, organization users cannot edit billing settings or invite new users to the organization. Organization users can create new projects, and project owners can add organization members to a project. New users have whatever role the organization owners and project owners grant them. Project owners can add users to a project if those users belong to the same organization as the project.  **Table 1: Organization roles and permissions**  | Organization role   | Permissions in organization    | | ------------------- | ------------------------------ | | Organization owner  | Project owner for all projects | |                     | Create projects                | |                     | Manage billing                 | |                     | Manags organization members    | | Organization member | Create projects                | |                     | Join projects when invited     | |                     | Read access to billing         |  ## Next steps  * [Add users to an organization](add-users-to-projects-and-organizations/) ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e275"
  },
  "title": "Collections",
  "category": "630fc5235d91a70054705fb8",
  "content": "## Overview  This document explains the concepts related to collections in Pinecone.  > ⚠️  Warning > > This is a **public preview** (\"Beta\") feature. Test thoroughly before > using this feature for production workloads. No SLAs or technical support > commitments are provided for this feature.  **A collection is a static copy of an index.** It is a non-queryable representation of a set of vectors and metadata. You can create a collection from an index, and you can create a new index from a collection. This new index can differ from the original source index: the new index can have a different number of pods, a different pod type, or a different similarity metric.  ## Use cases for collections  Creating a collection from your index is useful when performing tasks like the following:  + Temporarily shutting down an index + Copying the data from one index into a different index; + Making a backup of your index + Experimenting with different index configurations  To learn about creating backups with collections, see [Back up indexes](back-up-indexes/#create-a-backup-using-a-collection).  To learn about creating indexes from collections, see [Manage indexes](manage-indexes/#create-an-index-from-a-collection).   ## Public collections contain real world data  Public collections contain vectorized data from real-world datasets that you can use to [create indexes](manage-indexes/#create-an-index-from-a-public-collection). You can use these indexes to try out Pinecone with realistic example data and queries.   Pinecone offers public collections containing data from the following datasets:  + [OpenAI TREC](https://huggingface.co/datasets/trec) + [Cohere TREC](https://huggingface.co/datasets/trec) + [SQuAD](https://huggingface.co/datasets/squad)  ## Performance   Collections operations perform differently with different pod types.  + Creating a collection from an index takes approximately 10 minutes.  + Creating a p1 or s1 index from a collection takes approximately 10 minutes. + Creating a p2 index from a collection can take several hours.  ## Limitations  You cannot query or write to a collection after its creation. For this reason, a collection only incurs storage costs.  You can only perform operations on collections in the current Pinecone project. ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e277"
  },
  "title": "Indexes",
  "category": "630fc5235d91a70054705fb8",
  "content": "## Overview  This document describes concepts related to Pinecone indexes. To learn how to create or modify an index, see [Manage indexes](manage-indexes).  An index is the highest-level organizational unit of vector data in Pinecone. It accepts and stores vectors, serves queries over the vectors it contains, and does other vector operations over its contents. Each index runs on at least one **pod**.   ## Pods, pod types, and pod sizes  Pods are pre-configured units of hardware for running a Pinecone service. Each index runs on one or more pods. Generally, more pods mean more storage capacity, lower latency, and higher throughput. You can also create pods of different sizes.  Once an index is created using a particular pod type, you cannot change the pod type for that index. However, you can [create a new index from that collection](manage-indexes/#create-an-index-from-a-collection) with a different pod type.  Users on the Starter (free) plan are limited to 1 p1 pod or 1 s1 pod.  Different pod types are priced differently. See [pricing](https://www.pinecone.io/pricing/) for more details.  ### s1 pods  These storage-optimized pods provide large storage capacity and lower overall costs with slightly higher query latencies than p1 pods. They are ideal for very large indexes with moderate or relaxed latency requirements.  Each s1 pod has enough capacity for around 5M vectors of 768 dimensions.  ### p1 pods  These performance-optimized pods provide very low query latencies, but hold fewer vectors per pod than s1 pods. They are ideal for applications with low latency requirements (<100ms).  Each p1 pod has enough capacity for around 1M vectors of 768 dimensions.  ### p2 pods  The p2 pod type provides greater query throughput with lower latency. For vectors with fewer than 128 dimension and queries where `topK` is less than 50, p2 pods support up to 200 QPS per replica and return queries in less than 10ms. This means that query throughput and latency are better than s1 and p1.  Each p2 pod has enough capacity for around 1M vectors of 768 dimensions. However, capacity may vary with dimensionality.  The data ingestion rate for p2 pods is significantly slower than for p1 pods; this rate decreases as the number of dimensions increases. For example, a p2 pod containing vectors with 128 dimensions can upsert up to 300 updates per second; a p2 pod containing vectors with 768 dimensions or more supports  upsert of 50 updates per second. Because query latency and throughput for p2 pods vary from p1 pods, test p2 pod performance with your dataset.  ### Pod size and performance  Pod performance varies depending on a variety of factors. To observe how your workloads perform on a given pod type, experiment with your own data set.  Each pod type supports four pod sizes: `x1`, `x2`, `x4`, and `x8`. Your index storage and compute capacity doubles for each size step. The default pod size is `x1`. You can increase the size of a pod after index creation.  To learn about changing the pod size of an index, see [Manage indexes](manage-indexes/#changing-pod-sizes).  ### Distance metrics  You can choose from different metrics when creating a vector index:  - `euclidean`   - This is used to calculate the distance between two data points in a plane. It is one of the most commonly used distance metric. For an example, see our [image similarity search example](image-similarity-search/).   - When you use `metric='euclidean'`, the most similar results are those with the **lowest score**. - `cosine`   - This is often used to find similarities between different documents. The advantage is that the scores are normalized to [-1,1] range. - `dotproduct`   - This is used to multiply two vectors. You can use it to tell us how similar the two vectors are. The more positive the answer is, the closer the two vectors are in terms of their directions.  For the full list of parameters available to customize an index, see the [create_index API reference](reference/create_index/).  Depending on your application, some metrics have better recall and precision performance than others. For more information, see: [What is Vector Similarity Search?](https://www.pinecone.io/learn/what-is-similarity-search/) ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e279"
  },
  "title": "Projects",
  "category": "630fc5235d91a70054705fb8",
  "content": "## Overview  This document explains the concepts related to Pinecone projects.  ## Projects contain indexes and users  Each Pinecone project contains a number of [indexes](/indexes) and users. Only a user who belongs to the project can access the indexes in that project. Each project also has at least one project owner. All of the pods in a single project are located in a single environment.    ## Project settings  When you create a new project, you can choose the **name**, **deployment environment**, and **pod limit**.  ### Project environment  GCP US-East is the default environment on default projects, and the only available environment for new users on the Starter (free) plan.  Users on the Standard and Enterprise plans can choose from GCP US-West, GCP US-East, GCP EU-West, or AWS US-East.  These regions correspond to the following values of the `environment` parameter for the [init() operation](concepts):  | Cloud region                   | `environment` value | | -------------------------------| ------------------- | | GCP US-West-1 (N. California)  | us-west1-gcp        | | GCP US-East-1 (South Carolina) | us-east1-gcp        | | GCP EU-West-1 (Ireland)        | eu-west1-gcp        | | AWS US-East-1 (Virginia)       | us-east1-aws        |   [Contact us](http://www.pinecone.io/contact/) if you need a dedicated deployment in other regions.  The environment cannot be changed after the project is created.  ### Project pod limit  You can set the maximum number of pods that can be used in total across all indexes in a project. Use this to control costs.  The pod limit can be changed only by the project owner.  ### Project roles  There are two project roles: **Project owner** and **project member.** Table 1 below summarizes the permissions for each role.  **Table 1: Project roles and permissions**  | Project role        | Permissions in organization    | | ------------------- | ------------------------------ | | Project owner       | Manage project members         | |                     | Manage project API keys        | |                     | Manage pod limits              | | Project member      | Access API keys                | |                     | Create indexes in project      | |                     | Use indexes in project         | ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e27b"
  },
  "title": "Hybrid Search",
  "category": "630fc5235d91a70054705fb8",
  "content": "> ⚠️  Warning > This is an early access feature, which is available on an invitation basis and is not intended for production workloads. No SLAs or technical support commitments are provided. [Sign up](https://www.pinecone.io/hybrid-search-early-access/) for early access.  ## Overview  Pinecone supports **hybrid search**, which allows you to perform semantic and keyword search over your data in one query and combine the results for more relevant results. This topic describes what hybrid search does, why it is useful, and how it works in Pinecone.  ## Pinecone hybrid search allows keyword-aware semantic search  Pinecone hybrid search allows you to perform keyword-aware semantic search. Semantic search results for out-of-domain queries can be less relevant; [combining these with keyword search results can improve relevance](https://arxiv.org/abs/2210.11934).  Because Pinecone allows you to create your own sparse vectors, you can use hybrid search to solve the Maximum Inner Product Search (MIPS) problem for hybrid vectors of any real values. This includes emerging use-cases such as retrieval over learnt sparse representations for text data using [SPLADE](https://arxiv.org/abs/2107.05720).  ## Hybrid search workflow  Hybrid search involves the following general steps:  1. Create dense vectors using an external embedding model. 1. Create sparse vectors using an external tokenizer. 1. [Create a hybrid index](manage-indexes/#creating-an-index). 1. Upsert dense and sparse vectors to the hybrid upsert endpoint. 1. Search the hybrid index using the hybrid query endpoint. 1. Pinecone returns ranked hybrid vectors.  Figure 1 below illustrates these steps.  **Figure 1: Hybrid search workflow**  ![Hybrid search workflow](https://raw.githubusercontent.com/pinecone-io/img/main/hybrid-search-architecture.png)   ## Sparse versus dense vectors in Pinecone   Hybrid search combines [dense and sparse vectors](https://www.pinecone.io/learn/dense-vector-embeddings-nlp/#dense-vs-sparse-vectors); these types of vectors represent different types of information and enable distinct kinds of search. [Dense vectors](https://www.pinecone.io/learn/dense-vector-embeddings-nlp/) enable semantic search. Semantic search returns the most similar results according to a specific distance metric even if no exact matches are present. This is possible because dense vectors generated by embedding models such as [SBERT](https://huggingface.co/sentence-transformers) are numerical representations of semantic meaning.  Sparse vectors have very large number of dimensions, generally over 1M, where only a small proportion of values are non-zero. When used for keywords search, each sparse vector represents a document; the dimensions represent words from a dictionary, and the values represent the frequency of these words in the document. Keyword search algorithms like the [BM25](https://en.wikipedia.org/wiki/Okapi_BM25) algorithm compute the relevance of text documents based on the number of keyword matches, their frequency, and other factors.  ## Creating sparse vectors for use in hybrid search  Keyword-aware semantic search requires vector representations of documents. Because Pinecone hybrid indexes accept sparse indexes rather than documents, you can control the generation of sparse vectors to represent documents. You can choose a tokenizer or analyzer, such as [Hugging Face](https://huggingface.co/docs/tokenizers/index), [spaCy](https://spacy.io/api/tok2vec), [Lucene]() to convert documents into sparse vectors, such as term-frequency vectors. The result is a dictionary that maps token IDs to term frequencies.  **Example**  ``` sparse_vector = dict(Counter(tokenizer.encode(doc)))  # {5:1, 10500:1, 7:1, ... } ```  ## Pinecone creates hybrid vectors from your sparse and dense vectors  Hybrid vectors combine dense and sparse vectors. In Pinecone, each hybrid vector consists of a sparse vector and a dense vector. Your hybrid index accepts vector upserts and queries containing both dense and sparse vector parameters and combines these into hybrid vectors.  When you upsert a hybrid vector using the hybrid upsert endpoint, your index normalizes the sparse vector for BM25 ranking and stores the normalized version. If you upsert hybrid vectors using the standard `upsert` operation, your index stores them without normalization.   ## Hybrid indexes store hybrid vectors and keyword search parameters  Pinecone stores hybrid vectors in hybrid indexes. A hybrid index has all of the features of a default dense vector index as well as a set of parameters for BM25 ranking. Your hybrid index uses these parameters to perform BM25 ranking of keyword search results and combines these keyword results with semantic search results to produce hybrid results.   Hybrid indexes use the `s1h` [pod type](https://www.pinecone.io/docs/indexes/#pods-pod-types-and-pod-sizes).  ## Hybrid queries include sparse and dense vectors with weighting parameter  To query your hybrid index, you provide a hybrid query vector and a weight parameter `alpha` that determines the relative weight of similarity and keyword relevance in hybrid query results. Your index performs both a semantic or similarity search and a keyword search; then, your index ranks the vectors in your index based on a combination of similarity and keyword matching and returns the most relevant results. Hybrid query results contain both dense and sparse vector values.  If you query your hybrid index using the hybrid query endpoint, your hybrid index denormalizes the sparse component of the hybrid result vectors before returning them in query results, so that they match the upserted sparse vectors.  ## Sparse vector search returns BM25 ranked results  If you query the hybrid index through the hybrid query endpoint, the sparse rankings are similar to those produced by the BM25 algorithm.   If you query the hybrid index directly, then the hybrid index returns the sparse vectors with the highest dot product across the sparse component of the hybrid query vector.  ## Hybrid queries specify weight of dense and sparse rankings  When you query a hybrid index, you provide both dense and sparse vectors, which the hybrid query endpoint combines to create a hybrid query vector. Your index performs similarity search using the dense component of the hybrid vector and keyword search using the sparse component. The hybrid query endpoint normalizes the sparse vector component before searching. Your hybrid query also contains a parameter called `alpha` that determines the relative weight of the relevance rankings from the dense vector searches. You can adjust `alpha` to adjust the relative weight of semantic and keyword search rankings.  The equation in Figure 1 below expresses how alpha affects the relative weighting of lexical or keyword ranking and semantic ranking in hybrid query results.  **Equation 1: Linear combination with weighting parameter `alpha`**  <sub>hybrid</sub>(q,d)=(1-&alpha;)f<sub>lexical</sub>(q,d) + &alpha;f<sub>semantic</sub>(q,d)  Values for `alpha` between `.7` and `.9` result in the best performance for in-domain models. When using a model that is not trained for the corpus, or is out-of-domain, downweight the semantic score with lower values of `alpha` in the range 0.3-0.6. When the model is fine-tuned or in-domain, use values closer to 1.   Figure 2 below shows the relationship between the value of `alpha` and the NDCG relevance metric.  ![Relevance by alpha value for in- and out-of-domain models](https://raw.githubusercontent.com/pinecone-io/img/main/alpha-relevance.png) ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e27d"
  },
  "title": "Release notes",
  "category": "630fc5235d91a70054705fb9",
  "content": "This document contains details about Pinecone releases. For information about using specific features, see our [API reference](/reference/describe_index_stats/).  ## January 3, 2023  #### Pinecone Python client version 2.1.0 is now available on GitHub.  The [latest release of the Python client](https://github.com/pinecone-io/pinecone-python-client/releases/tag/2.1.0) makes the following changes:  * Fixes \"Connection Reset by peer\" error after long idle periods * Adds typing and explicit names for arguments in all client operations * Adds docstrings to all client operations * Adds Support for batch upserts by passing `batch_size` to the upsert method * Improves gRPC query results parsing performance  ## December 22, 2022  #### Pinecone is now available in GCP Marketplace  You can now [sign up for Pinecone billing through Google Cloud Platform Marketplace](setting-up-gcp-marketplace-billing).   ## December 6, 2022  #### Organizations are generally available  Pinecone now features [organizations](/organizations), which allow one or more users to control billing and project settings across multiple projects owned by the same organization.  #### p2 pod type is generally available  The [p2 pod type](indexes#p2-pods) is now generally available and ready for production workloads. p2 pods are now available in the Starter plan and support the [`dotproduct` distance metric](indexes#distance-metrics).  #### Performance improvements  * [Bulk vector_deletes](https://www.pinecone.io/docs/manage-data/#deleting-vectors) are now up to 10x faster in many circumstances.  * [Creating collections](https://www.pinecone.io/docs/back-up-indexes/) is now faster.   ## October 31, 2022  #### Hybrid search (Early access)  Pinecone now supports keyword-aware semantic search with the new [hybrid search](hybrid-search) indexes and endpoints. Hybrid search enables improved relevance for semantic search results by combining them with keyword search.  This is an **early access** feature and is available only by [signing up](https://www.pinecone.io/hybrid-search-early-access/).  ## October 17, 2022  #### Status page  The new [Pinecone Status Page](https://status.pinecone.io/) displays information about the status of the Pinecone service, including the status of individual cloud regions and a log of recent incidents.  ## September 16, 2022  #### Public collections   You can now [create indexes from public collections](/collections#public-collections), which are collections containing public data from real-world data sources. Currently, public collections include the Glue - SSTB collection, the TREC Question classification collection, and the SQuAD collection.  ## August 16, 2022  #### Collections (Public Preview)(\"Beta\")  You can now [make static copies of your index](back-up-indexes) using [collections](collections). After you create a collection from an index, you can create a new index from that collection. The new index can use any pod type and any number of pods. Collections only consume storage.  This is a **public preview** feature and is not appropriate for production workloads.  #### Vertical scaling  You can now [change the size of the pods](manage-indexes/#changing-pod-sizes) for a live index to accommodate more vectors or queries without interrupting reads or writes. The p1 and s1 pod types are now available in [4 different sizes](indexes/#pods-pod-types-and-pod-sizes): `1x`, `2x`, `4x`, and `8x`. Capacity and compute per pod double with each size increment.  #### p2 pod type (Public Preview)(\"Beta\")  The new [p2 pod type](indexes/#p2-pods) provides search speeds of around 5ms and throughput of 200 queries per second per replica, or approximately 10x faster speeds and higher throughput than the p1 pod type, depending on your data and network conditions.   This is a **public preview** feature and is not appropriate for production workloads.  #### Improved p1 and s1 performance  The [s1](indexes/#s1-pods) and [p1](indexes/#p1-pods) pod types now offer approximately 50% higher query throughput and 50% lower latency, depending on your workload.  ## July 26, 2022  You can now specify a [metadata filter](metadata-filtering/) to get results for a subset of the vectors in your index by calling [`describe_index_stats`](https://www.pinecone.io/docs/api/operation/describe_index_stats/) with a [`filter`](/reference/describe_index_stats/#!path=filter&t=request) object.  The `describe_index_stats` operation now uses the `POST` HTTP request type. The `filter` parameter is only accepted by `describe_index_stats` calls using the `POST` request type. Calls to `describe_index_stats` using the [`GET` request type](/reference/describe_index_stats1/) are now deprecated.   ## July 12, 2022  #### Pinecone Console Guided Tour  You can now choose to follow a guided tour in the [Pinecone Console](https://app.pinecone.io). This interactive tutorial walks you through creating your first index, upserting vectors, and querying your data. The purpose of the tour is to show you all the steps you need to start your first project in Pinecone.  ## June 24, 2022  #### Updated response codes  The [`create_index`](/reference/create_index/), [`delete_index`](/reference/delete_index/), and [`scale_index`](/reference/scale_index/) operations now use more specific HTTP response codes that describe the type of operation that succeeded.  ## June 7, 2022  #### Selective metadata indexing  You can now store more metadata and more unique metadata values! [Select which metadata fields you want to index for filtering](manage-indexes/#selective-metadata-indexing) and which fields you only wish to store and retrieve. When you index metadata fields, you can filter vector search queries using those fields. When you store metadata fields without indexing them, you keep memory utilization low, especially when you have many unique metadata values, and therefore can fit more vectors per pod.  #### Single-vector queries  You can now [specify a single query vector using the `vector` input](https://www.pinecone.io/docs/api/operation/query/#!path=vector&t=request). We now encourage all users to query using a single vector rather than a batch of vectors, because batching queries can lead to long response messages and query times, and single queries execute just as fast on the server side.  #### Query by ID  You can now [query your Pinecone index using only the ID for another vector](https://www.pinecone.io/docs/api/operation/query/#!path=id&t=request). This is useful when you want to search for the nearest neighbors of a vector that is already stored in Pinecone.   #### Improved index fullness accuracy  The index fullness metric in [`describe_index_stats()`](https://www.pinecone.io/docs/api/operation/describe_index_stats/#!c=200&path=indexFullness&t=response) results is now more accurate.  ## April 25, 2022  #### Partial updates (Public Preview)  You can now perform a [partial update](https://www.pinecone.io/docs/manage-data/#partial-update) by ID and individual value pairs. This allows you to update individual metadata fields without having to upsert a matching vector or update all metadata fields at once.   #### New metrics   Users on all plans can now see metrics for the past one (1) week in the Pinecone console. Users on the Enterprise and Enterprise Dedicated plan now have access to the following metrics via the [Prometheus metrics endpoint](monitoring/):  * `pinecone_vector_count` * `pinecone_request_count_total` * `pinecone_request_error_count_total` * `pinecone_request_latency_seconds` * `pinecone_index_fullness` (Public Preview)  **Note:** The accuracy of the `pinecone_index_fullness` metric is improved. This may result in changes from historic reported values. This metric is in public preview.  #### Spark Connector  Spark users who want to manage parallel upserts into Pinecone can now use the [official Spark connector for Pinecone](https://github.com/pinecone-io/spark-pinecone#readme) to upsert their data from a Spark dataframe.  #### Support for Boolean and float metadata in Pinecone indexes  You can now add `Boolean` and `float64` values to [metadata JSON objects associated with a Pinecone index.](https://www.pinecone.io/docs/metadata-filtering/#adding-metadata-in-pinecone-indexes)   #### New state field in describe_index results  The [`describe_index`](https://www.pinecone.io/docs/api/operation/describe_index/) operation results now contain a value for `state`, which describes the state of the index. The possible values for `state` are `Initializing`, `ScalingUp`, `ScalingDown`, `Terminating`, and `Ready`.  #### Delete by metadata filter  The [`Delete`](https://www.pinecone.io/docs/api/operation/delete/) operation now supports filtering my metadata. ",
  "source": "readmedocs"
},{
  "_id": {
    "$oid": "63e26b4df2e6c738d0f7e27f"
  },
  "title": "Limits",
  "category": "630fc5235d91a70054705fb9",
  "content": " This is a summary of current Pinecone limitations. For many of these, there is a workaround or we're working on increasing the limits.  ## Upserts  Max vector dimensionality is 20,000.  Max size for an upsert request is 2MB. Recommended upsert limit is 100 vectors per request.  Vectors may not be visible to queries immediately after upserting. You can check if the vectors were indexed by looking at the total with `describe_index_stats()`, although this method may not work if the index has multiple replicas. The database is eventually consistent.  ## Queries  Max value for `top_k`, the number of results to return, is 10,000. Max value for `top_k` for queries with `include_metadata=True` or `include_data=True` is 1,000.  ## Fetch and Delete  Max vectors per fetch or delete request is 1,000.  ## Namespaces  There is no limit to the number of [namespaces](namespaces) per index.  ## Pod storage capacity  Each **p1** pod has enough capacity for 1M vectors with 768 dimensions.  Each **s1** pod has enough capacity for 5M vectors with 768 dimensions.  ## Metadata  Max metadata size per vector is 10 KB.  Null metadata values are not supported. Instead of setting a key to hold a null value, we recommend you remove that key from the metadata payload.  Metadata with high cardinality, such as a unique value for every vector in a large index, will take up more memory than expected and cause the pods to become full.  ## Retention  Indexes of users on the Starter (free) plan are deleted after 7 days of inactivity. To prevent this, send any API request to Pinecone to reset the counter. ",
  "source": "readmedocs"
}]